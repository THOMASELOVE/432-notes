[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Data Science for Biological, Medical and Health Research",
    "section": "",
    "text": "Introduction\nThese Notes provide a series of examples using R to work through issues that are likely to come up in PQHS/CRSP/MPHP 432.\nWhile these Notes share some of the features of a textbook, they are neither comprehensive nor completely original. The main purpose is to give students in 432 a set of common materials on which to draw during the course. In class, we will sometimes:\n\nreiterate points made in this document,\namplify what is here,\nsimplify the presentation of things done here,\nuse new examples to show some of the same techniques,\nrefer to issues not mentioned in this document,\n\nbut what we don’t (always) do is follow these notes very precisely. We assume instead that you will read the materials and try to learn from them, just as you will attend classes and try to learn from them. We welcome feedback of all kinds on this document or anything else via our Campuswire discussion forum.\nWhat you will mostly find are brief explanations of a key idea or summary, accompanied (most of the time) by R code and a demonstration of the results of applying that code.\nEverything you see here is available to you in this HTML document. You will also have access to the Quarto files, which contain the code which generates everything in the document, including all of the R results. We will demonstrate the use of Quarto and R Studio (the “program” which we use to interface with the R language) in class. At the end of the semester, I hope to be able to get the PDF version of this document posted, but that may be a challenge.\nTo download the data and R code related to these notes, visit the 432-data page."
  },
  {
    "objectID": "incomplete.html",
    "href": "incomplete.html",
    "title": "This is work in progress",
    "section": "",
    "text": "These materials are not yet finalized, although they are close to complete. Additional materials may appear as the semester progresses. We thank you for your patience.\nIf you find any typographical or other errors, please let Dr. Love know about them, either through the Typos folder in Campuswire or by email to Thomas dot Love at case dot edu.\nThanks again."
  },
  {
    "objectID": "setup.html#general-theme-for-ggplot-work",
    "href": "setup.html#general-theme-for-ggplot-work",
    "title": "R Setup",
    "section": "General Theme for ggplot work",
    "text": "General Theme for ggplot work\nDr. Love prefers theme_bw() to the default choice.\n\ntheme_set(theme_bw())"
  },
  {
    "objectID": "nhanes.html#r-setup",
    "href": "nhanes.html#r-setup",
    "title": "1  Building the nh432 example",
    "section": "1.1 R Setup",
    "text": "1.1 R Setup\n\nknitr::opts_chunk$set(comment = NA)\n\nlibrary(janitor) \nlibrary(gt)\nlibrary(gtsummary)\nlibrary(Hmisc)\nlibrary(naniar)\nlibrary(nhanesA)\n\nlibrary(tidyverse) \n\ntheme_set(theme_bw())"
  },
  {
    "objectID": "nhanes.html#selecting-nhanes-variables",
    "href": "nhanes.html#selecting-nhanes-variables",
    "title": "1  Building the nh432 example",
    "section": "1.2 Selecting NHANES Variables",
    "text": "1.2 Selecting NHANES Variables\nWe’ll focus on NHANES data describing\n\nparticipating adults ages 30-59 years who\ncompleted both an NHANES interview and medical examination, and who\nalso completed an oral health examination, and who\nalso reported their overall health as either Excellent, Very Good, Good, Fair, or Poor\n\nWe will pull the following NHANES data elements using the nhanesA package:\n\n1.2.1 Demographics and Sample Weights\nFrom the Demographic Variables and Sample Weights database (P_DEMO), we collect the following variables.\n\n\n\n\n\n\n\n\nVariable\nDescription\nValues\n\n\n\n\nSEQN\nRespondent Sequence Number  (participant ID)\nlink across databases\n\n\nRIDSTATR\nInterview and MEC exam status\n1 = Interviewed only  2 = Interviewed & MEC examined\n\n\nRIDAGEYR\nAge at screening (years)  We will require ages 30-59.\nTop coded at 801\n\n\nRIDRETH3\nRace/Hispanic origin\n1 = Mexican American  2 = Other Hispanic  3 = Non-Hispanic White  4 = Non-Hispanic Black  6 = Non-Hispanic Asian  7 = Other Race, including Multi-Racial\n\n\nDMDEDUC2\nEducation Level\n1 = Less than 9th grade  2 = 9-11th grade  3 = High school graduate  4 = Some college or AA  5 = College graduate or above  7 = Refused (will treat as NA)  9 = Don’t Know (will treat as NA)\n\n\nRIAGENDR\nSex\n1 = Male, 2 = Female\n\n\nWTINTPRP\nFull sample interview weight\nSampling weight\n\n\nWTMECPRP\nFull sample MEC examination weight\nSampling weight\n\n\n\nNote As part of our inclusion criteria, we will require that all participants in our analytic data have RIDAGEYR values of 30-59 and thus drop the participants whose responses to that item are missing or outside that range.\nHere’s my code to select these variables from the P_DEMO data.\n\np_demo <- nhanes('P_DEMO') |>\n  select(SEQN, RIDSTATR, RIDAGEYR, RIDRETH3, DMDEDUC2, RIAGENDR,\n         WTINTPRP, WTMECPRP) \n\ndim(p_demo) # gives number of rows (participants) and columns (variables)\n\n[1] 15560     8\n\n\nDo we have any duplicate SEQN values?\n\np_demo |> get_dupes(SEQN)\n\nNo duplicate combinations found of: SEQN\n\n\n[1] SEQN       dupe_count RIDSTATR   RIDAGEYR   RIDRETH3   DMDEDUC2   RIAGENDR  \n[8] WTINTPRP   WTMECPRP  \n<0 rows> (or 0-length row.names)\n\n\nGood.\n\n\n1.2.2 Oral Health\nFrom the Oral Health - Recommendation of Care (P_OHXREF), we collect the following variables.\n\n\n\n\n\n\n\n\nVariable\nDescription\nValues\n\n\n\n\nSEQN\nRespondent Sequence Number  (participant ID)\nlink across databases\n\n\nOHDEXSTS\nOverall Oral Health Exam Status\n1 = Complete  2 = Partial  3 = Not Done\n\n\nOHAREC\nOverall Recommendation for Dental Care\n1 = See a dentist immediately  2 = See a dentist within the next 2 weeks  3 = See a dentist at your earliest convenience  4 = Continue your regular routine care\n\n\n\nNote In addition to requiring that all participants in our analytic data have OHDEXSTS = 1, we will (later) collapse values 1 and 2 in OHAREC because there are only a few participants with code 1 in OHAREC.\nHere’s my code to select these variables from the P_OHXREF data.\n\np_ohxref <- nhanes('P_OHXREF') |>\n  select(SEQN, OHDEXSTS, OHAREC)\n\ndim(p_ohxref)\n\n[1] 13772     3\n\n\n\n\n1.2.3 Hospital Utilization & Access to Care\nFrom the Questionnaire on Hospital Utilization & Access to Care (P_HUQ) we collect the following variables.\n\n\n\n\n\n\n\n\nVariable\nDescription\nValues\n\n\n\n\nSEQN\nRespondent Sequence Number  (participant ID)\nlink across databases\n\n\nHUQ010\nGeneral health condition  we require a 1-5 response\n1 = Excellent  2 = Very Good  3 = Good  4 = Fair  5 = Poor  7 = Refused (to be dropped)  9 = Don’t Know (to be dropped)\n\n\nHUQ071\nOvernight hospital patient in past 12 months\n1 = Yes, 2 = No  7 = Refused (will treat as NA)  9 = Don’t Know (will treat as NA)\n\n\nHUQ090\nSeen mental health professional in past 12 months\n1 = Yes, 2 = No  7 = Refused (will treat as NA)  9 = Don’t Know (will treat as NA)\n\n\n\nNote As part of our inclusion criteria, we will require that all participants in our analytic data have HUQ010 values of 1-5 and drop participants whose responses are missing, Refused or Don’t Know for that item.\nHere’s my code to select these variables from the P_HUQ data.\n\np_huq <- nhanes('P_HUQ') |>\n  select(SEQN, HUQ010, HUQ071, HUQ090)\n\ndim(p_huq)\n\n[1] 15560     4\n\n\n\n\n1.2.4 Body Measures\nFrom the Body Measures database (P_BMX), we collect the following variables.\n\n\n\n\n\n\n\n\nVariable\nDescription\nValues\n\n\n\n\nSEQN\nRespondent Sequence Number  (participant ID)\nlink across databases\n\n\nBMXWT\nBody weight (kg)\nMeasured in examination\n\n\nBMXHT\nStanding height (cm)\nMeasured in examination\n\n\nBMXWAIST\nWaist Circumference (cm)\nMeasured in examination\n\n\n\nHere’s my code to select these variables from the P_BMX data.\n\np_bmx <- nhanes('P_BMX') |>\n  select(SEQN, BMXWT, BMXHT, BMXWAIST)\n\ndim(p_bmx)\n\n[1] 14300     4\n\n\n\n\n1.2.5 Blood Pressure\nFrom the Blood Pressure - Oscillometric Measurement (P_BPXO), we collect the following variables.\n\n\n\n\n\n\n\n\nVariable\nDescription\nValues\n\n\n\n\nSEQN\nRespondent Sequence Number  (participant ID)\nlink across databases\n\n\nBPXOSY2\nSystolic BP  (2nd reading, in mm Hg)\nMeasured in examination\n\n\nBPXODI2\nDiastolic BP  (2nd reading, in mm Hg)\nMeasured in examination\n\n\nBPXOPLS1\nPulse  (1st reading, beats/minute)\nMeasured in examination\n\n\nBPXOPLS2\nPulse  (2nd reading, beats/minute)\nMeasured in examination\n\n\n\n\nA “normal” blood pressure for most adults is < 120 systolic and < 80 diastolic.\nA “normal” resting pulse rate for most adults is between 60 and 100 beats/minute.\n\nHere’s my code to select these variables from the P_BPXO data.\n\np_bpxo <- nhanes('P_BPXO') |>\n  select(SEQN, BPXOSY2, BPXODI2, BPXOPLS1, BPXOPLS2)\n\ndim(p_bpxo)\n\n[1] 11656     5\n\n\n\n\n1.2.6 Complete Blood Count\nFrom the Complete Blood Count with 5-Part Differential in Whole Blood (P_CBC), we collect the following variables.\n\n\n\n\n\n\n\n\nVariable\nDescription\nValues\n\n\n\n\nSEQN\nRespondent Sequence Number  (participant ID)\nlink across databases\n\n\nLBXWBCSI\nWhite blood cell count  in 1000 cells/uL\nnormal range: 4.5 - 11\n\n\nLBXPLTSI\nPlatelet count  in 1000 cells/uL\nnormal range: 150-450\n\n\n\nHere’s my code to select these variables from the P_CBC data.\n\np_cbc <- nhanes('P_CBC') |>\n  select(SEQN, LBXWBCSI, LBXPLTSI)\n\ndim(p_cbc)\n\n[1] 13772     3\n\n\n\n\n1.2.7 C-Reactive Protein\nFrom the High-Sensitivity C-Reactive Protein (P_HSCRP) we collect the following variables.\n\n\n\n\n\n\n\n\nVariable\nDescription\nValues\n\n\n\n\nSEQN\nRespondent Sequence Number  (participant ID)\nlink across databases\n\n\nLBXHSCRP\nHigh-Sensitivity  C-Reactive Protein (mg/L)\nnormal range: 1.0 - 3.0\n\n\n\nHere’s my code to select these variables from the P_HSCRP data.\n\np_hscrp <- nhanes('P_HSCRP') |>\n  select(SEQN, LBXHSCRP)\n\ndim(p_hscrp)\n\n[1] 13772     2\n\n\n\n\n1.2.8 Alcohol Use\nFrom the Questionnaire on Alcohol Use (P_ALQ) we collect the following variables.\n\n\n\n\n\n\n\n\nVariable\nDescription\nValues\n\n\n\n\nSEQN\nRespondent Sequence Number  (participant ID)\nlink across databases\n\n\nALQ111\nEver had a drink of alcohol?\n1 = Yes, 2 = No  7 = Refused (treat as NA)  9 = Don’t Know (treat as NA)\n\n\nALQ130\nAverage drinks per day  in past 12 months  (Top coded at 152)\ncount (set to 0 if ALQ111 is No)  777 = Refused (treat as NA)  999 = Don’t Know (treat as NA)\n\n\n\nHere’s my code to select these variables from the P_ALQ data.\n\np_alq <- nhanes('P_ALQ') |>\n  select(SEQN, ALQ111, ALQ130)\n\ndim(p_alq)\n\n[1] 8965    3\n\n\nAs noted above, we set the value of ALQ130 to be 0 if the response to ALQ111 is 2 (No).\n\np_alq <- p_alq |>\n  mutate(ALQ130 = ifelse(ALQ111 == 2, 0, ALQ130))\n\np_alq |> count(ALQ130, ALQ111)\n\n   ALQ130 ALQ111    n\n1       0      2  867\n2       1      1 2126\n3       2      1 1769\n4       3      1  843\n5       4      1  431\n6       5      1  231\n7       6      1  201\n8       7      1   44\n9       8      1   65\n10      9      1   13\n11     10      1   42\n12     11      1    3\n13     12      1   53\n14     13      1    3\n15     15      1   29\n16    777      1    1\n17    999      1    9\n18     NA      1 1640\n19     NA     NA  595\n\n\n\n\n1.2.9 Dermatology\nFrom the Questionnaire on Dermatology (P_DEQ) we collect the following variables.\n\n\n\n\n\n\n\n\nVariable\nDescription\nValues\n\n\n\n\nSEQN\nRespondent Sequence Number  (participant ID)\nlink across databases\n\n\nDEQ034D\nUse sunscreen if outside on very sunny day?\n1 = Always  2 = Most of the time  3 = Sometimes  4 = Rarely  5 = Never  7 = Refused (treat as NA)  9 = Don’t Know (treat as NA)\n\n\n\nHere’s my code to select these variables from the P_DEQ data.\n\np_deq <- nhanes('P_DEQ') |>\n  select(SEQN, DEQ034D)\n\ndim(p_deq)\n\n[1] 5810    2\n\n\n\n\n1.2.10 Depression Screener\nFrom the Questionnaire on Mental Health - Depression Screener (P_DPQ) we collect the following variables.\n\n\n\n\n\n\n\n\nVariable\nDescription\nValues\n\n\n\n\nSEQN\nRespondent Sequence Number  (participant ID)\nlink across databases\n\n\nDPQ010\nHave little interest in doing things\n0-3: codes below\n\n\nDPQ020\nFeeling down, depressed or hopeless\n0-3: codes below\n\n\nDPQ030\nTrouble sleeping or sleeping too much\n0-3: codes below\n\n\nDPQ040\nFeeling tired or having little energy\n0-3: codes below\n\n\nDPQ050\nPoor appetite or overeating\n0-3: codes below\n\n\nDPQ060\nFeeling bad about yourself\n0-3: codes below\n\n\nDPQ070\nTrouble concentrating on things\n0-3: codes below\n\n\nDPQ080\nMoving or speaking slowly or too fast\n0-3: codes below\n\n\nDPQ090\nThoughts you would be better off dead\n0-3: codes below\n\n\nDPQ100\nDifficulty these problems have caused\n0-3: codes below\n\n\n\n\nFor DPQ010 - DPQ090, the codes are 0 = Not at all, 1 = Several days in the past two weeks, 2 = More than half the days in the past two weeks, 3 = Nearly every day in the past two weeks, with 7 = Refused and 9 = Don’t Know which we will treat as NA.\nFor DPQ100, the codes are 0 = Not at all difficult, 1 = Somewhat difficult, 2 = Very difficult, 3 = Extremely difficult, with 7 = Refused and 9 = Don’t Know which we will treat as NA. Also, the DPQ100 score should be 0 if the scores on DPQ010 through DPQ090 are all zero.\n\nLater, we will sum the scores in DPQ010 - DPQ090 to produce a PHQ-9 score for each participant.\nHere’s my code to select these variables from the P_DPQ data.\n\np_dpq <- nhanes('P_DPQ') # we're actually pulling all available variables\n\ndim(p_dpq)\n\n[1] 8965   11\n\n\n\n\n1.2.11 Diet Behavior\nFrom the Questionnaire on Diet Behavior and Nutrition (P_DBQ) we collect the following variables.\n\n\n\n\n\n\n\n\nVariable\nDescription\nValues\n\n\n\n\nSEQN\nRespondent Sequence Number  (participant ID)\nlink across databases\n\n\nDBQ700\nHow healthy is your diet?\n1 = Excellent  2 = Very Good  3 = Good  4 = Fair  5 = Poor  7 = Refused (treat as NA)  9 = Don’t Know (treat as NA)\n\n\n\nHere’s my code to select these variables from the P_DBQ data.\n\np_dbq <- nhanes('P_DBQ') |>\n  select(SEQN, DBQ700)\n\ndim(p_dbq)\n\n[1] 15560     2\n\n\n\n\n1.2.12 Food Security\nFrom the Questionnaire on Food Security (P_FSQ) we collect the following variables.\n\n\n\n\n\n\n\n\nVariable\nDescription\nValues\n\n\n\n\nSEQN\nRespondent Sequence Number  (participant ID)\nlink across databases\n\n\nFSDAD\nAdult food security category for last 12m\n1 = Full food security  2 = Marginal  3 = Low  4 = Very low\n\n\n\nHere’s my code to select these variables from the P_FSQ data.\n\np_fsq <- nhanes('P_FSQ') |>\n  select(SEQN, FSDAD)\n\ndim(p_fsq)\n\n[1] 15560     2\n\n\n\n\n1.2.13 Health Insurance\nFrom the Questionnaire on Health Insurance (P_HIQ) we collect the following variables.\n\n\n\n\n\n\n\n\nVariable\nDescription\nValues\n\n\n\n\nSEQN\nRespondent Sequence Number  (participant ID)\nlink across databases\n\n\nHIQ011\nCovered by health insurance now?\n1 = Yes, 2 = No  7 = Refused (treat as NA)  9 = Don’t Know (treat as NA)\n\n\nHIQ210\nTime when no insurance in past year?\n1 = Yes, 2 = No  7 = Refused (treat as NA)  9 = Don’t Know (treat as NA)  (set to Yes if HIQ011 is No.)\n\n\n\nHere’s my code to select these variables from the P_HIQ data.\n\np_hiq <- nhanes('P_HIQ') |>\n  select(SEQN, HIQ011, HIQ210)\n\ndim(p_hiq)\n\n[1] 15560     3\n\n\nAs noted above, we set the value of HIQ210 to be 1 (Yes) if HIQ011 is 2 (No).\n\np_hiq <- p_hiq |>\n  mutate(HIQ210 = ifelse(HIQ011 == 2, 1, HIQ210))\n\np_hiq |> count(HIQ210, HIQ011)\n\n  HIQ210 HIQ011     n\n1      1      1   960\n2      1      2  1852\n3      2      1 12682\n4      7      1     2\n5      9      1    25\n6     NA      1     2\n7     NA      7     8\n8     NA      9    29\n\n\n\n\n1.2.14 Medical Conditions\nFrom the Questionnaire on Medical Conditions (P_MCQ) we collect the following variables.\n\n\n\n\n\n\n\n\nVariable\nDescription\nValues\n\n\n\n\nSEQN\nRespondent Sequence Number  (participant ID)\nlink across databases\n\n\nMCQ366A\nDoctor told you to control/lose weight in the past 12 months?\n1 = Yes, 2 = No  7 = Refused (treat as NA)  9 = Don’t Know (treat as NA)\n\n\nMCQ366B\nDoctor told you to exercise  in the past 12 months?\n1 = Yes, 2 = No  7 = Refused (treat as NA)  9 = Don’t Know (treat as NA)\n\n\nMCQ371A\nAre you now controlling or losing weight?\n1 = Yes, 2 = No  7 = Refused (treat as NA)  9 = Don’t Know (treat as NA)\n\n\nMCQ371B\nAre you now increasing exercise?\n1 = Yes, 2 = No  7 = Refused (treat as NA)  9 = Don’t Know (treat as NA)\n\n\n\nHere’s my code to select these variables from the P_MCQ data.\n\np_mcq <- nhanes('P_MCQ') |>\n  select(SEQN, MCQ366A, MCQ366B, MCQ371A, MCQ371B)\n\ndim(p_mcq)\n\n[1] 14986     5\n\n\n\n\n1.2.15 Oral Health\nFrom the Questionnaire on Oral Health (P_OHQ) we collect the following variables.\n\n\n\n\n\n\n\n\nVariable\nDescription\nValues\n\n\n\n\nSEQN\nRespondent Sequence Number  (participant ID)\nlink across databases\n\n\nOHQ870\nDays using dental floss  (in the last week)\ncount (0-7)  9 = Unknown (treat as NA)  99 = Don’t Know (treat as NA)\n\n\n\nHere’s my code to select these variables from the P_OHQ data.\n\np_ohq <- nhanes('P_OHQ') |>\n  select(SEQN, OHQ870)\n\ndim(p_ohq)\n\n[1] 14986     2\n\n\n\n\n1.2.16 Physical Activity\nFrom the Questionnaire on Physical Activity (P_PAQ) we collect the following variables.\n\n\n\n\n\n\n\n\nVariable\nDescription\nValues\n\n\n\n\nSEQN\nRespondent Sequence Number  (participant ID)\nlink across databases\n\n\nPAQ605\nVigorous work activity for 10 min/week?\n1 = Yes, 2 = No  7 = Refused (treat as NA)  9 = Don’t Know (treat as NA)\n\n\nPAQ610\n# of days of vigorous work activity  in past week\ncount (1-7)  77 = Refused (treat as NA)  99 = Don’t Know (treat as NA)  (set to 0 if PAQ605 is No.)\n\n\nPAQ650\nVigorous recreational activity for 10 min/week?\n1 = Yes, 2 = No  7 = Refused (treat as NA)  9 = Don’t Know (treat as NA)\n\n\nPAQ655\n# of days of vigorous recreational  activity in past week\ncount (1-7)  77 = Refused (treat as NA)  99 = Don’t Know (treat as NA)  (set to 0 if PAQ650 is No.)\n\n\nPAD680\nMinutes of sedentary activity (min/day)\nexcludes sleeping  7777 = Refused (treat as NA)  9999 = Don’t Know (treat as NA)\n\n\n\nHere’s my code to select these variables from the P_PAQ data.\n\np_paq <- nhanes('P_PAQ') |>\n  select(SEQN, PAQ605, PAQ610, PAQ650, PAQ655, PAD680)\n\ndim(p_paq)\n\n[1] 9693    6\n\n\nNow, let’s set the value of PAQ610 to be 0 if PAQ605 is 2 (No).\n\np_paq <- p_paq |>\n  mutate(PAQ610 = ifelse(PAQ605 == 2, 0, PAQ610))\n\nFinally, we set the value of PAQ655 to be 0 if PAQ650 is 2 (No).\n\np_paq <- p_paq |>\n  mutate(PAQ655 = ifelse(PAQ650 == 2, 0, PAQ655))\n\n\n\n1.2.17 Reproductive Health\nFrom the Questionnaire on Reproductive Health (P_RHQ) we collect the following variables.\n\n\n\n\n\n\n\n\nVariable\nDescription\nValues\n\n\n\n\nSEQN\nRespondent Sequence Number  (participant ID)\nlink across databases\n\n\nRHQ131\nEver been pregnant?\n1 = Yes, 2 = No  7 = Refused (treat as NA)  9 = Don’t Know (treat as NA)\n\n\nRHQ160\nHow many times have you been pregnant?\ncount (1-11)  77 = Refused (treat as NA)  99 = Don’t Know (treat as NA)  (set to 0 if RHQ131 is No.)\n\n\n\nHere’s my code to select these variables from the P_RHQ data.\n\np_rhq <- nhanes('P_RHQ') |>\n  select(SEQN, RHQ131, RHQ160)\n\ndim(p_rhq)\n\n[1] 5314    3\n\n\nNow, let’s set the value of RHQ160 to be 0 if RHQ131 is 2 (No).\n\np_rhq <- p_rhq |>\n  mutate(RHQ160 = ifelse(RHQ131 == 2, 0, RHQ160))\n\n\n\n1.2.18 Sleep Disorders\nFrom the Questionnaire on Sleep Disorders (P_SLQ) we collect the following variables.\n\n\n\n\n\n\n\n\nVariable\nDescription\nValues\n\n\n\n\nSEQN\nRespondent Sequence Number  (participant ID)\nlink across databases\n\n\nSLD012\nUsual hours of sleep (weekdays)\nhours limited to 2-14\n\n\nSLD013\nUsual hours of sleep (weekends)\nhours limited to 2-14\n\n\nSLQ030\nHow often do you snore in the past 12 months?\n0 = Never  1 = Rarely (1-2 nights/week)  2 = Occasionally (3-4 nights/week)  3 = Frequently (5+ nights/week)  7 = Refused (treat as NA)  9 = Don’t Know (treat as NA)\n\n\nSLQ050\nHave you ever told a doctor you had trouble sleeping?\n1 = Yes, 2 = No  7 = Refused (treat as NA)  9 = Don’t Know (treat as NA)\n\n\n\nHere’s my code to select these variables from the P_SLQ data.\n\np_slq <- nhanes('P_SLQ') |>\n  select(SEQN, SLD012, SLD013, SLQ030, SLQ050)\n\ndim(p_slq)\n\n[1] 10195     5\n\n\n\n\n1.2.19 Smoking Cigarettes\nFrom the Questionnaire on Smoking - Cigarette Use (P_SMQ) we collect the following variables.\n\n\n\n\n\n\n\n\nVariable\nDescription\nValues\n\n\n\n\nSEQN\nRespondent Sequence Number  (participant ID)\nlink across databases\n\n\nSMQ020\nSmoked at least 100 cigarettes in your life?\n1 = Yes, 2 = No  7 = Refused (treat as NA)  9 = Don’t Know (treat as NA)\n\n\nSMD641\nDays (in past 30) when you smoked a cigarette?\ncount (0-30)  77 = Refused (treat as NA)  99 = Don’t Know (treat as NA)  (set to 0 if SMQ020 is No.)\n\n\n\nHere’s my code to select these variables from the P_SMQ data.\n\np_smq <- nhanes('P_SMQ') |>\n  select(SEQN, SMQ020, SMD641)\n\ndim(p_smq)\n\n[1] 11137     3\n\n\nNow, let’s set the value of SMD641 to be 0 if SMQ020 is 2 (No).\n\np_smq <- p_smq |>\n  mutate(SMD641 = ifelse(SMQ020 == 2, 0, SMD641))\n\n\n\n1.2.20 Secondhand Smoke\nFrom the Questionnaire on Smoking - Secondhand Smoke Exposure (P_SMQSHS) we collect the following variables.\n\n\n\n\n\n\n\n\nVariable\nDescription\nValues\n\n\n\n\nSEQN\nRespondent Sequence Number  (participant ID)\nlink across databases\n\n\nSMQ856\nLast 7 days worked at a job not at home?\n1 = Yes, 2 = No  7 = Refused (treat as NA)  9 = Don’t Know (treat as NA)\n\n\nSMQ860\nLast 7 days spent time in a restaurant?\n1 = Yes, 2 = No  7 = Refused (treat as NA)  9 = Don’t Know (treat as NA)\n\n\nSMQ866\nLast 7 days spent time in a bar?\n1 = Yes, 2 = No  7 = Refused (treat as NA)  9 = Don’t Know (treat as NA)\n\n\n\nHere’s my code to select these variables from the P_SMQSHS data.\n\np_smqshs <- nhanes('P_SMQSHS') |>\n  select(SEQN, SMQ856, SMQ860, SMQ866)\n\ndim(p_smqshs)\n\n[1] 15560     4\n\n\n\n\n1.2.21 Weight History\nFrom the Questionnaire on Weight History (P_WHQ) we collect the following variables.\n\n\n\n\n\n\n\n\nVariable\nDescription\nValues\n\n\n\n\nSEQN\nRespondent Sequence Number  (participant ID)\nlink across databases\n\n\nWHD010\nCurrent self-reported height (in inches)\n49 to 82  7777 = Refused (treat as NA)  9999 = Don’t Know (treat as NA)\n\n\nWHD020\nCurrent self-reported weight (in pounds)\n67 to 578  7777 = Refused (treat as NA)  9999 = Don’t Know (treat as NA)\n\n\nWHQ040\nLike to weigh more, less, or same\n1 = More  2 = Less  3 = Stay about the same  7 = Refused (treat as NA)  9 = Don’t Know (treat as NA)\n\n\n\nHere’s my code to select these variables from the P_WHQ data.\n\np_whq <- nhanes('P_WHQ') |>\n  select(SEQN, WHD010, WHD020, WHQ040)\n\ndim(p_whq)\n\n[1] 10195     4"
  },
  {
    "objectID": "nhanes.html#filtering-for-inclusion",
    "href": "nhanes.html#filtering-for-inclusion",
    "title": "1  Building the nh432 example",
    "section": "1.3 Filtering for Inclusion",
    "text": "1.3 Filtering for Inclusion\nFirst, I’ll filter the demographic data (p_demo) to the participants with known ages (RIDAGEYR here) between 30 and 59 years (inclusive), and to those who were both interviewed and examined (so RIDSTATR is 2) to match our inclusion criteria.\n\np_demo <- p_demo |>\n  filter(RIDAGEYR >= 30 & RIDAGEYR <= 59,\n         RIDSTATR == 2)\n\ndim(p_demo)\n\n[1] 4133    8\n\n\nSecond, I’ll restrict the p_ohxref sample to the participants who had a complete oral health exam (so OHDEXSTS is 1) which is also part of our inclusion criteria.\n\np_ohxref <- p_ohxref |>\n  filter(OHDEXSTS == 1)\n\ndim(p_ohxref)\n\n[1] 13271     3\n\n\nThird, I’ll restrict the p_hug sample to the participants who gave one of our five available responses (codes 1-5) to the general health condition question in HUQ010, which is the final element of our inclusion criteria.\n\np_huq <- p_huq |>\n  filter(HUQ010 <= 5)\n\ndim(p_huq)\n\n[1] 15550     4\n\n\nSubjects that meet all of these requirements will be included in our analytic data. To achieve that end, we’ll begin merging the individual data bases."
  },
  {
    "objectID": "nhanes.html#merging-the-data",
    "href": "nhanes.html#merging-the-data",
    "title": "1  Building the nh432 example",
    "section": "1.4 Merging the Data",
    "text": "1.4 Merging the Data\n\n1.4.1 Merging Two Data Frames at a Time\nWe have two ways to merge our data. We can merge data sets two at a time. In this case, we’ll use inner_join() from the dplyr package to include only those participants with data in each of the two data frames we’re merging. For example, we’ll create temp01 to include data from both p_demo and p_ohxref for all participants (identified by their SEQN) that appear in each of those two data frames. Then, we’ll merge the resulting temp01 with p_huq to create temp02 in a similar way.\n\ntemp01 <- inner_join(p_demo, p_ohxref, by = \"SEQN\")\ntemp02 <- inner_join(temp01, p_huq, by = \"SEQN\")\n\ndim(temp02)\n\n[1] 3931   13\n\n\nNote that we now have 3931 participants in our data, and this should be the case after we merge in all of the other data sets, too. Rather than using inner_join() we will switch now to using left_join() many more times so that we always add new information only on those subjects who meet our inclusion criteria (as identified in temp02. For more on the various types of joins we can use from the dplyr package, visit <https://dplyr.tidyverse.org/reference/mutate-joins.html. The problem is that that approach would force us to create lots of new temporary files as we add in each new variable.\n\n\n1.4.2 Merging Many Data Frames Together\nA better approach is to use the reduce() function in the purrr package3, which will let us join this temp02 data frame with our remaining 17 data frames using left_join() in a much more streamlined way. We’ll also ensure that the final result (which we’ll call nh_raw) is a tibble, rather than just a data frame.\n\ndf_list <- list(temp02, p_bmx, p_bpxo, p_cbc, p_hscrp, \n                p_alq, p_deq, p_dpq, p_dbq, p_fsq, \n                p_hiq, p_mcq, p_ohq, p_paq, p_rhq,\n                p_slq, p_smqshs, p_smq, p_whq)\n\nnh_raw <- df_list |> \n  reduce(left_join, by = 'SEQN') |>\n  as_tibble()\n\ndim(nh_raw)\n\n[1] 3931   64"
  },
  {
    "objectID": "nhanes.html#the-raw-data",
    "href": "nhanes.html#the-raw-data",
    "title": "1  Building the nh432 example",
    "section": "1.5 The “Raw” Data",
    "text": "1.5 The “Raw” Data\nWhat does the data in nh_raw look like? Normally, I wouldn’t include this sort of intermediate description in a published bit of work, but it may be helpful to compare this description to the one we’ll generate at the end of the cleaning process in this case.\n\nsummary(nh_raw)\n\n      SEQN           RIDSTATR    RIDAGEYR        RIDRETH3        DMDEDUC2   \n Min.   :109271   Min.   :2   Min.   :30.00   Min.   :1.000   Min.   :1.00  \n 1st Qu.:113103   1st Qu.:2   1st Qu.:37.00   1st Qu.:3.000   1st Qu.:3.00  \n Median :117059   Median :2   Median :45.00   Median :3.000   Median :4.00  \n Mean   :117074   Mean   :2   Mean   :44.79   Mean   :3.561   Mean   :3.64  \n 3rd Qu.:121040   3rd Qu.:2   3rd Qu.:53.00   3rd Qu.:4.000   3rd Qu.:5.00  \n Max.   :124818   Max.   :2   Max.   :59.00   Max.   :7.000   Max.   :7.00  \n                                                                            \n    RIAGENDR        WTINTPRP         WTMECPRP         OHDEXSTS     OHAREC     \n Min.   :1.000   Min.   :  2467   Min.   :  2589   Min.   :1   Min.   :1.000  \n 1st Qu.:1.000   1st Qu.: 10615   1st Qu.: 11365   1st Qu.:1   1st Qu.:3.000  \n Median :2.000   Median : 17358   Median : 18422   Median :1   Median :4.000  \n Mean   :1.533   Mean   : 28434   Mean   : 30353   Mean   :1   Mean   :3.455  \n 3rd Qu.:2.000   3rd Qu.: 31476   3rd Qu.: 33155   3rd Qu.:1   3rd Qu.:4.000  \n Max.   :2.000   Max.   :311265   Max.   :321574   Max.   :1   Max.   :4.000  \n                                                                              \n     HUQ010          HUQ071          HUQ090          BMXWT       \n Min.   :1.000   Min.   :1.000   Min.   :1.000   Min.   : 36.90  \n 1st Qu.:2.000   1st Qu.:2.000   1st Qu.:2.000   1st Qu.: 69.30  \n Median :3.000   Median :2.000   Median :2.000   Median : 82.10  \n Mean   :2.741   Mean   :1.913   Mean   :1.883   Mean   : 86.31  \n 3rd Qu.:3.000   3rd Qu.:2.000   3rd Qu.:2.000   3rd Qu.: 99.10  \n Max.   :5.000   Max.   :2.000   Max.   :9.000   Max.   :254.30  \n                                                 NA's   :28      \n     BMXHT          BMXWAIST        BPXOSY2         BPXODI2      \n Min.   :135.3   Min.   : 57.9   Min.   : 69.0   Min.   : 31.00  \n 1st Qu.:160.0   1st Qu.: 89.1   1st Qu.:110.0   1st Qu.: 69.00  \n Median :166.9   Median : 99.2   Median :120.0   Median : 76.00  \n Mean   :167.4   Mean   :101.5   Mean   :121.5   Mean   : 77.03  \n 3rd Qu.:174.7   3rd Qu.:111.7   3rd Qu.:131.0   3rd Qu.: 84.00  \n Max.   :198.7   Max.   :178.0   Max.   :222.0   Max.   :136.00  \n NA's   :30      NA's   :149     NA's   :346     NA's   :346     \n    BPXOPLS1        BPXOPLS2         LBXWBCSI         LBXPLTSI    \n Min.   : 38.0   Min.   : 37.00   Min.   : 2.300   Min.   : 47.0  \n 1st Qu.: 62.0   1st Qu.: 63.00   1st Qu.: 5.700   1st Qu.:210.0  \n Median : 69.0   Median : 70.00   Median : 6.900   Median :246.0  \n Mean   : 70.3   Mean   : 70.96   Mean   : 7.254   Mean   :253.3  \n 3rd Qu.: 77.0   3rd Qu.: 78.00   3rd Qu.: 8.400   3rd Qu.:290.0  \n Max.   :126.0   Max.   :121.00   Max.   :22.800   Max.   :818.0  \n NA's   :615     NA's   :617      NA's   :176      NA's   :176    \n    LBXHSCRP           ALQ111          ALQ130          DEQ034D     \n Min.   :  0.110   Min.   :1.000   Min.   : 0.000   Min.   :1.000  \n 1st Qu.:  0.890   1st Qu.:1.000   1st Qu.: 1.000   1st Qu.:3.000  \n Median :  2.090   Median :1.000   Median : 2.000   Median :4.000  \n Mean   :  4.326   Mean   :1.089   Mean   : 2.345   Mean   :3.675  \n 3rd Qu.:  4.740   3rd Qu.:1.000   3rd Qu.: 3.000   3rd Qu.:5.000  \n Max.   :182.820   Max.   :2.000   Max.   :15.000   Max.   :5.000  \n NA's   :267       NA's   :205     NA's   :789      NA's   :19     \n     DPQ010           DPQ020           DPQ030           DPQ040      \n Min.   :0.0000   Min.   :0.0000   Min.   :0.0000   Min.   :0.0000  \n 1st Qu.:0.0000   1st Qu.:0.0000   1st Qu.:0.0000   1st Qu.:0.0000  \n Median :0.0000   Median :0.0000   Median :0.0000   Median :1.0000  \n Mean   :0.3907   Mean   :0.3732   Mean   :0.6529   Mean   :0.7612  \n 3rd Qu.:1.0000   3rd Qu.:1.0000   3rd Qu.:1.0000   3rd Qu.:1.0000  \n Max.   :9.0000   Max.   :7.0000   Max.   :9.0000   Max.   :9.0000  \n NA's   :212      NA's   :212      NA's   :212      NA's   :212     \n     DPQ050           DPQ060           DPQ070           DPQ080      \n Min.   :0.0000   Min.   :0.0000   Min.   :0.0000   Min.   :0.0000  \n 1st Qu.:0.0000   1st Qu.:0.0000   1st Qu.:0.0000   1st Qu.:0.0000  \n Median :0.0000   Median :0.0000   Median :0.0000   Median :0.0000  \n Mean   :0.4117   Mean   :0.2504   Mean   :0.2924   Mean   :0.1622  \n 3rd Qu.:1.0000   3rd Qu.:0.0000   3rd Qu.:0.0000   3rd Qu.:0.0000  \n Max.   :9.0000   Max.   :3.0000   Max.   :3.0000   Max.   :3.0000  \n NA's   :212      NA's   :213      NA's   :213      NA's   :213     \n     DPQ090          DPQ100           DBQ700         FSDAD      \n Min.   :0.000   Min.   :0.0000   Min.   :1.00   Min.   :1.000  \n 1st Qu.:0.000   1st Qu.:0.0000   1st Qu.:2.00   1st Qu.:1.000  \n Median :0.000   Median :0.0000   Median :3.00   Median :1.000  \n Mean   :0.053   Mean   :0.3447   Mean   :3.11   Mean   :1.737  \n 3rd Qu.:0.000   3rd Qu.:1.0000   3rd Qu.:4.00   3rd Qu.:2.000  \n Max.   :3.000   Max.   :3.0000   Max.   :9.00   Max.   :4.000  \n NA's   :214     NA's   :1448                    NA's   :231    \n     HIQ011          HIQ210         MCQ366A         MCQ366B     \n Min.   :1.000   Min.   :1.000   Min.   :1.000   Min.   :1.000  \n 1st Qu.:1.000   1st Qu.:1.000   1st Qu.:1.000   1st Qu.:1.000  \n Median :1.000   Median :2.000   Median :2.000   Median :2.000  \n Mean   :1.211   Mean   :1.742   Mean   :1.699   Mean   :1.574  \n 3rd Qu.:1.000   3rd Qu.:2.000   3rd Qu.:2.000   3rd Qu.:2.000  \n Max.   :9.000   Max.   :9.000   Max.   :9.000   Max.   :9.000  \n                 NA's   :12                                     \n    MCQ371A         MCQ371B          OHQ870           PAQ605     \n Min.   :1.000   Min.   :1.000   Min.   : 0.000   Min.   :1.000  \n 1st Qu.:1.000   1st Qu.:1.000   1st Qu.: 0.000   1st Qu.:1.000  \n Median :1.000   Median :1.000   Median : 3.000   Median :2.000  \n Mean   :1.371   Mean   :1.399   Mean   : 3.503   Mean   :1.723  \n 3rd Qu.:2.000   3rd Qu.:2.000   3rd Qu.: 7.000   3rd Qu.:2.000  \n Max.   :9.000   Max.   :9.000   Max.   :99.000   Max.   :9.000  \n                                 NA's   :1                       \n     PAQ610           PAQ650          PAQ655            PAD680      \n Min.   : 0.000   Min.   :1.000   Min.   : 0.0000   Min.   :   2.0  \n 1st Qu.: 0.000   1st Qu.:1.000   1st Qu.: 0.0000   1st Qu.: 180.0  \n Median : 0.000   Median :2.000   Median : 0.0000   Median : 300.0  \n Mean   : 1.244   Mean   :1.731   Mean   : 0.9201   Mean   : 363.7  \n 3rd Qu.: 2.000   3rd Qu.:2.000   3rd Qu.: 1.0000   3rd Qu.: 480.0  \n Max.   :99.000   Max.   :2.000   Max.   :99.0000   Max.   :9999.0  \n NA's   :4                                          NA's   :11      \n     RHQ131          RHQ160           SLD012           SLD013      \n Min.   :1.000   Min.   : 0.000   Min.   : 2.000   Min.   : 2.000  \n 1st Qu.:1.000   1st Qu.: 2.000   1st Qu.: 6.500   1st Qu.: 7.000  \n Median :1.000   Median : 3.000   Median : 7.500   Median : 8.000  \n Mean   :1.114   Mean   : 3.159   Mean   : 7.359   Mean   : 8.231  \n 3rd Qu.:1.000   3rd Qu.: 4.000   3rd Qu.: 8.000   3rd Qu.: 9.000  \n Max.   :7.000   Max.   :77.000   Max.   :14.000   Max.   :14.000  \n NA's   :1970    NA's   :1972     NA's   :34       NA's   :34      \n     SLQ030          SLQ050          SMQ856          SMQ860     \n Min.   :0.000   Min.   :1.000   Min.   :1.000   Min.   :1.000  \n 1st Qu.:1.000   1st Qu.:1.000   1st Qu.:1.000   1st Qu.:1.000  \n Median :2.000   Median :2.000   Median :1.000   Median :1.000  \n Mean   :2.015   Mean   :1.711   Mean   :1.323   Mean   :1.423  \n 3rd Qu.:3.000   3rd Qu.:2.000   3rd Qu.:2.000   3rd Qu.:2.000  \n Max.   :9.000   Max.   :9.000   Max.   :2.000   Max.   :9.000  \n                                                                \n     SMQ866          SMQ020        SMD641           WHD010         WHD020      \n Min.   :1.000   Min.   :1.0   Min.   : 0.000   Min.   :  50   Min.   :  86.0  \n 1st Qu.:2.000   1st Qu.:1.0   1st Qu.: 0.000   1st Qu.:  63   1st Qu.: 152.0  \n Median :2.000   Median :2.0   Median : 0.000   Median :  66   Median : 180.0  \n Mean   :1.846   Mean   :1.6   Mean   : 6.859   Mean   : 247   Mean   : 353.9  \n 3rd Qu.:2.000   3rd Qu.:2.0   3rd Qu.: 5.000   3rd Qu.:  70   3rd Qu.: 220.0  \n Max.   :2.000   Max.   :7.0   Max.   :99.000   Max.   :9999   Max.   :9999.0  \n                               NA's   :724      NA's   :24                     \n     WHQ040    \n Min.   :1.00  \n 1st Qu.:2.00  \n Median :2.00  \n Mean   :2.18  \n 3rd Qu.:2.00  \n Max.   :9.00"
  },
  {
    "objectID": "nhanes.html#cleaning-tasks",
    "href": "nhanes.html#cleaning-tasks",
    "title": "1  Building the nh432 example",
    "section": "1.6 Cleaning Tasks",
    "text": "1.6 Cleaning Tasks\nWe now have a tibble called nh_raw containing 3931 NHANES participants in the rows and 64 variables in the columns. What must we do to clean up the data?\n\nCheck that every identifier (here, SEQN) is unique.\nEnsure that all coded values for “Refused”, “Don’t Know” or “Missing” are interpreted as missing values by R.\nBe sure all quantitative variables have plausible minimum and maximum values.\nReplace the RIAGENDR variable with a new factor variable called SEX with levels Male and Female.\nConvert all binary Yes/No variables to 1/0 numeric variables where 1 = Yes, 0 = No.\nCreate the PHQ-9 score from the nine relevant items in the depression screener (P_DPQ).\nUse meaningful level names for all multi-categorical variables, and be sure R uses factors to represent them.\nClean and adjust the names of the variables to something more useful, as desired. (Usually, I will do this first, but in this case, I’ve decided to do it last.)\n\nOnce we’ve accomplished these cleaning tasks, we’ll save the resulting tibble as an R data set we can use later, and we’ll summarize our final analytic variables in a proper codebook."
  },
  {
    "objectID": "nhanes.html#our-identifying-variable",
    "href": "nhanes.html#our-identifying-variable",
    "title": "1  Building the nh432 example",
    "section": "1.7 Our identifying variable",
    "text": "1.7 Our identifying variable\nSEQN is meant to identify the rows (participants) in these data, with one row per SEQN. Is every SEQN unique?\n\nnrow(nh_raw)\n\n[1] 3931\n\nn_distinct(nh_raw$SEQN)\n\n[1] 3931\n\n\nIt looks like the number of rows in our tibble matches the number of unique (distinct) SEQN values, so we’re OK. I prefer to specify that the SEQN be maintained by R as a character variable, which reduces the chance of my accidentally including it in a model as if it were something meaningful.\n\nnh_fixing <- nh_raw |> mutate(SEQN = as.character(SEQN))"
  },
  {
    "objectID": "nhanes.html#refused-dont-know",
    "href": "nhanes.html#refused-dont-know",
    "title": "1  Building the nh432 example",
    "section": "1.8 “Refused” & “Don’t Know”",
    "text": "1.8 “Refused” & “Don’t Know”\nSome of our variables have “hidden” missing values coded as “Refused” or “Don’t Know”. We must ensure that R sees these values as missing.\n\nThe following variables use code 7 for Refused and 9 for Don’t Know:\n\nDMDEDUC2, HUQ071, HUQ090, ALQ111, DEQ034D,\nDPQ010, DPQ020, DPQ030, DPQ040, DPQ050,\nDPQ060, DPQ070, DPQ080, DPQ090, DPQ100,\nDBQ700, HIQ011, HIQ210, MCQ366A, MCQ366B,\nMCQ371A, MCQ371B, PAQ605, PAQ650, RHQ131,\nSLQ030, SLQ050, SMQ020, SMQ856, SMQ860,\nSMQ866, WHQ040\n\nThe following variables use code 9 for Unknown and 99 for Don’t Know:\n\nOHQ870\n\nThe following variables use code 77 for Refused and 99 for Unknown:\n\nPAQ610, PAQ655, RHQ160, SMD641\n\nThe following variables use code 777 for Refused and 999 for Don’t Know:\n\nALQ130\n\nThe following variables use code 7777 for Refused and 9999 for Don’t Know:\n\nPAD680, WHD010, WHD020\n\n\nThe replace_with_na() set of functions from the naniar package can be very helpful here4.\n\nnh_fixing <- nh_fixing %>%\n  replace_with_na_at(.vars = c(\"DMDEDUC2\", \"HUQ071\", \"HUQ090\", \"ALQ111\", \n                               \"DEQ034D\", \"DPQ010\", \"DPQ020\", \"DPQ030\",\n                               \"DPQ040\", \"DPQ050\", \"DPQ060\", \"DPQ070\",\n                               \"DPQ080\", \"DPQ090\", \"DPQ100\", \"DBQ700\",\n                               \"HIQ011\", \"HIQ210\", \"MCQ366A\", \"MCQ366B\",\n                               \"MCQ371A\", \"MCQ371B\", \"PAQ605\", \"PAQ650\", \n                               \"RHQ131\", \"SLQ030\", \"SLQ050\", \"SMQ020\",\n                               \"SMQ856\", \"SMQ860\", \"SMQ866\", \"WHQ040\"),\n                     condition = ~.x %in% c(7, 9)) %>%\n  replace_with_na_at(.vars = c(\"OHQ870\"),\n                     condition = ~.x %in% c(9, 99)) %>%\n  replace_with_na_at(.vars = c(\"PAQ610\", \"PAQ655\", \"RHQ160\", \"SMD641\"),\n                     condition = ~.x %in% c(77, 99)) %>%\n  replace_with_na_at(.vars = c(\"ALQ130\"),\n                     condition = ~.x %in% c(777, 999)) %>%\n  replace_with_na_at(.vars = c(\"PAD680\", \"WHD010\", \"WHD020\"),\n                     condition = ~.x %in% c(7777, 9999))"
  },
  {
    "objectID": "nhanes.html#variables-without-variation",
    "href": "nhanes.html#variables-without-variation",
    "title": "1  Building the nh432 example",
    "section": "1.9 Variables without Variation",
    "text": "1.9 Variables without Variation\nNote first that we have two variables which now have the same value for all participants.\n\n\n\nVariable\nDescription\nCodes\n\n\n\n\nRIDSTATR\nInterview and examination status\n2 = Both\n\n\nOHDEXSTS\nComplete oral health exam?\n1 = Yes\n\n\n\n\nnh_fixing |> count(RIDSTATR, OHDEXSTS)\n\n# A tibble: 1 × 3\n  RIDSTATR OHDEXSTS     n\n     <dbl>    <dbl> <int>\n1        2        1  3931\n\n\nWe won’t use these variables in our analyses, now that we’ve verified them."
  },
  {
    "objectID": "nhanes.html#quantitative-variables",
    "href": "nhanes.html#quantitative-variables",
    "title": "1  Building the nh432 example",
    "section": "1.10 Quantitative Variables",
    "text": "1.10 Quantitative Variables\nHere are our quantitative variables, and some key information about the values we observe, along with their units of measurement. Our job here is to check the ranges of these variables, and be sure we have no unreasonable values. It’s also helpful to keep an eye on how much missingness we might have to deal with.\nWe’re also going to rename each of these variables, as indicated.\n\n\n\n\n\n\n\n\n\n\n\nNew  Name\nNHANES  Name\nDescription\nUnits\n# NA\nRange\n\n\n\n\nAGE\nRIDAGEYR\nAge at screening\nyears\n0\n30, 59\n\n\nWEIGHT\nBMXWT\nBody weight\nkg\n28\n36.9, 254.3\n\n\nHEIGHT\nBMXHT\nStanding height\ncm\n30\n135.3, 198.7\n\n\nWAIST\nBMXWAIST\nWaist Circumference\ncm\n149\n57.9, 178\n\n\nSBP\nBPXOSY2\nSystolic BP (2nd reading)\nmm Hg\n346\n69, 222\n\n\nDBP\nBPXODI2\nDiastolic BP (2nd reading)\nmm Hg\n346\n31, 136\n\n\nPULSE1\nBPXOPLS1\nPulse (1st reading)\n\\(\\frac{beats}{minute}\\)\n615\n38, 126\n\n\nPULSE2\nBPXOPLS2\nPulse (2nd reading)\n\\(\\frac{beats}{minute}\\)\n617\n37, 121\n\n\nWBC\nLBXWBCSI\nWhite blood cell count\n\\(\\frac{\\mbox{1000 cells}}{uL}\\)\n176\n2.3, 22.8\n\n\nPLATELET\nLBXPLTSI\nPlatelet count\n\\(\\frac{\\mbox{1000 cells}}{uL}\\)\n176\n47, 818\n\n\nHSCRP\nLBXHSCRP\nHigh-Sensitivity  C-Reactive Protein\n\\(mg/L\\)\n267\n0.11, 182.82\n\n\nDRINKS\nALQ130\nAverage daily  Alcoholic drinks\ndrinks\n789\n0, 15\n\n\nFLOSS\nOHQ870\nDays using dental floss  in past week\ndays\n4\n0, 7\n\n\nVIGWK_D\nPAQ610\nAverage days per week  with Vigorous work\ndays\n5\n0, 7\n\n\nVIGREC_D\nPAQ655\nAverage days per week  with Vigorous recreation\ndays\n1\n0, 7\n\n\nSEDATE\nPAD680\nAverage daily  Sedentary activity\nminutes\n24\n2, 1320\n\n\nPREGNANT\nRHQ160\nPregnancies\ntimes\n1975\n0, 11\n\n\nSLPWKDAY\nSLD012\nUsual sleep (weekdays)\nhours\n34\n2, 14\n\n\nSLPWKEND\nSLD013\nUsual sleep (weekends)\nhours\n34\n2, 14\n\n\nSMOKE30\nSMD641\nDays in past 30 when  you smoked a cigarette\ndays\n726\n0, 30\n\n\nESTHT\nWHD010\nSelf-reported height\ninches\n95\n50, 81\n\n\nESTWT\nWHD020\nSelf-reported weight\npounds\n68\n86, 578\n\n\n\nTo insert the number of missing values and range (minimum, maximum among non-missing values) into the table, I used inline R code like this:\n\nn_miss(nh_fixing$BMXWT)\n\n[1] 28\n\nrange(nh_fixing$BMXWT, na.rm = TRUE)\n\n[1]  36.9 254.3\n\n\n\n1.10.1 Renaming the Quantities\nHere’s the renaming code:\n\nnh_fixing <- nh_fixing |>\n  rename(AGE = RIDAGEYR, WEIGHT = BMXWT, HEIGHT = BMXHT,\n         WAIST = BMXWAIST, SBP = BPXOSY2, DBP = BPXODI2,\n         PULSE1 = BPXOPLS1, PULSE2 = BPXOPLS2, WBC = LBXWBCSI,\n         PLATELET = LBXPLTSI, HSCRP = LBXHSCRP, DRINKS = ALQ130,\n         FLOSS = OHQ870, VIGWK_D = PAQ610, VIGREC_D = PAQ655,\n         SEDATE = PAD680, PREGS = RHQ160, SLPWKDAY = SLD012, \n         SLPWKEND = SLD013, SMOKE30 = SMD641, \n         ESTHT = WHD010, ESTWT = WHD020)\n\n\n\n1.10.2 Sampling Weights\nHere are the sampling weights, which I think of as unitless, typically, though they represent people.\n\n\n\nVariable\nDescription\n# NA\nRange\n\n\n\n\nWTINTPRP\nSampling Weight (interviews)\n0\n2467.1, 311265.2\n\n\nWTMECPRP\nSampling Weight (examinations)\n0\n2589.2, 321573.5\n\n\n\nNote that to obtain these ranges formatted like this, I had to use some additional code in the table:\n\nformat(round_half_up(range(nh_fixing$WTINTPRP, na.rm = TRUE),1), scientific = FALSE)\n\n[1] \"  2467.1\" \"311265.2\""
  },
  {
    "objectID": "nhanes.html#binary-variables",
    "href": "nhanes.html#binary-variables",
    "title": "1  Building the nh432 example",
    "section": "1.11 Binary Variables",
    "text": "1.11 Binary Variables\n\n1.11.1 Sex (RIAGENDR)\nTo start, let’s do something about the variable describing the participant’s biological sex (so we’ll rename it to a more useful name), and then we’ll recode the values of the SEX variable to more useful choices.\n\n\n\nNew Name\nNHANES Name\nDescription\n\n\n\n\nSEX\nRIAGENDR\nSex\n\n\n\nNote that we have more female than male subjects, and no missing values, as it turns out.\n\nnh_fixing |> count(RIAGENDR)\n\n# A tibble: 2 × 2\n  RIAGENDR     n\n     <dbl> <int>\n1        1  1837\n2        2  2094\n\n\nNow, let’s convert the information in RIAGENDR to SEX.\n\nnh_fixing <- nh_fixing |>\n  rename(SEX = RIAGENDR) |>\n  mutate(SEX = factor(ifelse(SEX == 1, \"Male\", \"Female\")))\n\nAnd we’ll run a little “sanity check” here to ensure that we’ve recoded this variable properly5.\n\nnh_fixing |> count(SEX)\n\n# A tibble: 2 × 2\n  SEX        n\n  <fct>  <int>\n1 Female  2094\n2 Male    1837\n\n\n\n\n1.11.2 Yes/No variables\nNow, let’s tackle the variables with code 1 = Yes, and 2 = No, and (potentially) some missing values. I’ll summarize each with the percentage of “Yes” responses (out of those with code 1 or 2) and the number of missing values.\n\n\n\n\n\n\n\n\n\n\nNew NAME\nNHANES NAME\nDescription\n% Yes\n# NA\n\n\n\n\nHOSPITAL\nHUQ071\nOvernight hospital patient in past 12m?\n8.7\n0\n\n\nMENTALH\nHUQ090\nSeen mental health professional past 12m?\n12.1\n2\n\n\nEVERALC\nALQ111\nEver had a drink of alcohol?\n91.1\n205\n\n\nINSURNOW\nHIQ011\nCovered by health insurance now?\n80.8\n10\n\n\nNOINSUR\nHIQ210\nTime when no insurance in past year?\n26.6\n16\n\n\nDR_LOSE\nMCQ366A\nDoctor told you to control/lose weight  in the past 12 months?\n30.3\n1\n\n\nDR_EXER\nMCQ366B\nDoctor told you to exercise  in the past 12 months?\n42.7\n1\n\n\nNOW_LOSE\nMCQ371A\nAre you now controlling or losing weight?\n63.2\n2\n\n\nNOW_EXER\nMCQ371B\nAre you now increasing exercise?\n60.3\n1\n\n\nWORK_V\nPAQ605\nVigorous work activity for 10 min/week?\n28.4\n4\n\n\nREC_V\nPAQ650\nVigorous recreational activity for 10 min/week?\n26.9\n0\n\n\nEVERPREG\nRHQ131\nEver been pregnant?\n89.2\n1972\n\n\nSLPTROUB\nSLQ050\nEver told a doctor you had trouble sleeping?\n29.5\n3\n\n\nCIG100\nSMQ020\nSmoked at least 100 cigarettes in your life?\n40.1\n1\n\n\nAWAYWORK\nSMQ856\nLast 7 days worked at a job not at home?\n67.7\n0\n\n\nAWAYREST\nSMQ860\nLast 7 days spent time in a restaurant?\n58.1\n2\n\n\nAWAYBAR\nSMQ866\nLast 7 days spent time in a bar?\n15.4\n0\n\n\n\nThe inline code I used in the tables was, for example:\n\nround_half_up(100 * sum(nh_fixing$HUQ090 == \"1\", na.rm = TRUE) / \n                sum(nh_fixing$HUQ090 %in% c(\"1\",\"2\"), na.rm = TRUE), 1)\n\n[1] 12.1\n\nn_miss(nh_fixing$HUQ090)\n\n[1] 2\n\n\nTo clean these (1 = Yes, 2 = No) variables, I’ll subtract the values from 2, to obtain variables where 1 = Yes and 0 = No. I’ll use the across() function within my mutate() statement so as to avoid having to type out each change individually6.\n\nnh_fixing <- nh_fixing |>\n  mutate(across(c(HUQ071, HUQ090, ALQ111, HIQ011, HIQ210,\n                  MCQ366A, MCQ366B, MCQ371A, MCQ371B, PAQ605,\n                  PAQ650, RHQ131, SLQ050, SMQ020, SMQ856,\n                  SMQ860, SMQ866), \n                ~ 2 - .x))\n\nLet’s do just one of the relevant sanity checks here. In addition to verifying that our new variable has the values 0 and 1 (instead of 2 and 1), we want to be certain that we’ve maintained any missing values.\n\nnh_fixing |> count(SLQ050)\n\n# A tibble: 3 × 2\n  SLQ050     n\n   <dbl> <int>\n1      0  2769\n2      1  1159\n3     NA     3\n\n\n\n\n1.11.3 Renaming Binary Variables\nHere’s the renaming code.\n\nnh_fixing <- nh_fixing |>\n  rename(HOSPITAL = HUQ071, MENTALH = HUQ090, EVERALC = ALQ111,\n         INSURNOW = HIQ011, NOINSUR = HIQ210, DR_LOSE = MCQ366A,\n         DR_EXER = MCQ366B, NOW_LOSE = MCQ371A, NOW_EXER = MCQ371B,\n         WORK_V = PAQ605, REC_V = PAQ650, EVERPREG = RHQ131, \n         SLPTROUB = SLQ050, CIG100 =  SMQ020, AWAYWORK = SMQ856,\n         AWAYREST = SMQ860, AWAYBAR = SMQ866)"
  },
  {
    "objectID": "nhanes.html#create-phq-9-scores",
    "href": "nhanes.html#create-phq-9-scores",
    "title": "1  Building the nh432 example",
    "section": "1.12 Create PHQ-9 Scores",
    "text": "1.12 Create PHQ-9 Scores\nThe questions below are asked to assess depression severity, following the prompt “Over the last two weeks, how often have you been bothered by any of the following problems?”\n\n\n\n\n\n\n\n\nVariable\nDescription\nValues\n\n\n\n\nSEQN\nRespondent Sequence Number  (participant ID)\nlink across databases\n\n\nDPQ010\nHave little interest in doing things\n0-3: see below\n\n\nDPQ020\nFeeling down, depressed or hopeless\n0-3: see below\n\n\nDPQ030\nTrouble sleeping or sleeping too much\n0-3: see below\n\n\nDPQ040\nFeeling tired or having little energy\n0-3: see below\n\n\nDPQ050\nPoor appetite or overeating\n0-3: see below\n\n\nDPQ060\nFeeling bad about yourself\n0-3: see below\n\n\nDPQ070\nTrouble concentrating on things\n0-3: see below\n\n\nDPQ080\nMoving or speaking slowly or too fast\n0-3: see below\n\n\nDPQ090\nThoughts you would be better off dead\n0-3: see below\n\n\n\n\nFor DPQ010 - DPQ090, the codes are 0 = Not at all, 1 = Several days in the past two weeks, 2 = More than half the days in the past two weeks, 3 = Nearly every day in the past two weeks.\n\n\n1.12.1 Forming the PHQ-9 Score\nOne way to use this information is to sum the scores from items DPQ010 through DPQ090 to obtain a result on a scale from 0 - 27. Cutoffs of 5, 10, 15, and 20 then represent mild, moderate, moderately severe, and severe levels of depressive symptoms, respectively7. If we had no missing values in our responses, then this would be relatively straightforward.\n\ntemp <- nh_fixing |>\n  mutate(PHQ9 = DPQ010 + DPQ020 + DPQ030 + DPQ040 + DPQ050 +\n           DPQ060 + DPQ070 + DPQ080 + DPQ090)\n\ntemp |> count(PHQ9) |> tail()\n\n# A tibble: 6 × 2\n   PHQ9     n\n  <dbl> <int>\n1    22     9\n2    23     5\n3    24     2\n4    25     2\n5    26     3\n6    NA   221\n\n\nIt turns out that this formulation of PHQ9 regards as missing the result for any participant who failed to answer all 9 questions. A common approach to dealing with missing data in creating PHQ-9 scores8 is to score all questionnaires with up to two missing values, replacing any missing values with the average score of the completed items.\nSo how many of our subjects are missing only one or two of the 9 items?\n\ntemp2 <- temp |>\n  select(SEQN, DPQ010, DPQ020, DPQ030, DPQ040, DPQ050, DPQ060, \n         DPQ070, DPQ080, DPQ090) \n\nmiss_case_table(temp2)\n\n# A tibble: 5 × 3\n  n_miss_in_case n_cases pct_cases\n           <int>   <int>     <dbl>\n1              0    3710   94.4   \n2              1       5    0.127 \n3              2       3    0.0763\n4              4       1    0.0254\n5              9     212    5.39  \n\n\nWith a little googling I found an R script online that will perform this task, and create three new variables:\n\nnvalid_phq9 = Number of Valid Responses (out of 9) to the PHQ-9 items\nPHQ9 = PHQ-9 score (0-27 scale, higher values indicate more depression)\nPHQ9_CAT = factor describing PHQ-9 score\n\nPHQ9 > 20 means PHQ9_CAT is”severe”,\n15-19 = “moderately severe”,\n10-14 = “moderate”\n5-9 = “mild”\n0-4 = “minimal”\n\n\n\nscoring_phq9 <- function(data, items.phq9) {\n  data %>%\n    mutate(nvalid_phq9 = rowSums(!is.na(select(., items.phq9))),\n           nvalid_phq9 = as.integer(nvalid_phq9),\n           mean.temp = rowSums(select(., items.phq9), na.rm = TRUE)/nvalid_phq9,\n           phq.01.temp = as.integer(unlist(data[items.phq9[1]])),\n           phq.02.temp = as.integer(unlist(data[items.phq9[2]])),\n           phq.03.temp = as.integer(unlist(data[items.phq9[3]])),\n           phq.04.temp = as.integer(unlist(data[items.phq9[4]])),\n           phq.05.temp = as.integer(unlist(data[items.phq9[5]])),\n           phq.06.temp = as.integer(unlist(data[items.phq9[6]])),\n           phq.07.temp = as.integer(unlist(data[items.phq9[7]])),\n           phq.08.temp = as.integer(unlist(data[items.phq9[8]])),\n           phq.09.temp = as.integer(unlist(data[items.phq9[9]]))) %>%\n    mutate_at(vars(phq.01.temp:phq.09.temp),\n              funs(ifelse(is.na(.), round(mean.temp), .))) %>%\n    mutate(score.temp = rowSums(select(., phq.01.temp:phq.09.temp), na.rm = TRUE),\n           PHQ9 = ifelse(nvalid_phq9 >= 7, as.integer(round(score.temp)), NA),\n           PHQ9_CAT = case_when(\n             PHQ9 >= 20 ~ 'severe',\n             PHQ9 >= 15 ~ 'moderately severe',\n             PHQ9 >= 10 ~ 'moderate',\n             PHQ9 >= 5 ~ 'mild',\n             PHQ9 < 5 ~ 'minimal'),\n             PHQ9_CAT = factor(PHQ9_CAT, levels = c('minimal', 'mild',\n                                                          'moderate', 'moderately severe',\n                                                          'severe'))) %>%\n    select(-ends_with(\"temp\"))\n \n}\n\nApplying this script to our nh_fixing data, our result is:\n\nitems.phq9 <- c(\"DPQ010\", \"DPQ020\", \"DPQ030\", \"DPQ040\", \"DPQ050\",\n                \"DPQ060\", \"DPQ070\", \"DPQ080\", \"DPQ090\")\nnh_fixing <- nh_fixing %>% scoring_phq9(., all_of(items.phq9))\n\nnh_fixing |> count(nvalid_phq9, PHQ9, PHQ9_CAT)\n\n# A tibble: 36 × 4\n   nvalid_phq9  PHQ9 PHQ9_CAT     n\n         <int> <int> <fct>    <int>\n 1           0    NA <NA>       212\n 2           5    NA <NA>         1\n 3           7     1 minimal      1\n 4           7     2 minimal      1\n 5           7     7 mild         1\n 6           8     1 minimal      2\n 7           8     2 minimal      1\n 8           8     3 minimal      1\n 9           8     8 mild         1\n10           9     0 minimal   1233\n# … with 26 more rows\n\n\n\n\n1.12.2 Distribution of PHQ-9 Score\nHere’s a quick look at the distribution of PHQ-9 scores in our nh_fixing data.\n\nnh_fixing |> filter(complete.cases(PHQ9, PHQ9_CAT)) %>%\n  ggplot(., aes(x = PHQ9, fill = PHQ9_CAT)) +\n  geom_histogram(binwidth = 1) +\n  scale_fill_viridis_d() + \n  labs(title = \"PHQ-9 Scores for subjects in `nh_fixing`\")\n\n\n\n\n\n\n1.12.3 Fixing the DPQ100 variable\nThe DPQ100 variable should be 0 (Not at all difficult) if the PHQ-9 score is zero. We need to fix this, because NHANES participants who answered 0 (Not at all) to each of the nine elements contained in the PHQ-9 were not asked the DPQ100 question. So, we set the value of DPQ100 to be 0 if PHQ9 is 0.\n\nnh_fixing <- nh_fixing |>\n  mutate(DPQ100 = ifelse(PHQ9 == 0, 0, DPQ100))\n\nThis will eliminate the “automatic missing” values in DPQ100.\n\nnh_fixing |> tabyl(DPQ100)\n\n DPQ100    n    percent valid_percent\n      0 3039 0.77308573    0.81781485\n      1  541 0.13762401    0.14558665\n      2   93 0.02365810    0.02502691\n      3   43 0.01093869    0.01157158\n     NA  215 0.05469346            NA"
  },
  {
    "objectID": "nhanes.html#multi-categorical-variables",
    "href": "nhanes.html#multi-categorical-variables",
    "title": "1  Building the nh432 example",
    "section": "1.13 Multi-Categorical Variables",
    "text": "1.13 Multi-Categorical Variables\nOur remaining categorical variables with more than two levels are:\n\n\n\n\n\n\n\n\n\nVariable\nDescription\nCodes\n# NAs\n\n\n\n\nRIDRETH3\nRace/Hispanic origin\n1, 2, 3, 4, 6, 7\n0\n\n\nDMDEDUC2\nEducation Level\n1, 2, 3, 4, 5\n1\n\n\nOHAREC\nOverall Recommendation for Care\n1, 2, 3, 4\n0\n\n\nHUQ010\nGeneral health condition\n1, 2, 3, 4, 5\n0\n\n\nDEQ034D\nUse sunscreen if outside on very sunny day?\n1, 2, 3, 4, 5\n19\n\n\nDPQ100\nDifficulty depression problems have caused\n0, 1, 2, 3\n215\n\n\nDBQ700\nHow healthy is your diet?\n1, 2, 3, 4, 5\n1\n\n\nFSDAD\nAdult food security in last 12m\n1, 2, 3, 4\n231\n\n\nSLQ030\nHow often do you snore?\n0, 1, 2, 3\n219\n\n\nWHQ040\nLike to weigh more, less, or same?\n1, 2, 3\n3\n\n\n\n\n1.13.1 Creating RACEETH from RIDRETH3\nAt the moment, our RIDRETH3 data look like this:\n\nt_ridreth3 <- nh_fixing |> tabyl(RIDRETH3) |> adorn_pct_formatting() |>\n  mutate(Code = c(\"Mexican American\", \"Other Hispanic\", \"Non-Hispanic White\", \n                  \"Non-Hispanic Black\", \"Non-Hispanic Asian\", \"Other Race\"))\n\ngt(t_ridreth3)\n\n\n\n\n\n  \n  \n    \n      RIDRETH3\n      n\n      percent\n      Code\n    \n  \n  \n    1\n500\n12.7%\nMexican American\n    2\n403\n10.3%\nOther Hispanic\n    3\n1192\n30.3%\nNon-Hispanic White\n    4\n1049\n26.7%\nNon-Hispanic Black\n    6\n588\n15.0%\nNon-Hispanic Asian\n    7\n199\n5.1%\nOther Race\n  \n  \n  \n\n\n\n\nNow, we’ll turn this RIDRETH3 variable into a new factor called RACEETH with meaningful levels, and then sort those levels by their frequency in the data. We’ll also collapse together the Mexican American and Other Hispanic levels, not because the distinction is irrelevant, but more to demonstrate how this might be done.\n\nnh_fixing <- nh_fixing |>\n  mutate(RACEETH = \n           fct_recode(\n             factor(RIDRETH3), \n             \"Hispanic\" = \"1\", \n             \"Hispanic\" = \"2\", \n             \"Non-H White\" = \"3\",\n             \"Non-H Black\" = \"4\",\n             \"Non-H Asian\" = \"6\",\n             \"Other Race\" = \"7\")) |>\n  mutate(RACEETH = fct_infreq(RACEETH))\n\nI’m using fct_infreq() here to sort the (nominal) Race and Ethnicity data so that the most common column appears first, and will thus be treated as the “baseline” level in models. Here is the resulting order.\n\nnh_fixing |> count(RACEETH)\n\n# A tibble: 5 × 2\n  RACEETH         n\n  <fct>       <int>\n1 Non-H White  1192\n2 Non-H Black  1049\n3 Hispanic      903\n4 Non-H Asian   588\n5 Other Race    199\n\n\nNow, let’s check9 to see if RACEETH and RIDRETH3 include the same information (after collapsing the Mexican American and Other Hispanic categories.)\n\nnh_fixing |> count(RACEETH, RIDRETH3)\n\n# A tibble: 6 × 3\n  RACEETH     RIDRETH3     n\n  <fct>          <dbl> <int>\n1 Non-H White        3  1192\n2 Non-H Black        4  1049\n3 Hispanic           1   500\n4 Hispanic           2   403\n5 Non-H Asian        6   588\n6 Other Race         7   199\n\n\n\n\n1.13.2 Creating EDUC from DMDEDUC2\nAt the moment, our DMDEDUC2 data look like this:\n\nt_dmdeduc2 <- nh_fixing |> tabyl(DMDEDUC2) |> adorn_pct_formatting() |>\n  mutate(Code = c(\"Less than 9th grade\", \"9th-11th grade\", \"High School Grad\", \n                  \"Some College / AA\", \"College Grad\", \"Missing\"))\n\ngt(t_dmdeduc2)\n\n\n\n\n\n  \n  \n    \n      DMDEDUC2\n      n\n      percent\n      valid_percent\n      Code\n    \n  \n  \n    1\n272\n6.9%\n6.9%\nLess than 9th grade\n    2\n424\n10.8%\n10.8%\n9th-11th grade\n    3\n850\n21.6%\n21.6%\nHigh School Grad\n    4\n1287\n32.7%\n32.7%\nSome College / AA\n    5\n1097\n27.9%\n27.9%\nCollege Grad\n    NA\n1\n0.0%\n-\nMissing\n  \n  \n  \n\n\n\n\nNow, we’ll turn this DMDEDUC2 variable into a new factor called EDUC with meaningful levels.\n\nnh_fixing <- nh_fixing |>\n  mutate(EDUC = \n           fct_recode(\n             factor(DMDEDUC2), \n             \"Less than 9th Grade\" = \"1\", \n             \"9th - 11th Grade\" = \"2\", \n             \"High School Grad\" = \"3\",\n             \"Some College / AA\" = \"4\",\n             \"College Grad\" = \"5\")) \n\nOnce again, checking our work…\n\nnh_fixing |> tabyl(EDUC, DMDEDUC2) |> gt()\n\n\n\n\n\n  \n  \n    \n      EDUC\n      1\n      2\n      3\n      4\n      5\n      NA_\n    \n  \n  \n    Less than 9th Grade\n272\n0\n0\n0\n0\n0\n    9th - 11th Grade\n0\n424\n0\n0\n0\n0\n    High School Grad\n0\n0\n850\n0\n0\n0\n    Some College / AA\n0\n0\n0\n1287\n0\n0\n    College Grad\n0\n0\n0\n0\n1097\n0\n    NA\n0\n0\n0\n0\n0\n1\n  \n  \n  \n\n\n\n\n\n\n1.13.3 Creating DENTAL from OHAREC\n\nt_oharec <- nh_fixing |> tabyl(OHAREC) |> adorn_pct_formatting() |>\n  mutate(Code = c(\"See a dentist immediately\", \"See a dentist within the next 2 weeks\", \"See a dentist at your earliest convenience\", \"Continue your regular routine care\"))\n\ngt(t_oharec)\n\n\n\n\n\n  \n  \n    \n      OHAREC\n      n\n      percent\n      Code\n    \n  \n  \n    1\n4\n0.1%\nSee a dentist immediately\n    2\n230\n5.9%\nSee a dentist within the next 2 weeks\n    3\n1671\n42.5%\nSee a dentist at your earliest convenience\n    4\n2026\n51.5%\nContinue your regular routine care\n  \n  \n  \n\n\n\n\nWe’ll collapse categories 1 and 2 together since they are quite small.\n\nnh_fixing <- nh_fixing |>\n  mutate(DENTAL = \n           fct_recode(\n             factor(OHAREC), \n             \"See dentist urgently\" = \"1\", \n             \"See dentist urgently\" = \"2\", \n             \"See dentist soon\" = \"3\",\n             \"Regular Routine\" = \"4\")) \n\nOnce again, checking our work…\n\nnh_fixing |> tabyl(DENTAL, OHAREC) |> gt()\n\n\n\n\n\n  \n  \n    \n      DENTAL\n      1\n      2\n      3\n      4\n    \n  \n  \n    See dentist urgently\n4\n230\n0\n0\n    See dentist soon\n0\n0\n1671\n0\n    Regular Routine\n0\n0\n0\n2026\n  \n  \n  \n\n\n\n\n\n\n1.13.4 Creating SROH from HUQ010\n\n\n\n\n\n\n\n\n\nVariable\nDescription\nCodes\n# NAs\n\n\n\n\nHUQ010\nGeneral health condition\n1 = Excellent  2 = Very Good  3 = Good  4 = Fair  5 = Poor\n0\n\n\n\n\nnh_fixing <- nh_fixing |>\n  mutate(SROH = \n           fct_recode(\n             factor(HUQ010), \n             \"Excellent\" = \"1\", \n             \"Very Good\" = \"2\", \n             \"Good\" = \"3\",\n             \"Fair\" = \"4\",\n             \"Poor\" = \"5\"))\n\nChecking our work…\n\nnh_fixing |> tabyl(SROH, HUQ010) |> gt()\n\n\n\n\n\n  \n  \n    \n      SROH\n      1\n      2\n      3\n      4\n      5\n    \n  \n  \n    Excellent\n495\n0\n0\n0\n0\n    Very Good\n0\n1071\n0\n0\n0\n    Good\n0\n0\n1462\n0\n0\n    Fair\n0\n0\n0\n765\n0\n    Poor\n0\n0\n0\n0\n138\n  \n  \n  \n\n\n\n\n\n\n1.13.5 Creating SUNSCR from DEQ034D\n\n\n\n\n\n\n\n\n\nVariable\nDescription\nCodes\n# NAs\n\n\n\n\nDEQ034D\nUse sunscreen if outside on very sunny day?\n1 = Always  2 = Most of the time  3 = Sometimes  4 = Rarely  5 = Never\n19\n\n\n\n\nnh_fixing <- nh_fixing |>\n  mutate(SUNSCR = \n           fct_recode(\n             factor(DEQ034D), \n             \"Always\" = \"1\", \n             \"Most of the time\" = \"2\", \n             \"Sometimes\" = \"3\",\n             \"Rarely\" = \"4\",\n             \"Never\" = \"5\")) \n\n\nnh_fixing |> tabyl(SUNSCR, DEQ034D) |> gt()\n\n\n\n\n\n  \n  \n    \n      SUNSCR\n      1\n      2\n      3\n      4\n      5\n      NA_\n    \n  \n  \n    Always\n351\n0\n0\n0\n0\n0\n    Most of the time\n0\n485\n0\n0\n0\n0\n    Sometimes\n0\n0\n831\n0\n0\n0\n    Rarely\n0\n0\n0\n662\n0\n0\n    Never\n0\n0\n0\n0\n1583\n0\n    NA\n0\n0\n0\n0\n0\n19\n  \n  \n  \n\n\n\n\n\n\n1.13.6 Creating DEPRDIFF from DPQ100\n\n\n\n\n\n\n\n\n\nVariable\nDescription\nCodes\n# NAs\n\n\n\n\nDPQ100\nDifficulty depression problems have caused\n0 = Not at all difficult  1 = Somewhat difficult  2 = Very difficult  3 = Extremely difficult\n215\n\n\n\n\nnh_fixing <- nh_fixing |>\n  mutate(DEPRDIFF = \n           fct_recode(\n             factor(DPQ100), \n             \"Not at all\" = \"0\", \n             \"Somewhat\" = \"1\", \n             \"Very\" = \"2\",\n             \"Extremely\" = \"3\")) \n\n\nnh_fixing |> tabyl(DEPRDIFF, DPQ100) |> gt()\n\n\n\n\n\n  \n  \n    \n      DEPRDIFF\n      0\n      1\n      2\n      3\n      NA_\n    \n  \n  \n    Not at all\n3039\n0\n0\n0\n0\n    Somewhat\n0\n541\n0\n0\n0\n    Very\n0\n0\n93\n0\n0\n    Extremely\n0\n0\n0\n43\n0\n    NA\n0\n0\n0\n0\n215\n  \n  \n  \n\n\n\n\n\n\n1.13.7 Creating DIETQUAL from DBQ700\n\n\n\n\n\n\n\n\n\nVariable\nDescription\nCodes\n# NAs\n\n\n\n\nDBQ700\nHow healthy is your diet?\n1 = Excellent  2 = Very Good  3 = Good  4 = Fair  5 = Poor\n1\n\n\n\n\nnh_fixing <- nh_fixing |>\n  mutate(DIETQUAL = \n           fct_recode(\n             factor(DBQ700), \n             \"Excellent\" = \"1\", \n             \"Very Good\" = \"2\", \n             \"Good\" = \"3\",\n             \"Fair\" = \"4\",\n             \"Poor\" = \"5\")) \n\n\nnh_fixing |> tabyl(DIETQUAL, DBQ700) |> gt()\n\n\n\n\n\n  \n  \n    \n      DIETQUAL\n      1\n      2\n      3\n      4\n      5\n      NA_\n    \n  \n  \n    Excellent\n260\n0\n0\n0\n0\n0\n    Very Good\n0\n758\n0\n0\n0\n0\n    Good\n0\n0\n1519\n0\n0\n0\n    Fair\n0\n0\n0\n1082\n0\n0\n    Poor\n0\n0\n0\n0\n311\n0\n    NA\n0\n0\n0\n0\n0\n1\n  \n  \n  \n\n\n\n\n\n\n1.13.8 Creating FOODSEC from FSDAD\n\n\n\n\n\n\n\n\n\nVariable\nDescription\nCodes\n# NAs\n\n\n\n\nFSDAD\nAdult food security category for last 12m\n1 = Full food security  2 = Marginal food security  3 = Low food security  4 = Very low food security\n231\n\n\n\n\nnh_fixing <- nh_fixing |>\n  mutate(FOODSEC = \n           fct_recode(\n             factor(FSDAD), \n             \"Full\" = \"1\", \n             \"Marginal\" = \"2\", \n             \"Low\" = \"3\",\n             \"Very Low\" = \"4\")) \n\n\nnh_fixing |> tabyl(FOODSEC, FSDAD) |> gt()\n\n\n\n\n\n  \n  \n    \n      FOODSEC\n      1\n      2\n      3\n      4\n      NA_\n    \n  \n  \n    Full\n2247\n0\n0\n0\n0\n    Marginal\n0\n565\n0\n0\n0\n    Low\n0\n0\n503\n0\n0\n    Very Low\n0\n0\n0\n385\n0\n    NA\n0\n0\n0\n0\n231\n  \n  \n  \n\n\n\n\n\n\n1.13.9 Creating SNORE from SLQ030\n\n\n\n\n\n\n\n\n\nVariable\nDescription\nCodes\n# NAs\n\n\n\n\nSLQ030\nHow often do you snore?\n0 = Never  1 = Rarely (1-2 nights/week)  2 = Occasionally (3-4 nights/week)  3 = Frequently (5+ nights/week)\n219\n\n\n\n\nnh_fixing <- nh_fixing |>\n  mutate(SNORE = \n           fct_recode(\n             factor(SLQ030), \n             \"Never\" = \"0\", \n             \"Rarely\" = \"1\", \n             \"Occasionally\" = \"2\",\n             \"Frequently\" = \"3\")) \n\n\nnh_fixing |> tabyl(SNORE, SLQ030) |> gt()\n\n\n\n\n\n  \n  \n    \n      SNORE\n      0\n      1\n      2\n      3\n      NA_\n    \n  \n  \n    Never\n855\n0\n0\n0\n0\n    Rarely\n0\n959\n0\n0\n0\n    Occasionally\n0\n0\n700\n0\n0\n    Frequently\n0\n0\n0\n1198\n0\n    NA\n0\n0\n0\n0\n219\n  \n  \n  \n\n\n\n\n\n\n1.13.10 Creating WTGOAL from WHQ040\n\n\n\n\n\n\n\n\n\nVariable\nDescription\nCodes\n# NAs\n\n\n\n\nWHQ040\nLike to weigh more, less, or same?\n1 = More  2 = Less  3 = Stay about the same\n3\n\n\n\nSince there’s a natural ordering here (more then same then less) I’ll adapt it using the fct_relevel() function from the forcats package10 for this variable.\n\nnh_fixing <- nh_fixing |>\n  mutate(WTGOAL = \n           fct_recode(\n             factor(WHQ040), \n             \"More\" = \"1\", \n             \"Less\" = \"2\", \n             \"Same\" = \"3\")) |>\n  mutate(WTGOAL = fct_relevel(WTGOAL, \"More\", \"Same\", \"Less\"))\n\n\nnh_fixing |> tabyl(WTGOAL, WHQ040) |> gt()\n\n\n\n\n\n  \n  \n    \n      WTGOAL\n      1\n      2\n      3\n      NA_\n    \n  \n  \n    More\n289\n0\n0\n0\n    Same\n0\n0\n974\n0\n    Less\n0\n2665\n0\n0\n    NA\n0\n0\n0\n3"
  },
  {
    "objectID": "nhanes.html#dropping-variables",
    "href": "nhanes.html#dropping-variables",
    "title": "1  Building the nh432 example",
    "section": "1.14 Dropping Variables",
    "text": "1.14 Dropping Variables\nWe’ll drop the following variables before saving an analytic tibble.\n\nOur two variables with no variation\n\nRIDSTATR, OHDEXSTS\n\nElements of the PHQ-9 we no longer need\n\nnvalid_phq, DPQ010, DPQ020, DPQ030, DPQ040\nDPQ050, DPQ060, DPQ070, DPQ080, DPQ090\n\nMulti-categorical variables that we renamed\n\nRIDRETH3, DMDEDUC2, OHAREC, HUQ010, DEQ034D\nDPQ100, DBQ700, FSDAD, SLQ030, WHQ040\n\n\n\nnh_fixing <- nh_fixing |>\n  select(-c(RIDSTATR, OHDEXSTS, nvalid_phq9, DPQ010, \n            DPQ020, DPQ030, DPQ040, DPQ050, DPQ060, \n            DPQ070, DPQ080, DPQ090, RIDRETH3, DMDEDUC2, \n            OHAREC, HUQ010, DEQ034D, DPQ100, DBQ700, \n            FSDAD, SLQ030, WHQ040))"
  },
  {
    "objectID": "nhanes.html#resorting-variables",
    "href": "nhanes.html#resorting-variables",
    "title": "1  Building the nh432 example",
    "section": "1.15 Resorting Variables",
    "text": "1.15 Resorting Variables\nI’d like to have the variables in the following order:\n\nnh432 <- nh_fixing |>\n  select(SEQN, AGE, RACEETH, EDUC, SEX, INSURNOW, \n         NOINSUR, SROH, WEIGHT, HEIGHT, WAIST, \n         SBP, DBP, PULSE1, PULSE2, WBC, PLATELET, HSCRP, \n         DR_LOSE, DR_EXER, NOW_LOSE, NOW_EXER,\n         ESTHT, ESTWT, WTGOAL, DIETQUAL, FOODSEC, \n         WORK_V, VIGWK_D, REC_V, VIGREC_D, SEDATE, \n         PHQ9, PHQ9_CAT, DEPRDIFF, MENTALH, \n         SLPWKDAY, SLPWKEND, SLPTROUB, SNORE,\n         HOSPITAL, EVERALC, DRINKS, CIG100, SMOKE30,\n         AWAYWORK, AWAYREST, AWAYBAR, DENTAL, FLOSS, \n         EVERPREG, PREGS, SUNSCR, WTINTPRP, WTMECPRP)"
  },
  {
    "objectID": "nhanes.html#nh432-analytic-tibble",
    "href": "nhanes.html#nh432-analytic-tibble",
    "title": "1  Building the nh432 example",
    "section": "1.16 nh432 analytic tibble",
    "text": "1.16 nh432 analytic tibble\n\nnh432\n\n# A tibble: 3,931 × 55\n   SEQN     AGE RACEETH    EDUC  SEX   INSUR…¹ NOINSUR SROH  WEIGHT HEIGHT WAIST\n   <chr>  <dbl> <fct>      <fct> <fct>   <dbl>   <dbl> <fct>  <dbl>  <dbl> <dbl>\n 1 109271    49 Non-H Whi… 9th … Male        1       0 Fair    98.8   182. 120. \n 2 109273    36 Non-H Whi… Some… Male        1       1 Good    74.3   184.  86.8\n 3 109284    44 Hispanic   9th … Fema…       0       1 Fair    91.1   153. 103. \n 4 109291    42 Non-H Asi… Coll… Fema…       1       0 Fair    81.4   161.  NA  \n 5 109292    58 Hispanic   High… Male        1       0 Very…   86     168. 108. \n 6 109293    44 Non-H Whi… High… Male        1       0 Good    99.4   182. 107  \n 7 109295    54 Hispanic   Less… Fema…       1       0 Good    61.7   157.  90.5\n 8 109297    30 Non-H Asi… Some… Fema…       1       0 Very…   55.4   155.  73.2\n 9 109300    54 Non-H Asi… Coll… Fema…       1       0 Exce…   62     145.  84.8\n10 109305    55 Non-H Asi… Coll… Male        1       0 Good    64     175.  82.5\n# … with 3,921 more rows, 44 more variables: SBP <dbl>, DBP <dbl>,\n#   PULSE1 <dbl>, PULSE2 <dbl>, WBC <dbl>, PLATELET <dbl>, HSCRP <dbl>,\n#   DR_LOSE <dbl>, DR_EXER <dbl>, NOW_LOSE <dbl>, NOW_EXER <dbl>, ESTHT <dbl>,\n#   ESTWT <dbl>, WTGOAL <fct>, DIETQUAL <fct>, FOODSEC <fct>, WORK_V <dbl>,\n#   VIGWK_D <dbl>, REC_V <dbl>, VIGREC_D <dbl>, SEDATE <dbl>, PHQ9 <int>,\n#   PHQ9_CAT <fct>, DEPRDIFF <fct>, MENTALH <dbl>, SLPWKDAY <dbl>,\n#   SLPWKEND <dbl>, SLPTROUB <dbl>, SNORE <fct>, HOSPITAL <dbl>, …\n\n\n\n1.16.1 Saving the tibble as nh432.Rds\n\nwrite_rds(nh432, \"data/nh432.Rds\")"
  },
  {
    "objectID": "nh432cb.html#r-setup",
    "href": "nh432cb.html#r-setup",
    "title": "2  Codebook for nh432",
    "section": "2.1 R Setup",
    "text": "2.1 R Setup\n\nknitr::opts_chunk$set(comment = NA)\n\nlibrary(janitor) \nlibrary(gt)\nlibrary(gtsummary)\nlibrary(Hmisc)\nlibrary(mosaic)\nlibrary(naniar)\nlibrary(tidyverse) \n\ntheme_set(theme_bw())\n\n\n2.1.1 Data Load\n\nnh432 <- read_rds(\"data/nh432.Rds\")"
  },
  {
    "objectID": "nh432cb.html#quantitative-variables-in-nh432",
    "href": "nh432cb.html#quantitative-variables-in-nh432",
    "title": "2  Codebook for nh432",
    "section": "2.2 Quantitative Variables in nh432",
    "text": "2.2 Quantitative Variables in nh432\n\nt1_quantitative <- df_stats(~ AGE + WEIGHT + HEIGHT + WAIST + SBP + DBP +\n           PULSE1 + PULSE2 + WBC + PLATELET + HSCRP +\n           ESTHT + ESTWT + VIGWK_D + VIGREC_D + SEDATE + PHQ9 +\n           SLPWKDAY + SLPWKEND + DRINKS + SMOKE30 + \n           FLOSS + PREGS, data = nh432) |>\n  mutate(across(.cols = -c(response, n, missing), \n              round_half_up, digits = 1)) |> \n  rename(med = median, \"NA\" = missing)\n\nt1_quantitative |>\n  mutate(description = \n           c(\"Age (years)\", \"Weight (kg)\", \"Height (cm)\", \n             \"Waist circumference (cm)\", \"Systolic BP (mm Hg)\", \n             \"Diastolic BP (mm Hg)\", \"1st Pulse (beats/min)\", \n             \"2nd Pulse (beats/min)\", \"White Blood Cell Count (1000 cells/uL)\",\n             \"Platelets (1000 cells/uL)\", \n             \"High-Sensitivity C-Reactive Protein (mg/L)\",\n             \"Self Estimate: Height (in)\", \"Self-Estimate: Weight (lb)\",\n             \"Vigorous Work per week (days)\", \n             \"Vigorous Recreation per week (days)\",\n             \"Sedentary Activity per day (minutes)\",\n             \"PHQ-9 Depression Screener Score (points)\",\n             \"Average weekday sleep (hours)\", \"Average weekend sleep (hours)\",\n             \"Average Alcohol per day (drinks)\", \n             \"Days smoked cigarette in last 30\",\n             \"Days Flossed in last 7\", \"Pregnancies\")) |>\n  select(response, description, everything()) |>\n  gt() |>\n  tab_header(title = \"Quantitative Variables in nh432\")\n\n\n\n\n\n  \n    \n      Quantitative Variables in nh432\n    \n    \n  \n  \n    \n      response\n      description\n      min\n      Q1\n      med\n      Q3\n      max\n      mean\n      sd\n      n\n      NA\n    \n  \n  \n    AGE\nAge (years)\n30.0\n37.0\n45.0\n53.0\n59.0\n44.8\n8.7\n3931\n0\n    WEIGHT\nWeight (kg)\n36.9\n69.3\n82.1\n99.1\n254.3\n86.3\n24.6\n3903\n28\n    HEIGHT\nHeight (cm)\n135.3\n160.0\n166.9\n174.7\n198.7\n167.4\n10.1\n3901\n30\n    WAIST\nWaist circumference (cm)\n57.9\n89.1\n99.2\n111.7\n178.0\n101.5\n17.7\n3782\n149\n    SBP\nSystolic BP (mm Hg)\n69.0\n110.0\n120.0\n131.0\n222.0\n121.5\n17.0\n3585\n346\n    DBP\nDiastolic BP (mm Hg)\n31.0\n69.0\n76.0\n84.0\n136.0\n77.0\n11.7\n3585\n346\n    PULSE1\n1st Pulse (beats/min)\n38.0\n62.0\n69.0\n77.0\n126.0\n70.3\n11.6\n3316\n615\n    PULSE2\n2nd Pulse (beats/min)\n37.0\n63.0\n70.0\n78.0\n121.0\n71.0\n11.6\n3314\n617\n    WBC\nWhite Blood Cell Count (1000 cells/uL)\n2.3\n5.7\n6.9\n8.4\n22.8\n7.3\n2.2\n3755\n176\n    PLATELET\nPlatelets (1000 cells/uL)\n47.0\n210.0\n246.0\n290.0\n818.0\n253.3\n66.4\n3755\n176\n    HSCRP\nHigh-Sensitivity C-Reactive Protein (mg/L)\n0.1\n0.9\n2.1\n4.7\n182.8\n4.3\n8.3\n3664\n267\n    ESTHT\nSelf Estimate: Height (in)\n50.0\n63.0\n66.0\n69.0\n81.0\n66.5\n4.2\n3836\n95\n    ESTWT\nSelf-Estimate: Weight (lb)\n86.0\n150.0\n180.0\n216.0\n578.0\n188.1\n52.2\n3863\n68\n    VIGWK_D\nVigorous Work per week (days)\n0.0\n0.0\n0.0\n2.0\n7.0\n1.2\n2.1\n3926\n5\n    VIGREC_D\nVigorous Recreation per week (days)\n0.0\n0.0\n0.0\n1.0\n7.0\n0.9\n1.7\n3930\n1\n    SEDATE\nSedentary Activity per day (minutes)\n2.0\n180.0\n300.0\n480.0\n1320.0\n332.7\n210.2\n3907\n24\n    PHQ9\nPHQ-9 Depression Screener Score (points)\n0.0\n0.0\n2.0\n5.0\n26.0\n3.3\n4.3\n3718\n213\n    SLPWKDAY\nAverage weekday sleep (hours)\n2.0\n6.5\n7.5\n8.0\n14.0\n7.4\n1.6\n3897\n34\n    SLPWKEND\nAverage weekend sleep (hours)\n2.0\n7.0\n8.0\n9.0\n14.0\n8.2\n1.8\n3897\n34\n    DRINKS\nAverage Alcohol per day (drinks)\n0.0\n1.0\n2.0\n3.0\n15.0\n2.3\n2.2\n3142\n789\n    SMOKE30\nDays smoked cigarette in last 30\n0.0\n0.0\n0.0\n5.0\n30.0\n6.8\n12.1\n3205\n726\n    FLOSS\nDays Flossed in last 7\n0.0\n0.0\n3.0\n7.0\n7.0\n3.5\n2.9\n3927\n4\n    PREGS\nPregnancies\n0.0\n2.0\n3.0\n4.0\n11.0\n3.0\n2.1\n1956\n1975"
  },
  {
    "objectID": "nh432cb.html#two-category-10-variables-in-nh432",
    "href": "nh432cb.html#two-category-10-variables-in-nh432",
    "title": "2  Codebook for nh432",
    "section": "2.3 Two-Category (1/0) Variables in nh432",
    "text": "2.3 Two-Category (1/0) Variables in nh432\n\nnh_dich_vars <- nh432 |>\n  select(HOSPITAL, MENTALH, EVERALC, INSURNOW, NOINSUR, DR_LOSE,\n         DR_EXER, NOW_LOSE, NOW_EXER, WORK_V, REC_V, EVERPREG,\n         SLPTROUB, CIG100, AWAYWORK, AWAYREST, AWAYBAR) \n\ntemp1 <- nh_dich_vars |>  summarise(across(.cols = everything(), \n                                           ~ sum(.x, na.rm = TRUE)))\n\ntemp2 <- nh_dich_vars |>  summarise(across(.cols = everything(), \n                           ~ round_half_up(100*mean(.x, na.rm = TRUE), 1)))\n\ntemp3 <- nh_dich_vars |>  summarise(across(.cols = everything(), \n                                           ~ n_miss(.x)))\n\nnh_dichotomous_summary <- bind_rows(temp1, temp2, temp3) |>\n  mutate(summary = c(\"Yes\", \"% Yes\", \"# NA\")) |>\n  relocate(summary) |>\n  pivot_longer(!summary, names_to = \"variable\") |>\n  pivot_wider(names_from = summary) |>\n  mutate(Description = \n           c(\"Overnight hospital patient in past 12m?\",\n             \"Seen mental health professional past 12m?\",\n             \"Ever had a drink of alcohol?\",\n             \"Covered by health insurance now?\",\n             \"Time when no insurance in past year?\",\n             \"Doctor said to control/lose weight past 12m?\",\n             \"Doctor said to exercise in past 12m?\",\n             \"Are you now controlling or losing weight?\",\n             \"Are you now increasing exercise?\",\n             \"Vigorous work activity for 10 min/week?\",\n             \"Vigorous recreational activity for 10 min/week?\",\n             \"Ever been pregnant?\",\n             \"Ever told a doctor you had trouble sleeping?\",\n             \"Smoked at least 100 cigarettes in your life?\",\n             \"Last 7 days worked at a job not at home?\",\n             \"Last 7 days spent time in a restaurant?\",\n             \"Last 7 days spent time in a bar?\"))\n\nnh_dichotomous_summary |> \n  gt()\n\n\n\n\n\n  \n  \n    \n      variable\n      Yes\n      % Yes\n      # NA\n      Description\n    \n  \n  \n    HOSPITAL\n343\n8.7\n0\nOvernight hospital patient in past 12m?\n    MENTALH\n475\n12.1\n2\nSeen mental health professional past 12m?\n    EVERALC\n3393\n91.1\n205\nEver had a drink of alcohol?\n    INSURNOW\n3169\n80.8\n10\nCovered by health insurance now?\n    NOINSUR\n1041\n26.6\n16\nTime when no insurance in past year?\n    DR_LOSE\n1189\n30.3\n1\nDoctor said to control/lose weight past 12m?\n    DR_EXER\n1680\n42.7\n1\nDoctor said to exercise in past 12m?\n    NOW_LOSE\n2485\n63.2\n2\nAre you now controlling or losing weight?\n    NOW_EXER\n2369\n60.3\n1\nAre you now increasing exercise?\n    WORK_V\n1116\n28.4\n4\nVigorous work activity for 10 min/week?\n    REC_V\n1056\n26.9\n0\nVigorous recreational activity for 10 min/week?\n    EVERPREG\n1747\n89.2\n1972\nEver been pregnant?\n    SLPTROUB\n1159\n29.5\n3\nEver told a doctor you had trouble sleeping?\n    CIG100\n1576\n40.1\n1\nSmoked at least 100 cigarettes in your life?\n    AWAYWORK\n2663\n67.7\n0\nLast 7 days worked at a job not at home?\n    AWAYREST\n2283\n58.1\n2\nLast 7 days spent time in a restaurant?\n    AWAYBAR\n605\n15.4\n0\nLast 7 days spent time in a bar?"
  },
  {
    "objectID": "nh432cb.html#factor-variables-in-nh432",
    "href": "nh432cb.html#factor-variables-in-nh432",
    "title": "2  Codebook for nh432",
    "section": "2.4 Factor Variables in nh432",
    "text": "2.4 Factor Variables in nh432\n\nnh_factor_vars <- nh432 |>\n  select(where(~ is.factor(.x)))\n\ntbl_summary(nh_factor_vars,\n            label = c(RACEETH = \"RACEETH: Race/Ethnicity\",\n                      EDUC = \"EDUC: Educational Attainment\",\n                      SROH = \"SROH: Self-reported Overall Health\",\n                      WTGOAL = \"WTGOAL: Like to weigh more/less/the same?\",\n                      DIETQUAL = \"DIETQUAL: How healthy is your diet?\",\n                      FOODSEC = \"FOODSEC: Adult food security (last 12m)\",\n                      PHQ9_CAT = \"PHQ9_CAT: Depression Screen Category\",\n                      DEPRDIFF = \"DEPRDIFF: Difficulty with Depression?\",\n                      SNORE = \"SNORE: How often do you snore?\",\n                      DENTAL = \"DENTAL: Recommendation for Dental Care?\",\n                      SUNSCR = \"SUNSCR: Use sunscreen on very sunny day?\"),\n            missing_text = \"(# NA)\")\n\n\n\n\n\n  \n  \n    \n      Characteristic\n      N = 3,9311\n    \n  \n  \n    RACEETH: Race/Ethnicity\n\n        Non-H White\n1,192 (30%)\n        Non-H Black\n1,049 (27%)\n        Hispanic\n903 (23%)\n        Non-H Asian\n588 (15%)\n        Other Race\n199 (5.1%)\n    EDUC: Educational Attainment\n\n        Less than 9th Grade\n272 (6.9%)\n        9th - 11th Grade\n424 (11%)\n        High School Grad\n850 (22%)\n        Some College / AA\n1,287 (33%)\n        College Grad\n1,097 (28%)\n        (# NA)\n1\n    SEX\n\n        Female\n2,094 (53%)\n        Male\n1,837 (47%)\n    SROH: Self-reported Overall Health\n\n        Excellent\n495 (13%)\n        Very Good\n1,071 (27%)\n        Good\n1,462 (37%)\n        Fair\n765 (19%)\n        Poor\n138 (3.5%)\n    WTGOAL: Like to weigh more/less/the same?\n\n        More\n289 (7.4%)\n        Same\n974 (25%)\n        Less\n2,665 (68%)\n        (# NA)\n3\n    DIETQUAL: How healthy is your diet?\n\n        Excellent\n260 (6.6%)\n        Very Good\n758 (19%)\n        Good\n1,519 (39%)\n        Fair\n1,082 (28%)\n        Poor\n311 (7.9%)\n        (# NA)\n1\n    FOODSEC: Adult food security (last 12m)\n\n        Full\n2,247 (61%)\n        Marginal\n565 (15%)\n        Low\n503 (14%)\n        Very Low\n385 (10%)\n        (# NA)\n231\n    PHQ9_CAT: Depression Screen Category\n\n        minimal\n2,748 (74%)\n        mild\n621 (17%)\n        moderate\n220 (5.9%)\n        moderately severe\n91 (2.4%)\n        severe\n38 (1.0%)\n        (# NA)\n213\n    DEPRDIFF: Difficulty with Depression?\n\n        Not at all\n3,039 (82%)\n        Somewhat\n541 (15%)\n        Very\n93 (2.5%)\n        Extremely\n43 (1.2%)\n        (# NA)\n215\n    SNORE: How often do you snore?\n\n        Never\n855 (23%)\n        Rarely\n959 (26%)\n        Occasionally\n700 (19%)\n        Frequently\n1,198 (32%)\n        (# NA)\n219\n    DENTAL: Recommendation for Dental Care?\n\n        See dentist urgently\n234 (6.0%)\n        See dentist soon\n1,671 (43%)\n        Regular Routine\n2,026 (52%)\n    SUNSCR: Use sunscreen on very sunny day?\n\n        Always\n351 (9.0%)\n        Most of the time\n485 (12%)\n        Sometimes\n831 (21%)\n        Rarely\n662 (17%)\n        Never\n1,583 (40%)\n        (# NA)\n19\n  \n  \n  \n    \n      1 n (%)"
  },
  {
    "objectID": "nh432cb.html#detailed-numerical-description-for-nh432",
    "href": "nh432cb.html#detailed-numerical-description-for-nh432",
    "title": "2  Codebook for nh432",
    "section": "2.5 Detailed Numerical Description for nh432",
    "text": "2.5 Detailed Numerical Description for nh432\n\ndescribe(nh432)\n\nnh432 \n\n 55  Variables      3931  Observations\n--------------------------------------------------------------------------------\nSEQN \n       n  missing distinct \n    3931        0     3931 \n\nlowest : 109271 109273 109284 109291 109292, highest: 124807 124810 124813 124815 124818\n--------------------------------------------------------------------------------\nAGE \n       n  missing distinct     Info     Mean      Gmd      .05      .10 \n    3931        0       30    0.999    44.79    10.09       31       33 \n     .25      .50      .75      .90      .95 \n      37       45       53       57       58 \n\nlowest : 30 31 32 33 34, highest: 55 56 57 58 59\n--------------------------------------------------------------------------------\nRACEETH \n       n  missing distinct \n    3931        0        5 \n\nlowest : Non-H White Non-H Black Hispanic    Non-H Asian Other Race \nhighest: Non-H White Non-H Black Hispanic    Non-H Asian Other Race \n                                                                      \nValue      Non-H White Non-H Black    Hispanic Non-H Asian  Other Race\nFrequency         1192        1049         903         588         199\nProportion       0.303       0.267       0.230       0.150       0.051\n--------------------------------------------------------------------------------\nEDUC \n       n  missing distinct \n    3930        1        5 \n\nlowest : Less than 9th Grade 9th - 11th Grade    High School Grad    Some College / AA   College Grad       \nhighest: Less than 9th Grade 9th - 11th Grade    High School Grad    Some College / AA   College Grad       \n                                                                      \nValue      Less than 9th Grade    9th - 11th Grade    High School Grad\nFrequency                  272                 424                 850\nProportion               0.069               0.108               0.216\n                                                  \nValue        Some College / AA        College Grad\nFrequency                 1287                1097\nProportion               0.327               0.279\n--------------------------------------------------------------------------------\nSEX \n       n  missing distinct \n    3931        0        2 \n                        \nValue      Female   Male\nFrequency    2094   1837\nProportion  0.533  0.467\n--------------------------------------------------------------------------------\nINSURNOW \n       n  missing distinct     Info      Sum     Mean      Gmd \n    3921       10        2    0.465     3169   0.8082   0.3101 \n\n--------------------------------------------------------------------------------\nNOINSUR \n       n  missing distinct     Info      Sum     Mean      Gmd \n    3915       16        2    0.586     1041   0.2659   0.3905 \n\n--------------------------------------------------------------------------------\nSROH \n       n  missing distinct \n    3931        0        5 \n\nlowest : Excellent Very Good Good      Fair      Poor     \nhighest: Excellent Very Good Good      Fair      Poor     \n                                                            \nValue      Excellent Very Good      Good      Fair      Poor\nFrequency        495      1071      1462       765       138\nProportion     0.126     0.272     0.372     0.195     0.035\n--------------------------------------------------------------------------------\nWEIGHT \n       n  missing distinct     Info     Mean      Gmd      .05      .10 \n    3903       28      969        1    86.31    26.59    54.20    58.82 \n     .25      .50      .75      .90      .95 \n   69.30    82.10    99.10   119.30   131.49 \n\nlowest :  36.9  39.4  39.6  39.8  39.9, highest: 204.4 204.6 210.8 242.6 254.3\n--------------------------------------------------------------------------------\nHEIGHT \n       n  missing distinct     Info     Mean      Gmd      .05      .10 \n    3901       30      484        1    167.4    11.45    152.0    154.8 \n     .25      .50      .75      .90      .95 \n   160.0    166.9    174.7    180.8    184.6 \n\nlowest : 135.3 138.3 139.7 141.4 141.9, highest: 195.8 195.9 196.6 198.3 198.7\n--------------------------------------------------------------------------------\nWAIST \n       n  missing distinct     Info     Mean      Gmd      .05      .10 \n    3782      149      781        1    101.5    19.68     75.9     80.4 \n     .25      .50      .75      .90      .95 \n    89.1     99.2    111.7    125.4    134.5 \n\nlowest :  57.9  62.7  63.2  64.5  64.9, highest: 166.0 167.1 170.8 173.1 178.0\n--------------------------------------------------------------------------------\nSBP \n       n  missing distinct     Info     Mean      Gmd      .05      .10 \n    3585      346      116        1    121.5    18.61       98      102 \n     .25      .50      .75      .90      .95 \n     110      120      131      143      152 \n\nlowest :  69  72  77  79  80, highest: 199 200 211 219 222\n--------------------------------------------------------------------------------\nDBP \n       n  missing distinct     Info     Mean      Gmd      .05      .10 \n    3585      346       81    0.999    77.03    13.01     59.2     63.0 \n     .25      .50      .75      .90      .95 \n    69.0     76.0     84.0     92.0     97.0 \n\nlowest :  31  44  45  46  47, highest: 121 122 126 127 136\n--------------------------------------------------------------------------------\nPULSE1 \n       n  missing distinct     Info     Mean      Gmd      .05      .10 \n    3316      615       76    0.999     70.3    12.92       53       57 \n     .25      .50      .75      .90      .95 \n      62       69       77       86       91 \n\nlowest :  38  40  41  42  44, highest: 114 115 120 121 126\n--------------------------------------------------------------------------------\nPULSE2 \n       n  missing distinct     Info     Mean      Gmd      .05      .10 \n    3314      617       80    0.999    70.96    12.92       54       57 \n     .25      .50      .75      .90      .95 \n      63       70       78       86       91 \n\nlowest :  37  39  40  41  42, highest: 117 118 119 120 121\n--------------------------------------------------------------------------------\nWBC \n       n  missing distinct     Info     Mean      Gmd      .05      .10 \n    3755      176      136        1    7.254    2.387      4.3      4.8 \n     .25      .50      .75      .90      .95 \n     5.7      6.9      8.4     10.1     11.3 \n\nlowest :  2.3  2.5  2.6  2.7  2.8, highest: 17.2 17.4 17.6 20.6 22.8\n--------------------------------------------------------------------------------\nPLATELET \n       n  missing distinct     Info     Mean      Gmd      .05      .10 \n    3755      176      372        1    253.3    72.04      159      179 \n     .25      .50      .75      .90      .95 \n     210      246      290      337      371 \n\nlowest :  47  48  54  57  61, highest: 583 602 638 662 818\n--------------------------------------------------------------------------------\nHSCRP \n       n  missing distinct     Info     Mean      Gmd      .05      .10 \n    3664      267     1065        1    4.326    5.271    0.350    0.470 \n     .25      .50      .75      .90      .95 \n   0.890    2.090    4.740    9.217   13.630 \n\nlowest :   0.11   0.16   0.17   0.18   0.19, highest: 102.94 104.48 109.81 138.81 182.82\n--------------------------------------------------------------------------------\nDR_LOSE \n       n  missing distinct     Info      Sum     Mean      Gmd \n    3930        1        2    0.633     1189   0.3025   0.4221 \n\n--------------------------------------------------------------------------------\nDR_EXER \n       n  missing distinct     Info      Sum     Mean      Gmd \n    3930        1        2    0.734     1680   0.4275   0.4896 \n\n--------------------------------------------------------------------------------\nNOW_LOSE \n       n  missing distinct     Info      Sum     Mean      Gmd \n    3929        2        2    0.697     2485   0.6325    0.465 \n\n--------------------------------------------------------------------------------\nNOW_EXER \n       n  missing distinct     Info      Sum     Mean      Gmd \n    3930        1        2    0.718     2369   0.6028    0.479 \n\n--------------------------------------------------------------------------------\nESTHT \n       n  missing distinct     Info     Mean      Gmd      .05      .10 \n    3836       95       29    0.995    66.46    4.722       60       61 \n     .25      .50      .75      .90      .95 \n      63       66       69       72       74 \n\nlowest : 50 53 54 55 56, highest: 76 77 78 79 81\n--------------------------------------------------------------------------------\nESTWT \n       n  missing distinct     Info     Mean      Gmd      .05      .10 \n    3863       68      255        1    188.1    56.51    120.0    130.0 \n     .25      .50      .75      .90      .95 \n   150.0    180.0    216.0    258.0    281.9 \n\nlowest :  86  88  90  93  95, highest: 416 434 450 457 578\n--------------------------------------------------------------------------------\nWTGOAL \n       n  missing distinct \n    3928        3        3 \n                            \nValue       More  Same  Less\nFrequency    289   974  2665\nProportion 0.074 0.248 0.678\n--------------------------------------------------------------------------------\nDIETQUAL \n       n  missing distinct \n    3930        1        5 \n\nlowest : Excellent Very Good Good      Fair      Poor     \nhighest: Excellent Very Good Good      Fair      Poor     \n                                                            \nValue      Excellent Very Good      Good      Fair      Poor\nFrequency        260       758      1519      1082       311\nProportion     0.066     0.193     0.387     0.275     0.079\n--------------------------------------------------------------------------------\nFOODSEC \n       n  missing distinct \n    3700      231        4 \n                                              \nValue          Full Marginal      Low Very Low\nFrequency      2247      565      503      385\nProportion    0.607    0.153    0.136    0.104\n--------------------------------------------------------------------------------\nWORK_V \n       n  missing distinct     Info      Sum     Mean      Gmd \n    3927        4        2     0.61     1116   0.2842    0.407 \n\n--------------------------------------------------------------------------------\nVIGWK_D \n       n  missing distinct     Info     Mean      Gmd \n    3926        5        8    0.632     1.22    1.904 \n\nlowest : 0 1 2 3 4, highest: 3 4 5 6 7\n                                                          \nValue          0     1     2     3     4     5     6     7\nFrequency   2811    79   127   184   112   352   132   129\nProportion 0.716 0.020 0.032 0.047 0.029 0.090 0.034 0.033\n--------------------------------------------------------------------------------\nREC_V \n       n  missing distinct     Info      Sum     Mean      Gmd \n    3931        0        2    0.589     1056   0.2686    0.393 \n\n--------------------------------------------------------------------------------\nVIGREC_D \n       n  missing distinct     Info     Mean      Gmd \n    3930        1        8    0.608   0.8952    1.436 \n\nlowest : 0 1 2 3 4, highest: 3 4 5 6 7\n                                                          \nValue          0     1     2     3     4     5     6     7\nFrequency   2875   126   211   301   166   154    46    51\nProportion 0.732 0.032 0.054 0.077 0.042 0.039 0.012 0.013\n--------------------------------------------------------------------------------\nSEDATE \n       n  missing distinct     Info     Mean      Gmd      .05      .10 \n    3907       24       44     0.99    332.7    232.2       60      120 \n     .25      .50      .75      .90      .95 \n     180      300      480      600      720 \n\nlowest :    2    3    5    8    9, highest:  960 1020 1080 1200 1320\n--------------------------------------------------------------------------------\nPHQ9 \n       n  missing distinct     Info     Mean      Gmd      .05      .10 \n    3718      213       27    0.958    3.324    4.201        0        0 \n     .25      .50      .75      .90      .95 \n       0        2        5        9       13 \n\nlowest :  0  1  2  3  4, highest: 22 23 24 25 26\n--------------------------------------------------------------------------------\nPHQ9_CAT \n       n  missing distinct \n    3718      213        5 \n\nlowest : minimal           mild              moderate          moderately severe severe           \nhighest: minimal           mild              moderate          moderately severe severe           \n                                                                \nValue                minimal              mild          moderate\nFrequency               2748               621               220\nProportion             0.739             0.167             0.059\n                                              \nValue      moderately severe            severe\nFrequency                 91                38\nProportion             0.024             0.010\n--------------------------------------------------------------------------------\nDEPRDIFF \n       n  missing distinct \n    3716      215        4 \n                                                      \nValue      Not at all   Somewhat       Very  Extremely\nFrequency        3039        541         93         43\nProportion      0.818      0.146      0.025      0.012\n--------------------------------------------------------------------------------\nMENTALH \n       n  missing distinct     Info      Sum     Mean      Gmd \n    3929        2        2    0.319      475   0.1209   0.2126 \n\n--------------------------------------------------------------------------------\nSLPWKDAY \n       n  missing distinct     Info     Mean      Gmd      .05      .10 \n    3897       34       22    0.984    7.359    1.735      5.0      5.5 \n     .25      .50      .75      .90      .95 \n     6.5      7.5      8.0      9.0     10.0 \n\nlowest :  2.0  3.0  3.5  4.0  4.5, highest: 11.0 11.5 12.0 13.0 14.0\n--------------------------------------------------------------------------------\nSLPWKEND \n       n  missing distinct     Info     Mean      Gmd      .05      .10 \n    3897       34       24    0.983    8.231    1.928        5        6 \n     .25      .50      .75      .90      .95 \n       7        8        9       10       11 \n\nlowest :  2.0  3.0  3.5  4.0  4.5, highest: 12.0 12.5 13.0 13.5 14.0\n--------------------------------------------------------------------------------\nSLPTROUB \n       n  missing distinct     Info      Sum     Mean      Gmd \n    3928        3        2    0.624     1159   0.2951   0.4161 \n\n--------------------------------------------------------------------------------\nSNORE \n       n  missing distinct \n    3712      219        4 \n                                                              \nValue             Never       Rarely Occasionally   Frequently\nFrequency           855          959          700         1198\nProportion        0.230        0.258        0.189        0.323\n--------------------------------------------------------------------------------\nHOSPITAL \n       n  missing distinct     Info      Sum     Mean      Gmd \n    3931        0        2    0.239      343  0.08726   0.1593 \n\n--------------------------------------------------------------------------------\nEVERALC \n       n  missing distinct     Info      Sum     Mean      Gmd \n    3726      205        2    0.244     3393   0.9106   0.1628 \n\n--------------------------------------------------------------------------------\nDRINKS \n       n  missing distinct     Info     Mean      Gmd      .05      .10 \n    3142      789       15    0.948    2.345    2.051        0        0 \n     .25      .50      .75      .90      .95 \n       1        2        3        5        6 \n\nlowest :  0  1  2  3  4, highest: 10 11 12 13 15\n                                                                            \nValue          0     1     2     3     4     5     6     7     8     9    10\nFrequency    333   912   903   412   228   123   105    21    37     4    19\nProportion 0.106 0.290 0.287 0.131 0.073 0.039 0.033 0.007 0.012 0.001 0.006\n                                  \nValue         11    12    13    15\nFrequency      1    26     2    16\nProportion 0.000 0.008 0.001 0.005\n--------------------------------------------------------------------------------\nCIG100 \n       n  missing distinct     Info      Sum     Mean      Gmd \n    3930        1        2    0.721     1576    0.401   0.4805 \n\n--------------------------------------------------------------------------------\nSMOKE30 \n       n  missing distinct     Info     Mean      Gmd      .05      .10 \n    3205      726       26    0.594    6.808     10.5        0        0 \n     .25      .50      .75      .90      .95 \n       0        0        5       30       30 \n\nlowest :  0  1  2  3  4, highest: 26 27 28 29 30\n--------------------------------------------------------------------------------\nAWAYWORK \n       n  missing distinct     Info      Sum     Mean      Gmd \n    3931        0        2    0.656     2663   0.6774   0.4371 \n\n--------------------------------------------------------------------------------\nAWAYREST \n       n  missing distinct     Info      Sum     Mean      Gmd \n    3929        2        2     0.73     2283   0.5811    0.487 \n\n--------------------------------------------------------------------------------\nAWAYBAR \n       n  missing distinct     Info      Sum     Mean      Gmd \n    3931        0        2    0.391      605   0.1539   0.2605 \n\n--------------------------------------------------------------------------------\nDENTAL \n       n  missing distinct \n    3931        0        3 \n                                                                         \nValue      See dentist urgently     See dentist soon      Regular Routine\nFrequency                   234                 1671                 2026\nProportion                0.060                0.425                0.515\n--------------------------------------------------------------------------------\nFLOSS \n       n  missing distinct     Info     Mean      Gmd \n    3927        4        8    0.934    3.476    3.248 \n\nlowest : 0 1 2 3 4, highest: 3 4 5 6 7\n                                                          \nValue          0     1     2     3     4     5     6     7\nFrequency   1104   288   395   329   230   179    44  1358\nProportion 0.281 0.073 0.101 0.084 0.059 0.046 0.011 0.346\n--------------------------------------------------------------------------------\nEVERPREG \n       n  missing distinct     Info      Sum     Mean      Gmd \n    1959     1972        2     0.29     1747   0.8918   0.1931 \n\n--------------------------------------------------------------------------------\nPREGS \n       n  missing distinct     Info     Mean      Gmd      .05      .10 \n    1956     1975       12    0.973    3.046     2.24        0        0 \n     .25      .50      .75      .90      .95 \n       2        3        4        6        7 \n\nlowest :  0  1  2  3  4, highest:  7  8  9 10 11\n                                                                            \nValue          0     1     2     3     4     5     6     7     8     9    10\nFrequency    212   205   421   420   297   191    95    58    22    10     9\nProportion 0.108 0.105 0.215 0.215 0.152 0.098 0.049 0.030 0.011 0.005 0.005\n                \nValue         11\nFrequency     16\nProportion 0.008\n--------------------------------------------------------------------------------\nSUNSCR \n       n  missing distinct \n    3912       19        5 \n\nlowest : Always           Most of the time Sometimes        Rarely           Never           \nhighest: Always           Most of the time Sometimes        Rarely           Never           \n                                                                              \nValue                Always Most of the time        Sometimes           Rarely\nFrequency               351              485              831              662\nProportion            0.090            0.124            0.212            0.169\n                           \nValue                 Never\nFrequency              1583\nProportion            0.405\n--------------------------------------------------------------------------------\nWTINTPRP \n       n  missing distinct     Info     Mean      Gmd      .05      .10 \n    3931        0     3677        1    28434    27437     5911     7199 \n     .25      .50      .75      .90      .95 \n   10615    17358    31476    65098    94422 \n\nlowest :   2467.054   2779.464   2833.287   2917.413   2967.271\nhighest: 246249.502 248091.496 264719.137 282883.648 311265.152\n--------------------------------------------------------------------------------\nWTMECPRP \n       n  missing distinct     Info     Mean      Gmd      .05      .10 \n    3931        0     3701        1    30353    29409     6217     7634 \n     .25      .50      .75      .90      .95 \n   11365    18422    33155    68569   102038 \n\nlowest :   2589.175   2782.738   3003.518   3009.532   3016.643\nhighest: 267064.352 268878.570 273958.374 308014.509 321573.519\n--------------------------------------------------------------------------------"
  },
  {
    "objectID": "nh432cb.html#missingness-in-nh432",
    "href": "nh432cb.html#missingness-in-nh432",
    "title": "2  Codebook for nh432",
    "section": "2.6 Missingness in nh432",
    "text": "2.6 Missingness in nh432\n\nmiss_case_table(nh432) |> gt()\n\n\n\n\n\n  \n  \n    \n      n_miss_in_case\n      n_cases\n      pct_cases\n    \n  \n  \n    0\n907\n23.0730094\n    1\n533\n13.5588909\n    2\n1030\n26.2019842\n    3\n591\n15.0343424\n    4\n307\n7.8097176\n    5\n161\n4.0956500\n    6\n87\n2.2131773\n    7\n106\n2.6965149\n    8\n68\n1.7298397\n    9\n18\n0.4578988\n    10\n20\n0.5087764\n    11\n39\n0.9921140\n    12\n27\n0.6868481\n    13\n14\n0.3561435\n    14\n9\n0.2289494\n    15\n14\n0.3561435\n  \n  \n  \n\n\n\n\n\ngg_miss_var(nh432)\n\n\n\n\n\nmiss_var_summary(nh432) |> gt()\n\n\n\n\n\n  \n  \n    \n      variable\n      n_miss\n      pct_miss\n    \n  \n  \n    PREGS\n1975\n50.24166879\n    EVERPREG\n1972\n50.16535233\n    DRINKS\n789\n20.07122869\n    SMOKE30\n726\n18.46858306\n    PULSE2\n617\n15.69575172\n    PULSE1\n615\n15.64487408\n    SBP\n346\n8.80183160\n    DBP\n346\n8.80183160\n    HSCRP\n267\n6.79216484\n    FOODSEC\n231\n5.87636734\n    SNORE\n219\n5.57110150\n    DEPRDIFF\n215\n5.46934622\n    PHQ9\n213\n5.41846858\n    PHQ9_CAT\n213\n5.41846858\n    EVERALC\n205\n5.21495803\n    WBC\n176\n4.47723226\n    PLATELET\n176\n4.47723226\n    WAIST\n149\n3.79038413\n    ESTHT\n95\n2.41668787\n    ESTWT\n68\n1.72983974\n    SLPWKDAY\n34\n0.86491987\n    SLPWKEND\n34\n0.86491987\n    HEIGHT\n30\n0.76316459\n    WEIGHT\n28\n0.71228695\n    SEDATE\n24\n0.61053167\n    SUNSCR\n19\n0.48333757\n    NOINSUR\n16\n0.40702111\n    INSURNOW\n10\n0.25438820\n    VIGWK_D\n5\n0.12719410\n    WORK_V\n4\n0.10175528\n    FLOSS\n4\n0.10175528\n    WTGOAL\n3\n0.07631646\n    SLPTROUB\n3\n0.07631646\n    NOW_LOSE\n2\n0.05087764\n    MENTALH\n2\n0.05087764\n    AWAYREST\n2\n0.05087764\n    EDUC\n1\n0.02543882\n    DR_LOSE\n1\n0.02543882\n    DR_EXER\n1\n0.02543882\n    NOW_EXER\n1\n0.02543882\n    DIETQUAL\n1\n0.02543882\n    VIGREC_D\n1\n0.02543882\n    CIG100\n1\n0.02543882\n    SEQN\n0\n0.00000000\n    AGE\n0\n0.00000000\n    RACEETH\n0\n0.00000000\n    SEX\n0\n0.00000000\n    SROH\n0\n0.00000000\n    REC_V\n0\n0.00000000\n    HOSPITAL\n0\n0.00000000\n    AWAYWORK\n0\n0.00000000\n    AWAYBAR\n0\n0.00000000\n    DENTAL\n0\n0.00000000\n    WTINTPRP\n0\n0.00000000\n    WTMECPRP\n0\n0.00000000\n  \n  \n  \n\n\n\n\n\nmiss_var_table(nh432) |> gt()\n\n\n\n\n\n  \n  \n    \n      n_miss_in_var\n      n_vars\n      pct_vars\n    \n  \n  \n    0\n12\n21.818182\n    1\n7\n12.727273\n    2\n3\n5.454545\n    3\n2\n3.636364\n    4\n2\n3.636364\n    5\n1\n1.818182\n    10\n1\n1.818182\n    16\n1\n1.818182\n    19\n1\n1.818182\n    24\n1\n1.818182\n    28\n1\n1.818182\n    30\n1\n1.818182\n    34\n2\n3.636364\n    68\n1\n1.818182\n    95\n1\n1.818182\n    149\n1\n1.818182\n    176\n2\n3.636364\n    205\n1\n1.818182\n    213\n2\n3.636364\n    215\n1\n1.818182\n    219\n1\n1.818182\n    231\n1\n1.818182\n    267\n1\n1.818182\n    346\n2\n3.636364\n    615\n1\n1.818182\n    617\n1\n1.818182\n    726\n1\n1.818182\n    789\n1\n1.818182\n    1972\n1\n1.818182\n    1975\n1\n1.818182"
  },
  {
    "objectID": "431review1.html#r-setup",
    "href": "431review1.html#r-setup",
    "title": "3  431 Review: Comparing Means",
    "section": "3.1 R Setup",
    "text": "3.1 R Setup\n\nknitr::opts_chunk$set(comment = NA)\n\nlibrary(janitor) \nlibrary(broom)\nlibrary(car)\nlibrary(glue)\nlibrary(gt)\nlibrary(gtsummary)\nlibrary(lmboot)\nlibrary(MKinfer)\nlibrary(mosaic)\nlibrary(naniar)\nlibrary(patchwork)\nlibrary(rstatix)\n\nlibrary(tidyverse) \n\ntheme_set(theme_bw())"
  },
  {
    "objectID": "431review1.html#data-ingest",
    "href": "431review1.html#data-ingest",
    "title": "3  431 Review: Comparing Means",
    "section": "3.2 Data Ingest",
    "text": "3.2 Data Ingest\nSince we’ve already got the nh432 file formatted as an R data set, we’ll use that.\n\nnh432 <- read_rds(\"data/nh432.Rds\")"
  },
  {
    "objectID": "431review1.html#testing-or-summarizing-normality",
    "href": "431review1.html#testing-or-summarizing-normality",
    "title": "3  431 Review: Comparing Means",
    "section": "3.3 Testing or Summarizing Normality?",
    "text": "3.3 Testing or Summarizing Normality?\nAs we’ll see, the two most useful strategies for dealing with problematic non-Normality when comparing means are (1) transformation of the outcome to make the assumption of Normality more tenable, and (2) alternate inference approaches (for example, using a bootstrap or rank-based procedure instead of a t test.)\nWhile it is possible to obtain numerical summaries of deviations from Normality, perhaps a measure of skewness (asymmetry) or kurtosis (heavy-tailed behavior), in practical work, I never use such summaries to overrule my assessment of the plots. It’s critical instead to focus on the pictures of a distribution, most especially Normal Q-Q plots.\nPerhaps the simplest skewness summary is \\(skew_1\\) = (mean-median)/(standard deviation), where values below -0.2 are meant to indicate (meaningful) left skew, and values above +0.2 indicate (meaningful) right skew. Unfortunately, this approach works poorly with many distributions (for example, multimodal distributions) and so do many other (more sophisticated) measures1.\nIt is also possible to develop hypothesis tests of whether a particular batch of data follows a Normal distribution, for example, the Kolmogorov-Smirnov test2, or the Shapiro-Wilk test3, but again, I find these to be without value in practical work and cannot recommend their use."
  },
  {
    "objectID": "431review1.html#comparing-two-means-using-paired-samples",
    "href": "431review1.html#comparing-two-means-using-paired-samples",
    "title": "3  431 Review: Comparing Means",
    "section": "3.4 Comparing Two Means using Paired Samples",
    "text": "3.4 Comparing Two Means using Paired Samples\nNow, we’ll demonstrate some approaches to comparing two means coming from paired samples. This will include:\n\na paired t test (one-sample t test on the paired differences), which we can obtain from a linear model, or from t.test()\n\nThese procedures based on the t distribution for paired samples require that the distribution of the sample paired differences is well-approximated by a Normal model. As an alternative without that requirement, we’ll focus primarily on a bootstrap comparison (not assuming Normality) from boot.t.test(), which comes from the MKinfer package. It is also possible to generate rank-based inference, such as using the Wilcoxon signed rank approach, but this introduces the major weakness of not estimating the population mean (or even the population median.)\nWe’ll assume a Missing Completely at Random (MCAR) mechanism for missing data, so that a complete case analysis makes sense, and we’ll also use functions from the broom package to tidy our output, and from the gt package to help present it in an attractive table."
  },
  {
    "objectID": "431review1.html#comparing-pulse1-to-pulse2",
    "href": "431review1.html#comparing-pulse1-to-pulse2",
    "title": "3  431 Review: Comparing Means",
    "section": "3.5 Comparing PULSE1 to PULSE2",
    "text": "3.5 Comparing PULSE1 to PULSE2\nWe have two measurements of pulse rate (in beats per minute) in nh432 for each participant. Let’s compare the two for all participants with two PULSE readings. Since we have a value of PULSE1 and PULSE2 for each participant, it makes sense to treat these as paired samples, and study the paired differences in pulse rate.\n\ndat1 <- nh432 |> select(SEQN, PULSE1, PULSE2) |>\n  drop_na() |>\n  mutate(PULSEDIFF = PULSE2 - PULSE1)\n\nsummary(dat1 |> select(-SEQN))\n\n     PULSE1          PULSE2         PULSEDIFF       \n Min.   : 38.0   Min.   : 37.00   Min.   :-22.0000  \n 1st Qu.: 62.0   1st Qu.: 63.00   1st Qu.: -1.0000  \n Median : 69.0   Median : 70.00   Median :  1.0000  \n Mean   : 70.3   Mean   : 70.96   Mean   :  0.6533  \n 3rd Qu.: 77.0   3rd Qu.: 78.00   3rd Qu.:  2.0000  \n Max.   :126.0   Max.   :121.00   Max.   : 26.0000  \n\ndf_stats(~ PULSE1 + PULSE2 + PULSEDIFF, data = dat1) |> \n  mutate(across(.cols = -c(response, n, missing), num, digits = 2)) |>\n  gt()\n\nWarning: There was 1 warning in `mutate()`.\nℹ In argument: `across(.cols = -c(response, n, missing), num, digits = 2)`.\nCaused by warning:\n! The `...` argument of `across()` is deprecated as of dplyr 1.1.0.\nSupply arguments directly to `.fns` through an anonymous function instead.\n\n  # Previously\n  across(a:b, mean, na.rm = TRUE)\n\n  # Now\n  across(a:b, \\(x) mean(x, na.rm = TRUE))\n\n\n\n\n\n\n  \n  \n    \n      response\n      min\n      Q1\n      median\n      Q3\n      max\n      mean\n      sd\n      n\n      missing\n    \n  \n  \n    PULSE1\n38.00\n62.00\n69.00\n77.00\n126.00\n70.30\n11.61\n3314\n0\n    PULSE2\n37.00\n63.00\n70.00\n78.00\n121.00\n70.96\n11.57\n3314\n0\n    PULSEDIFF\n-22.00\n-1.00\n1.00\n2.00\n26.00\n0.65\n3.43\n3314\n0\n  \n  \n  \n\n\n\n\n\n3.5.1 Distribution of Paired Differences\n\np1 <- ggplot(dat1, aes(sample = PULSEDIFF)) +\n  geom_qq(col = \"dodgerblue\") + geom_qq_line(col = \"navy\") + \n  theme(aspect.ratio = 1) + \n  labs(title = \"Normal Q-Q plot\")\n\np2 <- ggplot(dat1, aes(x = PULSEDIFF)) +\n  geom_histogram(aes(y = after_stat(density)), \n                 bins = 25, fill = \"dodgerblue\", col = \"white\") +\n  stat_function(fun = dnorm, \n                args = list(mean = mean(dat1$PULSEDIFF), \n                            sd = sd(dat1$PULSEDIFF)),\n                col = \"navy\", linewidth = 1.5) +\n  labs(title = \"Histogram and Normal Density\")\n\np3 <- ggplot(dat1, aes(x = PULSEDIFF, y = \"\")) +\n  geom_boxplot(fill = \"dodgerblue\", outlier.color = \"dodgerblue\") + \n  stat_summary(fun = \"mean\", geom = \"point\", \n               shape = 23, size = 3, fill = \"white\") +\n  labs(title = \"Boxplot with mean\", y = \"\")\n\np1 + (p2 / p3 + plot_layout(heights = c(4,1))) +\n  plot_annotation(title = \"Pulse 2 - Pulse 1 difference in nh432\",\n                  subtitle = glue(nrow(dat1), \" NHANES Participants ages 30-59 in nh432 data\"))\n\n\n\n\nThese data appear to come from a distribution that is essentially symmetric, but extremely heavy-tailed, with many outlier candidates on both the low and high end of the distribution. It seems unwise to assume a Normal distribution for these differences in pulse rate.\n\n\n3.5.2 Using t.test to obtain a 90% CI for the mean pulse difference\nNote that I use 90% as my confidence level here, mostly to make sure that we don’t always simply default to 95% without engaging our brains.\n\ntt1 <- t.test(dat1$PULSEDIFF, conf.level = 0.90)\n\ntt1\n\n\n    One Sample t-test\n\ndata:  dat1$PULSEDIFF\nt = 10.98, df = 3313, p-value < 2.2e-16\nalternative hypothesis: true mean is not equal to 0\n90 percent confidence interval:\n 0.5553989 0.7511792\nsample estimates:\nmean of x \n0.6532891 \n\ntidy(tt1, conf.int = TRUE, conf = 0.90) |>\n  select(estimate, low.90 = conf.low, hi.90 = conf.high, \n         \"Paired t\" = statistic, p.value, method) |>\n  mutate(across(.cols = -method, num, digits = 3)) |>\n  gt() |>\n  tab_header(title = \"Pulse 2 - Pulse 1 Difference Estimate with 90% T-based CI\",\n             subtitle = glue(nrow(dat1), \" NHANES Participants ages 30-59 in nh432 data\"))\n\n\n\n\n\n  \n    \n      Pulse 2 - Pulse 1 Difference Estimate with 90% T-based CI\n    \n    \n      3314 NHANES Participants ages 30-59 in nh432 data\n    \n  \n  \n    \n      estimate\n      low.90\n      hi.90\n      Paired t\n      p.value\n      method\n    \n  \n  \n    0.653\n0.555\n0.751\n10.980\n0.000\nOne Sample t-test\n  \n  \n  \n\n\n\n\n\n\n3.5.3 Using linear regression to obtain a 90% CI for the mean pulse difference\nA linear regression model predicting the paired differences with an intercept alone produces the same result as the paired t test.\n\nlm1 <- lm(PULSEDIFF ~ 1, data = dat1)\n\nsummary(lm1)\n\n\nCall:\nlm(formula = PULSEDIFF ~ 1, data = dat1)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-22.6533  -1.6533   0.3467   1.3467  25.3467 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)   0.6533     0.0595   10.98   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.425 on 3313 degrees of freedom\n\ntidy(lm1, conf.int = TRUE, conf = 0.90) |>\n  mutate(method = c(\"Linear Model\")) |>\n  select(estimate, low.90 = conf.low, hi.90 = conf.high, \n         \"Paired t\" = statistic, p.value, method) |>\n  mutate(across(.cols = -method, num, digits = 3)) |>\n  gt() |>\n  tab_header(title = \"Pulse 2 - Pulse 1 Difference Estimate with 90% T-based CI\",\n             subtitle = glue(nrow(dat1), \" NHANES Participants ages 30-59 in nh432 data\"))\n\n\n\n\n\n  \n    \n      Pulse 2 - Pulse 1 Difference Estimate with 90% T-based CI\n    \n    \n      3314 NHANES Participants ages 30-59 in nh432 data\n    \n  \n  \n    \n      estimate\n      low.90\n      hi.90\n      Paired t\n      p.value\n      method\n    \n  \n  \n    0.653\n0.555\n0.751\n10.980\n0.000\nLinear Model\n  \n  \n  \n\n\n\n\n\n\n3.5.4 Using the bootstrap to obtain a 90% CI for the mean pulse difference\nThis is a better choice than the t test if the distribution of the paired differences veer far away from a Normal distribution, but you are still interested in making inferences about the population mean. This is a different approach to obtaining a bootstrap than I have used in the past, but I prefer it because it works well with the tidy() function in the broom package.\n\nset.seed(4321)\nbs1 <- boot.t.test(dat1$PULSEDIFF, conf.level = 0.90, \n                           boot = TRUE, R = 999)\n\nbs1\n\n\n    Bootstrap One Sample t-test\n\ndata:  dat1$PULSEDIFF\nbootstrap p-value < 2.2e-16 \nbootstrap mean of x (SE) = 0.6554904 (0.05954107) \n90 percent bootstrap percentile confidence interval:\n 0.5554617 0.7526554\n\nResults without bootstrap:\nt = 10.98, df = 3313, p-value < 2.2e-16\nalternative hypothesis: true mean is not equal to 0\n90 percent confidence interval:\n 0.5553989 0.7511792\nsample estimates:\nmean of x \n0.6532891 \n\ntidy(bs1, conf.int = TRUE, conf = 0.90) |>\n  select(estimate, low.90 = conf.low, hi.90 = conf.high, \n         \"Bootstrap t\" = statistic, p.value, method) |>\n  mutate(across(.cols = -method, num, digits = 3)) |>\n  gt() |>\n  tab_header(title = \"Pulse 2 - Pulse 1 Difference Estimate with 90% Bootstrap CI\",\n             subtitle = glue(nrow(dat1), \" NHANES Participants ages 30-59 in nh432 data\"))\n\n\n\n\n\n  \n    \n      Pulse 2 - Pulse 1 Difference Estimate with 90% Bootstrap CI\n    \n    \n      3314 NHANES Participants ages 30-59 in nh432 data\n    \n  \n  \n    \n      estimate\n      low.90\n      hi.90\n      Bootstrap t\n      p.value\n      method\n    \n  \n  \n    0.653\n0.555\n0.751\n10.980\n0.000\nBootstrap One Sample t-test\n  \n  \n  \n\n\n\n\nGiven our large sample size, it is perhaps not overly surprising that even a small difference in mean pulse rate (0.653 beats per minute) turns out to have a 90% confidence interval well above the value (0) that would occur if there were no difference at all between the groups.\n\n\n3.5.5 Wilcoxon signed rank approach to comparing pulse rates\nWe can obtain a 90% confidence interval for the pseudo-median of our paired differences in pulse rate with the Wilcoxon signed rank approach.\n\nwt1 <- wilcox.test(dat1$PULSEDIFF, conf.int = TRUE, conf.level = 0.90)\n\nwt1\n\n\n    Wilcoxon signed rank test with continuity correction\n\ndata:  dat1$PULSEDIFF\nV = 2449203, p-value < 2.2e-16\nalternative hypothesis: true location is not equal to 0\n90 percent confidence interval:\n 0.5000466 0.9999290\nsample estimates:\n(pseudo)median \n     0.9999809 \n\n\nBut this is of limited value, even though it doesn’t assume Normality of the distribution of paired differences, because the summary statistic is a pseudo-median4, which isn’t straightforward to interpret, unless the true distribution of the paired differences is symmetric, in which case the pseudo-median and the median have the same value.\nLet’s consider another example using two paired samples to compare means, this time with a somewhat smaller sample size."
  },
  {
    "objectID": "431review1.html#comparing-weight-to-estwt",
    "href": "431review1.html#comparing-weight-to-estwt",
    "title": "3  431 Review: Comparing Means",
    "section": "3.6 Comparing WEIGHT to ESTWT",
    "text": "3.6 Comparing WEIGHT to ESTWT\nWe have two assessments of each participant’s weight in nh432: their WEIGHT (as measured using a scale, in kilograms) and their ESTWT (self-reported weight via questionnaire, in pounds.) First, let’s create a data set containing those values, and converting pounds to kilograms for the ESTWT results so that we can compare the two assessments fairly. To shrink the sample size a bit, let’s only look at people whose age is 43, and who describe their overall health as either Good or Fair.\n\ndat2 <- nh432 |> select(SEQN, AGE, SROH, WEIGHT, ESTWT) |>\n  filter(AGE == 43, SROH %in% c(\"Good\", \"Fair\")) |>\n  drop_na() |>\n  mutate(ESTWTKG = ESTWT*0.45359,\n         WTDIFF = WEIGHT - ESTWTKG)\n\nglimpse(dat2)\n\nRows: 70\nColumns: 7\n$ SEQN    <chr> \"109342\", \"109602\", \"109805\", \"110286\", \"110645\", \"111149\", \"1…\n$ AGE     <dbl> 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43…\n$ SROH    <fct> Good, Good, Good, Fair, Good, Good, Good, Good, Good, Fair, Go…\n$ WEIGHT  <dbl> 92.1, 76.5, 133.0, 86.8, 119.3, 74.1, 75.8, 106.8, 102.1, 77.0…\n$ ESTWT   <dbl> 200, 167, 260, 198, 230, 145, 167, 240, 223, 172, 150, 265, 22…\n$ ESTWTKG <dbl> 90.71800, 75.74953, 117.93340, 89.81082, 104.32570, 65.77055, …\n$ WTDIFF  <dbl> 1.38200, 0.75047, 15.06660, -3.01082, 14.97430, 8.32945, 0.050…\n\ndf_stats(~ WEIGHT + ESTWTKG + WTDIFF, data = dat2) |> \n  mutate(across(.cols = -c(response, n, missing), num, digits = 3)) |>\n  gt()\n\n\n\n\n\n  \n  \n    \n      response\n      min\n      Q1\n      median\n      Q3\n      max\n      mean\n      sd\n      n\n      missing\n    \n  \n  \n    WEIGHT\n36.900\n74.525\n89.400\n106.175\n204.600\n93.040\n29.211\n70\n0\n    ESTWTKG\n45.359\n74.842\n89.130\n103.759\n204.115\n92.532\n27.869\n70\n0\n    WTDIFF\n-9.871\n-2.256\n-0.028\n1.923\n15.067\n0.508\n4.671\n70\n0\n  \n  \n  \n\n\n\n\n\n3.6.1 Plotting The Paired Difference in Weight\n\np1 <- ggplot(dat2, aes(sample = WTDIFF)) +\n  geom_qq(col = \"seagreen\") + geom_qq_line(col = \"deeppink\") + \n  theme(aspect.ratio = 1) + \n  labs(title = \"Normal Q-Q plot\")\n\np2 <- ggplot(dat2, aes(x = WTDIFF)) +\n  geom_histogram(aes(y = after_stat(density)), \n                   bins = 15, fill = \"seagreen\", col = \"white\") +\n  stat_function(fun = dnorm, \n                args = list(mean = mean(dat2$WTDIFF), \n                            sd = sd(dat2$WTDIFF)),\n                col = \"deeppink\", linewidth = 1.5) +\n  labs(title = \"Histogram and Normal Density\")\n\np3 <- ggplot(dat2, aes(x = WTDIFF, y = \"\")) +\n  geom_boxplot(fill = \"seagreen\", outlier.color = \"seagreen\") + \n  stat_summary(fun = \"mean\", geom = \"point\", \n               shape = 23, size = 3, fill = \"white\") +\n  labs(title = \"Boxplot with mean\", y = \"\")\n\np1 + (p2 / p3 + plot_layout(heights = c(4,1))) +\n  plot_annotation(title = \"Measured - Self-reported Weight (in kilograms)\",\n                  subtitle = glue(nrow(dat2), \" participants in Good or Fair Health aged 43 in nh432\"))\n\n\n\n\nAs we saw with the differences in pulse rate, the differences in weight for this sample appear to come from a distribution that might be symmetric, but that still has several outlier candidates, especially on the high end of the distribution. We may want to consider whether the assumption of a t-based confidence interval is reasonable here, and whether we might be better off using a bootstrap approach.\n\n\n3.6.2 t.test 90% CI for the mean weight difference\n\ntt2 <- t.test(dat2$WTDIFF, conf.level = 0.90)\n\ntidy(tt2, conf.int = TRUE, conf = 0.90) |>\n  select(estimate, low.90 = conf.low, hi.90 = conf.high, \n         \"Paired t\" = statistic, p.value, method) |>\n  mutate(across(.cols = -method, num, digits = 3)) |>\n  gt() |>\n  tab_header(title = \"Measured - Self-reported Weight with 90% T-based CI\",\n             subtitle = glue(nrow(dat2), \" NHANES Participants aged 43 in Good or Fair Health in nh432 data\"))\n\n\n\n\n\n  \n    \n      Measured - Self-reported Weight with 90% T-based CI\n    \n    \n      70 NHANES Participants aged 43 in Good or Fair Health in nh432 data\n    \n  \n  \n    \n      estimate\n      low.90\n      hi.90\n      Paired t\n      p.value\n      method\n    \n  \n  \n    0.508\n-0.423\n1.438\n0.909\n0.366\nOne Sample t-test\n  \n  \n  \n\n\n\n\n\n\n3.6.3 Linear Regression: 90% CI for mean weight difference\n\nlm2 <- lm(WTDIFF ~ 1, data = dat2)\n\ntidy(lm2, conf.int = TRUE, conf = 0.90) |>\n  mutate(method = c(\"Linear Model\")) |>\n  select(estimate, low.90 = conf.low, hi.90 = conf.high, \n         \"Paired t\" = statistic, p.value, method) |>\n  mutate(across(.cols = -method, num, digits = 3)) |>\n  gt() |>\n  tab_header(title = \"Measured - Self-reported Weight with 90% T-based CI\",\n             subtitle = glue(nrow(dat2), \" NHANES Participants aged 43 in Good or Fair Health in nh432 data\"))\n\n\n\n\n\n  \n    \n      Measured - Self-reported Weight with 90% T-based CI\n    \n    \n      70 NHANES Participants aged 43 in Good or Fair Health in nh432 data\n    \n  \n  \n    \n      estimate\n      low.90\n      hi.90\n      Paired t\n      p.value\n      method\n    \n  \n  \n    0.508\n-0.423\n1.438\n0.909\n0.366\nLinear Model\n  \n  \n  \n\n\n\n\n\n\n3.6.4 Bootstrap 90% CI for the mean weight difference\n\nset.seed(4322)\nbs2 <- boot.t.test(dat2$WTDIFF, conf.level = 0.90, \n                           boot = TRUE, R = 999)\n\ntidy(bs2, conf.int = TRUE, conf = 0.90) |>\n  select(estimate, low.90 = conf.low, hi.90 = conf.high, \n         \"Bootstrap t\" = statistic, p.value, method) |>\n  mutate(across(.cols = -method, num, digits = 3)) |>\n  gt() |>\n  tab_header(title = \"Measured - Self-reported Weight\",\n             subtitle = \"with 90% Bootstrap CI\") |>\n  tab_footnote(footnote = glue(nrow(dat1), \" NHANES Participants age 43 in Good or Fair Health in nh432 data\"))\n\n\n\n\n\n  \n    \n      Measured - Self-reported Weight\n    \n    \n      with 90% Bootstrap CI\n    \n  \n  \n    \n      estimate\n      low.90\n      hi.90\n      Bootstrap t\n      p.value\n      method\n    \n  \n  \n    0.508\n-0.423\n1.438\n0.909\n0.366\nBootstrap One Sample t-test\n  \n  \n  \n    \n       3314 NHANES Participants age 43 in Good or Fair Health in nh432 data\n    \n  \n\n\n\n\nIn light of the clear issue with outliers in the plots of the weight differences, I think I would choose the bootstrap confidence interval, which clearly includes both negative and positive values as plausible estimates of the population mean difference.\n\n\n3.6.5 Wilcoxon signed rank approach to comparing weight estimates\nWe can obtain a 90% confidence interval for the pseudo-median of our paired differences in weight with the Wilcoxon signed rank approach.\n\nwt2 <- wilcox.test(dat2$WTDIFF, conf.int = TRUE, conf.level = 0.90)\n\nwt2\n\n\n    Wilcoxon signed rank test with continuity correction\n\ndata:  dat2$WTDIFF\nV = 1262, p-value = 0.9115\nalternative hypothesis: true location is not equal to 0\n90 percent confidence interval:\n -0.6347756  0.7941699\nsample estimates:\n(pseudo)median \n    0.06155979 \n\n\nBut this is of limited value, even though it doesn’t assume Normality of the distribution of paired differences, because the summary statistic is a pseudo-median5, which isn’t straightforward to interpret, unless the true distribution of the paired differences is symmetric, in which case the pseudo-median and the median have the same value."
  },
  {
    "objectID": "431review1.html#comparing-two-means-using-independent-samples",
    "href": "431review1.html#comparing-two-means-using-independent-samples",
    "title": "3  431 Review: Comparing Means",
    "section": "3.7 Comparing Two Means using Independent Samples",
    "text": "3.7 Comparing Two Means using Independent Samples\nNow, we’ll demonstrate some approaches to comparing two means coming from independent samples. This will include:\n\na pooled t test (t test assuming equal population variances), which we can obtain from a linear model, or from t.test()\na Welch t test (t test not assuming equal population variances), from t.test()\n\nEach of these t tests requires the distribution of each of our two independent samples to be well-approximated by a Normal model. As an alternative without that requirement, we’ll focus on a bootstrap comparison (not assuming equal variances or Normality) from boot.t.test() (again from the MKinfer package.) Once more, it is also possible to generate rank-based inference, such as using the Wilcoxon-Mann-Whitney rank sum approach, but again this does not provide us with estimates of either the difference in population means or medians, which limits its utility."
  },
  {
    "objectID": "431review1.html#comparing-white-blood-cell-count-by-hospitalization-status",
    "href": "431review1.html#comparing-white-blood-cell-count-by-hospitalization-status",
    "title": "3  431 Review: Comparing Means",
    "section": "3.8 Comparing White Blood Cell Count by Hospitalization Status",
    "text": "3.8 Comparing White Blood Cell Count by Hospitalization Status\nNow, we’ll use independent samples to compare subjects who were hospitalized in the past year to those who were not, in terms of their white blood cell count. The normal range of WBCs in the blood is 4.5 to 11 on the scale (1000 cells per microliter) our data is available.\n\n3.8.1 Exploring the Data\n\ndat3 <- nh432 |>\n  select(SEQN, HOSPITAL, WBC) |>\n  drop_na()\n\nggplot(dat3, aes(x = factor(HOSPITAL), y = WBC)) +\n  geom_violin(aes(fill = factor(HOSPITAL))) +\n  geom_boxplot(width = 0.3, notch = TRUE) +\n  stat_summary(aes(fill = factor(HOSPITAL)), fun = \"mean\", geom = \"point\", \n               shape = 23, size = 3) +\n  guides(fill = \"none\", col = \"none\") +\n  scale_fill_viridis_d(option = \"cividis\", alpha = 0.3) +\n  coord_flip() +\n  labs(x = \"Hospitalized in Past Year? (0 = No, 1 = Yes)\",\n       y = \"White blood cell count (1000 cells / uL)\",\n       title = \"White Blood Cell Count by Hospitalization Status\",\n       subtitle = glue(nrow(dat3), \" NHANES participants in nh432\"))\n\n\n\n\nEach of these distributions shows some signs of right skew, or at least more than a few outlier candidates on the upper end of the white blood cell count’s distribution, according to the boxplot. A pair of Normal Q-Q plots should help clarify issues for us.\n\nggplot(dat3, aes(sample = WBC)) +\n  geom_qq(aes(col = factor(HOSPITAL))) + geom_qq_line(col = \"red\") + \n  facet_wrap(~ HOSPITAL, labeller = \"label_both\") +\n  guides(col = \"none\") +\n  scale_color_viridis_d(option = \"cividis\", end = 0.5) +\n  labs(x = \"Expectation Under Standard Normal Distribution\",\n       y = \"Observed White Blood Cell Count (1000 cells/uL)\",\n       title = \"Normal Q-Q plots of White Blood Cell Count\",\n       subtitle = \"By Hospitalization Status in the Past Year\",\n       caption = glue(nrow(dat3), \" NHANES participants in nh432\"))\n\n\n\n\nIt seems unreasonable to assume that each of these samples comes from a distribution that is well-approximated by the Normal. There’s just too much skew here. Here are some key numerical summaries of the data in each sample.\n\nfavstats(WBC ~ HOSPITAL, data = dat3) |>\n  mutate(across(.cols = c(mean, sd), num, digits = 3)) |>\n  gt()\n\n\n\n\n\n  \n  \n    \n      HOSPITAL\n      min\n      Q1\n      median\n      Q3\n      max\n      mean\n      sd\n      n\n      missing\n    \n  \n  \n    0\n2.3\n5.7\n7.0\n8.4\n22.8\n7.246\n2.170\n3430\n0\n    1\n2.5\n5.6\n6.9\n8.7\n17.1\n7.332\n2.392\n325\n0\n  \n  \n  \n\n\n\n\n\n\n3.8.2 Pooled t test (assumes equal variances) via linear model\nThe pooled t test for comparison of two population means using independent samples assumes:\n\nthat the WBC (outcome) in each of the two HOSPITAL (exposure) groups follows a Normal distribution, and\nthat the population variances are equal in the two groups\n\nThe “equal population variances” assumption can be relaxed and a pooled t test used if we have a balanced design, with the same number of subjects in each exposure group.\nIn our setting, we shouldn’t be particularly comfortable with the assumption of Normality, as mentioned above. Were we able to get past that, though, we can see that the two distributions have fairly similar sample variances (remember this is just the square of the standard deviation.) The sample sizes are wildly different, with many more non-hospitalized subjects than hospitalized ones.\nFor completeness, though, we’ll start by running the pooled t test.\n\nlm3 <- lm(WBC ~ HOSPITAL, data = dat3)\n\nsummary(lm3)\n\n\nCall:\nlm(formula = WBC ~ HOSPITAL, data = dat3)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-4.9463 -1.5463 -0.3463  1.1537 15.5537 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  7.24633    0.03740 193.760   <2e-16 ***\nHOSPITAL     0.08567    0.12712   0.674      0.5    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.19 on 3753 degrees of freedom\nMultiple R-squared:  0.000121,  Adjusted R-squared:  -0.0001454 \nF-statistic: 0.4542 on 1 and 3753 DF,  p-value: 0.5004\n\nconfint(lm3, level = 0.90)\n\n                   5 %      95 %\n(Intercept)  7.1847964 7.3078567\nHOSPITAL    -0.1234734 0.2948204\n\ntidy(lm3, conf.int = TRUE, conf.level = 0.90) |>\n  filter(term == \"HOSPITAL\") |>\n  mutate(method = \"Pooled t\") |>\n  select(estimate, low.90 = conf.low, hi.90 = conf.high, \n         \"Pooled t\" = statistic, p.value, method) |>\n  mutate(across(.cols = -method, num, digits = 3)) |>\n  gt() |>\n  tab_header(title = \"White Blood Cell Count (Hospitalized - Non-Hospitalized)\",\n             subtitle = \"with 90% Pooled t-based CI via Linear Model\") |>\n  tab_footnote(footnote = glue(nrow(dat3), \" NHANES Participants ages 30-59 in nh432 data\"))\n\n\n\n\n\n  \n    \n      White Blood Cell Count (Hospitalized - Non-Hospitalized)\n    \n    \n      with 90% Pooled t-based CI via Linear Model\n    \n  \n  \n    \n      estimate\n      low.90\n      hi.90\n      Pooled t\n      p.value\n      method\n    \n  \n  \n    0.086\n-0.123\n0.295\n0.674\n0.500\nPooled t\n  \n  \n  \n    \n       3755 NHANES Participants ages 30-59 in nh432 data\n    \n  \n\n\n\n\n\n\n3.8.3 Pooled t test (assumes equal variances) via t.test\nNote that this approach estimates the difference with Not Hospitalized - Hospitalized, as opposed to the approach used in the linear model. Be careful to check the sample estimates provided in your output against the original summary of the sample data to avoid making a mistake.\n\ntt3p <- t.test(WBC ~ HOSPITAL, data = dat3, var.equal = TRUE, conf.level = 0.90)\n\ntt3p\n\n\n    Two Sample t-test\n\ndata:  WBC by HOSPITAL\nt = -0.67395, df = 3753, p-value = 0.5004\nalternative hypothesis: true difference in means between group 0 and group 1 is not equal to 0\n90 percent confidence interval:\n -0.2948204  0.1234734\nsample estimates:\nmean in group 0 mean in group 1 \n       7.246327        7.332000 \n\ntidy(tt3p, conf.int = TRUE, conf.level = 0.90) |>\n  select(estimate, low.90 = conf.low, hi.90 = conf.high, \n         \"Pooled t\" = statistic, p.value, method) |>\n  mutate(across(.cols = -method, num, digits = 3)) |>\n  gt() |>\n  tab_header(title = \"White Blood Cell Count (Non-Hospitalized minus Hospitalized)\",\n             subtitle = \"with 90% Pooled t-based Confidence Interval\") |>\n  tab_footnote(footnote = glue(nrow(dat3), \" NHANES Participants ages 30-59 in nh432 data\"))\n\n\n\n\n\n  \n    \n      White Blood Cell Count (Non-Hospitalized minus Hospitalized)\n    \n    \n      with 90% Pooled t-based Confidence Interval\n    \n  \n  \n    \n      estimate\n      low.90\n      hi.90\n      Pooled t\n      p.value\n      method\n    \n  \n  \n    -0.086\n-0.295\n0.123\n-0.674\n0.500\nTwo Sample t-test\n  \n  \n  \n    \n       3755 NHANES Participants ages 30-59 in nh432 data\n    \n  \n\n\n\n\nAgain, note that the t.test() approach estimates Non-Hospitalized - Hospitalized (so that the sample mean is negative.)\n\n\n3.8.4 Welch t test (doesn’t assume equal variance) via t.test\nThe Welch t test (which is actually the default t.test in R) assumes that the two groups each follow a Normal distribution, but does not require that those distributions have the same population variance.\n\ntt3w <- t.test(WBC ~ HOSPITAL, data = dat3, conf.level = 0.90)\n\ntt3w\n\n\n    Welch Two Sample t-test\n\ndata:  WBC by HOSPITAL\nt = -0.6218, df = 376.28, p-value = 0.5345\nalternative hypothesis: true difference in means between group 0 and group 1 is not equal to 0\n90 percent confidence interval:\n -0.3128672  0.1415202\nsample estimates:\nmean in group 0 mean in group 1 \n       7.246327        7.332000 \n\ntidy(tt3w, conf.int = TRUE, conf.level = 0.90) |>\n  select(estimate, low.90 = conf.low, hi.90 = conf.high, \n         \"Welch t\" = statistic, p.value, method) |>\n  mutate(across(.cols = -method, num, digits = 3)) |>\n  gt() |>\n  tab_header(title = \"White Blood Cell Count (Non-Hospitalized minus Hospitalized)\",\n             subtitle = \"with 90% Welch t-based Confidence Interval\") |>\n  tab_footnote(footnote = glue(nrow(dat3), \" NHANES Participants ages 30-59 in nh432 data\"))\n\n\n\n\n\n  \n    \n      White Blood Cell Count (Non-Hospitalized minus Hospitalized)\n    \n    \n      with 90% Welch t-based Confidence Interval\n    \n  \n  \n    \n      estimate\n      low.90\n      hi.90\n      Welch t\n      p.value\n      method\n    \n  \n  \n    -0.086\n-0.313\n0.142\n-0.622\n0.534\nWelch Two Sample t-test\n  \n  \n  \n    \n       3755 NHANES Participants ages 30-59 in nh432 data\n    \n  \n\n\n\n\n\n\n3.8.5 Bootstrap comparison of WBC by HOSPITAL\nThe bootstrap approach is appealing in part because it neither assumes Normality or equal population variances.\n\nset.seed(4323)\nbs3 <- boot.t.test(WBC ~ HOSPITAL, data = dat3, \n                   R = 999, conf.level = 0.90)\n\nbs3\n\n\n    Bootstrap Welch Two Sample t-test\n\ndata:  WBC by HOSPITAL\nbootstrap p-value = 0.5325 \nbootstrap difference of means (SE) = -0.08734076 (0.1376246) \n90 percent bootstrap percentile confidence interval:\n -0.3137606  0.1414317\n\nResults without bootstrap:\nt = -0.6218, df = 376.28, p-value = 0.5345\nalternative hypothesis: true difference in means is not equal to 0\n90 percent confidence interval:\n -0.3128672  0.1415202\nsample estimates:\nmean in group 0 mean in group 1 \n       7.246327        7.332000 \n\ntidy(bs3, conf.int = TRUE, conf.level = 0.90) |>\n  mutate(diff = estimate1 - estimate2) |>\n  select(est1 = estimate1, est2 = estimate2, diff, \n         low.90 = conf.low, hi.90 = conf.high, \n         p.value, method) |>\n  mutate(across(.cols = -method, num, digits = 3)) |>\n  gt() |>\n  tab_header(title = \"White Blood Cell Count (Non-Hospitalized minus Hospitalized)\",\n             subtitle = \"with 90% Bootstrap Confidence Interval\") |>\n  tab_footnote(footnote = glue(nrow(dat3), \" NHANES Participants ages 30-59 in nh432 data\"))\n\n\n\n\n\n  \n    \n      White Blood Cell Count (Non-Hospitalized minus Hospitalized)\n    \n    \n      with 90% Bootstrap Confidence Interval\n    \n  \n  \n    \n      est1\n      est2\n      diff\n      low.90\n      hi.90\n      p.value\n      method\n    \n  \n  \n    7.246\n7.332\n-0.086\n-0.313\n0.142\n0.534\nBootstrap Welch Two Sample t-test\n  \n  \n  \n    \n       3755 NHANES Participants ages 30-59 in nh432 data\n    \n  \n\n\n\n\nIn any case, though, we come to the same basic conclusion - both positive and negative differences in WBC count are plausible.\nGiven the huge imbalance between the two groups in terms of sample size, and the apparent skew in the distribution of each sample, I would probably be most comfortable with the bootstrap approach here than the t-based intervals.\n\n\n3.8.6 Transforming the WBC Counts\nSince the White Blood Cell counts are far from Normally distributed, and in fact appear to be substantially skewed (asymmetric) we might want to consider a transformation of the data. The Box-Cox approach can be used to suggest potential transformations even in a simple case like this. We can use the boxCox() function from the car package, for example.\n\nm3 <- lm(WBC ~ HOSPITAL, data = dat3)\n\nboxCox(m3)\n\n\n\n\nThe estimated power (\\(\\lambda\\)) shown in the plot is close to 0. The ladder of power transformations looks like this:\n\n\n\n$\nTransformation\nFormula\n\n\n\n\n-2\ninverse square\n\\(1/y^2\\)\n\n\n-1\ninverse\n\\(1/y\\)\n\n\n-0.5\ninverse square root\n\\(1/\\sqrt{y}\\)\n\n\n0\nlogarithm\n\\(log y\\)\n\n\n0.5\nsquare root\n\\(\\sqrt{y}\\)\n\n\n1\nno transformation\ny\n\n\n2\nsquare\n\\(y^2\\)\n\n\n3\ncube\n\\(y^3\\)\n\n\n\nSo in this case, the Box-Cox approach is suggesting we try the logarithm (we use the natural logarithm, with base e, here) of WBC.\nLet’s redraw our Normal Q-Q plots with this transformation applied.\n\nggplot(dat3, aes(sample = log(WBC))) +\n  geom_qq(aes(col = factor(HOSPITAL))) + geom_qq_line(col = \"red\") + \n  facet_wrap(~ HOSPITAL, labeller = \"label_both\") +\n  guides(col = \"none\") +\n  scale_color_viridis_d(option = \"cividis\", end = 0.5) +\n  labs(x = \"Expectation Under Standard Normal Distribution\",\n       y = \"Log of White Blood Cell Count (1000 cells/uL)\",\n       title = \"Normal Q-Q plots of log(WBC)\",\n       subtitle = \"By Hospitalization Status in the Past Year\",\n       caption = glue(nrow(dat3), \" NHANES participants in nh432\"))\n\n\n\n\nThe assumption of Normality now looks much more plausible for each of our samples. So we might try building a 90% confidence interval for the mean of log(WBC), as follows:\n\nfavstats(log(WBC) ~ HOSPITAL, data = dat3) |>\n  mutate(across(.cols = -c(HOSPITAL, n, missing), num, digits = 3)) |>\n  gt() |>\n  tab_header(title = \"log(WBC) by Hospitalization Status\",\n             subtitle = \"NHANES participants in nh432\")\n\n\n\n\n\n  \n    \n      log(WBC) by Hospitalization Status\n    \n    \n      NHANES participants in nh432\n    \n  \n  \n    \n      HOSPITAL\n      min\n      Q1\n      median\n      Q3\n      max\n      mean\n      sd\n      n\n      missing\n    \n  \n  \n    0\n0.833\n1.740\n1.946\n2.128\n3.127\n1.938\n0.293\n3430\n0\n    1\n0.916\n1.723\n1.932\n2.163\n2.839\n1.941\n0.322\n325\n0\n  \n  \n  \n\n\n\n\nWe see that there’s essentially no difference at all in the means of the log(WBC) values across the two levels of hospitalization status.\n\ntt3log <- t.test(log(WBC) ~ HOSPITAL, data = dat3, var.equal = TRUE, conf.level = 0.90)\n\ntidy(tt3log, conf.int = TRUE, conf.level = 0.90) |>\n  select(estimate1, estimate2, estimate, conf.low, conf.high, p.value)\n\n# A tibble: 1 × 6\n  estimate1 estimate2 estimate conf.low conf.high p.value\n      <dbl>     <dbl>    <dbl>    <dbl>     <dbl>   <dbl>\n1      1.94      1.94 -0.00320  -0.0314    0.0250   0.852\n\n\nLet’s consider a second example for comparing means from independent samples."
  },
  {
    "objectID": "431review1.html#comparing-waist-circumference-by-sleep-trouble",
    "href": "431review1.html#comparing-waist-circumference-by-sleep-trouble",
    "title": "3  431 Review: Comparing Means",
    "section": "3.9 Comparing Waist Circumference by Sleep Trouble",
    "text": "3.9 Comparing Waist Circumference by Sleep Trouble\nNow, we’ll restrict ourselves to NHANES participants who rated their overall health as “Fair”, and we’ll compare the mean waist circumference (WAIST, in cm) of people in that group who responded Yes (vs. No) to the question of whether they had told a doctor that they had trouble sleeping (gathered in the SLPTROUB variable.)\n\n3.9.1 Summarizing the Data\n\ndat4 <- nh432 |>\n  select(SEQN, SROH, SLPTROUB, WAIST) |>\n  filter(SROH == \"Fair\") |>\n  drop_na()\n\nggplot(dat4, aes(x = factor(SLPTROUB), y = WAIST)) +\n  geom_violin(aes(fill = factor(SLPTROUB))) +\n  geom_boxplot(width = 0.3, notch = TRUE) +\n  stat_summary(fill = \"red\", fun = \"mean\", geom = \"point\", \n               shape = 23, size = 3) +\n  guides(fill = \"none\", col = \"none\") +\n  scale_fill_brewer(palette = \"Accent\") +\n  labs(x = \"Reported Sleep Trouble to a Doctor? (0 = No, 1 = Yes)\",\n       y = \"Waist circumference (cm)\",\n       title = \"Waist Circumference by Sleep Trouble\",\n       subtitle = glue(nrow(dat4), \" NHANES participants in Fair health in nh432\"))\n\n\n\n\n\nggplot(dat4, aes(sample = WAIST)) +\n  geom_qq(aes(col = factor(SLPTROUB))) + geom_qq_line(col = \"red\") + \n  facet_wrap(~ SLPTROUB, labeller = \"label_both\") +\n  guides(col = \"none\") +\n  scale_color_brewer(palette = \"Accent\") +\n  labs(x = \"Expectation Under Standard Normal Distribution\",\n       y = \"Observed Waist Circumference (in cm)\",\n       title = \"Normal Q-Q plots of Waist Circumference\",\n       subtitle = \"By Reported Sleep Trouble\",\n       caption = glue(nrow(dat4), \" NHANES participants in Fair health in nh432\"))\n\n\n\n\nHere’s a situation where we might be willing to consider a t test, since a Normal distribution is a much better fit for the data in each of our two samples. Let’s look at some brief numerical summaries, too.\n\nfavstats(WAIST ~ SLPTROUB, data = dat4) |>\n  mutate(across(.cols = c(mean, sd), num, digits = 2)) |>\n  gt()\n\n\n\n\n\n  \n  \n    \n      SLPTROUB\n      min\n      Q1\n      median\n      Q3\n      max\n      mean\n      sd\n      n\n      missing\n    \n  \n  \n    0\n65.8\n91.7\n102.1\n117.5\n178\n104.71\n18.35\n425\n0\n    1\n69.6\n97.4\n109.4\n124.0\n166\n110.69\n18.86\n309\n0\n  \n  \n  \n\n\n\n\n\n\n3.9.2 Pooled t test (assumes equal variances) via linear model\nHere’s the pooled t test via linear model.\n\nlm4 <- lm(WAIST ~ SLPTROUB, data = dat4)\n\nsummary(lm4)\n\n\nCall:\nlm(formula = WAIST ~ SLPTROUB, data = dat4)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-41.093 -13.293  -2.293  12.790  73.290 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 104.7099     0.9006  116.27  < 2e-16 ***\nSLPTROUB      5.9830     1.3881    4.31 1.85e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 18.57 on 732 degrees of freedom\nMultiple R-squared:  0.02475,   Adjusted R-squared:  0.02342 \nF-statistic: 18.58 on 1 and 732 DF,  p-value: 1.853e-05\n\nconfint(lm4, level = 0.90)\n\n                   5 %       95 %\n(Intercept) 103.226621 106.193144\nSLPTROUB      3.696944   8.269052\n\ntidy(lm4, conf.int = TRUE, conf.level = 0.90) |>\n  filter(term == \"SLPTROUB\") |>\n  mutate(method = \"Pooled t\") |>\n  select(estimate, low.90 = conf.low, hi.90 = conf.high, \n         \"Pooled t\" = statistic, p.value, method) |>\n  mutate(across(.cols = -method, num, digits = 2)) |>\n  gt() |>\n  tab_header(title = \"Waist Circumference by Sleep Trouble\",\n             subtitle = \"with 90% Pooled t-based CI via Linear Model\") |>\n  tab_footnote(footnote = glue(nrow(dat4), \" NHANES Participants in Fair Health in nh432 data\"))\n\n\n\n\n\n  \n    \n      Waist Circumference by Sleep Trouble\n    \n    \n      with 90% Pooled t-based CI via Linear Model\n    \n  \n  \n    \n      estimate\n      low.90\n      hi.90\n      Pooled t\n      p.value\n      method\n    \n  \n  \n    5.98\n3.70\n8.27\n4.31\n0.00\nPooled t\n  \n  \n  \n    \n       734 NHANES Participants in Fair Health in nh432 data\n    \n  \n\n\n\n\n\n\n3.9.3 Pooled t test (assumes equal variances) via t.test\n\ntt4p <- t.test(WAIST ~ SLPTROUB, data = dat4, var.equal = TRUE, conf.level = 0.90)\n\ntt4p\n\n\n    Two Sample t-test\n\ndata:  WAIST by SLPTROUB\nt = -4.3103, df = 732, p-value = 1.853e-05\nalternative hypothesis: true difference in means between group 0 and group 1 is not equal to 0\n90 percent confidence interval:\n -8.269052 -3.696944\nsample estimates:\nmean in group 0 mean in group 1 \n       104.7099        110.6929 \n\ntidy(tt4p, conf.int = TRUE, conf.level = 0.90) |>\n  select(estimate, low.90 = conf.low, hi.90 = conf.high, \n         \"Pooled t\" = statistic, p.value, method) |>\n  mutate(across(.cols = -method, num, digits = 2)) |>\n  gt() |>\n  tab_header(title = \"Waist Circumference by Sleep Trouble\",\n             subtitle = \"with 90% Pooled t-based Confidence Interval\") |>\n  tab_footnote(footnote = glue(nrow(dat4), \" NHANES Participants in Fair Health in nh432 data\"))\n\n\n\n\n\n  \n    \n      Waist Circumference by Sleep Trouble\n    \n    \n      with 90% Pooled t-based Confidence Interval\n    \n  \n  \n    \n      estimate\n      low.90\n      hi.90\n      Pooled t\n      p.value\n      method\n    \n  \n  \n    -5.98\n-8.27\n-3.70\n-4.31\n0.00\nTwo Sample t-test\n  \n  \n  \n    \n       734 NHANES Participants in Fair Health in nh432 data\n    \n  \n\n\n\n\n\n\n3.9.4 Welch t test (doesn’t assume equal variance) via t.test\n\ntt4w <- t.test(WAIST ~ SLPTROUB, data = dat4, conf.level = 0.90)\n\ntt4w\n\n\n    Welch Two Sample t-test\n\ndata:  WAIST by SLPTROUB\nt = -4.2919, df = 653.24, p-value = 2.04e-05\nalternative hypothesis: true difference in means between group 0 and group 1 is not equal to 0\n90 percent confidence interval:\n -8.279237 -3.686759\nsample estimates:\nmean in group 0 mean in group 1 \n       104.7099        110.6929 \n\ntidy(tt4w, conf.int = TRUE, conf.level = 0.90) |>\n  select(estimate, low.90 = conf.low, hi.90 = conf.high, \n         \"Welch t\" = statistic, p.value, method) |>\n  mutate(across(.cols = -method, num, digits = 2)) |>\n  gt() |>\n  tab_header(title = \"Waist Circumference by Sleep Trouble\",\n             subtitle = \"with 90% Welch t-based Confidence Interval\") |>\n  tab_footnote(footnote = glue(nrow(dat4), \" NHANES Participants in Fair Health in nh432 data\"))\n\n\n\n\n\n  \n    \n      Waist Circumference by Sleep Trouble\n    \n    \n      with 90% Welch t-based Confidence Interval\n    \n  \n  \n    \n      estimate\n      low.90\n      hi.90\n      Welch t\n      p.value\n      method\n    \n  \n  \n    -5.98\n-8.28\n-3.69\n-4.29\n0.00\nWelch Two Sample t-test\n  \n  \n  \n    \n       734 NHANES Participants in Fair Health in nh432 data\n    \n  \n\n\n\n\n\n\n3.9.5 Bootstrap comparison of WAIST by SLPTROUB\n\nset.seed(4324)\nbs4 <- boot.t.test(WAIST ~ SLPTROUB, data = dat4, \n                   R = 999, conf.level = 0.90)\n\nbs4\n\n\n    Bootstrap Welch Two Sample t-test\n\ndata:  WAIST by SLPTROUB\nbootstrap p-value < 2.2e-16 \nbootstrap difference of means (SE) = -5.991814 (1.389937) \n90 percent bootstrap percentile confidence interval:\n -8.207833 -3.747965\n\nResults without bootstrap:\nt = -4.2919, df = 653.24, p-value = 2.04e-05\nalternative hypothesis: true difference in means is not equal to 0\n90 percent confidence interval:\n -8.279237 -3.686759\nsample estimates:\nmean in group 0 mean in group 1 \n       104.7099        110.6929 \n\ntidy(bs4, conf.int = TRUE, conf.level = 0.90) |>\n  mutate(diff = estimate1 - estimate2) |>\n  select(est1 = estimate1, est2 = estimate2, diff, \n         low.90 = conf.low, hi.90 = conf.high, \n         p.value, method) |>\n  mutate(across(.cols = -method, num, digits = 2)) |>\n  gt() |>\n  tab_header(title = \"Waist Circumference by Sleep Trouble\",\n             subtitle = \"with 90% Bootstrap Confidence Interval\") |>\n  tab_footnote(footnote = glue(nrow(dat4), \" NHANES Participants in Fair Health in nh432 data\"))\n\n\n\n\n\n  \n    \n      Waist Circumference by Sleep Trouble\n    \n    \n      with 90% Bootstrap Confidence Interval\n    \n  \n  \n    \n      est1\n      est2\n      diff\n      low.90\n      hi.90\n      p.value\n      method\n    \n  \n  \n    104.71\n110.69\n-5.98\n-8.28\n-3.69\n0.00\nBootstrap Welch Two Sample t-test\n  \n  \n  \n    \n       734 NHANES Participants in Fair Health in nh432 data\n    \n  \n\n\n\n\n\n\n3.9.6 Wilcoxon-Mann-Whitney Rank Sum Approach\nThe Wilcoxon-Mann-Whitney rank sum approach also allows us (like the bootstrap) to avoid the assumptions of Normality and equal population variances, but at the cost of no longer yielding direct inference about the population mean.\n\nwt4 <- wilcox.test(WAIST ~ SLPTROUB, data = dat4, \n            conf.int = TRUE, conf.level = 0.90, paired = FALSE)\nwt4\n\n\n    Wilcoxon rank sum test with continuity correction\n\ndata:  WAIST by SLPTROUB\nW = 53322, p-value = 1.355e-05\nalternative hypothesis: true location shift is not equal to 0\n90 percent confidence interval:\n -8.599997 -3.900026\nsample estimates:\ndifference in location \n             -6.299963 \n\n\nNote that the estimated “difference in location” here is not the difference in the medians across the two groups, but instead the median of the difference between a sample from the SLPTROUB = Yes group and a sample from the SLPTROUB = No group.\nJust to prove my point, here are the sample median WAIST results in the two SLPTROUB groups. You can see that the difference between these medians does not match the “difference in location” estimate from the Wilcoxon-Mann-Whitney rank sum output.\n\ndat4 |> group_by(SLPTROUB) |> summarise(median(WAIST))\n\n# A tibble: 2 × 2\n  SLPTROUB `median(WAIST)`\n     <dbl>           <dbl>\n1        0            102.\n2        1            109.\n\n\nIn conclusion, the confidence intervals (from any of these approaches) suggest that plausible means of waist circumference are around 3-8 centimeters larger in the “told Dr. about sleep problems” group, which I suppose isn’t especially surprising, at least in terms of its direction."
  },
  {
    "objectID": "431review1.html#comparing-3-means-using-independent-samples-systolic-bp-by-weight-goal",
    "href": "431review1.html#comparing-3-means-using-independent-samples-systolic-bp-by-weight-goal",
    "title": "3  431 Review: Comparing Means",
    "section": "3.10 Comparing 3 Means using Independent Samples: Systolic BP by Weight Goal",
    "text": "3.10 Comparing 3 Means using Independent Samples: Systolic BP by Weight Goal\nWe’ll compare systolic blood pressure means across the three samples defined by WTGOAL (goal is to weigh more, less or stay about the same), restricting to our participants of Hispanic or Latinx ethnicity in nh432.\n\ndat5 <- nh432 |>\n  select(SEQN, RACEETH, SBP, WTGOAL) |>\n  filter(RACEETH == \"Hispanic\") |>\n  drop_na()\n\n\n3.10.1 Summarizing SBP by WTGOAL\n\nggplot(dat5, aes(x = SBP, y = WTGOAL)) + \n  geom_violin(aes(fill = WTGOAL)) +\n  geom_boxplot(width = 0.3, notch = TRUE) + \n  guides(fill = \"none\") + \n  labs(title = \"Comparing Mean Systolic BP by Weight Goal\",\n       subtitle = glue(\"among \", nrow(dat5), \" Hispanic participants in nh432\"),\n    x = \"Systolic Blood Pressure (mm Hg)\", y = \"Weight Goal\")\n\n\n\n\n\nfavstats(SBP ~ WTGOAL, data = dat5) |> \n  as_tibble() |>\n  mutate(across(.cols = c(\"mean\", \"sd\"), num, digits = 2)) |>\n  gt()\n\n\n\n\n\n  \n  \n    \n      WTGOAL\n      min\n      Q1\n      median\n      Q3\n      max\n      mean\n      sd\n      n\n      missing\n    \n  \n  \n    More\n84\n104.5\n114.0\n123\n150\n114.08\n14.76\n36\n0\n    Same\n87\n107.0\n116.5\n128\n200\n119.26\n16.49\n188\n0\n    Less\n80\n109.0\n119.0\n131\n199\n120.70\n17.17\n564\n0\n  \n  \n  \n\n\n\n\nThe analysis of variance is our primary tool for comparing more than two means (this is the extension of the pooled t test, with similar assumptions.) So the assumptions we might want to think about here are:\n\nSBP in each Weight Goal group is assumed to follow a Normal distribution\nSBP in each Weight Goal group is assumed to have the same population variance\n\nThe ANOVA, however, is far more robust to minor violations of these assumptions than is the pooled t test. So we might go ahead and fit the ANOVA model anyway, despite the apparent right skew in the “Less” group.\n\n\n3.10.2 Fitting an ANOVA Model\n\nm5 <- lm(SBP ~ WTGOAL, data = dat5)\n\nanova(m5)\n\nAnalysis of Variance Table\n\nResponse: SBP\n           Df Sum Sq Mean Sq F value  Pr(>F)  \nWTGOAL      2   1639  819.45  2.8656 0.05754 .\nResiduals 785 224477  285.96                  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nA small p value (remember we are using 90% confidence in our 432 work) like this isn’t really very important - usually it simply steers us towards trying to identify confidence intervals for differences between pairs of SBP means defined by WTGOAL.\n\n3.10.2.1 ANOVA without assuming Equal Variances?\nR will also fit an ANOVA-style model and produce a p value without the assumption of equal population SBP variance across the three groups of WTGOAL.\n\noneway.test(SBP ~ WTGOAL, data = dat5)\n\n\n    One-way analysis of means (not assuming equal variances)\n\ndata:  SBP and WTGOAL\nF = 3.5255, num df = 2.000, denom df = 93.753, p-value = 0.0334\n\n\nI don’t use this approach much, as ANOVA is pretty robust to the assumption of equal variance. The huge differences in sample size in this study (many more participants are in the Less group than the More, for instance) are most of the cause of the difference we see here.\n\n\n3.10.2.2 Testing for Equal Population Variance?\nSome people like to perform tests for equal population variance to help choose between ANOVA and the oneway.test() approach, but I do not. If I’m happy with the assumption of Normality, I virtually always just use ANOVA. There are many such tests of “equal variance”, including:\n\nBartlett’s test\nLevene’s test (which in R comes from the car package)\nFligner-Killeen test\n\nBartlett’s test is the least reliable of these when the data in at least one sample appear to be poorly described by the Normal distribution. Either Levene or Fligner-Killeen is a better choice in that setting, but again, I don’t use any of these in my work.\n\nbartlett.test(SBP ~ WTGOAL, data = dat5)\n\n\n    Bartlett test of homogeneity of variances\n\ndata:  SBP by WTGOAL\nBartlett's K-squared = 1.6722, df = 2, p-value = 0.4334\n\n\n\nleveneTest(SBP ~ WTGOAL, data = dat5)\n\nLevene's Test for Homogeneity of Variance (center = median)\n       Df F value Pr(>F)\ngroup   2  0.6557 0.5193\n      785               \n\n\n\nfligner.test(SBP ~ WTGOAL, data = dat5)\n\n\n    Fligner-Killeen test of homogeneity of variances\n\ndata:  SBP by WTGOAL\nFligner-Killeen:med chi-squared = 1.26, df = 2, p-value = 0.5326\n\n\n\n\n3.10.2.3 Is there a bootstrap one-way ANOVA approach?\nIf all you are looking for is a p value for the ANOVA model, then yes, there is a bootstrap approach available to perform one-way ANOVA testing. But I don’t actually use it, again usually preferring the usual ANOVA if the data seem reasonably likely to have been drawn from a Normal distribution, and the Kruskal-Wallis rank-based test otherwise. If you are willing to install the lmboot package, and use its ANOVA.boot() function, you can do so, like this.\n\nbs5 <- ANOVA.boot(SBP ~ WTGOAL, B = 1000, seed = 4325, data = dat5)\nbs5$`p-value`\n\n[1] 0.052\n\n\nIn this case, it doesn’t seem that we have a wildly different result than we got from the original ANOVA. That is often the case, and I have never actually used ANOVA.boot() in practical work.\n\n\n\n3.10.3 Tukey HSD Pairwise Comparisons\nWhen pairwise comparisons are pre-planned, especially when the design is close to balanced, my favorite choice for generating adjusted inferences about the means is Tukey’s Honestly Significant Differences (HSD) approach.\nHere, we generate confidence intervals for the pairwise differences in the SBP means by WTGOAL group with a 90% family-wise confidence level.\n\nth5 <- TukeyHSD(aov(SBP ~ WTGOAL, data = dat5), \n                ordered = TRUE, conf.level = 0.90)\n\ntidy(th5) |> \n  select(-c(term, null.value)) |>\n  mutate(across(.cols = -contrast, num, digits = 3)) |>\n  arrange(desc(estimate)) |>\n  gt() |>\n  tab_header(title = \"Comparing Mean Systolic BP across pairs of WTGOAL groups\",\n             subtitle = \"90% Tukey HSD Confidence Intervals\") |>\n  tab_footnote(footnote = glue(nrow(dat5), \" Hispanic participants in nh432\"))\n\n\n\n\n\n  \n    \n      Comparing Mean Systolic BP across pairs of WTGOAL groups\n    \n    \n      90% Tukey HSD Confidence Intervals\n    \n  \n  \n    \n      contrast\n      estimate\n      conf.low\n      conf.high\n      adj.p.value\n    \n  \n  \n    Less-More\n6.617\n0.642\n12.592\n0.060\n    Same-More\n5.172\n-1.151\n11.495\n0.213\n    Less-Same\n1.445\n-1.482\n4.372\n0.568\n  \n  \n  \n    \n       788 Hispanic participants in nh432\n    \n  \n\n\n\ntidy(th5) |>\n  mutate(contrast = fct_reorder(contrast, estimate, .desc = TRUE)) %>%\n  ggplot(., aes(x = contrast, y = estimate, ymin = conf.low, ymax = conf.high)) +\n  geom_crossbar() +\n  geom_hline(yintercept = 0, col = \"red\", linetype = \"dashed\") +\n  geom_label(aes(label = round_half_up(estimate, 2))) +\n  labs(title = \"Comparing Mean Systolic BP across pairs of WTGOAL groups\",\n       subtitle = \"90% Tukey HSD Confidence intervals\",\n       caption = glue(nrow(dat5), \" Hispanic participants in nh432\"),\n       x = \"Pairwise Difference between WTGOAL groups\",\n       y = \"Difference in Systolic Blood Pressure (mm Hg)\")\n\n\n\n\nThe main problems here are that:\n\nthe sample sizes in the various levels of WTGOAL are very different from one another, and\nthe SBP data are not especially well-described by a Normal distribution, at least in the “Less” group.\n\n\n\n3.10.4 Holm pairwise comparisons of means\nAnother approach to developing pairwise inferences would be to use either Bonferroni or (my preference) Holm-adjusted p values for the relevant t tests. First, we’ll run the appropriate Holm comparison of means assuming equal population variances of SBP across all three WTGOAL groups.\n\nht5 <- pairwise.t.test(dat5$SBP, dat5$WTGOAL, pool.sd = TRUE, p.adjust.method = \"holm\")\ntidy(ht5) |>\n  gt()\n\n\n\n\n\n  \n  \n    \n      group1\n      group2\n      p.value\n    \n  \n  \n    Same\nMore\n0.18625400\n    Less\nMore\n0.06929224\n    Less\nSame\n0.31056211\n  \n  \n  \n\n\n\n\nThe results are merely p-values, and not confidence intervals. There’s nothing being estimated here of interest. We can also perform these Holm comparisons without assuming equal population variances, as shown below.\n\nht5un <- pairwise.t.test(dat5$SBP, dat5$WTGOAL, pool.sd = FALSE, \n                       p.adjust.method = \"holm\")\ntidy(ht5un) |>\n  gt()\n\n\n\n\n\n  \n  \n    \n      group1\n      group2\n      p.value\n    \n  \n  \n    Same\nMore\n0.12866665\n    Less\nMore\n0.04046259\n    Less\nSame\n0.30396220\n  \n  \n  \n\n\n\n\nAgain, the problem with this approach is that it’s only producing a p value, which tempts us into talking about useless things like “statistical significance.” This is part of the reason I prefer Tukey HSD approaches when appropriate."
  },
  {
    "objectID": "431review1.html#comparing-4-means-using-independent-samples-weight-by-food-security",
    "href": "431review1.html#comparing-4-means-using-independent-samples-weight-by-food-security",
    "title": "3  431 Review: Comparing Means",
    "section": "3.11 Comparing 4 Means using Independent Samples: Weight by Food Security",
    "text": "3.11 Comparing 4 Means using Independent Samples: Weight by Food Security\n\ndat6 <- nh432 |>\n  select(SEQN, WEIGHT, FOODSEC) |>\n  drop_na()\n\n\n3.11.1 Summarizing the Data\n\nggplot(dat6, aes(x = FOODSEC, y = WEIGHT)) + \n  geom_violin(aes(fill = FOODSEC)) +\n  geom_boxplot(width = 0.3, notch = TRUE) + \n  guides(fill = \"none\") + \n  scale_fill_viridis_d(option = \"rocket\") +\n  labs(title = \"Comparing Mean Weight by Food Security\",\n       subtitle = glue(\"among \", nrow(dat6), \" participants in nh432\"),\n    x = \"Food Security Category\", y = \"Weight (kg)\")\n\n\n\n\n\nfavstats(WEIGHT ~ FOODSEC, data = dat6) |> \n  as_tibble() |>\n  mutate(across(.cols = -c(\"FOODSEC\", \"n\", \"missing\"), num, digits = 1)) |>\n  gt()\n\n\n\n\n\n  \n  \n    \n      FOODSEC\n      min\n      Q1\n      median\n      Q3\n      max\n      mean\n      sd\n      n\n      missing\n    \n  \n  \n    Full\n36.9\n68.6\n81.0\n97.7\n210.8\n85.2\n24.5\n2233\n0\n    Marginal\n39.9\n69.2\n83.0\n100.2\n242.6\n87.0\n25.0\n560\n0\n    Low\n40.9\n70.4\n83.6\n102.3\n201.0\n88.2\n24.6\n501\n0\n    Very Low\n46.1\n73.4\n85.8\n101.3\n254.3\n90.1\n24.4\n379\n0\n  \n  \n  \n\n\n\n\n\n\n3.11.2 Fitting the ANOVA model\n\nm6 <- lm(WEIGHT ~ FOODSEC, data = dat6)\n\nanova(m6)\n\nAnalysis of Variance Table\n\nResponse: WEIGHT\n            Df  Sum Sq Mean Sq F value    Pr(>F)    \nFOODSEC      3   10021  3340.3   5.525 0.0008786 ***\nResiduals 3669 2218162   604.6                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nDoes the bootstrap ANOVA give a meaningfully different result? No.\n\nbs6 <- ANOVA.boot(WEIGHT ~ FOODSEC, B = 5000, seed = 4326, data = dat6)\nbs6$`p-value`\n\n[1] 8e-04\n\n\n\n\n3.11.3 Tukey HSD Pairwise Comparisons\n\nth6 <- TukeyHSD(aov(WEIGHT ~ FOODSEC, data = dat6), \n                ordered = TRUE, conf.level = 0.90)\n\ntidy(th6) |> \n  select(-c(term, null.value)) |>\n  mutate(across(.cols = -contrast, num, digits = 3)) |>\n  arrange(desc(estimate)) |>\n  gt() |>\n  tab_header(title = \"Comparing Mean Weight across pairs of Food Security groups\",\n             subtitle = \"90% Tukey HSD Confidence Intervals\") |>\n  tab_footnote(footnote = glue(nrow(dat6), \" participants in nh432\"))\n\n\n\n\n\n  \n    \n      Comparing Mean Weight across pairs of Food Security groups\n    \n    \n      90% Tukey HSD Confidence Intervals\n    \n  \n  \n    \n      contrast\n      estimate\n      conf.low\n      conf.high\n      adj.p.value\n    \n  \n  \n    Very Low-Full\n4.877\n1.746\n8.008\n0.002\n    Very Low-Marginal\n3.103\n-0.646\n6.851\n0.229\n    Low-Full\n2.930\n0.144\n5.717\n0.075\n    Very Low-Low\n1.946\n-1.891\n5.783\n0.650\n    Marginal-Full\n1.774\n-0.889\n4.438\n0.422\n    Low-Marginal\n1.156\n-2.310\n4.622\n0.870\n  \n  \n  \n    \n       3673 participants in nh432\n    \n  \n\n\n\ntidy(th6) |> \n  mutate(contrast = fct_reorder(contrast, estimate, .desc = TRUE)) %>%\n  ggplot(., aes(x = contrast, y = estimate, ymin = conf.low, ymax = conf.high)) +\n  geom_pointrange() +\n  geom_hline(yintercept = 0, col = \"blue\", linetype = \"dashed\") +\n  geom_label(aes(label = round_half_up(estimate, 2))) +\n  labs(title = \"Comparing Mean Weight across pairs of Food Security groups\",\n       subtitle = \"90% Tukey HSD Confidence intervals\",\n       caption = glue(nrow(dat5), \" non-Hispanic Black participants in nh432\"),\n       x = \"Pairwise Difference between FOODSEC groups\",\n       y = \"Difference in Weight (kg)\")\n\n\n\n\n\n\n3.11.4 Kruskal-Wallis Test\nWhen the assumption of Normality is really unreasonable, many people (including me) will instead use a rank-based method, called the Kruskal-Wallis test to compare the locations of WEIGHT across levels of FOODSEC.\n\nkruskal.test(WEIGHT ~ FOODSEC, data = dat6)\n\n\n    Kruskal-Wallis rank sum test\n\ndata:  WEIGHT by FOODSEC\nKruskal-Wallis chi-squared = 21.102, df = 3, p-value = 0.0001003\n\n\n\n\n3.11.5 Dunn Test for Pairwise Comparisons after Kruskal-Wallis Test\nShould you develop a Kruskal-Wallis test result which implies that running a set of pairwise comparisons is important, I would suggest the use of the Dunn test, available in the dunn_test() function from the rstatix package.\n\ndunn_test(data = dat6, WEIGHT ~ FOODSEC, \n                   p.adjust.method = \"holm\", detailed = TRUE) |>\n  select(group1, group2, p.adj, n1, n2, estimate1, estimate2, estimate) |>\n  mutate(across(.cols = -c(group1, group2, n1, n2), num, digits = 3)) |>\n  gt() |>\n  tab_header(title = \"Dunn Tests comparing WEIGHT by FOODSEC\",\n             subtitle = \"Pairwise Comparisons after Kruskal-Wallis test\") |>\n  tab_footnote(footnote = glue(nrow(dat6), \" participants in nh432\"))\n\n\n\n\n\n  \n    \n      Dunn Tests comparing WEIGHT by FOODSEC\n    \n    \n      Pairwise Comparisons after Kruskal-Wallis test\n    \n  \n  \n    \n      group1\n      group2\n      p.adj\n      n1\n      n2\n      estimate1\n      estimate2\n      estimate\n    \n  \n  \n    Full\nMarginal\n0.269\n2233\n560\n1780.595\n1865.621\n85.026\n    Full\nLow\n0.049\n2233\n501\n1780.595\n1916.147\n135.551\n    Full\nVery Low\n0.000\n2233\n379\n1780.595\n2022.412\n241.816\n    Marginal\nLow\n0.438\n560\n501\n1865.621\n1916.147\n50.525\n    Marginal\nVery Low\n0.105\n560\n379\n1865.621\n2022.412\n156.790\n    Low\nVery Low\n0.282\n501\n379\n1916.147\n2022.412\n106.265\n  \n  \n  \n    \n       3673 participants in nh432\n    \n  \n\n\n\n\nAgain, a problem with this approach is that all it provides is a set of adjusted p values for these comparisons, but if we’re not willing to assume even very approximate Normality (and thus use an ANOVA approach) this is what we’ll have to cope with."
  },
  {
    "objectID": "431review2.html#r-setup",
    "href": "431review2.html#r-setup",
    "title": "4  431 Review: Comparing Rates",
    "section": "4.1 R Setup",
    "text": "4.1 R Setup\n\nknitr::opts_chunk$set(comment = NA)\n\nlibrary(janitor) \nlibrary(broom)\nlibrary(DescTools)\nlibrary(Epi)\nlibrary(gt)\nlibrary(Hmisc)\nlibrary(vcd)\n\nlibrary(tidyverse) \n\ntheme_set(theme_bw())\n\n\n4.1.1 Data Load\n\nnh432 <- read_rds(\"data/nh432.Rds\")"
  },
  {
    "objectID": "431review2.html#x2-contingency-table-dr_lose-and-nowlose",
    "href": "431review2.html#x2-contingency-table-dr_lose-and-nowlose",
    "title": "4  431 Review: Comparing Rates",
    "section": "4.2 2x2 Contingency Table: DR_LOSE and NOWLOSE",
    "text": "4.2 2x2 Contingency Table: DR_LOSE and NOWLOSE\nLet’s compare the probability that NOWLOSE is 1 (The subject is currently working on losing or controlling their body weight) between NHANES participants who have (vs. who have not) been told by a doctor to lose or control their weight in the past 12 months (DR_LOSE). Each of these (DR_LOSE and NOWLOSE) is stored in R as a numeric variable with non-missing values equal to 0 or 1.\n\ntemp <- nh432 |> \n  select(SEQN, DR_LOSE, NOW_LOSE) |> \n  drop_na()\n\nAs with any categorical variable, we start by counting, and the natural way to display the counts of these two variables (DR_LOSE and NOW_LOSE) is in a table, rather than a graph, I think.\n\ntemp |> \n  tabyl(DR_LOSE, NOW_LOSE) |> \n  adorn_totals(where = c(\"row\", \"col\")) |> \n  adorn_title() \n\n         NOW_LOSE           \n DR_LOSE        0    1 Total\n       0     1198 1541  2739\n       1      246  943  1189\n   Total     1444 2484  3928\n\n\nNow that we have a 2x2 table, we could consider obtaining some more detailed summary statistics, with a tool like the twoby2() function in the Epi package. There is a problem with this, though.\n\ntwoby2(temp$DR_LOSE, temp$NOW_LOSE)\n\n2 by 2 table analysis: \n------------------------------------------------------ \nOutcome   : 0 \nComparing : 0 vs. 1 \n\n     0    1    P(0) 95% conf. interval\n0 1198 1541  0.4374    0.4189   0.4560\n1  246  943  0.2069    0.1848   0.2309\n\n                                   95% conf. interval\n             Relative Risk: 2.1140    1.8766   2.3815\n         Sample Odds Ratio: 2.9801    2.5412   3.4949\nConditional MLE Odds Ratio: 2.9793    2.5350   3.5096\n    Probability difference: 0.2305    0.2002   0.2594\n\n             Exact P-value: 0.0000 \n        Asymptotic P-value: 0.0000 \n------------------------------------------------------\n\n\nThe code runs fine, but the table isn’t really in a useful format. This table shows the probability that NOWLOSE = 0 (“No”) comparing DR_LOSE = 0 (“No”) to DR_LOSE = 1 (“Yes”), and that’s just confusing.\nIt would be much better if we did two things:\n\nused factors with meaningful labels to represent the 1/0 variables for this table\nset up the table in standard epidemiological format, and then made a better choice as to what combination should be in the top left of the 2x2 table.\n\nSo let’s do that.\n\n4.2.1 Standard Epidemiological Format\nStandard Epidemiological Format for a 2x2 table places the exposure in the rows, and the outcome in the columns, with the top left representing the combination of interest when we obtain things like an odds ratio or probability difference. Typically this means we want to put the “Yes” and “Yes” combination in the top left.\nFirst, let’s create factor versions (with more meaningful labels than 1 and 0) out of the two variables of interest: DR_LOSE and NOW_LOSE.\n\ndat1 <- nh432 |> \n  select(SEQN, DR_LOSE, NOW_LOSE) |> \n  drop_na() |>\n  mutate(DR_LOSE_f = fct_recode(factor(DR_LOSE), \"Dr_said_Lose_Wt\" = \"1\", No = \"0\"),\n         DR_LOSE_f = fct_relevel(DR_LOSE_f, \"Dr_said_Lose_Wt\", \"No\"),\n         NOW_LOSE_f = fct_recode(factor(NOW_LOSE), \"Now_losing_Wt\" = \"1\", No = \"0\"),\n         NOW_LOSE_f = fct_relevel(NOW_LOSE_f, \"Now_losing_Wt\", \"No\"))\n\nNote that after recoding the levels to more meaningful labels, we also re-leveled the factors so that the “Yes” result comes first rather than last.\nThis produces the following table, which is now in standard epidemiological format, where we are using the DR_LOSE_f information to predict NOW_LOSE_f.\n\ndat1 |> \n  tabyl(DR_LOSE_f, NOW_LOSE_f) |> \n  adorn_totals(where = c(\"row\", \"col\")) |> \n  adorn_title() \n\n                    NOW_LOSE_f           \n       DR_LOSE_f Now_losing_Wt   No Total\n Dr_said_Lose_Wt           943  246  1189\n              No          1541 1198  2739\n           Total          2484 1444  3928\n\n\nWe could, I suppose, make the table even prettier.\n\ntab1 <- dat1 |> \n  tabyl(DR_LOSE_f, NOW_LOSE_f) |> \n  adorn_totals(where = c(\"row\", \"col\")) \n\ngt(tab1, rowname_col = \"DR_LOSE_f\") |>\n  tab_header(title = \"DR_LOSE vs. NOW_LOSE\",\n             subtitle = \"Standard Epidemiological Format\") |>\n  tab_stubhead(label = \"Dr said Lose Weight?\") |>\n  tab_spanner(label = \"Currently Losing Weight?\", \n              columns = c(Now_losing_Wt, No))\n\n\n\n\n\n  \n    \n      DR_LOSE vs. NOW_LOSE\n    \n    \n      Standard Epidemiological Format\n    \n  \n  \n    \n      Dr said Lose Weight?\n      \n        Currently Losing Weight?\n      \n      Total\n    \n    \n      Now_losing_Wt\n      No\n    \n  \n  \n    Dr_said_Lose_Wt\n943\n246\n1189\n    No\n1541\n1198\n2739\n    Total\n2484\n1444\n3928\n  \n  \n  \n\n\n\n\n\n\n4.2.2 Obtaining Key Summaries with twoby2()\nAnd, finally, we can obtain necessary summaries (including estimates and confidence intervals) using the twoby2() function.\n\ntwoby2(dat1$DR_LOSE_f, dat1$NOW_LOSE_f, conf.level = 0.90)\n\n2 by 2 table analysis: \n------------------------------------------------------ \nOutcome   : Now_losing_Wt \nComparing : Dr_said_Lose_Wt vs. No \n\n                Now_losing_Wt   No    P(Now_losing_Wt) 90% conf. interval\nDr_said_Lose_Wt           943  246              0.7931    0.7731   0.8118\nNo                       1541 1198              0.5626    0.5470   0.5781\n\n                                   90% conf. interval\n             Relative Risk: 1.4097    1.3586   1.4627\n         Sample Odds Ratio: 2.9801    2.6071   3.4065\nConditional MLE Odds Ratio: 2.9793    2.5998   3.4195\n    Probability difference: 0.2305    0.2052   0.2548\n\n             Exact P-value: 0.0000 \n        Asymptotic P-value: 0.0000 \n------------------------------------------------------\n\n\nSome brief descriptions of these results:\n\nThe probability that a participant is now losing weight (NOW_LOSE is 1) is estimated to be 0.79 (with 90% CI 0.77, 0.81) if the participant has been told to lose weight by a doctor in the past 12 months (DR_LOSE = 1), but only 0.56 (with 90% CI 0.55, 0.58) if the participant has not been told this.\nThe relative risk of a participant now losing weight is estimated to be \\(\\frac{0.7931}{0.5626}\\) = 1.41 (with 90% CI 1.36, 1.46) for a participant who has been told to lose weight vs. a participant who has not.\nThe odds of a participant now losing weight are \\(\\frac{0.7931(1-0.5626)}{0.5626(1-0.7931)}\\) = 2.98 times as high for a participant who has been told to lose weight than for one who has not, with 90% CI (2.61, 3.41).\nThe difference in probability is estimated to be 0.7931 - 0.5626 = 0.2305 (90% CI: 0.21, 0.25), indicating again that the true probability of now losing weight is higher in participants who have been told to lose weight than in those who have not.\n\nThe “exact” p-value listed comes from the Fisher exact test, while the “asymptotic” p-value comes from a Pearson \\(\\chi^2\\) (chi-squared) test. I would focus on the meaningful estimates (those with confidence intervals) in making comparisons, rather than on trying to determine “statistical significance” with the p-values."
  },
  {
    "objectID": "431review2.html#x2-table-sedate-category-and-now_exer",
    "href": "431review2.html#x2-table-sedate-category-and-now_exer",
    "title": "4  431 Review: Comparing Rates",
    "section": "4.3 2x2 Table: SEDATE category and NOW_EXER",
    "text": "4.3 2x2 Table: SEDATE category and NOW_EXER\nLet’s now look at another example, where we compare the probability that a participant is “now exercising” (NOW_EXER = 1) on the basis of their level of sedentary activity in a typical day (collected in the SEDATE variable, in minutes.)\n\ndat2 <- nh432 |> \n  select(SEQN, SEDATE, NOW_EXER) |> \n  drop_na()\n\nsummary(dat2 |> select(-SEQN))\n\n     SEDATE          NOW_EXER     \n Min.   :   2.0   Min.   :0.0000  \n 1st Qu.: 180.0   1st Qu.:0.0000  \n Median : 300.0   Median :1.0000  \n Mean   : 332.8   Mean   :0.6019  \n 3rd Qu.: 480.0   3rd Qu.:1.0000  \n Max.   :1320.0   Max.   :1.0000  \n\n\nAs you can see above, the information in SEDATE is quantitative, and suppose we want to compare a High SEDATE group vs. a Low SEDATE group.\n\n4.3.1 Creating a Low and High Group on SEDATE\nWe can use the cut2() function from the Hmisc package to partition the data by the SEDATE variable into three groups of equal sample size. At the same time, we’ll make NOW_EXER into a more useful (for tabulation) factor with more meaningful level descriptions.\n\ndat2 <- dat2 |>\n  mutate(SED_f = cut2(SEDATE, g = 3),\n         NOW_EXER_f = fct_recode(factor(NOW_EXER), \"Now_exercising\" = \"1\", No = \"0\"),\n         NOW_EXER_f = fct_relevel(NOW_EXER_f, \"Now_exercising\", \"No\"))\n\nAs you can see, we now have three groups defined by their SEDATE values, of roughly equal sample sizes.\n\ndat2 |> tabyl(SED_f)\n\n      SED_f    n   percent\n [  2, 200) 1323 0.3387097\n [200, 420) 1301 0.3330773\n [420,1320] 1282 0.3282130\n\n\nThe group labeled [2, 200) contains the 1323 subjects who had SEDATE values ranging from 2 up to (but not including) 200 minutes, for example.\n\nggplot(dat2, aes(x = SEDATE)) +\n  geom_histogram(aes(fill = SED_f), col = \"black\", bins = 25) +\n  scale_fill_manual(values = c(\"seagreen\", \"white\", \"seagreen\")) +\n  labs(title = \"Comparing Low SEDATE to High SEDATE\",\n       subtitle = \"Identification of Groups\")\n\n\n\n\nNow, we want to compare the Lowest SEDATE group (SED_F = [2, 200)) to the Highest SEDATE group (SED_F = [420, 1320]). To do that, we’ll drop the middle group, and then look at the cross-tabulation of our two remaining SEDATE groups with our outcome: NOW_EXER (in factor form.)\n\ndat2 <- dat2 |>\n  filter(SED_f != \"[200, 420)\") |>\n  mutate(SED_f = fct_drop(SED_f))\n\ndat2 |> tabyl(SED_f, NOW_EXER_f)\n\n      SED_f Now_exercising  No\n [  2, 200)            776 547\n [420,1320]            789 493\n\n\n\n\n4.3.2 Two-by-Two Table Summaries\nLet’s look at the analytic results for this table.\n\ntwoby2(dat2$SED_f, dat2$NOW_EXER_f) \n\n2 by 2 table analysis: \n------------------------------------------------------ \nOutcome   : Now_exercising \nComparing : [  2, 200) vs. [420,1320] \n\n           Now_exercising  No    P(Now_exercising) 95% conf. interval\n[  2, 200)            776 547               0.5865    0.5598   0.6128\n[420,1320]            789 493               0.6154    0.5885   0.6417\n\n                                    95% conf. interval\n             Relative Risk:  0.9530    0.8952   1.0146\n         Sample Odds Ratio:  0.8864    0.7577   1.0371\nConditional MLE Odds Ratio:  0.8865    0.7553   1.0403\n    Probability difference: -0.0289   -0.0664   0.0087\n\n             Exact P-value: 0.1388 \n        Asymptotic P-value: 0.1322 \n------------------------------------------------------\n\n\nUh, oh. There’s a bit of a problem here now. We have the right rows and the right columns, but they’re not in the best possible order, since the estimated probability of Now Exercising for the group on top (SED = [2, 200)) is smaller than it is for the people in the high group in terms of sedentary activity As a result of this problem with ordering, our relative risk and odds ratio estimates are less than 1, and our probability difference is negative.\n\n\n4.3.3 Flipping Levels\nSince which exposure goes at the top is an arbitrary decision, let’s switch the factor levels in SED_f, so that the people with high sedentary activity and who are now exercising are shown in the top left cell of the table. This should flip the point estimates of the relative risk and odds ratio above 1, and the estimated probability difference to a positive number. Note the use of the fct_rev() function from the forcats package to accomplish this.\n\ndat2 <- dat2 |>\n  mutate(SED_f = fct_rev(SED_f))\n\ndat2 |> tabyl(SED_f, NOW_EXER_f) |> \n  adorn_totals(where = c(\"row\", \"col\")) |> \n  adorn_title() \n\n                NOW_EXER_f           \n      SED_f Now_exercising   No Total\n [420,1320]            789  493  1282\n [  2, 200)            776  547  1323\n      Total           1565 1040  2605\n\ntwoby2(dat2$SED_f, dat2$NOW_EXER_f, conf.level = 0.90)\n\n2 by 2 table analysis: \n------------------------------------------------------ \nOutcome   : Now_exercising \nComparing : [420,1320] vs. [  2, 200) \n\n           Now_exercising  No    P(Now_exercising) 90% conf. interval\n[420,1320]            789 493               0.6154    0.5929   0.6375\n[  2, 200)            776 547               0.5865    0.5641   0.6086\n\n                                   90% conf. interval\n             Relative Risk: 1.0493    0.9956   1.1059\n         Sample Odds Ratio: 1.1281    0.9889   1.2869\nConditional MLE Odds Ratio: 1.1281    0.9858   1.2910\n    Probability difference: 0.0289   -0.0027   0.0604\n\n             Exact P-value: 0.1388 \n        Asymptotic P-value: 0.1322 \n------------------------------------------------------\n\n\nWe conclude now that the participants who were in the high SEDATION group (as compared to those in the low SEDATION group) had:\n\na relative risk of 1.05 (90% CI: 0.995, 1.106) for Now exercising,\na sample odds ratio of 1.13 (90% CI: 0.989, 1.287) for Now exercising,\nand probability for Now exercising that was 0.029 higher (-0.003, 0.060) than for those in the low SEDATION group."
  },
  {
    "objectID": "431review2.html#a-larger-5x3-2-way-table-dietqual-and-wtgoal-in-lighter-men",
    "href": "431review2.html#a-larger-5x3-2-way-table-dietqual-and-wtgoal-in-lighter-men",
    "title": "4  431 Review: Comparing Rates",
    "section": "4.4 A Larger (5x3) 2-Way Table: DIETQUAL and WTGOAL in Lighter Men",
    "text": "4.4 A Larger (5x3) 2-Way Table: DIETQUAL and WTGOAL in Lighter Men\nHere, we’ll look at Male participants who weighed less than 100 kg (approximately 220 pounds) and ask whether their DIETQUAL (diet quality: self-rated as Excellent to poor in 5 categories) response is associated with their response to WTGOAL (would you like to weigh more, about the same, or less than you do now: 3 categories.)\nThe resulting two-way contingency table includes 5 rows and 3 columns. We are interested in evaluating the relationship between the rows and the columns. It’s called a two-way table because there are two categorical variables (DIETQUAL and WTGOAL) under study.\nIf the rows and columns were found to be independent of one another, this would mean that the probabilities of falling in each column do not change, regardless of what row of the table we look at.\nIf the rows and columns are associated, then the probabilities of falling in each column do depend on which row we’re looking at.\n\ndat3 <- nh432 |> \n  select(SEQN, DIETQUAL, WTGOAL, WEIGHT, SEX) |>\n  filter(WEIGHT < 100 & SEX == \"Male\") |>\n  drop_na()\n\ndat3 |> \n  tabyl(DIETQUAL, WTGOAL)\n\n  DIETQUAL More Same Less\n Excellent   16   53   32\n Very Good   37  153  117\n      Good   68  179  238\n      Fair   44  111  144\n      Poor   15   18   43\n\n\nIf we want a graphical representation of a two-way table, the most common choice is probably a mosaic plot.\n\nvcd::mosaic(~ DIETQUAL + WTGOAL, data = dat3,\n            highlighting = \"WTGOAL\")\n\n\n\n\nLarger observed frequencies in the contingency table show up with larger tile areas in the in the mosaic plot. So, for instance, we see the larger proportion of “less” WTGOAL in the “Poor” DIETQUAL category, as compared to most of the other DIETQUAL categories.\n\n4.4.1 What would independence look like?\nA mosaic plot displaying perfect independence (using simulated data) might look something like this:\n\nvar1 <- c(rep(\"A\", 48), rep(\"B\", 54), rep(\"C\", 60), rep(\"D\", 24) )\nvar2 <- c( rep(c(\"G1\", \"G1\", \"G2\", \"G2\", \"G2\", \"G3\"), 31) )\ntemp_tab <- tibble(var1, var2); rm(var1, var2)\nvcd::mosaic(~ var1 + var2, data = temp_tab, highlighting = \"var1\")\n\n\n\n\nHere’s the table for our simulated data, where independence holds perfectly.\n\nxtabs(~ var1 + var2, data = temp_tab)\n\n    var2\nvar1 G1 G2 G3\n   A 16 24  8\n   B 18 27  9\n   C 20 30 10\n   D  8 12  4\n\n\nNote that in these simulated data, we have the same fraction of people in each of the four var1 categories (A, B, C, and D) regardless of which of the three var2 categories (G1, G2 and G3) we are in, and vice versa. That’s what it means for rows and columns to be independent.\n\n\n4.4.2 Back to the DIETQUAL and WTGOAL table\nNow, returning to our problem, to obtain detailed results from the Pearson \\(\\chi^2\\) test, I use the xtabs() function and then the chisq.test() function, like this:\n\nchi3 <- chisq.test(xtabs(~ DIETQUAL + WTGOAL, data = dat3))\n\nchi3\n\n\n    Pearson's Chi-squared test\n\ndata:  xtabs(~DIETQUAL + WTGOAL, data = dat3)\nX-squared = 32.603, df = 8, p-value = 7.261e-05\n\n\nThe null hypothesis being tested here is that DIETQUAL and WTGOAL are independent of each other. A small p value like this is indicative of an association between the two variables.\nThe chi3 object we have created also contains:\n\nthe observed frequencies in each cell, as well as\nthe expected frequencies under the hypothesis of independence of the rows and the columns1, and\nthe Pearson residuals \\((\\mbox{observed - expected})/\\sqrt{\\mbox{expected}}\\) for each cell, among other things.\n\n\nchi3$observed\n\n           WTGOAL\nDIETQUAL    More Same Less\n  Excellent   16   53   32\n  Very Good   37  153  117\n  Good        68  179  238\n  Fair        44  111  144\n  Poor        15   18   43\n\nchi3$expected\n\n           WTGOAL\nDIETQUAL        More      Same      Less\n  Excellent 14.33754  40.94164  45.72082\n  Very Good 43.58044 124.44637 138.97319\n  Good      68.84858 196.60095 219.55047\n  Fair      42.44479 121.20347 135.35174\n  Poor      10.78864  30.80757  34.40379\n\nchi3$residuals # Pearson residuals\n\n           WTGOAL\nDIETQUAL          More       Same       Less\n  Excellent  0.4390501  1.8845411 -2.0291917\n  Very Good -0.9968028  2.5595886 -1.8639211\n  Good      -0.1022694 -1.2552875  1.2451396\n  Fair       0.2387127 -0.9268093  0.7433564\n  Poor       1.2821492 -2.3074805  1.4655618\n\n\nAn association plot presents a graphical description of the Pearson residuals, with the area of each box shown proportional to the difference between the observed and expected frequencies.\n\nIf the observed frequency of a cell is greater than the expectation under the hypothesis of independence, then the box rises above the baseline.\n\nAn example here is the (DIETQUAL = Very Good, WTGOAL = Same) which had an observed frequency of 153 but an expected frequency of 124.4, yielding the largest positive Pearson residual at 2.56.\n\nBoxes shown below the baseline indicate that the observed frequency was less than the expectation under the independence hypothesis.\n\nThe largest negative Pearson residual is the (DIETQUAL = Poor, WTGOAL = Same) cell, where we observed 18 observations but the independence model would predict 30.8, yielding a Pearson residual of -2.31.\n\n\n\nvcd::assoc(~ DIETQUAL + WTGOAL, data = dat3)\n\n\n\n\nSome people also like to calculate a correlation between categorical variables. If each of your categorical variables is ordinal (as in this case) then Kendall’s tau (version b) is probably the best choice. As with a Pearson correlation for quantities, the value for this measure ranges from -1 to 1, with -1 indicating a strong negative correlation, and +1 a strong positive correlation, with 0 indicating no correlation.\nTo use this approach, though, we first have to be willing to treat our multi-categorical variables as if they were numeric, which may or may not be reasonable.\n\ndat3 <- dat3 |>\n  mutate(DIETQUAL_num = as.numeric(DIETQUAL))\n\ndat3 |> tabyl(DIETQUAL_num, DIETQUAL)\n\n DIETQUAL_num Excellent Very Good Good Fair Poor\n            1       101         0    0    0    0\n            2         0       307    0    0    0\n            3         0         0  485    0    0\n            4         0         0    0  299    0\n            5         0         0    0    0   76\n\n\n\ndat3 <- dat3 |>\n  mutate(WTGOAL_num = as.numeric(WTGOAL))\n\ndat3 |> tabyl(WTGOAL_num, WTGOAL)\n\n WTGOAL_num More Same Less\n          1  180    0    0\n          2    0  514    0\n          3    0    0  574\n\n\n\ncor(dat3$DIETQUAL_num, dat3$WTGOAL_num, method = \"kendall\")\n\n[1] 0.07193663\n\n\nIf you want to obtain a confidence interval for this correlation coefficient, then you would need to use the KendallTauB() function from the DescTools package.\n\nKendallTauB(dat3$DIETQUAL_num, dat3$WTGOAL_num, conf.level = 0.90)\n\n     tau_b     lwr.ci     upr.ci \n0.07193663 0.03147130 0.11240196 \n\n\nAgain, it’s just a number, and not especially valuable."
  },
  {
    "objectID": "431review2.html#phq9-category-and-raceethnicity",
    "href": "431review2.html#phq9-category-and-raceethnicity",
    "title": "4  431 Review: Comparing Rates",
    "section": "4.5 PHQ9 Category and Race/Ethnicity",
    "text": "4.5 PHQ9 Category and Race/Ethnicity\nLet’s look next at the association of race-ethnicity (RACEETH, which has 5 levels) and the depression category (minimal, mild, moderate, moderately severe, or severe) available in PHQ9_CAT, which we derived from the PHQ-9 depression screener score. We’ll restrict this small analysis to NHANES participants who did not receive care from a mental health provider (so MENTALH is 0) in the last 12 months.\n\ntemp <- nh432 |> \n  select(SEQN, RACEETH, PHQ9_CAT, MENTALH) |>\n  filter(MENTALH == 0) |>\n  drop_na()\n\nSo here’s our first attempt at a 5x5 table describing this association.\n\ntemp |> \n  tabyl(RACEETH, PHQ9_CAT)\n\n     RACEETH minimal mild moderate moderately severe severe\n Non-H White     770  151       41                20      8\n Non-H Black     668  143       34                13      1\n    Hispanic     581  133       36                12      5\n Non-H Asian     428   49       11                 1      1\n  Other Race     105   34       12                 5      1\n\n\nWe note some very small observed frequencies, especially in the bottom right of the table. Should we try to run a Pearson \\(\\chi^2\\) test on these results, we will generate a warning that the Chi-square approximation may be incorrect.\n\nxtabs( ~ RACEETH + PHQ9_CAT, data = temp ) |>\n  chisq.test()\n\nWarning in stats::chisq.test(x, y, ...): Chi-squared approximation may be\nincorrect\n\n\n\n    Pearson's Chi-squared test\n\ndata:  xtabs(~RACEETH + PHQ9_CAT, data = temp)\nX-squared = 49.288, df = 16, p-value = 2.974e-05\n\n\n\n4.5.1 The Cochran conditions\nR sets off this warning when the “Cochran conditions” are not met. The Cochran conditions require that we have:\n\nno cells with 0 counts\nat least 80% of the cells in our table with counts of 5 or higher\nexpected counts in each cell of the table should be 5 or more\n\nIn our table, we have four cells with observed counts below 5 (all have count 1) and two more with observed counts of exactly 5. If we look at the expected frequencies under the hypothesis of independence, what do we see?\n\ntemp_chi <- xtabs( ~ RACEETH + PHQ9_CAT, data = temp ) |>\n  chisq.test()\n\nWarning in stats::chisq.test(x, y, ...): Chi-squared approximation may be\nincorrect\n\ntemp_chi$expected\n\n             PHQ9_CAT\nRACEETH        minimal      mild  moderate moderately severe    severe\n  Non-H White 774.2813 154.73491 40.655838         15.473491 4.8544284\n  Non-H Black 671.8259 134.25988 35.276126         13.425988 4.2120748\n  Hispanic    599.8725 119.88048 31.498008         11.988048 3.7609562\n  Non-H Asian 383.2302  76.58596 20.122587          7.658596 2.4026969\n  Other Race  122.7901  24.53877  6.447441          2.453877 0.7698437\n\n\nEvery cell in the “severe” category has an expected frequency below 5, and we also have some generally small counts, in the Non-Hispanic Asian and Other Race categories, as well as the “moderately severe” category.\n\n\n4.5.2 Collapsing Categories\nSo what might we do about this?\nLet us consider two approaches that we’ll use simultaneously:\n\ndrop two of the RACEETH groups, and just use the top 3 (Non-H White, Non-H Black and Hispanic) using filter()\ncollapse together the two right-most levels of PHQ9_CAT (moderately severe and severe) into a new level which I’ll call “More Severe”, using fct_lump_n()\n\n\ndat5 <- nh432 |> \n  select(SEQN, RACEETH, PHQ9_CAT, MENTALH) |>\n  filter(MENTALH == 0) |>\n  filter(RACEETH %in% c(\"Non-H White\", \"Non-H Black\", \"Hispanic\")) |>\n  drop_na() |>\n  mutate(RACEETH = fct_drop(RACEETH),\n         PHQ9_CAT = fct_lump_n(PHQ9_CAT, 3, \n                               other_level = \"More Severe\"))\n\ndat5 |> \n  tabyl(RACEETH, PHQ9_CAT)\n\n     RACEETH minimal mild moderate More Severe\n Non-H White     770  151       41          28\n Non-H Black     668  143       34          14\n    Hispanic     581  133       36          17\n\n\nNow, we have at least 14 participants in every cell of the table.\n\n\n4.5.3 Pearson \\(\\chi^2\\) Analysis\nNow, let’s consider what the Pearson \\(\\chi^2\\) test suggests.\n\ntab5 <- xtabs(~ RACEETH + PHQ9_CAT, data = dat5)\n\ntab5\n\n             PHQ9_CAT\nRACEETH       minimal mild moderate More Severe\n  Non-H White     770  151       41          28\n  Non-H Black     668  143       34          14\n  Hispanic        581  133       36          17\n\nchisq.test(tab5)\n\n\n    Pearson's Chi-squared test\n\ndata:  tab5\nX-squared = 5.0157, df = 6, p-value = 0.5418\n\n\nNow we have no warning, and notice also how large a change this has meant in terms of the p-value, as compared to our original \\(\\chi^2\\) result.\n\n\n4.5.4 Mosaic Plot\nHere’s a mosaic plot2 of the table.\n\nvcd::mosaic(tab5, highlighting = \"PHQ9_CAT\")\n\n\n\n\n\n\n4.5.5 Examining the Fit\nWe’ll finish up with a look at the expected frequencies, and a table and association plot of the Pearson residuals.\n\nchisq.test(tab5)$observed\n\n             PHQ9_CAT\nRACEETH       minimal mild moderate More Severe\n  Non-H White     770  151       41          28\n  Non-H Black     668  143       34          14\n  Hispanic        581  133       36          17\n\nchisq.test(tab5)$expected\n\n             PHQ9_CAT\nRACEETH        minimal     mild moderate More Severe\n  Non-H White 764.0711 161.5940 42.00688    22.32798\n  Non-H Black 662.9667 140.2114 36.44839    19.37347\n  Hispanic    591.9622 125.1946 32.54472    17.29855\n\nchisq.test(tab5)$residuals\n\n             PHQ9_CAT\nRACEETH           minimal        mild    moderate More Severe\n  Non-H White  0.21449006 -0.83339100 -0.15535235  1.20036381\n  Non-H Black  0.19548040  0.23550271 -0.40554793 -1.22081874\n  Hispanic    -0.45055624  0.69759600  0.60567876 -0.07178083\n\nassoc(tab5)"
  },
  {
    "objectID": "431review3.html#r-setup",
    "href": "431review3.html#r-setup",
    "title": "5  431 Review: Fitting Linear Models",
    "section": "5.1 R Setup",
    "text": "5.1 R Setup\n\nknitr::opts_chunk$set(comment = NA)\n\nlibrary(janitor) \nlibrary(broom)\nlibrary(car)\nlibrary(equatiomatic)\nlibrary(GGally)\nlibrary(gt)\nlibrary(Hmisc)\nlibrary(patchwork)\n\nlibrary(tidyverse) \n\ntheme_set(theme_bw())\n\n\n5.1.1 Data Load\n\nnh432 <- read_rds(\"data/nh432.Rds\")"
  },
  {
    "objectID": "431review3.html#modeling-weekend-sleep-hours",
    "href": "431review3.html#modeling-weekend-sleep-hours",
    "title": "5  431 Review: Fitting Linear Models",
    "section": "5.2 Modeling Weekend Sleep Hours",
    "text": "5.2 Modeling Weekend Sleep Hours\nIn this example, we’ll try to build an effective model to predict our outcome: average weekend hours of sleep (SLPWKEND) on the basis of four predictors:\n\naverage weekday hours of sleep (SLPWKDAY)\nsystolic blood pressure (SBP)\nPHQ-9 depression screener score (PHQ9), and\nwhether or not the participant has mentioned trouble sleeping to a physician (SLPTROUB)\n\nWe’ll compare a model using all four of these predictors to a model using just the two directly related to sleep (SLPWKDAY and SLPTROUB), and we’ll restrict our analysis to those participants whose self-reported overall health (SROH) was “Good”.\n\ndat1 <- nh432 |>\n  select(SEQN, SLPWKEND, SLPWKDAY, SBP, PHQ9, SLPTROUB, SROH) |>\n  filter(SROH == \"Good\") |>\n  drop_na()\n\ndat1\n\n# A tibble: 1,293 × 7\n   SEQN   SLPWKEND SLPWKDAY   SBP  PHQ9 SLPTROUB SROH \n   <chr>     <dbl>    <dbl> <dbl> <int>    <dbl> <fct>\n 1 109273      8        6.5   110    15        1 Good \n 2 109293      6.5      7.5   130     3        0 Good \n 3 109295      7        7     161     0        1 Good \n 4 109305      6.5      6     125     0        0 Good \n 5 109307     11        7.5   114     0        0 Good \n 6 109315      5        5     123     1        0 Good \n 7 109336      8        4     148     1        1 Good \n 8 109342      8        6.5   107    16        1 Good \n 9 109365      9.5      9.5   133     7        0 Good \n10 109378      9        9     133     0        0 Good \n# … with 1,283 more rows\n\n\n\n5.2.1 Should we transform our outcome?\nWe can develop a Box-Cox plot to help us choose between potential transformations of our outcome, so as to improve the adherence to regression assumptions. To do so, we first fit our larger model.\n\nm1 <- lm(SLPWKEND ~ SLPWKDAY + SBP + PHQ9 + SLPTROUB, data = dat1)\n\nboxCox(m1)\n\n\n\n\nThe Box-Cox suggested set of transformations centers near \\(\\lambda = 1\\). As we saw back in Chapter 3, The ladder of power transformations looks like this:\n\n\n\n\\(\\lambda\\)\nTransformation\nFormula\n\n\n\n\n-2\ninverse square\n\\(1/y^2\\)\n\n\n-1\ninverse\n\\(1/y\\)\n\n\n-0.5\ninverse square root\n\\(1/\\sqrt{y}\\)\n\n\n0\nlogarithm\n\\(log y\\)\n\n\n0.5\nsquare root\n\\(\\sqrt{y}\\)\n\n\n1\nno transformation\ny\n\n\n2\nsquare\n\\(y^2\\)\n\n\n3\ncube\n\\(y^3\\)\n\n\n\nSo, in this case, the Box-Cox approach (again, with \\(\\lambda\\) near 1) suggests that we leave the existing SLPWKEND outcome alone.\n\n\n5.2.2 Scatterplot Matrix\n\nggpairs(dat1, columns = c(3:6, 2), switch = \"both\",\n        lower=list(combo=wrap(\"facethist\", bins=25)))\n\n\n\n\n\nThe reason I included column 2 (our outcome: SLPWKEND) last in this plot is so that the bottom row would include each of our predictors plotted on the X (horizontal) axis against the outcome on the Y (vertical) axis, next to a density plot of the outcome.\nI also switched the locations of the facet labels on both the x and y axis from their defaults, so that the labels are to the left and below the plots, since I find that a bit easier to work with.\nThe lower business is to avoid getting a warning about binwidths.\nThe binary variable (SLPTROUB) is included here as a 1-0 numeric variable, rather than a factor, which is why the scatterplot matrix looks as it does, rather than creating a series of boxplots (as we’ll see when we work with a factor later.)\n\n\n\n5.2.3 Collinearity?\nIn any multiple regression setting, two or more of the predictors might be highly correlated with one another, and this is referred to as multicollinearity or just collinearity. If we have a serious problem with collinearity, this can cause several problems, including difficulty fitting and interpreting the resulting model.\nIs collinearity a serious concern in our situation? Looking at the scatterplot matrix, we see that the largest observed correlation between two predictors is between PHQ9 and SLPTROUB. Does that rise to the level of a problem?\nI usually use the vif() function from the car package to help make this decision. The variance inflation factor (or VIF) measures how much the variance of a regression coefficient is inflated due to collinearity in the model. The smallest possible VIF value is 1, and VIFs near 1, as we’ll see here, indicate no problems with collinearity worth worrying about.\n\nvif(m1)\n\nSLPWKDAY      SBP     PHQ9 SLPTROUB \n1.002807 1.006778 1.102341 1.098187 \n\n\nShould we see a VIF (or generalized VIF, which is produced by the vif() function when we have factor variables in the model) above, say, 5, that would be an indication that the model would be improved by not including the variable that exhibits collinearity. Here, we have no such issues, and will proceed to fit the model including all of these predictors.\n\n\n5.2.4 Fitting and Displaying Model m1\nHere are the coefficients obtained from fitting the model m1.\n\nm1 <- lm(SLPWKEND ~ SLPWKDAY + SBP + PHQ9 + SLPTROUB, data = dat1)\n\nm1\n\n\nCall:\nlm(formula = SLPWKEND ~ SLPWKDAY + SBP + PHQ9 + SLPTROUB, data = dat1)\n\nCoefficients:\n(Intercept)     SLPWKDAY          SBP         PHQ9     SLPTROUB  \n    5.26266      0.52661     -0.00560     -0.02945     -0.20813  \n\n\nWe can use the extract_eq() function from the equatiomatic package to display the model attractively.\n\nextract_eq(m1, use_coefs = TRUE, coef_digits = 3,\n           wrap = TRUE, operator_location = \"start\")\n\n\\[\n\\begin{aligned}\n\\operatorname{\\widehat{SLPWKEND}} &= 5.263 + 0.527(\\operatorname{SLPWKDAY}) - 0.006(\\operatorname{SBP}) - 0.029(\\operatorname{PHQ9})\\\\\n&\\quad - 0.208(\\operatorname{SLPTROUB})\n\\end{aligned}\n\\]\n\n\nIf Harry and Sally have the same values of SLPWKDAY, SBP and SLPTROUB, but Harry’s PHQ9 is one point higher than Sally’s, then model m1 predicts that Harry will sleep 0.029 hours less than Sally on the weekends.\nA summary of the regression model m1 provides lots of useful information about the parameters (including their standard errors) and the quality of fit (at least as measured by \\(R^2\\) and adjusted \\(R^2\\).)\n\nsummary(m1)\n\n\nCall:\nlm(formula = SLPWKEND ~ SLPWKDAY + SBP + PHQ9 + SLPTROUB, data = dat1)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-5.1173 -0.9609 -0.1005  0.9248  6.3659 \n\nCoefficients:\n             Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  5.262657   0.388427  13.549  < 2e-16 ***\nSLPWKDAY     0.526612   0.028950  18.190  < 2e-16 ***\nSBP         -0.005600   0.002515  -2.227  0.02613 *  \nPHQ9        -0.029450   0.010955  -2.688  0.00727 ** \nSLPTROUB    -0.208129   0.098758  -2.107  0.03527 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.554 on 1288 degrees of freedom\nMultiple R-squared:  0.2171,    Adjusted R-squared:  0.2147 \nF-statistic:  89.3 on 4 and 1288 DF,  p-value: < 2.2e-16\n\n\n\n\n5.2.5 Using broom functions on Model m1\nIf we want to actually use the information in the model summary elsewhere, we use the tidy() and glance() functions from the broom package to help us.\n\ntidy(m1, conf.int = TRUE, conf.level = 0.90)\n\n# A tibble: 5 × 7\n  term        estimate std.error statistic  p.value conf.low conf.high\n  <chr>          <dbl>     <dbl>     <dbl>    <dbl>    <dbl>     <dbl>\n1 (Intercept)  5.26      0.388       13.5  3.41e-39  4.62      5.90   \n2 SLPWKDAY     0.527     0.0290      18.2  5.52e-66  0.479     0.574  \n3 SBP         -0.00560   0.00251     -2.23 2.61e- 2 -0.00974  -0.00146\n4 PHQ9        -0.0294    0.0110      -2.69 7.27e- 3 -0.0475   -0.0114 \n5 SLPTROUB    -0.208     0.0988      -2.11 3.53e- 2 -0.371    -0.0456 \n\n\nHere’s a cleaner presentation of the tidy() output.\n\ntidy(m1, conf.int = TRUE, conf.level = 0.90) |> \n  mutate(across(.cols = -c(term), num, digits = 3)) |>\n  gt()\n\nWarning: There was 1 warning in `mutate()`.\nℹ In argument: `across(.cols = -c(term), num, digits = 3)`.\nCaused by warning:\n! The `...` argument of `across()` is deprecated as of dplyr 1.1.0.\nSupply arguments directly to `.fns` through an anonymous function instead.\n\n  # Previously\n  across(a:b, mean, na.rm = TRUE)\n\n  # Now\n  across(a:b, \\(x) mean(x, na.rm = TRUE))\n\n\n\n\n\n\n  \n  \n    \n      term\n      estimate\n      std.error\n      statistic\n      p.value\n      conf.low\n      conf.high\n    \n  \n  \n    (Intercept)\n5.263\n0.388\n13.549\n0.000\n4.623\n5.902\n    SLPWKDAY\n0.527\n0.029\n18.190\n0.000\n0.479\n0.574\n    SBP\n-0.006\n0.003\n-2.227\n0.026\n-0.010\n-0.001\n    PHQ9\n-0.029\n0.011\n-2.688\n0.007\n-0.047\n-0.011\n    SLPTROUB\n-0.208\n0.099\n-2.107\n0.035\n-0.371\n-0.046\n  \n  \n  \n\n\n\n\nNote that none of the 90% confidence intervals here cross zero. This just means that we have a pretty good handle on the direction of effects - for example, our estimate for the slope of SLPWKDAY is positive, suggesting that people who sleep more during the week also sleep more on the weekend, after accounting for SBP, PHQ9 and SLPTROUB.\n\nglance(m1)\n\n# A tibble: 1 × 12\n  r.squared adj.r.squa…¹ sigma stati…²  p.value    df logLik   AIC   BIC devia…³\n      <dbl>        <dbl> <dbl>   <dbl>    <dbl> <dbl>  <dbl> <dbl> <dbl>   <dbl>\n1     0.217        0.215  1.55    89.3 4.93e-67     4 -2402. 4817. 4848.   3111.\n# … with 2 more variables: df.residual <int>, nobs <int>, and abbreviated\n#   variable names ¹​adj.r.squared, ²​statistic, ³​deviance\n\n\nModel m1 shows an \\(R^2\\) value of 0.217, which means that 21.7% of the variation in our outcome SLPWKEND is accounted for by the model using SLPWKDAY, SBP, PHQ9 and SLPTROUBLE.\nThe adjusted \\(R^2\\) value isn’t a percentage or proportion of anything, but it is a handy index when comparing two models fit to the same outcome for the same observations. It penalizes the raw \\(R^2\\) value for models that require more coefficients to be fit. If the raw \\(R^2\\) is much larger than the adjusted \\(R^2\\) value, this is also an indication that the model may be “overfit” - capitalizing on noise in the data more than we’d like, so that the amount of signal in the predictors may be overstated by raw \\(R^2\\).\nHere’s a cleaner presentation of some of the more important elements in the glance() output:\n\nglance(m1) |> \n  select(r2 = r.squared, adjr2 = adj.r.squared, \n         AIC, BIC, sigma, nobs, df, df.res = df.residual) |>\n  gt()\n\n\n\n\n\n  \n  \n    \n      r2\n      adjr2\n      AIC\n      BIC\n      sigma\n      nobs\n      df\n      df.res\n    \n  \n  \n    0.217108\n0.2146766\n4816.624\n4847.613\n1.55416\n1293\n4\n1288\n  \n  \n  \n\n\n\n\n\nAIC and BIC are measures used to compare models for the same outcome using the same data, so we’ll see those again when we fit a second model to these data. In those comparisons, smaller values of AIC and BIC indicate better fitting models.\nnobs is the number of observations used to actually fit our model m1,\ndf indicates the number of degrees of freedom used by the model, and represents the number of estimated coefficients fit, while\ndf.res = nobs - df - 1 = residual degrees of freedom.\n\n\n\n5.2.6 Residual Plots for Model m1\nThe key assumptions for a linear regression model include:\n\nLinearity of the association under study\nNormality of the residuals\nConstant Variance (Homoscedasticity)\nIndependence (not an issue with cross-sectional data like this)\n\nA residual for a point in a regression model is just the observed value of our outcome (here, SLPWKEND) minus the value predicted by the model based on the predictor values (also called the fitted value.)\nThe four key plots that R will generate for you to help assess these results are shown below for model m1.\n\n## I used \n## #| fig.height: 8 \n## at the top of this code chunk\n## to make the plots tall enough to see well\n\npar(mfrow = c(2,2)); plot(m1); par(mfrow = c(1,1))\n\n\n\n\n\n5.2.6.1 Residuals vs. Fitted values\nThe top left plot (Residuals vs. Fitted Values) helps us to assess the linearity and constant variance assumptions.\n\nWe want to see a “fuzzy football” shape.\nA clear curve is indicative of a problem with linearity, and suggests that perhaps a transformation of the outcome (or perhaps one or more predictors) may be in order\nA fan shape, with much more variation at one end of the fitted values (left or right) than the other indicates a problem with the constant variance assumption, and again a transformation may be needed.\n\nThe diagonal lines we see in the Residuals vs. Fitted plot are the result of the fact that both the outcome (SLPWKEND) and a key predictor (SLPWKDAYS) aren’t really continuous in the data, as most of the responses to those questions were either integers, or used 0.5 as the fraction. So those two variables are more discrete than we might have expected.\n\n\n5.2.6.2 Normal Q-Q plot of standardized residuals\nThe top right plot (Normal Q-Q) is a Normal Q-Q plot of the standardized regression residuals for our model m1. Substantial issues with skew (a curve in the plot) or a major problem with outliers (as indicated by a reverse S-shape) indicate potential concerns with the Normality assumption. Since the y-axis here shows standardized residuals, we can also assess whether what we’re seeing is especially surprising relative to our expectations for any standardized values (for example, we should see values above +3 or below -3 approximately 3 times in 1000).\n\nRemember that this plot represents nobs = 1293 residuals, so a few values near 3 in absolute value aren’t surprising.\nWe’re looking for big deviations from Normality here.\nThe plot() function in R will always identify three of the cases, by default, in these four plots.\n\nSuppose we wanted to look at the data for case 210, identified by these plots as a potential outlier, or at least a poorly fit point.\n\ndat1_aug <- augment(m1, data = dat1)\n\ndat1_aug |> slice(210) |>\n  select(SEQN, SLPWKEND, .fitted, .resid, .std.resid, everything())\n\n# A tibble: 1 × 13\n  SEQN  SLPWK…¹ .fitted .resid .std.…² SLPWK…³   SBP  PHQ9 SLPTR…⁴ SROH     .hat\n  <chr>   <dbl>   <dbl>  <dbl>   <dbl>   <dbl> <dbl> <int>   <dbl> <fct>   <dbl>\n1 1116…      13    6.63   6.37    4.11       4   126     1       0 Good  0.00504\n# … with 2 more variables: .sigma <dbl>, .cooksd <dbl>, and abbreviated\n#   variable names ¹​SLPWKEND, ²​.std.resid, ³​SLPWKDAY, ⁴​SLPTROUB\n\n\nWe see that row 210 contains SEQN 111688, whose\n\nobserved SLPWKEND is 13\nfitted SLPWKEND is 6.63\nyielding a residual of 6.37,\nor a standardized residual of 4.11\n\nWe can use the outlierTest() function in the car package to help assess whether this value is unusual enough to merit more careful consideration. This function actually works with the studentized residual, which is similar to the standardized residual we saw above. Here, this point (SEQN 111688) is fit poorly enough to be flagged by the Bonferroni outlier test as a mean-shift outlier.\n\noutlierTest(m1)\n\n    rstudent unadjusted p-value Bonferroni p\n210 4.131972         3.8289e-05     0.049508\n\n\nHaving seen that, though, I’m going to essentially ignore it for the moment, and press on to the rest of our residual analysis.\n\n\n5.2.6.3 Scale-Location plot\nThe bottom left plot in our set of four residual plots is the Scale-Location plot, which presents the square root of the standardized residuals against the fitted values. This plot provides another check on the “equal variance” assumption - if the plot shows a clear trend either up or down as we move left to right, then that indicates an issue with constant variance. While a loess smooth is provided (red curve) to help guide our thinking, it’s important not to get too excited about small changes or changes associated with small numbers of observations.\nYou’ll also note the presence of curves (in particular, little “V” shapes) formed by the points of the plot. Again, this is caused by the discrete nature of the outcome (and one of the key predictors) and wouldn’t be evident if our outcome was more continuous.\nDespite the drop in the red loess smooth as fitted values move from 5 to about 8, I don’t see much of a pattern here to indicate trouble with non-constant variance.\n\n\n5.2.6.4 Residuals vs. Leverage plot\nThe bottom-left plot is a plot of residuals vs. leverage, with influence contours.\nHighly leveraged points have unusual combinations of predictor values.\nHighly influential points have a big impact on the model, in that the model’s coefficients or quality of fit would change markedly were those points to be removed from the model. To measure influence, we combine leverage and residuals together, with a measure like Cook’s distance.\n\nTo look for points with substantial leverage on the model by virtue of having unusual values of the predictors - look for points whose leverage is at least 3 times as large as the average leverage value.\nThe average leverage is always k/n, where k is the number of coefficients fit by the model (including the slopes and intercept), and n is the number of observations in the model.\nTo obtain the leverage values, the augment() function stores them in .hat.\nTo look for points with substantial influence on the model, that is, removing them from the model would change it substantially, consider the Cook’s distance, plotted in contours here.\nAny Cook’s distance point > 1 will likely have a substantial impact on the model.\nAny points with Cook’s distance > 0.5, if any, will be indicated in the bottom-right (Residuals vs. Leverage) plot, and are worthy of investigation.\nIn model m1, we have no points with values of Cook’s distance > 0.5. To obtain the Cook’s distance values for each point, use the augment() function, which stores them in .cooksd.\n\nHere, for example, we identify the points with largest leverage and with largest Cook’s distance, across the points used to fit m1.\n\ndat1_aug <- augment(m1, data = dat1)\n\ndat1_aug |> slice_max(.hat) |>\n  select(SEQN, .hat, .resid, .fitted, .cooksd, everything())\n\n# A tibble: 1 × 13\n  SEQN     .hat .resid .fitted .cooksd SLPWK…¹ SLPWK…²   SBP  PHQ9 SLPTR…³ SROH \n  <chr>   <dbl>  <dbl>   <dbl>   <dbl>   <dbl>   <dbl> <dbl> <int>   <dbl> <fct>\n1 123474 0.0268  -1.03    8.03 0.00250       7       8   126    25       0 Good \n# … with 2 more variables: .sigma <dbl>, .std.resid <dbl>, and abbreviated\n#   variable names ¹​SLPWKEND, ²​SLPWKDAY, ³​SLPTROUB\n\ndat1_aug |> slice_max(.cooksd) |>\n  select(SEQN, .hat, .resid, .fitted, .cooksd, everything())\n\n# A tibble: 1 × 13\n  SEQN     .hat .resid .fitted .cooksd SLPWK…¹ SLPWK…²   SBP  PHQ9 SLPTR…³ SROH \n  <chr>   <dbl>  <dbl>   <dbl>   <dbl>   <dbl>   <dbl> <dbl> <int>   <dbl> <fct>\n1 122894 0.0113   5.38    5.62  0.0277      11       2   114     2       0 Good \n# … with 2 more variables: .sigma <dbl>, .std.resid <dbl>, and abbreviated\n#   variable names ¹​SLPWKEND, ²​SLPWKDAY, ³​SLPTROUB\n\n\nIt turns out that SEQN 123474 has the largest value of leverage (.hat) and SEQN 122894 has the largest value of influence (.cooksd) in our model. We will worry about .cooksd values above 0.5, but the largest value in this model is much smaller than that, so I think we’re OK for now.\n\n\n\n5.2.7 Fitting and Displaying Model m2\nWe will now move on to compare the results of this model (m1) to a smaller model.\nOur second model, m2 is a subset of m1, including only the two predictors directly related to sleep, SLPWKDAY and SLPTROUB.\n\nm2 <- lm(SLPWKEND ~ SLPWKDAY + SLPTROUB, data = dat1)\n\nm2\n\n\nCall:\nlm(formula = SLPWKEND ~ SLPWKDAY + SLPTROUB, data = dat1)\n\nCoefficients:\n(Intercept)     SLPWKDAY     SLPTROUB  \n     4.4813       0.5301      -0.2862  \n\n\n\nextract_eq(m2, use_coefs = TRUE, coef_digits = 3,\n           wrap = TRUE, operator_location = \"start\")\n\n\\[\n\\begin{aligned}\n\\operatorname{\\widehat{SLPWKEND}} &= 4.481 + 0.53(\\operatorname{SLPWKDAY}) - 0.286(\\operatorname{SLPTROUB})\n\\end{aligned}\n\\]\n\n\nNote that the slopes of both SLPWKDAY and SLPTROUB have changed from model m1 (although not very much), and that the intercept has changed more substantially.\n\n\n5.2.8 Using broom functions on m2\n\ntidy(m2, conf.int = TRUE, conf.level = 0.90) |> \n  mutate(across(.cols = -c(term), num, digits = 3)) |>\n  gt()\n\n\n\n\n\n  \n  \n    \n      term\n      estimate\n      std.error\n      statistic\n      p.value\n      conf.low\n      conf.high\n    \n  \n  \n    (Intercept)\n4.481\n0.219\n20.475\n0.000\n4.121\n4.842\n    SLPWKDAY\n0.530\n0.029\n18.268\n0.000\n0.482\n0.578\n    SLPTROUB\n-0.286\n0.095\n-3.025\n0.003\n-0.442\n-0.130\n  \n  \n  \n\n\n\n\n\nglance(m2) |> \n  select(r2 = r.squared, adjr2 = adj.r.squared, \n         AIC, BIC, sigma, nobs, df, df.res = df.residual) |>\n  gt()\n\n\n\n\n\n  \n  \n    \n      r2\n      adjr2\n      AIC\n      BIC\n      sigma\n      nobs\n      df\n      df.res\n    \n  \n  \n    0.2101295\n0.2089049\n4824.099\n4844.758\n1.559861\n1293\n2\n1290\n  \n  \n  \n\n\n\n\nSince we want to compare the fit of m1 to that of m2, we probably want to do so in a single table, like this:\n\ntemp1 <- glance(m1) |> \n  select(r2 = r.squared, adjr2 = adj.r.squared, \n         AIC, BIC, sigma, nobs, df, df.res = df.residual) |>\n  mutate(model = \"m1\") |>\n  relocate(model)\n\ntemp2 <- glance(m2) |> \n  select(r2 = r.squared, adjr2 = adj.r.squared, \n         AIC, BIC, sigma, nobs, df, df.res = df.residual) |>\n  mutate(model = \"m2\") |>\n  relocate(model)\n\nbind_rows(temp1, temp2) |>\n  gt()\n\n\n\n\n\n  \n  \n    \n      model\n      r2\n      adjr2\n      AIC\n      BIC\n      sigma\n      nobs\n      df\n      df.res\n    \n  \n  \n    m1\n0.2171080\n0.2146766\n4816.624\n4847.613\n1.554160\n1293\n4\n1288\n    m2\n0.2101295\n0.2089049\n4824.099\n4844.758\n1.559861\n1293\n2\n1290\n  \n  \n  \n\n\n\n\nEach model uses the same number of observations to predict the same outcome (SLPWKEND). So we can compare them directly. As compared to model m2, model m1 has:\n\nthe larger \\(R^2\\) (as it must, since model m2 includes a subset of the predictors in model m1),\nthe larger adjusted \\(R^2\\),\nthe smaller AIC (Akaike Information Criterion: smaller values are better),\nthe larger BIC (Bayes Information Criterion: again, smaller values are better),\n\nand the smaller residual standard error (\\(\\sigma\\)) (smaller values are better.)\n\nThe key realizations for these data are that the AIC, adjusted \\(R^2\\) and \\(\\sigma\\) results favor model m1 while the BIC favors model m2.\n\n\n5.2.9 Residual Plots for Model m2\n\n## I used #| fig.height: 8 in this code chunk\n## to make the plots tall enough to see well\n\npar(mfrow = c(2,2)); plot(m2); par(mfrow = c(1,1))\n\n\n\n\nThe residual plots here show (even more starkly than in model m1) the discrete nature of our outcome and the two variables we’re using to predict it. I see no especially serious problems with the assumptions of linearity or constant variance here, and while there are still some fairly poorly fit values, there are no highly influential points, so I’ll accept these residual plots as indicative of a fairly reasonable model on the whole.\n\n\n5.2.10 Conclusions\nThree of our four in-sample measures of fit quality (AIC, \\(\\sigma\\) and adjusted \\(R^2\\)) favor the larger model m1 over m2, but there’s not a lot to choose from here. Neither model showed important problems with regression assumptions, so I would probably wind up choosing m1 based on the analyses we’ve done in this Chapter.\nHowever, a more appropriate strategy for prediction assessment would be to partition the data into separate samples for model training (the development or building sample) and model testing. We adopt such a model validation strategy in our next little study."
  },
  {
    "objectID": "431review3.html#modeling-high-sensitivity-c-reactive-protein",
    "href": "431review3.html#modeling-high-sensitivity-c-reactive-protein",
    "title": "5  431 Review: Fitting Linear Models",
    "section": "5.3 Modeling High-Sensitivity C-Reactive Protein",
    "text": "5.3 Modeling High-Sensitivity C-Reactive Protein\nIn this, our second linear modeling example, we will try to predict High-Sensitivity C-Reactive Protein levels (HSCRP) on the basis of these three predictor variables:\n\nthe participant’s mean pulse rate, specifically the mean of the two gathered pulse rates, PULSE1 and PULSE2\nthe participant’s self-reported overall health (SROH, which is an ordinal factor with levels Excellent, Very Good, Good, Fair and Poor)\nHOSPITAL, a 1-0 binary variable indicating whether or not the participant was hospitalized in the past year.\n\nIn this case, we’ll use all NHANES participants with complete data on the relevant variables to fit the three-predictor model, and then a second model using mean pulse rate alone.\n\ndat2 <- nh432 |>\n  select(SEQN, HSCRP, PULSE1, PULSE2, SROH, HOSPITAL) |>\n  drop_na() |>\n  mutate(MEANPULSE = 0.5*(PULSE1 + PULSE2))\n\nglimpse(dat2)\n\nRows: 3,117\nColumns: 7\n$ SEQN      <chr> \"109271\", \"109273\", \"109291\", \"109292\", \"109293\", \"109295\", …\n$ HSCRP     <dbl> 28.68, 0.98, 5.31, 3.08, 15.10, 6.28, 0.56, 1.45, 0.32, 0.86…\n$ PULSE1    <dbl> 73, 71, 77, 93, 62, 93, 74, 59, 66, 83, 64, 55, 54, 63, 68, …\n$ PULSE2    <dbl> 71, 70, 76, 91, 64, 93, 74, 58, 64, 87, 68, 55, 54, 63, 70, …\n$ SROH      <fct> Fair, Good, Fair, Very Good, Good, Good, Very Good, Excellen…\n$ HOSPITAL  <dbl> 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ MEANPULSE <dbl> 72.0, 70.5, 76.5, 92.0, 63.0, 93.0, 74.0, 58.5, 65.0, 85.0, …\n\n\n\n5.3.1 Partitioning the Data\nBefore partitioning, it’s always a good idea to be sure that the number of rows in the tibble matches the number of distinct (unique) values in the identifier column.\n\nidentical(nrow(dat2), n_distinct(dat2 |> select(SEQN)))\n\n[1] TRUE\n\n\nOK. Now, be sure to set a seed so that we can replicate the selection. We’ll put 70% of the data in our training sample, setting aside the remaining 30% for the test sample.\n\nset.seed(432005)\n\ndat2_train <- slice_sample(dat2, prop = 0.70)\n\ndat2_test <- anti_join(dat2, dat2_train, by = \"SEQN\")\n\nc(nrow(dat2), nrow(dat2_train), nrow(dat2_test))\n\n[1] 3117 2181  936\n\n\nIn what follows, we’ll work with the dat2_train sample, and set aside the dat2_test sample for a while.\n\n\n5.3.2 Transforming the Outcome?\nLet’s use the Box-Cox approach to help us think about which potential transformations of our outcome might be helpful, within our training sample.\n\nm_temp <- lm(HSCRP ~ MEANPULSE + SROH + HOSPITAL, data = dat2_train)\n\nboxCox(m_temp)\n\n\n\n\nThe estimated \\(\\lambda\\) value is very close to 0, which according to the ladder of power transformations, suggests we take the logarithm of our outcome, so as to improve the residual plots for the model. This will also, as it turns out, lead to a much less right-skewed outcome variable.\n\np1 <- ggplot(dat2_train, aes(sample = HSCRP)) +\n  geom_qq() + geom_qq_line(col = \"red\") +\n  labs(title = \"Normal Q-Q: untransformed HSCRP\")\n\np2 <- ggplot(dat2_train, aes(sample = log(HSCRP))) +\n  geom_qq() + geom_qq_line(col = \"red\") +\n  labs(title = \"Normal Q-Q: Natural Log of HSCRP\")\n\np1 + p2\n\n\n\n\nClearly, one benefit of the transformation is some improvement in the Normality of our outcome’s distribution.\n\n\n5.3.3 Scatterplot Matrix and Collinearity\nTo build the relevant scatterplot matrix with our transformed outcome, I’ll create a variable containing the result of the transformation within our training sample.\n\ndat2_train <- dat2_train |>\n  mutate(logHSCRP = log(HSCRP))\n\nnames(dat2_train)\n\n[1] \"SEQN\"      \"HSCRP\"     \"PULSE1\"    \"PULSE2\"    \"SROH\"      \"HOSPITAL\" \n[7] \"MEANPULSE\" \"logHSCRP\" \n\nggpairs(dat2_train, columns = c(7,5,6,8), switch = \"both\",\n        lower=list(combo=wrap(\"facethist\", bins=25)))\n\n\n\n\nAs a collinearity check, we’ll run vif() from the car package here.\n\nm3 <- lm(log(HSCRP) ~ MEANPULSE + SROH + HOSPITAL,\n         data = dat2_train)\n\nvif(m3)\n\n              GVIF Df GVIF^(1/(2*Df))\nMEANPULSE 1.036382  1        1.018028\nSROH      1.063545  4        1.007731\nHOSPITAL  1.027860  1        1.013834\n\n\nAgain, no signs of meaningful collinearity. Note the presentation of the factor variable SROH in the scatterplot matrix, and in the generalized VIF output.\n\n\n5.3.4 Fit Model m3\n\nm3 <- lm(log(HSCRP) ~ MEANPULSE + SROH + HOSPITAL,\n         data = dat2_train)\n\n\nextract_eq(m3, use_coefs = TRUE, coef_digits = 3,\n           terms_per_line = 3, wrap = TRUE, \n           operator_location = \"start\")\n\n\\[\n\\begin{aligned}\n\\operatorname{\\widehat{log(HSCRP)}} &= -1.108 + 0.02(\\operatorname{MEANPULSE}) + 0.237(\\operatorname{SROH}_{\\operatorname{Very\\ Good}})\\\\\n&\\quad + 0.532(\\operatorname{SROH}_{\\operatorname{Good}}) + 0.641(\\operatorname{SROH}_{\\operatorname{Fair}}) + 0.86(\\operatorname{SROH}_{\\operatorname{Poor}})\\\\\n&\\quad + 0.052(\\operatorname{HOSPITAL})\n\\end{aligned}\n\\]\n\n\n\ntidy(m3, conf.int = TRUE, conf.level = 0.90) |>\n  mutate(across(.cols = -c(term), num, digits = 3)) |>\n  gt()\n\n\n\n\n\n  \n  \n    \n      term\n      estimate\n      std.error\n      statistic\n      p.value\n      conf.low\n      conf.high\n    \n  \n  \n    (Intercept)\n-1.108\n0.160\n-6.914\n0.000\n-1.372\n-0.845\n    MEANPULSE\n0.020\n0.002\n9.359\n0.000\n0.017\n0.024\n    SROHVery Good\n0.237\n0.081\n2.939\n0.003\n0.104\n0.370\n    SROHGood\n0.532\n0.078\n6.857\n0.000\n0.404\n0.660\n    SROHFair\n0.641\n0.087\n7.353\n0.000\n0.497\n0.784\n    SROHPoor\n0.860\n0.143\n6.020\n0.000\n0.625\n1.095\n    HOSPITAL\n0.052\n0.088\n0.594\n0.552\n-0.092\n0.197\n  \n  \n  \n\n\n\n\n\nIf Harry and Sally have the same values of HOSPITAL and MEANPULSE, but Harry’s SROH is “Very Good” while Sally’s is “Excellent”, then model m3 predicts that Harry will have a log(HSCRP) that is 0.237 (90% CI: 0.104, 0.370) larger than Sally’s log(HSCRP).\nOn the other hand, if Harry and Gary have the same values of HOSPITAL and MEANPULSE, but Harry’s SROH remains “Very Good” while Gary’s is only “Good”, then model m3 predicts that Gary will have a log(HSCRP) that is (0.532 - 0.237 = 0.295) larger than Harry’s log(HSCRP).\n\n\n\n5.3.5 Residual Plots for m3\n\n## don't forget to use #| fig.height: 8\n## to make the residual plots taller\n\npar(mfrow = c(2,2)); plot(m3); par(mfrow = c(1,1))\n\n\n\n\nI see no serious concerns with regression assumptions here. The residuals vs. fitted plot shows no signs of meaningful non-linearity or heteroscedasticity. The standardized residuals in the Normal Q-Q plot follow the reference line closely. There is no clear trend in the scale-location plot, and the residuals vs. leverage plot reveals no particularly influential points.\n\n\n5.3.6 Fit Model m4\nLet’s now fit the simple regression model, m4, with only MEANPULSE as a predictor of the log of HSCRP.\n\nm4 <- lm(log(HSCRP) ~ MEANPULSE,\n         data = dat2_train)\n\n\nextract_eq(m4, use_coefs = TRUE, coef_digits = 3,\n           wrap = TRUE, operator_location = \"start\")\n\n\\[\n\\begin{aligned}\n\\operatorname{\\widehat{log(HSCRP)}} &= -0.929 + 0.023(\\operatorname{MEANPULSE})\n\\end{aligned}\n\\]\n\n\nNow, let’s look at the tidied coefficients.\n\ntidy(m4, conf.int = TRUE, conf.level = 0.90) |>\n  mutate(across(.cols = -c(term), num, digits = 3)) |>\n  gt()\n\n\n\n\n\n  \n  \n    \n      term\n      estimate\n      std.error\n      statistic\n      p.value\n      conf.low\n      conf.high\n    \n  \n  \n    (Intercept)\n-0.929\n0.154\n-6.048\n0.000\n-1.181\n-0.676\n    MEANPULSE\n0.023\n0.002\n10.934\n0.000\n0.020\n0.027\n  \n  \n  \n\n\n\n\n\nIf Harry’s mean pulse rate is one beat per minute higher than Sally’s, then model m4 predicts that the logarithm of Harry’s HSCRP will be 0.023 higher than Sally’s, with 90% CI (0.020, 0.027).\nNote that if Harry’s mean pulse rate is ten beats per minute higher than Sally’s, then model m4 predicts that the logarithm of Harry’s HSCRP will be 0.23 higher than Sally’s, with 90% CI (0.20, 0.27).\n\n\n\n5.3.7 Residual Plots for m4\n\npar(mfrow = c(2,2)); plot(m4); par(mfrow = c(1,1))\n\n\n\n\n\n\n5.3.8 In-Sample Fit Quality Comparison (m3 vs. m4)\n\ng3 <- glance(m3) |> \n  select(r2 = r.squared, adjr2 = adj.r.squared, \n         AIC, BIC, sigma, nobs, df, df.res = df.residual) |>\n  mutate(model = \"m3\") |>\n  relocate(model)\n\ng4 <- glance(m4) |> \n  select(r2 = r.squared, adjr2 = adj.r.squared, \n         AIC, BIC, sigma, nobs, df, df.res = df.residual) |>\n  mutate(model = \"m4\") |>\n  relocate(model)\n\nbind_rows(g3, g4) |> gt()\n\n\n\n\n\n  \n  \n    \n      model\n      r2\n      adjr2\n      AIC\n      BIC\n      sigma\n      nobs\n      df\n      df.res\n    \n  \n  \n    m3\n0.09070792\n0.08819837\n6636.024\n6681.525\n1.105532\n2181\n6\n2174\n    m4\n0.05200815\n0.05157309\n6716.928\n6733.990\n1.127517\n2181\n1\n2179\n  \n  \n  \n\n\n\n\nThe larger model (model m3) has better results than model m4 in the sense that it produces a larger adjusted \\(R^2\\), and smaller values for AIC, BIC and \\(\\sigma\\). Based on this comparison within the training sample, we clearly prefer m3, since each model shows reasonable adherence to the assumptions of a linear regression model.\n\n\n5.3.9 Testing the models in new data\nAt last we return to the dat2_test sample which was not used in fitting models m3 and m4 to investigate which of these models has better predictive results in new data. When doing this sort of testing, I recommend a look at the following 4 summaries, each of which is based on the fitted (predicted) and observed values of our outcome in our new data, using the models we want to compare:\n\nsquared correlation of predicted with observed values (validated \\(R^2\\); higher values are better)\nmean absolute prediction error (MAPE; smaller values indicate smaller errors, hence better prediction)\nsquare root of the mean squared prediction error (RMSPE; again, smaller values indicate better predictions)\nmaximum (in absolute value) prediction error (Max Error)\n\nTo obtain observed, predicted, and error (observed - predicted) values for each new data point when we apply model m3, we first use the augment() function from the broom package to obtain our .fitted values.\n\nm3_test_aug <- augment(m3, newdata = dat2_test)\nhead(m3_test_aug)\n\n# A tibble: 6 × 9\n  SEQN   HSCRP PULSE1 PULSE2 SROH      HOSPITAL MEANPULSE .fitted .resid\n  <chr>  <dbl>  <dbl>  <dbl> <fct>        <dbl>     <dbl>   <dbl>  <dbl>\n1 109273  0.98     71     70 Good             0      70.5   0.839 -0.859\n2 109293 15.1      62     64 Good             0      63     0.688  2.03 \n3 109312  0.86     83     87 Very Good        0      85     0.835 -0.985\n4 109332  2.29     63     63 Excellent        0      63     0.156  0.673\n5 109340  4.64     78     78 Fair             0      78     1.10   0.437\n6 109342  5.51     72     70 Good             0      71     0.849  0.858\n\n\nRemember, however, that our models m3 and m4 do not predict HSCRP, but rather the logarithm of HSCRP, so we need to exponentiate the .fitted values to get what we want.\n\nm3_test_aug <- augment(m3, newdata = dat2_test) |>\n  mutate(fits = exp(.fitted),\n         resid = HSCRP - fits) |>\n  select(SEQN, HSCRP, fits, resid, everything())\n\nhead(m3_test_aug)\n\n# A tibble: 6 × 11\n  SEQN   HSCRP  fits resid PULSE1 PULSE2 SROH     HOSPI…¹ MEANP…² .fitted .resid\n  <chr>  <dbl> <dbl> <dbl>  <dbl>  <dbl> <fct>      <dbl>   <dbl>   <dbl>  <dbl>\n1 109273  0.98  2.31 -1.33     71     70 Good           0    70.5   0.839 -0.859\n2 109293 15.1   1.99 13.1      62     64 Good           0    63     0.688  2.03 \n3 109312  0.86  2.30 -1.44     83     87 Very Go…       0    85     0.835 -0.985\n4 109332  2.29  1.17  1.12     63     63 Excelle…       0    63     0.156  0.673\n5 109340  4.64  3.00  1.64     78     78 Fair           0    78     1.10   0.437\n6 109342  5.51  2.34  3.17     72     70 Good           0    71     0.849  0.858\n# … with abbreviated variable names ¹​HOSPITAL, ²​MEANPULSE\n\n\nNow, we can obtain our summaries, as follows.\n\nm3_test_results <- m3_test_aug |>\n  summarize(validated_R_sq = cor(HSCRP, fits)^2,\n            MAPE = mean(abs(resid)),\n            RMSPE = sqrt(mean(resid^2)),\n            max_Error = max(abs(resid)))\n\nm3_test_results\n\n# A tibble: 1 × 4\n  validated_R_sq  MAPE RMSPE max_Error\n           <dbl> <dbl> <dbl>     <dbl>\n1          0.114  3.45  10.7      177.\n\n\nFor model m4, we have:\n\nm4_test_aug <- augment(m4, newdata = dat2_test) |>\n  mutate(fits = exp(.fitted),\n         resid = HSCRP - fits) |>\n  select(SEQN, HSCRP, fits, resid, everything())\n\nm4_test_results <- m4_test_aug |>\n  summarize(validated_R_sq = cor(HSCRP, fits)^2,\n            MAPE = mean(abs(resid)),\n            RMSPE = sqrt(mean(resid^2)),\n            max_Error = max(abs(resid)))\n\nm4_test_results\n\n# A tibble: 1 × 4\n  validated_R_sq  MAPE RMSPE max_Error\n           <dbl> <dbl> <dbl>     <dbl>\n1          0.102  3.50  10.8      177.\n\n\nAnd we can put the two sets of results together into a nice table.\n\nbind_rows(m3_test_results, m4_test_results) |>\n  mutate(model = c(\"m3\", \"m4\")) |>\n  relocate(model) |>\n  gt()\n\n\n\n\n\n  \n  \n    \n      model\n      validated_R_sq\n      MAPE\n      RMSPE\n      max_Error\n    \n  \n  \n    m3\n0.1135858\n3.451295\n10.72894\n176.9855\n    m4\n0.1015280\n3.496673\n10.79613\n177.0716\n  \n  \n  \n\n\n\n\nBased on these out-of-sample validation results, it seems that Model m3 has the better results across each of these four summaries than Model 4 does.\n\n\n5.3.10 Conclusions\nWe fit two models to predict HSCRP, a larger model (m3) containing three predictors (MEANPULSE, SROH and HOSPITAL), and a smaller model (m4) containing only the MEANPULSE as a predictor.\n\nBoth models (after transforming to log(HSCRP) for our outcome) seem to generally meet the assumptions of linear regression\nModel m3 had a raw \\(R^2\\) value of 0.091, so it accounted for about 9.1% of the variation in log(HSCRP) within our training sample. Model m4 accounted for 5.2%.\nIn our in-sample checks, Model m3 had better results in terms of adjusted \\(R^2\\), AIC, BIC and \\(\\sigma\\).\nIn a validation (test) sample, our Model m3 also showed superior predictive performance, including better results in terms of MAPE, RMSPE and maximum absolute error, as well as a validated \\(R^2\\) of 11.4%, higher than model m4’s result of 10.2%.\n\nOverall, model m3 seems like the meaningfully better choice."
  },
  {
    "objectID": "smart.html#r-setup-used-here",
    "href": "smart.html#r-setup-used-here",
    "title": "6  BRFSS SMART Data",
    "section": "6.1 R Setup Used Here",
    "text": "6.1 R Setup Used Here\n\nknitr::opts_chunk$set(comment = NA)\n\nlibrary(janitor) \nlibrary(Hmisc)\nlibrary(patchwork)\nlibrary(tidyverse) \n\ntheme_set(theme_bw())"
  },
  {
    "objectID": "smart.html#key-resources",
    "href": "smart.html#key-resources",
    "title": "6  BRFSS SMART Data",
    "section": "6.2 Key resources",
    "text": "6.2 Key resources\n\nthe “raw” data, in the form of the 2017 SMART BRFSS MMSA Data, found in a zipped SAS Transport Format file. The data were released in October 2018.\nthe MMSA Variable Layout which simply lists the variables included in the data file\nthe Calculated Variables PDF which describes the risk factors by data variable names - there is also an online summary matrix of these calculated variables.\nthe lengthy 2017 Survey Questions PDF which lists all questions asked as part of the BRFSS in 2017\nthe enormous Codebook for the 2017 BRFSS Survey PDF which identifies the variables by name for us.\n\nAlso, for each subject, we are also provided with a sampling weight, in _MMSAWT, which will help us incorporate the sampling design later. These weights are at the MMSA level, and are used for generating MMSA-level estimates for variables in the data set. Details on the weighting methodology are available at this PDF."
  },
  {
    "objectID": "smart.html#ingesting-the-raw-data",
    "href": "smart.html#ingesting-the-raw-data",
    "title": "6  BRFSS SMART Data",
    "section": "6.3 Ingesting the Raw Data",
    "text": "6.3 Ingesting the Raw Data\nTo create the data files we’ll use, I used the read_xpt function from the haven package to bring in the SAS XPT data file that is provided by CDC. The codes I used (but won’t use in these Notes) were:\n\nsmart_raw <- read_xpt(\"MMSA2017/MMSA2017.xpt\")\n\nThis gives the nationwide data, which has 230,875 rows and 177 columns.\nBut for the purposes of putting these Notes online, I needed to crank down the sample size enormously. To that end, I created a new data file, which I developed by\n\nimporting the MMSA2017.xpt file as above\nfiltering away all observations except those from MMSAs which include Ohio in their name, and\nsaving the result, which now has 7,412 rows and 177 columns.\n\nThe code (again, not run here) that I used to filter to the OH-based MMSAs was:\n\nsmart_ohio_raw <- smart_raw |> \n    filter(str_detect(MMSANAME, \"OH\"))\n\nwrite_csv(smart_ohio_raw, \"data/smart_ohio_raw.csv\")\n\nSo, for purposes of these notes, our complete data set is actually coming from smart_ohio_raw.csv and consists only of the 7,412 observations associated with the six MMSAs that include Ohio in their names."
  },
  {
    "objectID": "smart.html#ingesting-from-our-csv-file",
    "href": "smart.html#ingesting-from-our-csv-file",
    "title": "6  BRFSS SMART Data",
    "section": "6.4 Ingesting from our CSV file",
    "text": "6.4 Ingesting from our CSV file\nNote that the smart_ohio_raw.csv and other data files we’re developing in this Chapter are available on our 432-Data website\n\nsmart_ohio_raw <- read_csv(\"data/smart_ohio_raw.csv\", show_col_types = FALSE)\n\ndim(smart_ohio_raw)\n\n[1] 7412  177"
  },
  {
    "objectID": "smart.html#what-does-the-raw-data-look-like",
    "href": "smart.html#what-does-the-raw-data-look-like",
    "title": "6  BRFSS SMART Data",
    "section": "6.5 What does the raw data look like?",
    "text": "6.5 What does the raw data look like?\nHere is a list of all variable names included in this file. We’re not going to use all of those variables, but this will give you a sense of what is available.\n\nnames(smart_ohio_raw)\n\n  [1] \"DISPCODE\" \"STATERE1\" \"SAFETIME\" \"HHADULT\"  \"GENHLTH\"  \"PHYSHLTH\"\n  [7] \"MENTHLTH\" \"POORHLTH\" \"HLTHPLN1\" \"PERSDOC2\" \"MEDCOST\"  \"CHECKUP1\"\n [13] \"BPHIGH4\"  \"BPMEDS\"   \"CHOLCHK1\" \"TOLDHI2\"  \"CHOLMED1\" \"CVDINFR4\"\n [19] \"CVDCRHD4\" \"CVDSTRK3\" \"ASTHMA3\"  \"ASTHNOW\"  \"CHCSCNCR\" \"CHCOCNCR\"\n [25] \"CHCCOPD1\" \"HAVARTH3\" \"ADDEPEV2\" \"CHCKIDNY\" \"DIABETE3\" \"DIABAGE2\"\n [31] \"LMTJOIN3\" \"ARTHDIS2\" \"ARTHSOCL\" \"JOINPAI1\" \"SEX\"      \"MARITAL\" \n [37] \"EDUCA\"    \"RENTHOM1\" \"NUMHHOL2\" \"NUMPHON2\" \"CPDEMO1A\" \"VETERAN3\"\n [43] \"EMPLOY1\"  \"CHILDREN\" \"INCOME2\"  \"INTERNET\" \"WEIGHT2\"  \"HEIGHT3\" \n [49] \"PREGNANT\" \"DEAF\"     \"BLIND\"    \"DECIDE\"   \"DIFFWALK\" \"DIFFDRES\"\n [55] \"DIFFALON\" \"SMOKE100\" \"SMOKDAY2\" \"STOPSMK2\" \"LASTSMK2\" \"USENOW3\" \n [61] \"ECIGARET\" \"ECIGNOW\"  \"ALCDAY5\"  \"AVEDRNK2\" \"DRNK3GE5\" \"MAXDRNKS\"\n [67] \"FRUIT2\"   \"FRUITJU2\" \"FVGREEN1\" \"FRENCHF1\" \"POTATOE1\" \"VEGETAB2\"\n [73] \"EXERANY2\" \"EXRACT11\" \"EXEROFT1\" \"EXERHMM1\" \"EXRACT21\" \"EXEROFT2\"\n [79] \"EXERHMM2\" \"STRENGTH\" \"SEATBELT\" \"FLUSHOT6\" \"FLSHTMY2\" \"PNEUVAC3\"\n [85] \"SHINGLE2\" \"HIVTST6\"  \"HIVTSTD3\" \"HIVRISK5\" \"CASTHDX2\" \"CASTHNO2\"\n [91] \"CALLBCKZ\" \"WDUSENOW\" \"WDINFTRK\" \"WDHOWOFT\" \"WDSHARE\"  \"NAMTRIBE\"\n [97] \"NAMOTHR\"  \"_URBNRRL\" \"_STSTR\"   \"_IMPSEX\"  \"_RFHLTH\"  \"_PHYS14D\"\n[103] \"_MENT14D\" \"_HCVU651\" \"_RFHYPE5\" \"_CHOLCH1\" \"_RFCHOL1\" \"_MICHD\"  \n[109] \"_LTASTH1\" \"_CASTHM1\" \"_ASTHMS1\" \"_DRDXAR1\" \"_LMTACT1\" \"_LMTWRK1\"\n[115] \"_LMTSCL1\" \"_PRACE1\"  \"_MRACE1\"  \"_HISPANC\" \"_RACE\"    \"_RACEG21\"\n[121] \"_RACEGR3\" \"_AGEG5YR\" \"_AGE65YR\" \"_AGE80\"   \"_AGE_G\"   \"WTKG3\"   \n[127] \"_BMI5\"    \"_BMI5CAT\" \"_RFBMI5\"  \"_EDUCAG\"  \"_INCOMG\"  \"_SMOKER3\"\n[133] \"_RFSMOK3\" \"_ECIGSTS\" \"_CURECIG\" \"DRNKANY5\" \"_RFBING5\" \"_DRNKWEK\"\n[139] \"_RFDRHV5\" \"FTJUDA2_\" \"FRUTDA2_\" \"GRENDA1_\" \"FRNCHDA_\" \"POTADA1_\"\n[145] \"VEGEDA2_\" \"_MISFRT1\" \"_MISVEG1\" \"_FRTRES1\" \"_VEGRES1\" \"_FRUTSU1\"\n[151] \"_VEGESU1\" \"_FRTLT1A\" \"_VEGLT1A\" \"_FRT16A\"  \"_VEG23A\"  \"_FRUITE1\"\n[157] \"_VEGETE1\" \"_TOTINDA\" \"_MINAC11\" \"_MINAC21\" \"_PACAT1\"  \"_PAINDX1\"\n[163] \"_PA150R2\" \"_PA300R2\" \"_PA30021\" \"_PASTRNG\" \"_PAREC1\"  \"_PASTAE1\"\n[169] \"_RFSEAT2\" \"_RFSEAT3\" \"_FLSHOT6\" \"_PNEUMO2\" \"_AIDTST3\" \"_MMSA\"   \n[175] \"_MMSAWT\"  \"SEQNO\"    \"MMSANAME\""
  },
  {
    "objectID": "smart.html#cleaning-the-brfss-data",
    "href": "smart.html#cleaning-the-brfss-data",
    "title": "6  BRFSS SMART Data",
    "section": "6.6 Cleaning the BRFSS Data",
    "text": "6.6 Cleaning the BRFSS Data\n\n6.6.1 Identifying Information\nThe identifying variables for each subject are gathered in SEQNO, which I’ll leave alone.\n\nEach statistical (geographic) area is identified by a _MMSA variable, which I’ll rename mmsa_code, and by an MMSANAME which I’ll rename as mmsa_name\nFor each subject, we are also provided with a sampling weight, in _MMSAWT, which will help us incorporate the sampling design later in the semester. We’ll rename this as mmsa_wt. Details on the weighting methodology are available at https://www.cdc.gov/brfss/annual_data/2017/pdf/2017_SMART_BRFSS_MMSA_Methodology-508.pdf\n\n\nsmart_ohio_raw <- smart_ohio_raw |>\n    mutate(mmsa_code = `_MMSA`,\n           mmsa_name = `MMSANAME`,\n           mmsa_wt = `_MMSAWT`)\n\nsmart_ohio_raw |> count(mmsa_code, mmsa_name)\n\n# A tibble: 6 × 3\n  mmsa_code mmsa_name                                                       n\n      <dbl> <chr>                                                       <int>\n1     17140 Cincinnati, OH-KY-IN, Metropolitan Statistical Area          1737\n2     17460 Cleveland-Elyria, OH, Metropolitan Statistical Area          1133\n3     18140 Columbus, OH, Metropolitan Statistical Area                  2033\n4     19380 Dayton, OH, Metropolitan Statistical Area                     587\n5     26580 Huntington-Ashland, WV-KY-OH, Metropolitan Statistical Area  1156\n6     45780 Toledo, OH, Metropolitan Statistical Area                     766\n\n\nThose names are very long. I’ll build some shorter ones, by dropping everything after the comma.\n\nsmart_ohio_raw <- smart_ohio_raw |>\n    mutate(mmsa = str_replace_all(string = mmsa_name, pattern=\"\\\\,.*$\",replacement=\" \"))\n\nsmart_ohio_raw |> count(mmsa, mmsa_name)\n\n# A tibble: 6 × 3\n  mmsa                  mmsa_name                                              n\n  <chr>                 <chr>                                              <int>\n1 \"Cincinnati \"         Cincinnati, OH-KY-IN, Metropolitan Statistical Ar…  1737\n2 \"Cleveland-Elyria \"   Cleveland-Elyria, OH, Metropolitan Statistical Ar…  1133\n3 \"Columbus \"           Columbus, OH, Metropolitan Statistical Area         2033\n4 \"Dayton \"             Dayton, OH, Metropolitan Statistical Area            587\n5 \"Huntington-Ashland \" Huntington-Ashland, WV-KY-OH, Metropolitan Statis…  1156\n6 \"Toledo \"             Toledo, OH, Metropolitan Statistical Area            766\n\n\nAnd here are the sampling weights for the subjects in the Cleveland-Elyria MSA.\n\nsmart_ohio_raw |> \n    filter(mmsa_code == 17460) %>%\n    ggplot(., aes(x = mmsa_wt)) +\n    geom_histogram(bins = 30, fill = \"blue\", col = \"white\")\n\n\n\n\n\n\n6.6.2 Survey Method\n\n6.6.2.1 DISPCODE and its cleanup to completed\nDISPCODE which is 1100 if the subject completed the interview, and 1200 if they partially completed the interview. We’ll create a variable called completed that indicates (1 = complete, 0 = not) whether the subject completed the interview.\n\nsmart_ohio_raw <- smart_ohio_raw |>\n    mutate(completed = 12 - (DISPCODE/100))\n\nsmart_ohio_raw |> count(DISPCODE, completed)\n\n# A tibble: 2 × 3\n  DISPCODE completed     n\n     <dbl>     <dbl> <int>\n1     1100         1  6277\n2     1200         0  1135\n\n\n\n\n6.6.2.2 STATERE1 and SAFETIME and their reduction to landline\nBRFSSS is conducted by telephone. The next two variables help us understand whether the subject was contacted via land line or via cellular phone.\n\nSTATERE1 is 1 if the subject is a resident of the state (only asked of people in the land line version of the survey).\nSAFETIME is 1 if this is a safe time to talk (only asked of people in the cell phone version of the survey).\nWe’ll use STATERE1 and SAFETIME to create an indicator variable landline that specifies how the respondent was surveyed (1 = land line, 0 = cell phone), as follows…\n\n\nsmart_ohio_raw <- smart_ohio_raw |> \n    mutate(landline = replace_na(STATERE1, 0))\n\nsmart_ohio_raw |> count(STATERE1, SAFETIME, landline)\n\n# A tibble: 2 × 4\n  STATERE1 SAFETIME landline     n\n     <dbl>    <dbl>    <dbl> <int>\n1        1       NA        1  3649\n2       NA        1        0  3763\n\n\n\n\n6.6.2.3 HHADULT and its cleanup to hhadults\n\nHHADULT is the response to “How many members of your household, including yourself, are 18 years of age or older?”\n\nThe permitted responses range from 1-76, with special values 77 for Don’t Know/Not Sure and 99 for refused, with BLANK for missing or not asked.\nSo we should change all numerical values above 76 to NA for our analyses (the blanks are already regarded as NAs by R in the ingestion process.)\n\n\n\nsmart_ohio_raw |> tabyl(HHADULT)\n\n HHADULT    n      percent valid_percent\n       1  274 0.0369670804   0.236206897\n       2  603 0.0813545602   0.519827586\n       3  170 0.0229357798   0.146551724\n       4   73 0.0098488937   0.062931034\n       5   28 0.0037776579   0.024137931\n       6    4 0.0005396654   0.003448276\n       7    3 0.0004047491   0.002586207\n       8    1 0.0001349164   0.000862069\n      10    1 0.0001349164   0.000862069\n      11    1 0.0001349164   0.000862069\n      99    2 0.0002698327   0.001724138\n      NA 6252 0.8434970318            NA\n\n\n\nsmart_ohio_raw <- smart_ohio_raw |> \n    mutate(hhadults = HHADULT,\n           hhadults = replace(hhadults, hhadults > 76, NA))\n\nsmart_ohio_raw |> count(HHADULT, hhadults) |> tail()\n\n# A tibble: 6 × 3\n  HHADULT hhadults     n\n    <dbl>    <dbl> <int>\n1       7        7     3\n2       8        8     1\n3      10       10     1\n4      11       11     1\n5      99       NA     2\n6      NA       NA  6252\n\n\n\n\n\n6.6.3 Health Status (1 item)\nThe next variable describes relate to the subject’s health status.\n\n6.6.3.1 GENHLTH and its cleanup to genhealth\n\nGENHLTH, the General Health variable, which is the response to “Would you say that in general your health is …”\n\n1 = Excellent\n2 = Very good\n3 = Good\n4 = Fair\n5 = Poor\n7 = Don’t know/Not sure\n9 = Refused\nBLANK = Not asked or missing\n\n\nTo clean up the GENHLTH data into a new variable called genhealth we’ll need to - convince R that the 7 and 9 values are in fact best interpreted as NA, - and perhaps change the variable to a factor and incorporate the names into the levels.\n\nsmart_ohio_raw <- smart_ohio_raw |> \n    mutate(genhealth = fct_recode(factor(GENHLTH), \n                                \"1_Excellent\" = \"1\",\n                                \"2_VeryGood\" = \"2\",\n                                \"3_Good\" = \"3\",\n                                \"4_Fair\" = \"4\", \n                                \"5_Poor\" = \"5\",\n                                NULL = \"7\",\n                                NULL = \"9\"))\n\nsmart_ohio_raw |> count(GENHLTH, genhealth)\n\n# A tibble: 7 × 3\n  GENHLTH genhealth       n\n    <dbl> <fct>       <int>\n1       1 1_Excellent  1057\n2       2 2_VeryGood   2406\n3       3 3_Good       2367\n4       4 4_Fair       1139\n5       5 5_Poor        428\n6       7 <NA>           10\n7       9 <NA>            5\n\n\n\n\n\n6.6.4 Healthy Days - Health-Related Quality of Life (3 items)\nThe next three variables describe the subject’s health-related quality of life.\n\n6.6.4.1 PHYSHLTH and its cleanup to physhealth\nPHYSHLTH`, the Number of Days Physical Health Not Good variable, which is the response to “Now thinking about your physical health, which includes physical illness and injury, for how many days during the past 30 days was your physical health not good?”\n\nValues of 1-30 are numeric and reasonable.\nA value of 88 indicates “none” and should be recoded to 0.\n77 is the code for Don’t know/Not sure\n99 is the code for Refused\nBLANK indicates Not asked or missing, and R recognizes this as NA properly.\n\nTo clean up PHYSHLTH to a new variable called physhealth, we’ll need: - to convince R that the 77 and 99 values are in fact best interpreted as NA, and - to convince R that the 88 should be interpreted as 0.\n\nsmart_ohio_raw <- smart_ohio_raw |> \n    mutate(physhealth = PHYSHLTH,\n           physhealth = replace(physhealth, physhealth %in% c(77, 99), NA),\n           physhealth = replace(physhealth, physhealth == 88, 0))\n\nsmart_ohio_raw |> count(PHYSHLTH, physhealth) |> tail()\n\n# A tibble: 6 × 3\n  PHYSHLTH physhealth     n\n     <dbl>      <dbl> <int>\n1       28         28    12\n2       29         29    14\n3       30         30   677\n4       77         NA   123\n5       88          0  4380\n6       99         NA    15\n\n\nNote that we present the tail of the counts in this case so we can see what happens to the key values (77, 88, 99) of our original variable PHYSHLTH.\n\n\n6.6.4.2 MENTHLTH and its cleanup to menthealth\nMENTHLTH`, the Number of Days Mental Health Not Good variable, which is the response to “Now thinking about your mental health, which includes stress, depression, and problems with emotions, for how many days during the past 30 days was your mental health not good?”\n\nThis is coded just like the PHYSHLTH variable, so we need to do the same cleaning we did there.\n\nTo clean up MENTHLTH to a new variable called menthealth, we’ll need: - to convince R that the 77 and 99 values are in fact best interpreted as NA, and - to convince R that the 88 should be interpreted as 0.\n\nsmart_ohio_raw <- smart_ohio_raw |> \n    mutate(menthealth = MENTHLTH,\n           menthealth = replace(menthealth, menthealth %in% c(77, 99), NA),\n           menthealth = replace(menthealth, menthealth == 88, 0))\n\nsmart_ohio_raw |> count(MENTHLTH, menthealth) |> tail()\n\n# A tibble: 6 × 3\n  MENTHLTH menthealth     n\n     <dbl>      <dbl> <int>\n1       28         28     7\n2       29         29    10\n3       30         30   475\n4       77         NA    86\n5       88          0  4823\n6       99         NA    28\n\n\n\n\n6.6.4.3 POORHLTH and its cleanup to poorhealth\nPOORHLTH, the Poor Physical or Mental Health variable, which is the response to “During the past 30 days, for about how many days did poor physical or mental health keep you from doing your usual activities, such as self-care, work, or recreation?”\n\nAgain, we recode just like the PHYSHLTH variable.\n\n\nsmart_ohio_raw <- smart_ohio_raw |> \n    mutate(poorhealth = POORHLTH,\n           poorhealth = replace(poorhealth, poorhealth %in% c(77, 99), NA),\n           poorhealth = replace(poorhealth, poorhealth == 88, 0))\n\nsmart_ohio_raw |> count(POORHLTH, poorhealth) |> tail()\n\n# A tibble: 6 × 3\n  POORHLTH poorhealth     n\n     <dbl>      <dbl> <int>\n1       29         29     4\n2       30         30   382\n3       77         NA    64\n4       88          0  2194\n5       99         NA    11\n6       NA         NA  3337\n\n\nThere’s a lot more missingness in the poorhealth counts than in the other health-related quality of life measures. There’s also a strong mode at 0, and a smaller mode at 30 in each variable.\n\np1 <- ggplot(smart_ohio_raw, aes(x = physhealth)) +\n    geom_histogram(binwidth = 1, fill = \"orange\") + \n    labs(title = paste0(\"Bad Physical Health Days (\",\n                        sum(is.na(smart_ohio_raw$physhealth)),\n                        \" NA)\"))\n\np2 <- ggplot(smart_ohio_raw, aes(x = menthealth)) +\n    geom_histogram(binwidth = 1, fill = \"blue\") + \n    labs(title = paste0(\"Bad Mental Health Days (\",\n                        sum(is.na(smart_ohio_raw$menthealth)), \n                        \" NA)\"))\n\np3 <- ggplot(smart_ohio_raw, aes(x = poorhealth)) +\n    geom_histogram(binwidth = 1, fill = \"red\") + \n    labs(title = paste0(\"Unable to Do Usual Activities Days (\",\n                        sum(is.na(smart_ohio_raw$poorhealth)), \n                        \" NA)\"))\n\n(p1 + p2) / p3 +\n    plot_annotation(title = \"Health Related Quality of Life Measures in BRFSS/SMART (Ohio MMSAs)\")\n\n\n\n\n\n\n\n6.6.5 Health Care Access (4 items)\nThe next four variables relate to the subject’s health care access.\n\n6.6.5.1 HLTHPLN1 and its cleanup to healthplan\nHLTHPLN1, the Have any health care coverage variable, is the response to “Do you have any kind of health care coverage, including health insurance, prepaid plans such as HMOs, or government plans such as Medicare, or Indian Health Service?”\n\n1 = Yes\n2 = No\n7 = Don’t know/Not sure\n9 = Refused\n\nTo clean up the HLTHPLN1 data into a new variable called healthplan we’ll\n- convince R that the 7 and 9 values are in fact best interpreted as NA, - and turn it into an indicator variable, e.g., we will leave the variable as numeric, but change the values to 1 = Yes and 0 = No.\n\nsmart_ohio_raw <- smart_ohio_raw |> \n    mutate(healthplan = HLTHPLN1,\n           healthplan = replace(healthplan, healthplan %in% c(7, 9), NA),\n           healthplan = replace(healthplan, healthplan == 2, 0))\n\nsmart_ohio_raw |> count(HLTHPLN1, healthplan)\n\n# A tibble: 4 × 3\n  HLTHPLN1 healthplan     n\n     <dbl>      <dbl> <int>\n1        1          1  6994\n2        2          0   398\n3        7         NA    10\n4        9         NA    10\n\n\n\n\n6.6.5.2 PERSDOC2 and its cleanup to hasdoc and to numdocs2\nPERSDOC2, the Multiple Health Care Professionals variable, is the response to “Do you have one person you think of as your personal doctor or health care provider?” where if the response is “No”, the survey then asks “Is there more than one or is there no person who you think of as your personal doctor or health care provider?”\n\n1 = Yes, only one\n2 = More than one\n3 = No\n7 = Don’t know/Not sure\n9 = Refused\nBLANK = Not asked or missing\n\nTo clean up the PERSDOC2 data into a new variable called hasdoc we’ll\n- convince R that the 7 and 9 values are in fact best interpreted as NA, - and turn it into an indicator variable, e.g., we will leave the variable as numeric, but change the values to 1 = Yes and 0 = No, so that the original 1 and 2 become 1, and the original 3 becomes 0.\n\nsmart_ohio_raw <- smart_ohio_raw |> \n    mutate(hasdoc = PERSDOC2,\n           hasdoc = replace(hasdoc, hasdoc %in% c(7, 9), NA),\n           hasdoc = replace(hasdoc, hasdoc %in% c(1, 2), 1),\n           hasdoc = replace(hasdoc, hasdoc == 3, 0))\n\nsmart_ohio_raw |> count(PERSDOC2, hasdoc)\n\n# A tibble: 5 × 3\n  PERSDOC2 hasdoc     n\n     <dbl>  <dbl> <int>\n1        1      1  5784\n2        2      1   623\n3        3      0   990\n4        7     NA    14\n5        9     NA     1\n\n\n\n\n6.6.5.3 MEDCOST and its cleanup to costprob\nMEDCOST, the Could Not See Doctor Because of Cost variable, is the response to “Was there a time in the past 12 months when you needed to see a doctor but could not because of cost?”\n\n1 = Yes\n2 = No\n7 = Don’t know/Not sure\n9 = Refused\nBLANK = Not asked or missing\n\nThis is just like HLTHPLAN.\n\nsmart_ohio_raw <- smart_ohio_raw |> \n    mutate(costprob = MEDCOST,\n           costprob = replace(costprob, costprob %in% c(7, 9), NA),\n           costprob = replace(costprob, costprob == 2, 0))\n\nsmart_ohio_raw |> count(MEDCOST, costprob)\n\n# A tibble: 4 × 3\n  MEDCOST costprob     n\n    <dbl>    <dbl> <int>\n1       1        1   714\n2       2        0  6680\n3       7       NA    14\n4       9       NA     4\n\n\n\n\n6.6.5.4 CHECKUP1 and its cleanup to t_checkup\nCHECKUP1, the Length of time since last routine checkup variable, is the response to “About how long has it been since you last visited a doctor for a routine checkup? [A routine checkup is a general physical exam, not an exam for a specific injury, illness, or condition.]”\n\n1 = Within past year (anytime less than 12 months ago)\n2 = Within past 2 years (1 year but less than 2 years ago)\n3 = Within past 5 years (2 years but less than 5 years ago)\n4 = 5 or more years ago\n7 = Don’t know/Not sure\n8 = Never\n9 = Refused\nBLANK = Not asked or missing\n\nTo clean up the CHECKUP1 data into a new variable called t_checkup we’ll - convince R that the 7 and 9 values are in fact best interpreted as NA, - relabel options 1, 2, 3, 4 and 8 while turning the variable into a factor.\n\nsmart_ohio_raw <- smart_ohio_raw |> \n    mutate(t_checkup = fct_recode(factor(CHECKUP1),\n                                 \"1_In-past-year\" = \"1\",\n                                 \"2_1-to-2-years\" = \"2\",\n                                 \"3_2-to-5-years\" = \"3\",\n                                 \"4_5_plus_years\" = \"4\",\n                                 \"8_Never\" = \"8\",\n                                 NULL = \"7\",\n                                 NULL = \"9\"))\n\nsmart_ohio_raw |> count(CHECKUP1, t_checkup)\n\n# A tibble: 7 × 3\n  CHECKUP1 t_checkup          n\n     <dbl> <fct>          <int>\n1        1 1_In-past-year  5803\n2        2 2_1-to-2-years   714\n3        3 3_2-to-5-years   413\n4        4 4_5_plus_years   376\n5        7 <NA>              68\n6        8 8_Never           32\n7        9 <NA>               6\n\n\n\n\n\n6.6.6 Blood Pressure (2 measures)\n\n6.6.6.1 BPHIGH4 and its cleanup to bp_high\nBPHIGH4 is asking about awareness of a hypertension diagnosis. It’s the response to the question: “Have you EVER been told by a doctor, nurse or other health professional that you have high blood pressure?” In addition, if the answer was “Yes” and the respondent is female, they were then asked “Was this only when you were pregnant?”\nThe available codes are:\n\n1 = Yes\n2 = Yes, but female told only during pregnancy\n3 = No\n4 = Told borderline high or pre-hypertensive\n7 = Don’t know/Not sure\n9 = Refused\nBLANK = Not asked or missing\n\nTo clean up the BPHIGH4 data into a new variable called bp_high we’ll - convince R that the 7 and 9 values are in fact best interpreted as NA, - relabel (and re-order) options 1, 2, 3, 4 while turning the variable into a factor.\n\nsmart_ohio_raw <- smart_ohio_raw |> \n    mutate(bp_high = fct_recode(factor(BPHIGH4),\n                                 \"0_No\" = \"3\",\n                                 \"1_Yes\" = \"1\",\n                                 \"2_Only_while_pregnant\" = \"2\",\n                                 \"4_Borderline\" = \"4\",\n                                 NULL = \"7\",\n                                 NULL = \"9\"),\n           bp_high = fct_relevel(bp_high,\n                                 \"0_No\", \"1_Yes\", \n                                 \"2_Only_while_pregnant\", \n                                 \"4_Borderline\"))\n\nsmart_ohio_raw |> count(BPHIGH4, bp_high)\n\n# A tibble: 6 × 3\n  BPHIGH4 bp_high                   n\n    <dbl> <fct>                 <int>\n1       1 1_Yes                  3161\n2       2 2_Only_while_pregnant    67\n3       3 0_No                   4114\n4       4 4_Borderline             49\n5       7 <NA>                     19\n6       9 <NA>                      2\n\n\n\n\n6.6.6.2 BPMEDS and its cleanup to bp_meds\nBPMEDS is the response to the question “Are you currently taking medicine for your high blood pressure?”\n\n1 = Yes\n2 = No\n7 = Don’t know/Not sure\n9 = Refused\nBLANK = Not asked or missing\n\nTo clean up the BPMEDS data into a new variable called bp_meds we’ll treat it just as we did with HLTHPLN1 and - convince R that the 7 and 9 values are in fact best interpreted as NA, - and turn it into an indicator variable, e.g., we will leave the variable as numeric, but change the values to 1 = Yes and 0 = No.\n\nsmart_ohio_raw <- smart_ohio_raw |> \n    mutate(bp_meds = BPMEDS,\n           bp_meds = replace(bp_meds, bp_meds %in% c(7, 9), NA),\n           bp_meds = replace(bp_meds, bp_meds == 2, 0))\n\nsmart_ohio_raw |> count(BPMEDS, bp_meds)\n\n# A tibble: 5 × 3\n  BPMEDS bp_meds     n\n   <dbl>   <dbl> <int>\n1      1       1  2675\n2      2       0   481\n3      7      NA     4\n4      9      NA     1\n5     NA      NA  4251\n\n\nWhat is the relationship between our two blood pressure variables? Only the people with bp_meds = “1_Yes” were asked the bp_meds question.\n\nsmart_ohio_raw |> tabyl(bp_high, bp_meds)\n\n               bp_high   0    1  NA_\n                  0_No   0    0 4114\n                 1_Yes 481 2675    5\n 2_Only_while_pregnant   0    0   67\n          4_Borderline   0    0   49\n                  <NA>   0    0   21\n\n\n\n\n\n6.6.7 Cholesterol (3 items)\n\n6.6.7.1 CHOLCHK1 and its cleanup to t_chol\nCHOLCHK1, the Length of time since cholesterol was checked, is the response to “Blood cholesterol is a fatty substance found in the blood. About how long has it been since you last had your blood cholesterol checked?”\n\n1 = Never\n2 = Within past year (anytime less than 12 months ago)\n3 = Within past 2 years (1 year but less than 2 years ago)\n4 = Within past 5 years (2 years but less than 5 years ago)\n5 = 5 or more years ago\n7 = Don’t know/Not sure\n9 = Refused\nBLANK = Not asked or missing\n\nTo clean up the CHOLCHK1 data into a new variable called t_chol we’ll - convince R that the 7 and 9 values are in fact best interpreted as NA, - relabel options 1, 2, 3, 4 and 8 while turning the variable into a factor.\n\nsmart_ohio_raw <- smart_ohio_raw |> \n    mutate(t_chol = fct_recode(factor(CHOLCHK1),\n                                 \"1_Never\" = \"1\",\n                                 \"2_In-past-year\" = \"2\",\n                                 \"3_1-to-2-years\" = \"3\",\n                                 \"4_2-to-5-years\" = \"4\",\n                                 \"5_5_plus_years\" = \"5\",\n                                 NULL = \"7\",\n                                 NULL = \"9\"))\n\nsmart_ohio_raw |> count(CHOLCHK1, t_chol)\n\n# A tibble: 8 × 3\n  CHOLCHK1 t_chol             n\n     <dbl> <fct>          <int>\n1        1 1_Never          424\n2        2 2_In-past-year  5483\n3        3 3_1-to-2-years   559\n4        4 4_2-to-5-years   289\n5        5 5_5_plus_years   272\n6        7 <NA>             376\n7        9 <NA>               8\n8       NA <NA>               1\n\n\nThe next two measures are not gathered from the people who answered “Never” to this question.\n\n\n6.6.7.2 TOLDHI2 and its cleanup to chol_high\nTOLDHI2 is asking about awareness of a diagnosis of high cholesterol. It’s the response to the question: “Have you EVER been told by a doctor, nurse or other health professional that your blood cholesterol is high?”\nThe available codes are:\n\n1 = Yes\n2 = No\n7 = Don’t know/Not sure\n9 = Refused\nBLANK = Not asked or missing\n\nTo clean up the TOLDHI2 data into a new variable called chol_high we’ll treat it like BPMEDS and HLTHPLN1 - convince R that the 7 and 9 values are in fact best interpreted as NA, - and turn it into an indicator variable, e.g., we will leave the variable as numeric, but change the values to 1 = Yes and 0 = No.\n\nsmart_ohio_raw <- smart_ohio_raw |> \n    mutate(chol_high = TOLDHI2,\n           chol_high = replace(chol_high, chol_high %in% c(7, 9), NA),\n           chol_high = replace(chol_high, chol_high == 2, 0))\n\nsmart_ohio_raw |> count(TOLDHI2, chol_high)\n\n# A tibble: 5 × 3\n  TOLDHI2 chol_high     n\n    <dbl>     <dbl> <int>\n1       1         1  2612\n2       2         0  4286\n3       7        NA    70\n4       9        NA     4\n5      NA        NA   440\n\n\n\n\n6.6.7.3 CHOLMED1 and its cleanup to chol_meds\nCHOLMED1 is the response to the question “Are you currently taking medicine prescribed by a doctor or other health professional for your blood cholesterol?”\n\n1 = Yes\n2 = No\n7 = Don’t know/Not sure\n9 = Refused\nBLANK = Not asked or missing\n\nTo clean up the CHOLMED1 data into a new variable called chol_meds we’ll treat it just as we did with HLTHPLN1 and - convince R that the 7 and 9 values are in fact best interpreted as NA, - and turn it into an indicator variable, e.g., we will leave the variable as numeric, but change the values to 1 = Yes and 0 = No.\n\nsmart_ohio_raw <- smart_ohio_raw |> \n    mutate(chol_meds = CHOLMED1,\n           chol_meds = replace(chol_meds, chol_meds %in% c(7, 9), NA),\n           chol_meds = replace(chol_meds, chol_meds == 2, 0))\n\nsmart_ohio_raw |> count(CHOLMED1, chol_meds)\n\n# A tibble: 4 × 3\n  CHOLMED1 chol_meds     n\n     <dbl>     <dbl> <int>\n1        1         1  1781\n2        2         0   826\n3        7        NA     5\n4       NA        NA  4800\n\n\n\n\n\n6.6.8 Chronic Health Conditions (14 items)\n\n6.6.8.1 Self-reported diagnosis history (11 items)\nThe next few variables describe whether or not the subject meets a particular standard, and are all coded in the raw data the same way:\n\n1 = Yes\n2 = No\n7 = Don’t know/Not sure\n9 = Refused\nBLANK = Not asked or missing\n\nand we’ll recode them all to 1 = Yes, 0 = No, otherwise NA, as we’ve done previously.\nThe questions are all started with “Has a doctor, nurse, or other health professional ever told you that you had any of the following? For each, tell me Yes, No, or you’re Not sure.”\n\n\n\n\n\n\n\n\nOriginal\nRevised\nDetails\n\n\n\n\nCVDINFR4\nhx_mi\n(Ever told) you had a heart attack, also called a myocardial infarction?\n\n\nCVDCRHD4\nhx_chd\n(Ever told) you had angina or coronary heart disease?\n\n\nCVDSTRK3\nhx_stroke\n(Ever told) you had a stroke?\n\n\nASTHMA3\nhx_asthma\n(Ever told) you had asthma?\n\n\nASTHNOW\nnow_asthma\nDo you still have asthma? (only asked of those with Yes in ASTHMA3)\n\n\nCHCSCNCR\nhx_skinc\n(Ever told) you had skin cancer?\n\n\nCHCOCNCR\nhx_otherc\n(Ever told) you had any other types of cancer?\n\n\nCHCCOPD1\nhx_copd\n(Ever told) you have Chronic Obstructive Pulmonary Disease or COPD, emphysema or chronic bronchitis?\n\n\nHAVARTH3\nhx_arthr\n(Ever told) you have some form of arthritis, rheumatoid arthritis, gout, lupus, or fibromyalgia? (Arthritis diagnoses include: rheumatism, polymyalgia rheumatica; osteoarthritis (not osteporosis); tendonitis, bursitis, bunion, tennis elbow; carpal tunnel syndrome, tarsal tunnel syndrome; joint infection, etc.)\n\n\nADDEPEV2\nhx_depress\n(Ever told) you that you have a depressive disorder, including depression, major depression, dysthymia, or minor depression?\n\n\nCHCKIDNY\nhx_kidney\n(Ever told) you have kidney disease? Do NOT include kidney stones, bladder infection or incontinence.\n\n\n\n\nsmart_ohio_raw <- smart_ohio_raw |> \n    mutate(hx_mi = CVDINFR4,\n           hx_mi = replace(hx_mi, hx_mi %in% c(7, 9), NA),\n           hx_mi = replace(hx_mi, hx_mi == 2, 0),\n           hx_chd = CVDCRHD4,\n           hx_chd = replace(hx_chd, hx_chd %in% c(7, 9), NA),\n           hx_chd = replace(hx_chd, hx_chd == 2, 0),\n           hx_stroke = CVDSTRK3,\n           hx_stroke = replace(hx_stroke, hx_stroke %in% c(7, 9), NA),\n           hx_stroke = replace(hx_stroke, hx_stroke == 2, 0),\n           hx_asthma = ASTHMA3,\n           hx_asthma = replace(hx_asthma, hx_asthma %in% c(7, 9), NA),\n           hx_asthma = replace(hx_asthma, hx_asthma == 2, 0),\n           now_asthma = ASTHNOW,\n           now_asthma = replace(now_asthma, now_asthma %in% c(7, 9), NA),\n           now_asthma = replace(now_asthma, now_asthma == 2, 0),\n           hx_skinc = CHCSCNCR,\n           hx_skinc = replace(hx_skinc, hx_skinc %in% c(7, 9), NA),\n           hx_skinc = replace(hx_skinc, hx_skinc == 2, 0),\n           hx_otherc = CHCOCNCR,\n           hx_otherc = replace(hx_otherc, hx_otherc %in% c(7, 9), NA),\n           hx_otherc = replace(hx_otherc, hx_otherc == 2, 0),\n           hx_copd = CHCCOPD1,\n           hx_copd = replace(hx_copd, hx_copd %in% c(7, 9), NA),\n           hx_copd = replace(hx_copd, hx_copd == 2, 0),\n           hx_arthr = HAVARTH3,\n           hx_arthr = replace(hx_arthr, hx_arthr %in% c(7, 9), NA),\n           hx_arthr = replace(hx_arthr, hx_arthr == 2, 0),\n           hx_depress = ADDEPEV2,\n           hx_depress = replace(hx_depress, hx_depress %in% c(7, 9), NA),\n           hx_depress = replace(hx_depress, hx_depress == 2, 0),\n           hx_kidney = CHCKIDNY,\n           hx_kidney = replace(hx_kidney, hx_kidney %in% c(7, 9), NA),\n           hx_kidney = replace(hx_kidney, hx_kidney == 2, 0))\n\nWe definitely should have written a function to do that, of course.\n\n\n6.6.8.2 _ASTHMS1 and its cleanup to asthma\n_ASTHMS1 categorizes subjects by asthma status as:\n\n1 = Current\n2 = Former\n3 = Never\n9 = Don’t Know / Not Sure / Refused / Missing\n\nWe’ll turn this into a factor with appropriate levels and NA information.\n\nsmart_ohio_raw <- smart_ohio_raw |> \n    mutate(asthma = fct_recode(\n        factor(`_ASTHMS1`),\n        \"Current\" = \"1\",\n        \"Former\" = \"2\",\n        \"Never\" = \"3\",\n        NULL = \"9\"))\n\nsmart_ohio_raw |> count(`_ASTHMS1`, asthma)\n\n# A tibble: 4 × 3\n  `_ASTHMS1` asthma      n\n       <dbl> <fct>   <int>\n1          1 Current   734\n2          2 Former    248\n3          3 Never    6376\n4          9 <NA>       54\n\n\n\n\n6.6.8.3 DIABETE3 and its cleanup to hx_diabetes and dm_status\nDIABETE3, the (Ever told) you have diabetes variable, is the response to “(Ever told) you have diabetes (If Yes and respondent is female, ask Was this only when you were pregnant?. If Respondent says pre-diabetes or borderline diabetes, use response code 4.)”\n\n1 = Yes\n2 = Yes, but female told only during pregnancy\n3 = No\n4 = No, pre-diabetes or borderline diabetes\n7 = Don’t know/Not sure\n9 = Refused\nBLANK = Not asked or missing\n\nI’ll create one variable called hx_diabetes which is 1 if DIABETE3 = 1, and 0 otherwise, with appropriate NAs, like our other variables. Then I’ll create dm_status to include all of this information in a factor, but again recode the missing values properly.\n\nsmart_ohio_raw <- smart_ohio_raw |> \n    mutate(hx_diabetes = DIABETE3,\n           hx_diabetes = replace(hx_diabetes, hx_diabetes %in% c(7, 9), NA),\n           hx_diabetes = replace(hx_diabetes, hx_diabetes %in% 2:4, 0),\n           dm_status = fct_recode(factor(DIABETE3),\n                                  \"Diabetes\" = \"1\",\n                                  \"Pregnancy-Induced\" = \"2\",\n                                  \"No-Diabetes\" = \"3\",\n                                  \"Pre-Diabetes\" = \"4\",\n                                  NULL = \"7\",\n                                  NULL = \"9\"),\n           dm_status = fct_relevel(dm_status,\n                                   \"No-Diabetes\",\n                                   \"Pre-Diabetes\",\n                                   \"Pregnancy-Induced\",\n                                   \"Diabetes\"))\n\nsmart_ohio_raw |> count(DIABETE3, hx_diabetes, dm_status)\n\n# A tibble: 6 × 4\n  DIABETE3 hx_diabetes dm_status             n\n     <dbl>       <dbl> <fct>             <int>\n1        1           1 Diabetes           1098\n2        2           0 Pregnancy-Induced    67\n3        3           0 No-Diabetes        6100\n4        4           0 Pre-Diabetes        133\n5        7          NA <NA>                 12\n6        9          NA <NA>                  2\n\n\n\n\n6.6.8.4 DIABAGE2 and its cleanup to dm_age\nDIABAGE2, the Age When Told Diabetic variable, is the response to “How old were you when you were told you have diabetes?” It is asked only of people with DIABETE3 = 1 (Yes).\n\nThe response is 1-97, with special values 98 for Don’t Know/Not Sure and 99 for refused, with BLANK for missing or not asked. People 97 years of age and above were listed as 97.\n\n\nsmart_ohio_raw <- smart_ohio_raw |> \n    mutate(dm_age = DIABAGE2,\n           dm_age = replace(dm_age, dm_age > 97, NA))\n\nsmart_ohio_raw |> count(DIABAGE2, dm_age) |> tail()\n\n# A tibble: 6 × 3\n  DIABAGE2 dm_age     n\n     <dbl>  <dbl> <int>\n1       84     84     1\n2       85     85     2\n3       90     90     1\n4       98     NA    61\n5       99     NA     4\n6       NA     NA  6314\n\n\n\n\n\n6.6.9 Arthritis Burden (4 items)\nThe first two measures are only asked of people with hx_arthr = 1, and are coded as:\n\n1 = Yes\n2 = No\n7 = Don’t know/Not sure\n9 = Refused\nBLANK = Not asked or missing\n\nand we’ll recode them to 1 = Yes, 0 = No, otherwise NA, as we’ve done previously.\n\n6.6.9.1 LMTJOIN3 (Limited because of joint symptoms), and its cleanup to arth_lims\nThis is the response to “Are you now limited in any way in any of your usual activities because of arthritis or joint symptoms?”\n\nsmart_ohio_raw <- smart_ohio_raw |> \n    mutate(arth_lims = LMTJOIN3,\n           arth_lims = replace(arth_lims, arth_lims %in% c(7, 9), NA),\n           arth_lims = replace(arth_lims, arth_lims == 2, 0))\n\nsmart_ohio_raw |> count(hx_arthr, LMTJOIN3, arth_lims)\n\n# A tibble: 6 × 4\n  hx_arthr LMTJOIN3 arth_lims     n\n     <dbl>    <dbl>     <dbl> <int>\n1        0       NA        NA  4587\n2        1        1         1  1378\n3        1        2         0  1388\n4        1        7        NA    17\n5        1        9        NA     2\n6       NA       NA        NA    40\n\n\n\n\n6.6.9.2 ARTHDIS2 (Does Arthritis Affect Whether You Work), and its cleanup to arth_work\nThis is the response to “Do arthritis or joint symptoms now affect whether you work, the type of work you do or the amount of work you do?”\n\nsmart_ohio_raw <- smart_ohio_raw |> \n    mutate(arth_work = ARTHDIS2,\n           arth_work = replace(arth_work, arth_work %in% c(7, 9), NA),\n           arth_work = replace(arth_work, arth_work == 2, 0))\n\nsmart_ohio_raw |> count(ARTHDIS2, arth_work)\n\n# A tibble: 5 × 3\n  ARTHDIS2 arth_work     n\n     <dbl>     <dbl> <int>\n1        1         1   925\n2        2         0  1808\n3        7        NA    42\n4        9        NA    10\n5       NA        NA  4627\n\n\n\n\n6.6.9.3 ARTHSOCL (Social Activities Limited Because of Joint Symptoms) and its cleanup to arth_soc\nThis is the response to “During the past 30 days, to what extent has your arthritis or joint symptoms interfered with your normal social activities, such as going shopping, to the movies, or to religious or social gatherings?”\nThe responses are:\n\n1 = A lot\n2 = A little\n3 = Not at all\n7 = Don’t know/Not sure\n9 = Refused\nBLANK = Not asked or missing\n\n\nsmart_ohio_raw <- smart_ohio_raw |> \n    mutate(arth_soc = fct_recode(factor(ARTHSOCL),\n                                  \"A lot\" = \"1\",\n                                  \"A little\" = \"2\",\n                                  \"Not at all\" = \"3\",\n                                  NULL = \"7\",\n                                  NULL = \"9\"))\n\nsmart_ohio_raw |> count(ARTHSOCL, arth_soc)\n\n# A tibble: 6 × 3\n  ARTHSOCL arth_soc       n\n     <dbl> <fct>      <int>\n1        1 A lot        606\n2        2 A little     734\n3        3 Not at all  1427\n4        7 <NA>          15\n5        9 <NA>           3\n6       NA <NA>        4627\n\n\n\n\n6.6.9.4 JOINPAI1 (How Bad Was Joint Pain - scale of 0-10) and its cleanup to joint_pain\nThis is the response to the following question: “Please think about the past 30 days, keeping in mind all of your joint pain or aching and whether or not you have taken medication. On a scale of 0 to 10 where 0 is no pain or aching and 10 is pain or aching as bad as it can be, DURING THE PAST 30 DAYS, how bad was your joint pain ON AVERAGE?”\nThe available values are 0-10, plus codes 77 (Don’t Know / Not Sure), 99 (Refused) and BLANK.\nTo clean up JOINPAI1 to a new variable called joint_pain, we’ll need to convince R that the 77 and 99 values are, like BLANK, in fact best interpreted as NA.\n\nsmart_ohio_raw <- smart_ohio_raw |> \n    mutate(joint_pain = JOINPAI1,\n           joint_pain = replace(joint_pain, joint_pain %in% c(77, 99), NA))\n\nsmart_ohio_raw |> count(JOINPAI1, joint_pain) |> tail()\n\n# A tibble: 6 × 3\n  JOINPAI1 joint_pain     n\n     <dbl>      <dbl> <int>\n1        8          8   277\n2        9          9    72\n3       10         10   158\n4       77         NA    28\n5       99         NA     5\n6       NA         NA  4627\n\n\n\n\n\n6.6.10 Demographics (25 items)\n\n6.6.10.1 _AGEG5YR, which we’ll edit into agegroup\nThe _AGEG5YR variable is a calculated variable (by CDC) obtained from the subject’s age. Since the age data are not available, we instead get these groupings, which we’ll rearrange into the agegroup factor.\n\n\n\n_AGEG5YR\nAge range\nagegroup\n\n\n\n\n1\n18 <= AGE <= 24\n18-24\n\n\n2\n25 <= AGE <= 29\n25-29\n\n\n3\n30 <= AGE <= 34\n30-34\n\n\n4\n35 <= AGE <= 39\n35-39\n\n\n5\n40 <= AGE <= 44\n40-44\n\n\n6\n45 <= AGE <= 49\n45-49\n\n\n7\n50 <= AGE <= 54\n50-54\n\n\n8\n55 <= AGE <= 59\n55-59\n\n\n9\n60 <= AGE <= 64\n60-64\n\n\n10\n65 <= AGE <= 69\n65-69\n\n\n11\n70 <= AGE <= 74\n70-74\n\n\n12\n75 <= AGE <= 79\n75-79\n\n\n13\nAGE >= 80\n80plus\n\n\n14\nDon’t Know, Refused or Missing\nNA\n\n\n\n\nsmart_ohio_raw <- smart_ohio_raw |> \n    mutate(agegroup = fct_recode(factor(`_AGEG5YR`),\n                                \"18-24\" = \"1\",\n                                \"25-29\" = \"2\",\n                                \"30-34\" = \"3\",\n                                \"35-39\" = \"4\",\n                                \"40-44\" = \"5\",\n                                \"45-49\" = \"6\",\n                                \"50-54\" = \"7\",\n                                \"55-59\" = \"8\",\n                                \"60-64\" = \"9\",\n                                \"65-69\" = \"10\",\n                                \"70-74\" = \"11\",\n                                \"75-79\" = \"12\",\n                                \"80-96\" = \"13\",\n                                NULL = \"14\"))\n\nsmart_ohio_raw |> count(`_AGEG5YR`, agegroup)\n\n# A tibble: 14 × 3\n   `_AGEG5YR` agegroup     n\n        <dbl> <fct>    <int>\n 1          1 18-24      448\n 2          2 25-29      327\n 3          3 30-34      375\n 4          4 35-39      446\n 5          5 40-44      426\n 6          6 45-49      509\n 7          7 50-54      604\n 8          8 55-59      786\n 9          9 60-64      837\n10         10 65-69      810\n11         11 70-74      685\n12         12 75-79      499\n13         13 80-96      592\n14         14 <NA>        68\n\n\n\n\n6.6.10.2 _MRACE1 recoded to race\nWe’ll create three variables describing race/ethnicity. The first comes from the _MRACE1 variable categorized by CDC, and the available responses are:\n\n1 = White only\n2 = Black or African-American only\n3 = American Indian or Alaskan Native only\n4 = Asian only\n5 = Native Hawaiian or Pacific Islander only\n6 = Other race only\n7 = Multiracial\n77 = Don’t know / Not Sure\n99 = Refused\nBLANK = Missing\n\nWe’ll create a factor out of this information, with appropriate level names.\n\nsmart_ohio_raw <- smart_ohio_raw |> \n    mutate(race = fct_recode(factor(`_MRACE1`),\n                                  \"White\" = \"1\",\n                                  \"Black or African A\" = \"2\",\n                                  \"Amer Indian or Alaskan\" = \"3\",\n                                  \"Asian\" = \"4\", \n                                  \"Hawaiian or Pac Island\" = \"5\",\n                                  \"Other Race\" = \"6\",\n                                  \"Multiracial\" = \"7\",\n                                  NULL = \"77\",\n                                  NULL = \"99\"))\n\nsmart_ohio_raw |> count(`_MRACE1`, race)\n\n# A tibble: 9 × 3\n  `_MRACE1` race                       n\n      <dbl> <fct>                  <int>\n1         1 White                   6177\n2         2 Black or African A       739\n3         3 Amer Indian or Alaskan    66\n4         4 Asian                    115\n5         5 Hawaiian or Pac Island     5\n6         6 Other Race                43\n7         7 Multiracial              153\n8        77 <NA>                      14\n9        99 <NA>                     100\n\n\n\n\n6.6.10.3 _HISPANC recoded to hispanic\nThe _HISPANC variable specifies whether or not the respondent is of Hispanic or Latinx origin. The available responses are:\n\n1 = Hispanic, Latinx or Spanish origin\n2 = Not of Hispanic, Latinx or Spanish origin\n9 = Don’t Know, Refused, or Missing\n\nWe’ll turn the 9s into NA, and create an indicator variable (1 = Hispanic or Latinx, 0 = not)\n\nsmart_ohio_raw <- smart_ohio_raw |> \n    mutate(hispanic = 2 - `_HISPANC`,\n           hispanic = replace(hispanic, hispanic < 0, NA))\n\nsmart_ohio_raw |> count(`_HISPANC`, hispanic)\n\n# A tibble: 3 × 3\n  `_HISPANC` hispanic     n\n       <dbl>    <dbl> <int>\n1          1        1   146\n2          2        0  7217\n3          9       NA    49\n\n\n\n\n6.6.10.4 _RACEGR3 recoded to race_eth\nThe _RACEGR3 variable is a five-level combination of race and ethnicity. The responses are:\n\n1 = White non-Hispanic\n2 = Black non-Hispanic\n3 = Other race non-Hispanic\n4 = Multiracial non-Hispanic\n5 = Hispanic\n9 = Don’t Know / Not Sure / Refused\n\nWe’ll create a factor out of this information, with appropriate level names.\n\nsmart_ohio_raw <- smart_ohio_raw |> \n    mutate(race_eth = fct_recode(\n        factor(`_RACEGR3`),\n        \"White non-Hispanic\" = \"1\",\n        \"Black non-Hispanic\" = \"2\",\n        \"Other race non-Hispanic\" = \"3\",\n        \"Multiracial non-Hispanic\" = \"4\", \n        \"Hispanic\" = \"5\",\n        NULL = \"9\"))\n\nsmart_ohio_raw |> count(`_RACEGR3`, race_eth)\n\n# A tibble: 6 × 3\n  `_RACEGR3` race_eth                     n\n       <dbl> <fct>                    <int>\n1          1 White non-Hispanic        6086\n2          2 Black non-Hispanic         725\n3          3 Other race non-Hispanic    193\n4          4 Multiracial non-Hispanic   143\n5          5 Hispanic                   146\n6          9 <NA>                       119\n\n\n\n\n6.6.10.5 SEX recoded to female\nThe available levels of SEX are:\n\n1 = Male\n2 = Female\n9 = Refused\n\nWe’ll recode that to female = 1 for Female, 0 Male, otherwise NA. Note the trick here is to subtract one from the coded SEX to get the desired female, but this requires that we move 8 to NA, rather than 9.\n\nsmart_ohio_raw <- smart_ohio_raw |> \n    mutate(female = SEX - 1,\n           female = replace(female, female == 8, NA))\n\nsmart_ohio_raw |> count(SEX, female)\n\n# A tibble: 2 × 3\n    SEX female     n\n  <dbl>  <dbl> <int>\n1     1      0  3136\n2     2      1  4276\n\n\n\n\n6.6.10.6 MARITAL status, revised to marital\nThe available levels of MARITAL are:\n\n1 = Married\n2 = Divorced\n3 = Widowed\n4 = Separated\n5 = Never married\n6 = A member of an unmarried couple\n9 = Refused\nBLANK = Not asked or missing\n\nWe’ll just turn this into a factor, and move 9 to NA.\n\nsmart_ohio_raw <- smart_ohio_raw |> \n    mutate(marital = fct_recode(factor(MARITAL),\n                                \"Married\" = \"1\",\n                                \"Divorced\" = \"2\",\n                                \"Widowed\" = \"3\",\n                                \"Separated\" = \"4\",\n                                \"Never_Married\" = \"5\",\n                                \"Unmarried_Couple\" = \"6\",\n                                NULL = \"9\"))\n\nsmart_ohio_raw |> count(MARITAL, marital)\n\n# A tibble: 7 × 3\n  MARITAL marital              n\n    <dbl> <fct>            <int>\n1       1 Married           3668\n2       2 Divorced          1110\n3       3 Widowed            978\n4       4 Separated          142\n5       5 Never_Married     1248\n6       6 Unmarried_Couple   208\n7       9 <NA>                58\n\n\n\n\n6.6.10.7 EDUCA recoded to educgroup\nThe available levels of EDUCA (Education Level) are responses to: “What is the highest grade or year of school you completed?”\n\n1 = Never attended school or only kindergarten\n2 = Grades 1 through 8 (Elementary)\n3 = Grades 9 through 11 (Some high school)\n4 = Grade 12 or GED (High school graduate)\n5 = College 1 year to 3 years (Some college or technical school)\n6 = College 4 years or more (College graduate)\n9 = Refused\nBLANK = Not asked or missing\n\nWe’ll just turn this into a factor, and move 9 to NA.\n\nsmart_ohio_raw <- smart_ohio_raw |> \n    mutate(educgroup = fct_recode(factor(EDUCA),\n                                \"Kindergarten\" = \"1\",\n                                \"Elementary\" = \"2\",\n                                \"Some_HS\" = \"3\",\n                                \"HS_Grad\" = \"4\",\n                                \"Some_College\" = \"5\",\n                                \"College_Grad\" = \"6\",\n                                NULL = \"9\"))\n\nsmart_ohio_raw |> count(EDUCA, educgroup)\n\n# A tibble: 7 × 3\n  EDUCA educgroup        n\n  <dbl> <fct>        <int>\n1     1 Kindergarten     3\n2     2 Elementary     117\n3     3 Some_HS        332\n4     4 HS_Grad       2209\n5     5 Some_College  2079\n6     6 College_Grad  2646\n7     9 <NA>            26\n\n\n\n\n6.6.10.8 RENTHOM1 recoded to home_own\nThe available levels of RENTHOM1 (Own or Rent Home) are responses to: “Do you own or rent your home? (Home is defined as the place where you live most of the time/the majority of the year.)”\n\n1 = Own\n2 = Rent\n3 = Other Arrangement\n7 = Don’t know/Not Sure\n9 = Refused\nBLANK = Not asked or missing\n\nWe’ll recode as home_own = 1 if they own their home, and 0 otherwise, and dealing with missingness properly.\n\nsmart_ohio_raw <- smart_ohio_raw |> \n    mutate(home_own = RENTHOM1,\n           home_own = replace(home_own, home_own %in% c(7,9), NA),\n           home_own = replace(home_own, home_own %in% c(2,3), 0))\n\nsmart_ohio_raw |> count(RENTHOM1, home_own)\n\n# A tibble: 5 × 3\n  RENTHOM1 home_own     n\n     <dbl>    <dbl> <int>\n1        1        1  5216\n2        2        0  1793\n3        3        0   348\n4        7       NA    28\n5        9       NA    27\n\n\n\n\n6.6.10.9 CPDEMO1A and its cleanup to cell_own\nCPDEMO1A is the response to “Including phones for business and personal use, do you have a cell phone for personal use?”\nAvailable responses are:\n\n1 = Yes\n2 = No\n7 = Don’t know/Not sure\n9 = Refused\nBLANK = Not asked or missing\n\nand we’ll recode them to 1 = Yes, 0 = No, otherwise NA, as we’ve done previously.\n\nsmart_ohio_raw <- smart_ohio_raw |> \n    mutate(cell_own = 2 - CPDEMO1A,\n           cell_own = replace(cell_own, cell_own < 0, NA))\n\nsmart_ohio_raw |> count(CPDEMO1A, cell_own)\n\n# A tibble: 5 × 3\n  CPDEMO1A cell_own     n\n     <dbl>    <dbl> <int>\n1        1        1  2930\n2        2        0   698\n3        7       NA     2\n4        9       NA    19\n5       NA       NA  3763\n\n\n\n\n6.6.10.10 VETERAN3 and its cleanup to veteran\nVETERAN3, the Are You A Veteran variable, is the response to “Have you ever served on active duty in the United States Armed Forces, either in the regular military or in a National Guard or military reserve unit? (Active duty does not include training for the Reserves or National Guard, but DOES include activation, for example, for the Persian Gulf War.)”\n\n1 = Yes\n2 = No\n7 = Don’t know/Not sure\n9 = Refused\nBLANK = Not asked or missing\n\n\nsmart_ohio_raw <- smart_ohio_raw |> \n    mutate(veteran = VETERAN3,\n           veteran = replace(veteran, veteran %in% c(7, 9), NA),\n           veteran = replace(veteran, veteran == 2, 0))\n\nsmart_ohio_raw |> count(VETERAN3, veteran)\n\n# A tibble: 3 × 3\n  VETERAN3 veteran     n\n     <dbl>   <dbl> <int>\n1        1       1   927\n2        2       0  6479\n3        9      NA     6\n\n\n\n\n6.6.10.11 EMPLOY1 and its cleanup to employment\nEMPLOY1, the Employment Status variable, is the response to “Are you currently … ?”\n\n1 = Employed for wages\n2 = Self-employed\n3 = Out of work for 1 year or more\n4 = Out of work for less than 1 year\n5 = A homemaker\n6 = A student\n7 = Retired\n8 = Unable to work\n9 = Refused\nBLANK = Not asked or missing\n\nWe’ll just turn this into a factor, and move 9 to NA.\n\nsmart_ohio_raw <- smart_ohio_raw |> \n    mutate(employment = fct_recode(factor(EMPLOY1),\n                                \"Employed_for_wages\" = \"1\",\n                                \"Self-employed\" = \"2\",\n                                \"Outofwork_1yearormore\" = \"3\",\n                                \"Outofwork_lt1year\" = \"4\",\n                                \"Homemaker\" = \"5\",\n                                \"Student\" = \"6\",\n                                \"Retired\" = \"7\",\n                                \"Unable_to_work\" = \"8\",\n                                NULL = \"9\"))\n\nsmart_ohio_raw |> count(EMPLOY1, employment)\n\n# A tibble: 9 × 3\n  EMPLOY1 employment                n\n    <dbl> <fct>                 <int>\n1       1 Employed_for_wages     3119\n2       2 Self-employed           466\n3       3 Outofwork_1yearormore   254\n4       4 Outofwork_lt1year       134\n5       5 Homemaker               411\n6       6 Student                 190\n7       7 Retired                2202\n8       8 Unable_to_work          603\n9       9 <NA>                     33\n\n\n\n\n6.6.10.12 CHILDREN and its cleanup to kids\nCHILDREN, the Number of Children in Household variable, is the response to “How many children less than 18 years of age live in your household?”\n\n1-87 = legitimate responses\n88 = None\n99 = Refused\nBLANK = Not asked or missing\n\n\nsmart_ohio_raw <- smart_ohio_raw |> \n    mutate(kids = CHILDREN,\n           kids = replace(kids, kids == 99, NA),\n           kids = replace(kids, kids == 88, 0))\n\nsmart_ohio_raw |> count(CHILDREN, kids) |> tail()\n\n# A tibble: 6 × 3\n  CHILDREN  kids     n\n     <dbl> <dbl> <int>\n1        6     6     7\n2        7     7     5\n3        8     8     2\n4       12    12     1\n5       88     0  5449\n6       99    NA    43\n\n\n\n\n6.6.10.13 INCOME2 to incomegroup\nThe available levels of INCOME2 (Income Level) are responses to: “Is your annual household income from all sources …”\n\n1 = Less than $10,000\n2 = $10,000 to less than $15,000\n3 = $15,000 to less than $20,000\n4 = $20,000 to less than $25,000\n5 = $25,000 to less than $35,000\n6 = $35,000 to less than $50,000\n7 = $50,000 to less than $75,000\n8 = $75,000 or more\n77 = Don’t know/Not sure\n99 = Refused\nBLANK = Not asked or missing\n\nWe’ll just turn this into a factor, and move 77 and 99 to NA.\n\nsmart_ohio_raw <- smart_ohio_raw |> \n    mutate(incomegroup = fct_recode(factor(`INCOME2`),\n                                \"0-9K\" = \"1\",\n                                \"10-14K\" = \"2\",\n                                \"15-19K\" = \"3\",\n                                \"20-24K\" = \"4\",\n                                \"25-34K\" = \"5\",\n                                \"35-49K\" = \"6\",\n                                \"50-74K\" = \"7\",\n                                \"75K+\" = \"8\",\n                                NULL = \"77\",\n                                NULL = \"99\"))\n\nsmart_ohio_raw |> count(`INCOME2`, incomegroup)\n\n# A tibble: 11 × 3\n   INCOME2 incomegroup     n\n     <dbl> <fct>       <int>\n 1       1 0-9K          285\n 2       2 10-14K        306\n 3       3 15-19K        477\n 4       4 20-24K        589\n 5       5 25-34K        685\n 6       6 35-49K        922\n 7       7 50-74K        928\n 8       8 75K+         1910\n 9      77 <NA>          610\n10      99 <NA>          678\n11      NA <NA>           22\n\n\n\n\n6.6.10.14 INTERNET and its cleanup to internet30\nINTERNET, the Internet use in the past 30 days variable, is the response to “Have you used the internet in the past 30 days?”\n\n1 = Yes\n2 = No\n7 = Don’t know/Not sure\n9 = Refused\nBLANK = Not asked or missing\n\n\nsmart_ohio_raw <- smart_ohio_raw |> \n    mutate(internet30 = INTERNET,\n           internet30 = replace(internet30, internet30 %in% c(7, 9), NA),\n           internet30 = replace(internet30, internet30 == 2, 0))\n\nsmart_ohio_raw |> count(INTERNET, internet30)\n\n# A tibble: 5 × 3\n  INTERNET internet30     n\n     <dbl>      <dbl> <int>\n1        1          1  6020\n2        2          0  1335\n3        7         NA    10\n4        9         NA    10\n5       NA         NA    37\n\n\n\n\n6.6.10.15 WTKG3 is weight_kg\nWTKG3 is computed by CDC, as the respondent’s weight in kilograms with two implied decimal places. We calculate the actual weight in kg, with the following:\n\nsmart_ohio_raw <- smart_ohio_raw |> \n    mutate(weight_kg = WTKG3/100)\n\nsmart_ohio_raw |> count(WTKG3, weight_kg) |> tail()\n\n# A tibble: 6 × 3\n  WTKG3 weight_kg     n\n  <dbl>     <dbl> <int>\n1 19051      191.     1\n2 19278      193.     1\n3 19504      195.     1\n4 20412      204.     2\n5 20865      209.     1\n6    NA       NA    462\n\n\n\n\n6.6.10.16 HEIGHT3 is replaced with height_m\nHEIGHT3 is strangely gathered to allow people to specify their height in either feet and inches or in meters and centimeters.\n\n200-711 indicates height in feet (first digit) and inches (second two digits)\n9000 - 9998 indicates height in meters (second digit) and centimeters (last two digits)\n7777 = Don’t know/Not sure\n9999 = Refused\n\nNote that there is one impossible value of 575 in the data set. We’ll make that an NA, and we’ll also make NA any heights below 3 feet, or above 2.24 meters. Specifically, we calculate the actual height in meters, with the following:\n\nsmart_ohio_raw <- smart_ohio_raw |>\n    mutate(height_m = case_when(\n        HEIGHT3 >= 300 & HEIGHT3 <= 511 ~ round((12*floor(HEIGHT3/100) + (HEIGHT3 - 100*floor(HEIGHT3/100)))*0.0254,2),\n        HEIGHT3 >= 600 & HEIGHT3 <= 711 ~ round((12*floor(HEIGHT3/100) + (HEIGHT3 - 100*floor(HEIGHT3/100)))*0.0254,2),\n        HEIGHT3 >= 9000 & HEIGHT3 <= 9224 ~ ((HEIGHT3 - 9000)/100)))\n\nsmart_ohio_raw |> count(HEIGHT3, height_m) |> tail()\n\n# A tibble: 6 × 3\n  HEIGHT3 height_m     n\n    <dbl>    <dbl> <int>\n1     607     2.01     2\n2     608     2.03     6\n3     609     2.06     1\n4    7777    NA       27\n5    9999    NA       86\n6      NA    NA       67\n\n\n\n\n6.6.10.17 bmi is calculated from height_m and weight_kg\nWe’ll calculate body-mass index from height and weight.\n\nsmart_ohio_raw <- smart_ohio_raw |> \n    mutate(bmi = round(weight_kg/(height_m)^2,2))\n\nsmart_ohio_raw |> count(height_m, weight_kg, bmi)# |> tail()\n\n# A tibble: 1,806 × 4\n   height_m weight_kg   bmi     n\n      <dbl>     <dbl> <dbl> <int>\n 1     1.35      39.0  21.4     1\n 2     1.35      52.2  28.6     1\n 3     1.4       89.8  45.8     1\n 4     1.42      31.8  15.8     1\n 5     1.42      45.4  22.5     1\n 6     1.42      55.8  27.7     1\n 7     1.42      58.5  29.0     1\n 8     1.42      59.9  29.7     1\n 9     1.42      60.8  30.1     1\n10     1.42      71.2  35.3     1\n# … with 1,796 more rows\n\n\n\n\n6.6.10.18 bmigroup is calculated from bmi\nWe’ll then divide the respondents into adult BMI categories, in the usual way.\n\nBMI < 18.5 indicates underweight\nBMI from 18.5 up to 25 indicates normal weight\nBMI from 25 up to 30 indicates overweight\nBMI of 30 and higher indicates obesity\n\n\nsmart_ohio_raw <- smart_ohio_raw |>\n    mutate(bmigroup = factor(cut2(as.numeric(bmi), \n                           cuts = c(18.5, 25.0, 30.0))))\n\nsmart_ohio_raw |> count(bmigroup)\n\n# A tibble: 5 × 2\n  bmigroup        n\n  <fct>       <int>\n1 [13.3,18.5)   119\n2 [18.5,25.0)  2017\n3 [25.0,30.0)  2445\n4 [30.0,75.5]  2338\n5 <NA>          493\n\n\n\n\n6.6.10.19 PREGNANT and its cleanup to pregnant\nPREGNANT, the Pregnancy Status variable, is the response to “To your knowledge, are you now pregnant?”\n\n1 = Yes\n2 = No\n7 = Don’t know/Not sure\n9 = Refused\nBLANK = Not asked or missing (includes SEX = male)\n\n\nsmart_ohio_raw <- smart_ohio_raw |> \n    mutate(pregnant = PREGNANT,\n           pregnant = replace(pregnant, pregnant %in% c(7, 9), NA),\n           pregnant = replace(pregnant, pregnant == 2, 0))\n\nsmart_ohio_raw |> count(PREGNANT, pregnant)\n\n# A tibble: 5 × 3\n  PREGNANT pregnant     n\n     <dbl>    <dbl> <int>\n1        1        1    41\n2        2        0  1329\n3        7       NA     3\n4        9       NA     3\n5       NA       NA  6036\n\n\n\n\n6.6.10.20 DEAF and its cleanup to deaf\nDEAF, the Are you deaf or do you have serious difficulty hearing variable, is the response to “Are you deaf or do you have serious difficulty hearing?”\n\n1 = Yes\n2 = No\n7 = Don’t know/Not sure\n9 = Refused\nBLANK = Not asked or missing\n\n\nsmart_ohio_raw <- smart_ohio_raw |> \n    mutate(deaf = DEAF,\n           deaf = replace(deaf, deaf %in% c(7, 9), NA),\n           deaf = replace(deaf, deaf == 2, 0))\n\nsmart_ohio_raw |> count(DEAF, deaf)\n\n# A tibble: 5 × 3\n   DEAF  deaf     n\n  <dbl> <dbl> <int>\n1     1     1   708\n2     2     0  6551\n3     7    NA    15\n4     9    NA     4\n5    NA    NA   134\n\n\n\n\n6.6.10.21 BLIND and its cleanup to blind\nBLIND, the Blind or Difficulty seeing variable, is the response to “Are you blind or do you have serious difficulty seeing, even when wearing glasses?”\n\n1 = Yes\n2 = No\n7 = Don’t know/Not sure\n9 = Refused\nBLANK = Not asked or missing\n\n\nsmart_ohio_raw <- smart_ohio_raw |> \n    mutate(blind = BLIND,\n           blind = replace(blind, blind %in% c(7, 9), NA),\n           blind = replace(blind, blind == 2, 0))\n\nsmart_ohio_raw |> count(BLIND, blind)\n\n# A tibble: 5 × 3\n  BLIND blind     n\n  <dbl> <dbl> <int>\n1     1     1   415\n2     2     0  6834\n3     7    NA    14\n4     9    NA     1\n5    NA    NA   148\n\n\n\n\n6.6.10.22 DECIDE and its cleanup to decide\nDECIDE, the Difficulty Concentrating or Remembering variable, is the response to “Because of a physical, mental, or emotional condition, do you have serious difficulty concentrating, remembering, or making decisions?”\n\n1 = Yes\n2 = No\n7 = Don’t know/Not sure\n9 = Refused\nBLANK = Not asked or missing\n\n\nsmart_ohio_raw <- smart_ohio_raw |> \n    mutate(decide = DECIDE,\n           decide = replace(decide, decide %in% c(7, 9), NA),\n           decide = replace(decide, decide == 2, 0))\n\nsmart_ohio_raw |> count(DECIDE, decide)\n\n# A tibble: 5 × 3\n  DECIDE decide     n\n   <dbl>  <dbl> <int>\n1      1      1   870\n2      2      0  6348\n3      7     NA    30\n4      9     NA     2\n5     NA     NA   162\n\n\n\n\n6.6.10.23 DIFFWALK and its cleanup to diffwalk\nDIFFWALK, the Difficulty Walking or Climbing Stairs variable, is the response to “Do you have serious difficulty walking or climbing stairs?”\n\n1 = Yes\n2 = No\n7 = Don’t know/Not sure\n9 = Refused\nBLANK = Not asked or missing\n\n\nsmart_ohio_raw <- smart_ohio_raw |> \n    mutate(diffwalk = DIFFWALK,\n           diffwalk = replace(diffwalk, diffwalk %in% c(7, 9), NA),\n           diffwalk = replace(diffwalk, diffwalk == 2, 0))\n\nsmart_ohio_raw |> count(DIFFWALK, diffwalk)\n\n# A tibble: 5 × 3\n  DIFFWALK diffwalk     n\n     <dbl>    <dbl> <int>\n1        1        1  1482\n2        2        0  5738\n3        7       NA    19\n4        9       NA     2\n5       NA       NA   171\n\n\n\n\n6.6.10.24 DIFFDRES and its cleanup to diffdress\nDIFFDRES, the Difficulty Dressing or Bathing variable, is the response to “Do you have difficulty dressing or bathing?”\n\n1 = Yes\n2 = No\n7 = Don’t know/Not sure\n9 = Refused\nBLANK = Not asked or missing\n\n\nsmart_ohio_raw <- smart_ohio_raw |> \n    mutate(diffdress = DIFFDRES,\n           diffdress = replace(diffdress, diffdress %in% c(7, 9), NA),\n           diffdress = replace(diffdress, diffdress == 2, 0))\n\nsmart_ohio_raw |> count(DIFFDRES, diffdress)\n\n# A tibble: 5 × 3\n  DIFFDRES diffdress     n\n     <dbl>     <dbl> <int>\n1        1         1   352\n2        2         0  6868\n3        7        NA    12\n4        9        NA     1\n5       NA        NA   179\n\n\n\n\n6.6.10.25 DIFFALON and its cleanup to diffalone\nDIFFALON, the Difficulty Doing Errands Alone variable, is the response to “Because of a physical, mental, or emotional condition, do you have difficulty doing errands alone such as visiting a doctor’s office or shopping?”\n\n1 = Yes\n2 = No\n7 = Don’t know/Not sure\n9 = Refused\nBLANK = Not asked or missing\n\n\nsmart_ohio_raw <- smart_ohio_raw |> \n    mutate(diffalone = DIFFALON,\n           diffalone = replace(diffalone, diffalone %in% c(7, 9), NA),\n           diffalone = replace(diffalone, diffalone == 2, 0))\n\nsmart_ohio_raw |> count(DIFFALON, diffalone)\n\n# A tibble: 5 × 3\n  DIFFALON diffalone     n\n     <dbl>     <dbl> <int>\n1        1         1   636\n2        2         0  6560\n3        7        NA    15\n4        9        NA     4\n5       NA        NA   197\n\n\n\n\n\n6.6.11 Tobacco Use (2 items)\n\n6.6.11.1 SMOKE100 and its cleanup to smoke100\nSMOKE100, the Smoked at Least 100 Cigarettes variable, is the response to “Have you smoked at least 100 cigarettes in your entire life? [Note: 5 packs = 100 cigarettes]”\n\n1 = Yes\n2 = No\n7 = Don’t know/Not sure\n9 = Refused\n\n\nsmart_ohio_raw <- smart_ohio_raw |> \n    mutate(smoke100 = SMOKE100,\n           smoke100 = replace(smoke100, smoke100 %in% c(7, 9), NA),\n           smoke100 = replace(smoke100, smoke100 == 2, 0))\n\nsmart_ohio_raw |> count(SMOKE100, smoke100)\n\n# A tibble: 5 × 3\n  SMOKE100 smoke100     n\n     <dbl>    <dbl> <int>\n1        1        1  3294\n2        2        0  3881\n3        7       NA    31\n4        9       NA     4\n5       NA       NA   202\n\n\n\n\n6.6.11.2 _SMOKER3 and its cleanup to smoker\n_SMOKER3, is a calculated variable which categorizes subjects by their smoking status:\n\n1 = Current smoker who smokes daily\n2 = Current smoker but not every day\n3 = Former smoker\n4 = Never smoked\n9 = Don’t Know / Refused / Missing\n\nWe’ll reclassify this as a factor with appropriate labels and NAs.\n\nsmart_ohio_raw <- smart_ohio_raw |> \n    mutate(smoker = fct_recode(factor(`_SMOKER3`),\n                                \"Current_daily\" = \"1\",\n                                \"Current_not_daily\" = \"2\",\n                                \"Former\" = \"3\",\n                                \"Never\" = \"4\",\n                                NULL = \"9\"))\n\nsmart_ohio_raw |> count(`_SMOKER3`, smoker)\n\n# A tibble: 5 × 3\n  `_SMOKER3` smoker                n\n       <dbl> <fct>             <int>\n1          1 Current_daily       990\n2          2 Current_not_daily   300\n3          3 Former             1999\n4          4 Never              3881\n5          9 <NA>                242\n\n\n\n\n\n6.6.12 E-Cigarettes (2 items)\n\n6.6.12.1 ECIGARET and its cleanup to ecig_ever\nECIGARET, the Ever used an e-cigarette variable, is the response to “Have you ever used an e-cigarette or other electronic vaping product, even just one time, in your entire life?”\n\n1 = Yes\n2 = No\n7 = Don’t know/Not sure\n9 = Refused\n\n\nsmart_ohio_raw <- smart_ohio_raw |> \n    mutate(ecig_ever = ECIGARET,\n           ecig_ever = replace(ecig_ever, ecig_ever %in% c(7, 9), NA),\n           ecig_ever = replace(ecig_ever, ecig_ever == 2, 0))\n\nsmart_ohio_raw |> count(ECIGARET, ecig_ever)\n\n# A tibble: 5 × 3\n  ECIGARET ecig_ever     n\n     <dbl>     <dbl> <int>\n1        1         1  1354\n2        2         0  5799\n3        7        NA     9\n4        9        NA     3\n5       NA        NA   247\n\n\n\n\n6.6.12.2 _ECIGSTS and its cleanup to ecigs\n_ECIGSTS, is a calculated variable which categorizes subjects by their smoking status:\n\n1 = Current and uses daily\n2 = Current user but not every day\n3 = Former user\n4 = Never used e-cigarettes\n9 = Don’t Know / Refused / Missing\n\nWe’ll reclassify this as a factor with appropriate labels and NAs.\n\nsmart_ohio_raw <- smart_ohio_raw |> \n    mutate(ecigs = fct_recode(factor(`_ECIGSTS`),\n                                \"Current_daily\" = \"1\",\n                                \"Current_not_daily\" = \"2\",\n                                \"Former\" = \"3\",\n                                \"Never\" = \"4\",\n                                NULL = \"9\"))\n\nsmart_ohio_raw |> count(`_ECIGSTS`, ecigs)\n\n# A tibble: 5 × 3\n  `_ECIGSTS` ecigs                 n\n       <dbl> <fct>             <int>\n1          1 Current_daily       102\n2          2 Current_not_daily   165\n3          3 Former             1085\n4          4 Never              5799\n5          9 <NA>                261\n\n\n\n\n\n6.6.13 Alcohol Consumption (6 items)\n\n6.6.13.1 ALCDAY5 and its cleanup to alcdays\nALCDAY5, the Days in past 30 had alcoholic beverage variable, is the response to “During the past 30 days, how many days per week or per month did you have at least one drink of any alcoholic beverage such as beer, wine, a malt beverage or liquor?”\n\n101-107 = # of days per week (101 = 1 day per week, 107 = 7 days per week)\n201-230 = # of days in past 30 days (201 = 1 day in last 30, 230 = 30 days in last 30)\n777 = Don’t know/Not sure\n888 = No drinks in past 30 days\n999 = Refused\nBLANK = Not asked or Missing\n\nWe’re going to convert this to a single numeric value. Answers in days per week (in the past 7 days) will be converted (after rounding) to days in the past 30. This is a little bit of a mess, really, but we can do it.\n\nsmart_ohio_raw <- smart_ohio_raw |> \n    mutate(alcdays = as.numeric(ALCDAY5)) |>\n    mutate(alcdays = replace(alcdays, alcdays == 888, 0),\n           alcdays = replace(alcdays, alcdays %in% c(777, 999), NA)) |>\n    mutate(alcdays = case_when(ALCDAY5 > 199 & ALCDAY5 < 231 ~ ALCDAY5 - 200,\n                               ALCDAY5 > 100 & ALCDAY5 < 108 ~ round((ALCDAY5 - 100)*30/7,0),\n                               TRUE ~ alcdays))\n\nsmart_ohio_raw |> count(ALCDAY5, alcdays)\n\n# A tibble: 39 × 3\n   ALCDAY5 alcdays     n\n     <dbl>   <dbl> <int>\n 1     101       4   263\n 2     102       9   197\n 3     103      13   142\n 4     104      17    76\n 5     105      21    53\n 6     106      26    18\n 7     107      30   114\n 8     201       1   621\n 9     202       2   448\n10     203       3   233\n# … with 29 more rows\n\n\n\n\n6.6.13.2 AVEDRNK2 and its cleanup to avgdrinks\nAVEDRNK2, the Avg alcoholic drinks per day in past 30 variable, is the response to “One drink is equivalent to a 12-ounce beer, a 5-ounce glass of wine, or a drink with one shot of liquor. During the past 30 days, on the days when you drank, about how many drinks did you drink on the average? (A 40 ounce beer would count as 3 drinks, or a cocktail drink with 2 shots would count as 2 drinks.)”\n\n1-76 = # of drinks per day\n77 = Don’t know/Not sure\n99 = Refused\nBLANK = Not asked or Missing (always happens when ALCDAY5 = 777, 888 or 999)\n\n\nsmart_ohio_raw <- smart_ohio_raw |> \n    mutate(avgdrinks = AVEDRNK2,\n           avgdrinks = replace(avgdrinks, avgdrinks > 76, NA))\n\nsmart_ohio_raw |> count(AVEDRNK2, avgdrinks) |> tail()\n\n# A tibble: 6 × 3\n  AVEDRNK2 avgdrinks     n\n     <dbl>     <dbl> <int>\n1       42        42     1\n2       60        60     2\n3       76        76     1\n4       77        NA    46\n5       99        NA     5\n6       NA        NA  3876\n\n\n\n\n6.6.13.3 MAXDRNKS and its cleanup to maxdrinks\nMAXDRINKS, the most drinks on a single occasion in the past 30 days variable, is the response to “During the past 30 days, what is the largest number of drinks you had on any occasion?”\n\n1-76 = # of drinks\n77 = Don’t know/Not sure\n99 = Refused\nBLANK = Not asked or Missing (always happens when ALCDAY5 = 777, 888 or 999)\n\n\nsmart_ohio_raw <- smart_ohio_raw |> \n    mutate(maxdrinks = MAXDRNKS,\n           maxdrinks = replace(maxdrinks, maxdrinks > 76, NA))\n\nsmart_ohio_raw |> count(MAXDRNKS, maxdrinks) |> tail()\n\n# A tibble: 6 × 3\n  MAXDRNKS maxdrinks     n\n     <dbl>     <dbl> <int>\n1       42        42     1\n2       48        48     1\n3       76        76     2\n4       77        NA    94\n5       99        NA    11\n6       NA        NA  3899\n\n\n\n\n6.6.13.4 _RFBING5 and its cleanup to binge\n_RFBING5 identifies binge drinkers (males having five or more drinks on one occasion, females having four or more drinks on one occasion in the past 30 days)\nThe values are\n\n1 = No\n2 = Yes\n9 = Don’t Know / Refused / Missing\n\nPeople who reported no alcdays are reported here as “No”, so we’ll adjust this into an indicator variable, and create the necessary NAs.\n\nsmart_ohio_raw <- smart_ohio_raw |> \n    mutate(binge = `_RFBING5` - 1,\n           binge = replace(binge, binge > 1, NA))\n\nsmart_ohio_raw |> count(`_RFBING5`, binge)\n\n# A tibble: 3 × 3\n  `_RFBING5` binge     n\n       <dbl> <dbl> <int>\n1          1     0  6035\n2          2     1  1000\n3          9    NA   377\n\n\n\n\n6.6.13.5 _DRNKWEK and its cleanup to drinks_wk\n_DRNKWEK provides the computed number of alcoholic drinks per week, with two implied decimal places. The code 99900 is used for “Don’t know / Not sure / Refused / Missing” so we’ll fix that, and also divide by 100 to get an average with a decimal point.\nNote: We’re also going to treat all results of 100 or more drinks per week as incorrect, and thus indicate them as missing data here.\n\nsmart_ohio_raw <- smart_ohio_raw |> \n    mutate(drinks_wk = `_DRNKWEK` / 100,\n           drinks_wk = replace(drinks_wk, drinks_wk > 99, NA))\n\nsmart_ohio_raw |> count(`_DRNKWEK`, drinks_wk) |> tail(12)\n\n# A tibble: 12 × 3\n   `_DRNKWEK` drinks_wk     n\n        <dbl>     <dbl> <int>\n 1       9333      93.3     2\n 2      10000      NA       1\n 3      10500      NA       2\n 4      11667      NA       1\n 5      14000      NA       2\n 6      16800      NA       2\n 7      17500      NA       1\n 8      18200      NA       1\n 9      28000      NA       1\n10      29400      NA       1\n11      53200      NA       1\n12      99900      NA     379\n\n\n\n\n6.6.13.6 _RFDRHV5 and its cleanup to drink_heavy\n_RFDRHV5 identifies heavy drinkers (males having 14 or more drinks per week, females having 7 or more drinks per week)\nThe values are\n\n1 = No\n2 = Yes\n9 = Don’t Know / Refused / Missing\n\nPeople who reported no alcdays are reported here as “No”, so we’ll adjust this into an indicator variable, and create the necessary NAs.\n\nsmart_ohio_raw <- smart_ohio_raw |> \n    mutate(drink_heavy = `_RFDRHV5` - 1,\n           drink_heavy = replace(drink_heavy, drink_heavy > 1, NA))\n\nsmart_ohio_raw |> count(`_RFDRHV5`, drink_heavy)\n\n# A tibble: 3 × 3\n  `_RFDRHV5` drink_heavy     n\n       <dbl>       <dbl> <int>\n1          1           0  6607\n2          2           1   426\n3          9          NA   379\n\n\n\n\n\n6.6.14 Fruits and Vegetables (8 items)\n\n6.6.14.1 _FRUTSU1 and its cleanup to fruit_day\n_FRUTSU1 provides the computed number of fruit servings consumed per day, with two implied decimal places. We’ll divide by 100 to insert the decimal point.\nNote: We’re also going to treat all results exceeding 16 servings per day as implausible, and thus indicate them as missing data here, following some CDC procedures.\n\nsmart_ohio_raw <- smart_ohio_raw |> \n    mutate(fruit_day = `_FRUTSU1` / 100,\n           fruit_day = replace(fruit_day, fruit_day > 16, NA))\n\nsmart_ohio_raw |> count(`_FRUTSU1`, fruit_day) |> tail()\n\n# A tibble: 6 × 3\n  `_FRUTSU1` fruit_day     n\n       <dbl>     <dbl> <int>\n1        913      9.13     1\n2       1000     10        4\n3       1400     14        1\n4       3000     NA        1\n5       7600     NA        1\n6         NA     NA      555\n\n\n\n\n6.6.14.2 _VEGESU1 and its cleanup to veg_day\n_VEGESU1 provides the computed number of vegetable servings consumed per day, with two implied decimal places. We’ll divide by 100 to insert the decimal point.\nNote: We’re also going to treat all results exceeding 23 servings per day as implausible, and thus indicate them as missing data here, following some CDC procedures.\n\nsmart_ohio_raw <- smart_ohio_raw |> \n    mutate(veg_day = `_VEGESU1` / 100,\n           veg_day = replace(veg_day, veg_day > 23, NA))\n\nsmart_ohio_raw |> count(`_VEGESU1`, veg_day) |> tail()\n\n# A tibble: 6 × 3\n  `_VEGESU1` veg_day     n\n       <dbl>   <dbl> <int>\n1       1414    14.1     1\n2       1603    16.0     1\n3       1891    18.9     1\n4       2167    21.7     1\n5       3150    NA       1\n6         NA    NA     666\n\n\n\n\n6.6.14.3 FTJUDA2_ and its cleanup to eat_juice\nFTJUDA2_ provides the servings of fruit juice consumed per day, with two implied decimal places. We’ll divide by 100 to insert the decimal point.\nNote: We’re also going to treat all results exceeding 16 servings per day as implausible, and thus indicate them as missing data here.\n\nsmart_ohio_raw <- smart_ohio_raw |> \n    mutate(eat_juice = `FTJUDA2_` / 100,\n           eat_juice = replace(eat_juice, eat_juice > 16, NA))\n\nsmart_ohio_raw |> count(`FTJUDA2_`, eat_juice) |> tail()\n\n# A tibble: 6 × 3\n  FTJUDA2_ eat_juice     n\n     <dbl>     <dbl> <int>\n1      500         5     6\n2      600         6     1\n3      700         7     1\n4     1200        12     1\n5     7500        NA     1\n6       NA        NA   469\n\n\n\n\n6.6.14.4 FRUTDA2_ and its cleanup to eat_fruit\nFRUTDA2_ provides the servings of fruit consumed per day, with two implied decimal places. We’ll divide by 100 to insert the decimal point.\nNote: We’re also going to treat all results exceeding 16 servings per day as implausible, and thus indicate them as missing data here.\n\nsmart_ohio_raw <- smart_ohio_raw |> \n    mutate(eat_fruit = `FRUTDA2_` / 100,\n           eat_fruit = replace(eat_fruit, eat_fruit > 16, NA))\n\nsmart_ohio_raw |> count(`FRUTDA2_`, eat_fruit) |> tail()\n\n# A tibble: 6 × 3\n  FRUTDA2_ eat_fruit     n\n     <dbl>     <dbl> <int>\n1      700         7     5\n2      800         8     3\n3      900         9     1\n4     1000        10     1\n5     3000        NA     1\n6       NA        NA   456\n\n\n\n\n6.6.14.5 GRENDA1_ and its cleanup to eat_greenveg\nGRENDA1_ provides the servings of dark green vegetables consumed per day, with two implied decimal places. We’ll divide by 100 to insert the decimal point.\nNote: We’re also going to treat all results exceeding 16 servings per day as implausible, and thus indicate them as missing data here.\n\nsmart_ohio_raw <- smart_ohio_raw |> \n    mutate(eat_greenveg = `GRENDA1_` / 100,\n           eat_greenveg = replace(eat_greenveg, eat_greenveg > 16, NA))\n\nsmart_ohio_raw |> count(`GRENDA1_`, eat_greenveg) |> tail()\n\n# A tibble: 6 × 3\n  GRENDA1_ eat_greenveg     n\n     <dbl>        <dbl> <int>\n1      700         7        4\n2      786         7.86     1\n3      800         8        2\n4     2000        NA        1\n5     3000        NA        1\n6       NA        NA      447\n\n\n\n\n6.6.14.6 FRNCHDA_ and its cleanup to eat_fries\nFRNCHDA_ provides the servings of french fries consumed per day, with two implied decimal places. We’ll divide by 100 to insert the decimal point.\nNote: We’re also going to treat all results exceeding 16 servings per day as implausible, and thus indicate them as missing data here.\n\nsmart_ohio_raw <- smart_ohio_raw |> \n    mutate(eat_fries = `FRNCHDA_` / 100,\n           eat_fries = replace(eat_fries, eat_fries > 16, NA))\n\nsmart_ohio_raw |> count(`FRNCHDA_`, eat_fries) |> tail()\n\n# A tibble: 6 × 3\n  FRNCHDA_ eat_fries     n\n     <dbl>     <dbl> <int>\n1      300      3        9\n2      314      3.14     1\n3      400      4        3\n4      500      5        1\n5      700      7        1\n6       NA     NA      453\n\n\n\n\n6.6.14.7 POTADA1_ and its cleanup to eat_potato\nPOTADA1_ provides the servings of potatoes consumed per day, with two implied decimal places. We’ll divide by 100 to insert the decimal point.\nNote: We’re also going to treat all results exceeding 16 servings per day as implausible, and thus indicate them as missing data here.\n\nsmart_ohio_raw <- smart_ohio_raw |> \n    mutate(eat_potato = `POTADA1_` / 100,\n           eat_potato = replace(eat_potato, eat_potato > 16, NA))\n\nsmart_ohio_raw |> count(`POTADA1_`, eat_potato) |> tail()\n\n# A tibble: 6 × 3\n  POTADA1_ eat_potato     n\n     <dbl>      <dbl> <int>\n1      314       3.14     1\n2      329       3.29     1\n3      400       4        3\n4      471       4.71     1\n5      700       7        1\n6       NA      NA      501\n\n\n\n\n6.6.14.8 VEGEDA2_ and its cleanup to eat_otherveg\nVEGEDA2_ provides the servings of other vegetables consumed per day, with two implied decimal places. We’ll divide by 100 to insert the decimal point.\nNote: We’re also going to treat all results exceeding 16 servings per day as implausible, and thus indicate them as missing data here.\n\nsmart_ohio_raw <- smart_ohio_raw |> \n    mutate(eat_otherveg = `VEGEDA2_` / 100,\n           eat_otherveg = replace(eat_otherveg, eat_otherveg > 16, NA))\n\nsmart_ohio_raw |> count(`VEGEDA2_`, eat_otherveg) |> tail()\n\n# A tibble: 6 × 3\n  VEGEDA2_ eat_otherveg     n\n     <dbl>        <dbl> <int>\n1      600            6     3\n2      700            7    11\n3      800            8     1\n4     1000           10     2\n5     1100           11     1\n6       NA           NA   509\n\n\n\n\n\n6.6.15 Exercise and Physical Activity (8 items)\n\n6.6.15.1 _TOTINDA and its cleanup to exerany\n_TOTINDA, the Exercise in Past 30 Days variable, is the response to “During the past month, other than your regular job, did you participate in any physical activities or exercises such as running, calisthenics, golf, gardening, or walking for exercise?”\n\n1 = Yes\n2 = No\n7 = Don’t know/Not sure\n9 = Refused\nBLANK = Not asked or missing\n\nThis is just like HLTHPLAN.\n\nsmart_ohio_raw <- smart_ohio_raw |> \n    mutate(exerany = `_TOTINDA`,\n           exerany = replace(exerany, exerany %in% c(7, 9), NA),\n           exerany = replace(exerany, exerany == 2, 0))\n\nsmart_ohio_raw |> count(`_TOTINDA`, exerany)\n\n# A tibble: 3 × 3\n  `_TOTINDA` exerany     n\n       <dbl>   <dbl> <int>\n1          1       1  4828\n2          2       0  2137\n3          9      NA   447\n\n\n\n\n6.6.15.2 _PACAT1 and its cleanup to activity\n_PACAT1 contains physical activity categories, estimated from responses to the BRFSS. The categories are:\n\n1 = Highly Active\n2 = Active\n3 = Insufficiently Active\n4 = Inactive\n9 = Don’t Know / Not Sure / Refused / Missing\n\nSo we’ll create a factor.\n\nsmart_ohio_raw <- smart_ohio_raw |>\n    mutate(activity = factor(`_PACAT1`),\n           activity = fct_recode(activity,\n                               \"Highly_Active\" = \"1\",\n                               \"Active\" = \"2\",\n                               \"Insufficiently_Active\" = \"3\",\n                               \"Inactive\" = \"4\",\n                               NULL = \"9\"))\n\nsmart_ohio_raw |> count(`_PACAT1`, activity)\n\n# A tibble: 5 × 3\n  `_PACAT1` activity                  n\n      <dbl> <fct>                 <int>\n1         1 Highly_Active          2053\n2         2 Active                 1132\n3         3 Insufficiently_Active  1293\n4         4 Inactive               2211\n5         9 <NA>                    723\n\n\n\n\n6.6.15.3 _PAINDX1 and its cleanup to rec_aerobic\n_PAINDX1 indicates whether the respondent’s stated levels of physical activity meet recommendations for aerobic activity. The responses are:\n\n1 = Yes\n2 = No\n9 = Don’t know/Not sure/Refused/Missing\n\n\nsmart_ohio_raw <- smart_ohio_raw |> \n    mutate(rec_aerobic = 2 - `_PAINDX1`,\n           rec_aerobic = replace(rec_aerobic, rec_aerobic < 0, NA))\n\nsmart_ohio_raw |> count(`_PAINDX1`, rec_aerobic)\n\n# A tibble: 3 × 3\n  `_PAINDX1` rec_aerobic     n\n       <dbl>       <dbl> <int>\n1          1           1  3228\n2          2           0  3504\n3          9          NA   680\n\n\n\n\n6.6.15.4 _PASTRNG and its cleanup to rec_strength\n_PASTRNG indicates whether the respondent’s stated levels of physical activity meet recommendations for strength-building activity. The responses are:\n\n1 = Yes\n2 = No\n9 = Don’t know/Not sure/Refused/Missing\n\n\nsmart_ohio_raw <- smart_ohio_raw |> \n    mutate(rec_strength = 2 - `_PASTRNG`,\n           rec_strength = replace(rec_strength, rec_strength < 0, NA))\n\nsmart_ohio_raw |> count(`_PASTRNG`, rec_strength)\n\n# A tibble: 3 × 3\n  `_PASTRNG` rec_strength     n\n       <dbl>        <dbl> <int>\n1          1            1  1852\n2          2            0  5004\n3          9           NA   556\n\n\n\n\n6.6.15.5 EXRACT11 and its cleanup to exer1_type\nRespondents are asked “What type of physical activity or exercise did you spend the most time doing during the past month?” and these responses are gathered into a set of 76 named categories, including an “other” category. Codes 77 (Don’t Know / Not Sure) and 99 (Refused) are dropped into NA in my code below, and Code 98 (“Other type of activity”) remains. Then I went through the tedious work of converting the factor levels from numbers to names, following the value labels provided by BRFSS.\n\nsmart_ohio_raw <- smart_ohio_raw |>\n    mutate(exer1_type = factor(EXRACT11),\n           exer1_type = fct_recode(\n               exer1_type,\n               \"Active Gaming Devices\" = \"1\",\n               \"Aerobics video or class\" = \"2\",\n               \"Backpacking\" = \"3\",\n               \"Badminton\" = \"4\",\n               \"Basketball\" = \"5\",\n               \"Bicycling machine\" = \"6\",\n               \"Bicycling\" = \"7\",\n               \"Boating\" = \"8\",\n               \"Bowling\" = \"9\",\n               \"Boxing\" = \"10\",\n               \"Calisthenics\" = \"11\",\n               \"Canoeing\" = \"12\",\n               \"Carpentry\" = \"13\",\n               \"Dancing\" = \"14\",\n               \"Elliptical machine\" = \"15\",\n               \"Fishing\" = \"16\",\n               \"Frisbee\" = \"17\",\n               \"Gardening\" = \"18\",\n               \"Golf with cart\" = \"19\",\n               \"Golf without cart\" = \"20\",\n               \"Handball\" = \"21\",\n               \"Hiking\" = \"22\",\n               \"Hockey\" = \"23\",\n               \"Horseback riding\" = \"24\",\n               \"Hunting large game\" = \"25\",\n               \"Hunting small game\" = \"26\",\n               \"Inline skating\" = \"27\",\n               \"Jogging\" = \"28\",\n               \"Lacrosse\" = \"29\",\n               \"Mountain climbing\" = \"30\",\n               \"Mowing lawn\" = \"31\",\n               \"Paddleball\" = \"32\",\n               \"Painting house\" = \"33\",\n               \"Pilates\" = \"34\",\n               \"Racquetball\" = \"35\",\n               \"Raking lawn\" = \"36\",\n               \"Running\" = \"37\",\n               \"Rock climbing\" = \"38\",\n               \"Rope skipping\" = \"39\",\n               \"Rowing machine\" = \"40\",\n               \"Rugby\" = \"41\",\n               \"Scuba diving\" = \"42\",\n               \"Skateboarding\" = \"43\",\n               \"Skating\" = \"44\",\n               \"Sledding\" = \"45\",\n               \"Snorkeling\" = \"46\",\n               \"Snow blowing\" = \"47\",\n               \"Snow shoveling\" = \"48\",\n               \"Snow skiing\" = \"49\",\n               \"Snowshoeing\" = \"50\",\n               \"Soccer\" = \"51\",\n               \"Softball/Baseball\" = \"52\",\n               \"Squash\" = \"53\",\n               \"Stair Climbing\" = \"54\",\n               \"Stream fishing\" = \"55\",\n               \"Surfing\" = \"56\",\n               \"Swimming\" = \"57\",\n               \"Swimming in laps\" = \"58\",\n               \"Table tennis\" = \"59\",\n               \"Tai Chi\" = \"60\",\n               \"Tennis\" = \"61\",\n               \"Touch football\" = \"62\",\n               \"Volleyball\" = \"63\",\n               \"Walking\" = \"64\",\n               \"Waterskiing\" = \"66\",\n               \"Weight lifting\" = \"67\",\n               \"Wrestling\" = \"68\",\n               \"Yoga\" = \"69\",\n               \"Child Care\" = \"71\",\n               \"Farm Work\" = \"72\",\n               \"Household Activities\" = \"73\",\n               \"Martial Arts\" = \"74\",\n               \"Upper Body Cycle\" = \"75\",\n               \"Yard Work\" = \"76\",\n               \"Other Activities\" = \"98\",\n               NULL = \"77\", \n               NULL = \"99\")\n    )\n\nWarning: There was 1 warning in `mutate()`.\nℹ In argument: `exer1_type = fct_recode(...)`.\nCaused by warning:\n! Unknown levels in `f`: 3, 17, 21, 32, 36, 41, 42, 45, 47, 53, 55, 56, 59\n\n\nThe warning generated here is caused by the fact that some of the available types of exercise were not mentioned by people in our sample. Looking at the last few results, we can see how many people fell into several categories.\n\nsmart_ohio_raw |> count(EXRACT11, exer1_type) |> tail()\n\n# A tibble: 6 × 3\n  EXRACT11 exer1_type           n\n     <dbl> <fct>            <int>\n1       75 Upper Body Cycle     6\n2       76 Yard Work           78\n3       77 <NA>                10\n4       98 Other Activities   276\n5       99 <NA>                 4\n6       NA <NA>              2588\n\n\nThe most common activities are:\n\nsmart_ohio_raw |> count(exer1_type, sort = TRUE) |> head(10)\n\n# A tibble: 10 × 2\n   exer1_type                  n\n   <fct>                   <int>\n 1 Walking                  2605\n 2 <NA>                     2602\n 3 Running                   324\n 4 Other Activities          276\n 5 Gardening                 242\n 6 Weight lifting            189\n 7 Aerobics video or class   103\n 8 Bicycling machine         103\n 9 Bicycling                  96\n10 Golf with cart             90\n\n\n\n\n6.6.15.6 EXRACT21 and its cleanup to exer2_type\nAs a follow-up, respondents are asked “What other type of physical activity gave you the next most exercise during the past month?” and these responses are also gathered into the same set of 76 named categories, including an “other” category, but now also adding a “No Other Activity” category (code 88). Codes 77 (Don’t Know / Not Sure) and 99 (Refused) are dropped into NA in my code below, and Code 98 (“Other type of activity”) remains. Then I went through the tedious work of converting the factor levels from numbers to names, following the value labels provided by BRFSS. I’m sure there’s a better way to do this.\n\nsmart_ohio_raw <- smart_ohio_raw |>\n    mutate(exer2_type = factor(EXRACT21),\n           exer2_type = fct_recode(\n               exer2_type,\n               \"Active Gaming Devices\" = \"1\",\n               \"Aerobics video or class\" = \"2\",\n               \"Backpacking\" = \"3\",\n               \"Badminton\" = \"4\",\n               \"Basketball\" = \"5\",\n               \"Bicycling machine\" = \"6\",\n               \"Bicycling\" = \"7\",\n               \"Boating\" = \"8\",\n               \"Bowling\" = \"9\",\n               \"Boxing\" = \"10\",\n               \"Calisthenics\" = \"11\",\n               \"Canoeing\" = \"12\",\n               \"Carpentry\" = \"13\",\n               \"Dancing\" = \"14\",\n               \"Elliptical machine\" = \"15\",\n               \"Fishing\" = \"16\",\n               \"Frisbee\" = \"17\",\n               \"Gardening\" = \"18\",\n               \"Golf with cart\" = \"19\",\n               \"Golf without cart\" = \"20\",\n               \"Handball\" = \"21\",\n               \"Hiking\" = \"22\",\n               \"Hockey\" = \"23\",\n               \"Horseback riding\" = \"24\",\n               \"Hunting large game\" = \"25\",\n               \"Hunting small game\" = \"26\",\n               \"Inline skating\" = \"27\",\n               \"Jogging\" = \"28\",\n               \"Lacrosse\" = \"29\",\n               \"Mountain climbing\" = \"30\",\n               \"Mowing lawn\" = \"31\",\n               \"Paddleball\" = \"32\",\n               \"Painting house\" = \"33\",\n               \"Pilates\" = \"34\",\n               \"Racquetball\" = \"35\",\n               \"Raking lawn\" = \"36\",\n               \"Running\" = \"37\",\n               \"Rock climbing\" = \"38\",\n               \"Rope skipping\" = \"39\",\n               \"Rowing machine\" = \"40\",\n               \"Rugby\" = \"41\",\n               \"Scuba diving\" = \"42\",\n               \"Skateboarding\" = \"43\",\n               \"Skating\" = \"44\",\n               \"Sledding\" = \"45\",\n               \"Snorkeling\" = \"46\",\n               \"Snow blowing\" = \"47\",\n               \"Snow shoveling\" = \"48\",\n               \"Snow skiing\" = \"49\",\n               \"Snowshoeing\" = \"50\",\n               \"Soccer\" = \"51\",\n               \"Softball/Baseball\" = \"52\",\n               \"Squash\" = \"53\",\n               \"Stair Climbing\" = \"54\",\n               \"Stream fishing\" = \"55\",\n               \"Surfing\" = \"56\",\n               \"Swimming\" = \"57\",\n               \"Swimming in laps\" = \"58\",\n               \"Table tennis\" = \"59\",\n               \"Tai Chi\" = \"60\",\n               \"Tennis\" = \"61\",\n               \"Touch football\" = \"62\",\n               \"Volleyball\" = \"63\",\n               \"Walking\" = \"64\",\n               \"Waterskiing\" = \"66\",\n               \"Weight lifting\" = \"67\",\n               \"Wrestling\" = \"68\",\n               \"Yoga\" = \"69\",\n               \"Child Care\" = \"71\",\n               \"Farm Work\" = \"72\",\n               \"Household Activities\" = \"73\",\n               \"Martial Arts\" = \"74\",\n               \"Upper Body Cycle\" = \"75\",\n               \"Yard Work\" = \"76\",\n               \"No Other Activity\" = \"88\",\n               \"Other Activities\" = \"98\",\n               NULL = \"77\", \n               NULL = \"99\")\n    )\n\nWarning: There was 1 warning in `mutate()`.\nℹ In argument: `exer2_type = fct_recode(...)`.\nCaused by warning:\n! Unknown levels in `f`: 3, 21, 30, 39, 41, 46, 50, 62\n\nsmart_ohio_raw |> count(EXRACT21, exer2_type) |> tail()\n\n# A tibble: 6 × 3\n  EXRACT21 exer2_type            n\n     <dbl> <fct>             <int>\n1       76 Yard Work           153\n2       77 <NA>                 26\n3       88 No Other Activity  1854\n4       98 Other Activities    246\n5       99 <NA>                 19\n6       NA <NA>               2627\n\n\nThe most common activity types in this group are:\n\nsmart_ohio_raw |> count(exer2_type, sort = TRUE) |> head(10)\n\n# A tibble: 10 × 2\n   exer2_type               n\n   <fct>                <int>\n 1 <NA>                  2672\n 2 No Other Activity     1854\n 3 Walking                629\n 4 Weight lifting         272\n 5 Other Activities       246\n 6 Gardening              202\n 7 Household Activities   169\n 8 Yard Work              153\n 9 Running                148\n10 Bicycling              118\n\n\n\n\n6.6.15.7 _MINAC11 and its cleanup to exer1_min\n_MINAC11 is minutes of physical activity per week for the first activity (listed as exer1_type above.) Since there are only about 10,080 minutes in a typical week, we’ll treat as implausible any values larger than 4200 minutes (which would indicate 70 hours per week.)\n\nsmart_ohio_raw <- smart_ohio_raw |> \n    mutate(exer1_min = `_MINAC11`,\n           exer1_min = replace(exer1_min, exer1_min > 4200, NA))\n\nsmart_ohio_raw |> count(`_MINAC11`, exer1_min) |> tail()\n\n# A tibble: 6 × 3\n  `_MINAC11` exer1_min     n\n       <dbl>     <dbl> <int>\n1       3780      3780     8\n2       3959      3959     1\n3       3960      3960     1\n4       4193      4193     6\n5      27000        NA     1\n6         NA        NA  2760\n\n\n\n\n6.6.15.8 _MINAC21 and its cleanup to exer2_min\n_MINAC21 is minutes of physical activity per week for the second activity (listed as exer2_type above.) Again, we’ll treat as implausible any values larger than 4200 minutes (which would indicate 70 hours per week.)\n\nsmart_ohio_raw <- smart_ohio_raw |> \n    mutate(exer2_min = `_MINAC21`,\n           exer2_min = replace(exer2_min, exer2_min > 4200, NA))\n\nsmart_ohio_raw |> count(`_MINAC21`, exer2_min) |> tail()\n\n# A tibble: 6 × 3\n  `_MINAC21` exer2_min     n\n       <dbl>     <dbl> <int>\n1       3360      3360     3\n2       3780      3780     7\n3       4193      4193     3\n4       6120        NA     1\n5       8400        NA     1\n6         NA        NA  2770\n\n\n\n\n\n6.6.16 Seatbelt Use (1 item)\n\n6.6.16.1 SEATBELT and its cleanup to seatbelt\nThis question asks “How often do you use seat belts when you drive or ride in a car?” Possible responses are:\n\n1 = Always\n2 = Nearly always\n3 = Sometimes\n4 = Seldom\n5 = Never\n7 = Don’t know / Not sure\n8 = Never drive or ride in a car\n9 = Refused\n\nWe’ll treat codes 7, 8 and 9 as NA, and turn this into a factor.\n\nsmart_ohio_raw <- smart_ohio_raw |> \n    mutate(seatbelt = fct_recode(factor(SEATBELT),\n                                  \"Always\" = \"1\",\n                                  \"Nearly_always\" = \"2\",\n                                  \"Sometimes\" = \"3\",\n                                  \"Seldom\" = \"4\", \n                                  \"Never\" = \"5\",\n                                  NULL = \"7\",\n                                  NULL = \"8\",\n                                  NULL = \"9\"))\n\nsmart_ohio_raw |> count(SEATBELT, seatbelt)\n\n# A tibble: 9 × 3\n  SEATBELT seatbelt          n\n     <dbl> <fct>         <int>\n1        1 Always         6047\n2        2 Nearly_always   409\n3        3 Sometimes       191\n4        4 Seldom           81\n5        5 Never           148\n6        7 <NA>              7\n7        8 <NA>             21\n8        9 <NA>              2\n9       NA <NA>            506\n\n\n\n\n\n6.6.17 Immunization (3 items)\n\n6.6.17.1 FLUSHOT6 and its cleanup to vax_flu\nFLUSHOT6 gives the response to “During the past 12 months, have you had either a flu shot or a flu vaccine that was sprayed in your nose?” The responses are:\n\n1 = Yes\n2 = No\n7 = Don’t know/Not sure\n9 = Refused\n\n\nsmart_ohio_raw <- smart_ohio_raw |> \n    mutate(vax_flu = 2 - FLUSHOT6,\n           vax_flu = replace(vax_flu, vax_flu < 0, NA))\n\nsmart_ohio_raw |> count(FLUSHOT6, vax_flu)\n\n# A tibble: 5 × 3\n  FLUSHOT6 vax_flu     n\n     <dbl>   <dbl> <int>\n1        1       1  3453\n2        2       0  3410\n3        7      NA    26\n4        9      NA     3\n5       NA      NA   520\n\n\n\n\n6.6.17.2 PNEUVAC3 and its cleanup to vax_pneumo\nPNEUVAC3 gives the response to “A pneumonia shot or pneumococcal vaccine is usually given only once or twice in a person’s lifetime and is different from the flu shot. Have you ever had a pneumonia shot?” The responses are:\n\n1 = Yes\n2 = No\n7 = Don’t know/Not sure\n9 = Refused\n\n\nsmart_ohio_raw <- smart_ohio_raw |> \n    mutate(vax_pneumo = 2 - PNEUVAC3,\n           vax_pneumo = replace(vax_pneumo, vax_pneumo < 0, NA))\n\nsmart_ohio_raw |> count(PNEUVAC3, vax_pneumo)\n\n# A tibble: 5 × 3\n  PNEUVAC3 vax_pneumo     n\n     <dbl>      <dbl> <int>\n1        1          1  3112\n2        2          0  3262\n3        7         NA   509\n4        9         NA     3\n5       NA         NA   526\n\n\n\n\n6.6.17.3 SHINGLE2 and its cleanup to vax_shingles\nSHINGLE2 gives the response to “Have you ever had the shingles or zoster vaccine?” The responses are:\n\n1 = Yes\n2 = No\n7 = Don’t know/Not sure\n9 = Refused\n\n\nsmart_ohio_raw <- smart_ohio_raw |> \n    mutate(vax_shingles = 2 - SHINGLE2,\n           vax_shingles = replace(vax_shingles, vax_shingles < 0, NA))\n\nsmart_ohio_raw |> count(SHINGLE2, vax_shingles)\n\n# A tibble: 4 × 3\n  SHINGLE2 vax_shingles     n\n     <dbl>        <dbl> <int>\n1        1            1  1503\n2        2            0  2979\n3        7           NA    78\n4       NA           NA  2852\n\n\n\n\n\n6.6.18 HIV/AIDS (2 items)\n\n6.6.18.1 HIVTST6 and its cleanup to hiv_test\nHIVTST6 gives the response to “Have you ever been tested for HIV? Do not count tests you may have had as part of a blood donation. Include testing fluid from your mouth.” The responses are:\n\n1 = Yes\n2 = No\n7 = Don’t know/Not sure\n9 = Refused\n\n\nsmart_ohio_raw <- smart_ohio_raw |> \n    mutate(hiv_test = 2 - HIVTST6,\n           hiv_test = replace(hiv_test, hiv_test < 0, NA))\n\nsmart_ohio_raw |> count(HIVTST6, hiv_test)\n\n# A tibble: 5 × 3\n  HIVTST6 hiv_test     n\n    <dbl>    <dbl> <int>\n1       1        1  2017\n2       2        0  4565\n3       7       NA   260\n4       9       NA    14\n5      NA       NA   556\n\n\n\n\n6.6.18.2 HIVRISK5 and its cleanup to hiv_risk\nHIVRISK5 gives the response to “I am going to read you a list. When I am done, please tell me if any of the situations apply to you. You do not need to tell me which one. You have injected any drug other than those prescribed for you in the past year. You have been treated for a sexually transmitted disease or STD in the past year. You have given or received money or drugs in exchange for sex in the past year.” The responses are:\n\n1 = Yes\n2 = No\n7 = Don’t know/Not sure\n9 = Refused\n\n\nsmart_ohio_raw <- smart_ohio_raw |> \n    mutate(hiv_risk = 2 - HIVRISK5,\n           hiv_risk = replace(hiv_risk, hiv_risk < 0, NA))\n\nsmart_ohio_raw |> count(HIVRISK5, hiv_risk)\n\n# A tibble: 5 × 3\n  HIVRISK5 hiv_risk     n\n     <dbl>    <dbl> <int>\n1        1        1   277\n2        2        0  6537\n3        7       NA     2\n4        9       NA    17\n5       NA       NA   579"
  },
  {
    "objectID": "smart.html#imputing-age-and-income-as-quantitative-from-thin-air",
    "href": "smart.html#imputing-age-and-income-as-quantitative-from-thin-air",
    "title": "6  BRFSS SMART Data",
    "section": "6.7 Imputing Age and Income as Quantitative from Thin Air",
    "text": "6.7 Imputing Age and Income as Quantitative from Thin Air\nThis section is purely for teaching purposes. I would never use the variables created in this section for research work.\n\n6.7.1 age_imp: Imputing Age Data\nI want a quantitative age variable, so I’m going to create an imputed age_imp value for each subject based on their agegroup. For each age group, I will assume that each of the ages represented by a value in that age group will be equally likely, and will draw from the relevant uniform distribution to impute age.\n\nset.seed(2020432002)\n\nsmart_ohio_raw <- smart_ohio_raw |>\n    mutate(age_low = as.numeric(str_sub(as.character(agegroup), 1, 2))) |>\n    mutate(age_high = as.numeric(str_sub(as.character(agegroup), 4, 5))) |>\n    rowwise() |>\n    mutate(age_imp = ifelse(!is.na(agegroup), \n                            round(runif(1, min = age_low, max = age_high),0),\n                            NA)) \n\nsmart_ohio_raw |> count(agegroup, age_imp) #|> tail()\n\n# A tibble: 80 × 3\n# Rowwise: \n   agegroup age_imp     n\n   <fct>      <dbl> <int>\n 1 18-24         18    46\n 2 18-24         19    75\n 3 18-24         20    76\n 4 18-24         21    82\n 5 18-24         22    80\n 6 18-24         23    54\n 7 18-24         24    35\n 8 25-29         25    42\n 9 25-29         26    93\n10 25-29         27    77\n# … with 70 more rows\n\n\nHere is a histogram of the age_imp variable.\n\nggplot(smart_ohio_raw, aes(x = age_imp)) +\n    geom_histogram(fill = \"navy\", col = \"white\",\n                   binwidth = 1) +\n    scale_x_continuous(breaks = c(18, 25, 35, 45, 55, 65, 75, 85, 96)) +\n    labs(x = \"Imputed Age in Years\",\n         title = paste0(\"Imputed Income: \", \n                        sum(is.na(smart_ohio_raw$age_imp)), \n                        \" respondents have missing age group\"))\n\n\n\n\n\n\n6.7.2 inc_imp: Imputing Income Data\nI want a quantitative income variable, so I’m going to create an imputed inc_imp value for each subject based on their incomegroup. For most income groups, I will assume that each of the incomes represented by a value in that income group will be equally likely, and will draw from the relevant uniform distribution to impute income. The exception is the highest income group, where I will impute a value drawn from a distribution that places all values at $75,000 or more, but has a substantial right skew and long tail.\n\nset.seed(2020432001)\n\nsmart_ohio_raw <- smart_ohio_raw |>\n    mutate(inc_imp = case_when(\n        incomegroup == \"0-9K\" ~ round(runif(1, min = 100, max = 9999)),\n        incomegroup == \"10-14K\" ~ round(runif(1, min = 10000, max = 14999)),\n        incomegroup == \"15-19K\" ~ round(runif(1, min = 15000, max = 19999)),\n        incomegroup == \"20-24K\" ~ round(runif(1, min = 20000, max = 24999)),\n        incomegroup == \"25-34K\" ~ round(runif(1, min = 25000, max = 34999)),\n        incomegroup == \"35-49K\" ~ round(runif(1, min = 35000, max = 49999)),\n        incomegroup == \"50-74K\" ~ round(runif(1, min = 50000, max = 74999)),\n        incomegroup == \"75K+\" ~ round((rnorm(n = 1, mean = 0, sd = 300)^2) + 74999)))\n\nsmart_ohio_raw |> count(incomegroup, inc_imp) |> tail()\n\n# A tibble: 6 × 3\n# Rowwise: \n  incomegroup inc_imp     n\n  <fct>         <dbl> <int>\n1 75K+         774009     1\n2 75K+         798174     1\n3 75K+         806161     1\n4 75K+         847758     1\n5 75K+        1085111     1\n6 <NA>             NA  1310\n\n\nHere are density plots of the inc_imp variable. The top picture shows the results on a linear scale, and the bottom shows them on a log (base 10) scale.\n\np1 <- ggplot(smart_ohio_raw, aes(x = inc_imp/1000)) +\n    geom_density(fill = \"darkgreen\", col = \"white\") +\n    labs(x = \"Imputed Income in Thousands of Dollars\",\n         title = \"Imputed Income on the Linear scale\") + \n    scale_x_continuous(breaks = c(25, 75, 250, 1000))\n\np2 <- ggplot(smart_ohio_raw, aes(x = inc_imp/1000)) +\n    geom_density(fill = \"darkgreen\", col = \"white\") +\n    labs(x = \"Imputed Income in Thousands of Dollars\",\n         title = \"Imputed Income on the Log (base 10) scale\") + \n    scale_x_log10(breaks = c(0.1, 1, 5, 25, 75, 250, 1000))\n\np1 / p2 + \n    plot_annotation(title = \n                        paste0(\"Imputed Income: \", sum(is.na(smart_ohio_raw$inc_imp)), \" respondents have missing income group\"))"
  },
  {
    "objectID": "smart.html#clean-data-in-the-state-of-ohio",
    "href": "smart.html#clean-data-in-the-state-of-ohio",
    "title": "6  BRFSS SMART Data",
    "section": "6.8 Clean Data in the State of Ohio",
    "text": "6.8 Clean Data in the State of Ohio\nThere are six MMSAs associated with the state of Ohio. We’re going to create a smart_ohio that includes each of them. First, I’ll ungroup the data that I created earlier, so I get a clean tibble.\n\nsmart_ohio_raw <- smart_ohio_raw |> ungroup()\n\nNext, I’ll select the variables I want to retain (they are the ones I created, plus SEQNO.)\n\nsmart_ohio <- smart_ohio_raw |>\n    select(SEQNO, mmsa, mmsa_code, mmsa_name, mmsa_wt, completed,\n           landline, hhadults, \n           genhealth, physhealth, menthealth, poorhealth, \n           agegroup, age_imp, race, hispanic, race_eth, \n           female, marital, kids, educgroup, home_own, \n           veteran, employment, incomegroup, inc_imp,\n           cell_own, internet30, \n           weight_kg, height_m, bmi, bmigroup, \n           pregnant, deaf, blind, decide, \n           diffwalk, diffdress, diffalone, \n           smoke100, smoker, ecig_ever, ecigs, \n           healthplan, hasdoc, costprob, t_checkup, \n           bp_high, bp_meds, \n           t_chol, chol_high, chol_meds,\n           asthma, hx_asthma, now_asthma, \n           hx_mi, hx_chd, hx_stroke, hx_skinc, hx_otherc, \n           hx_copd, hx_depress, hx_kidney, \n           hx_diabetes, dm_status, dm_age, \n           hx_arthr, arth_lims, arth_work, arth_soc, \n           joint_pain, alcdays, avgdrinks, maxdrinks, \n           binge, drinks_wk, drink_heavy, \n           fruit_day, veg_day, eat_juice, eat_fruit, \n           eat_greenveg, eat_fries, eat_potato, \n           eat_otherveg, exerany, activity, rec_aerobic, \n           rec_strength, exer1_type, exer2_type, \n           exer1_min, exer2_min, seatbelt,\n           vax_flu, vax_pneumo, vax_shingles, \n           hiv_test, hiv_risk)\n\nsaveRDS(smart_ohio, \"data/smart_ohio.Rds\")\n\nwrite_csv(smart_ohio, \"data/smart_ohio.csv\")\n\nThe smart_ohio file should contain 99 variables, describing 7412 respondents."
  },
  {
    "objectID": "smart.html#clean-cleveland-elyria-data",
    "href": "smart.html#clean-cleveland-elyria-data",
    "title": "6  BRFSS SMART Data",
    "section": "6.9 Clean Cleveland-Elyria Data",
    "text": "6.9 Clean Cleveland-Elyria Data\n\n6.9.1 Cleveland - Elyria Data\nThe mmsa_name variable is probably the simplest way for us to filter our data down to the MMSA we are interested in. Here, I’m using the str_detect function to identify the values of mmsa_name that contain the text “Cleveland”.\n\nsmart_cle <- smart_ohio |> \n  filter(str_detect(mmsa_name, 'Cleveland')) \n\nsaveRDS(smart_cle, \"data/smart_cle.Rds\")\n\nIn the Cleveland-Elyria MSA, we have 1133 observations on the same 99 variables.\nWe’ll build a variety of smaller subsets from these data, eventually."
  },
  {
    "objectID": "singleimputation.html#r-setup-used-here",
    "href": "singleimputation.html#r-setup-used-here",
    "title": "7  Dealing with Missingness: Single Imputation",
    "section": "7.1 R Setup Used Here",
    "text": "7.1 R Setup Used Here\n\nknitr::opts_chunk$set(comment = NA)\n\nlibrary(janitor) \nlibrary(ggridges)\nlibrary(knitr)\nlibrary(naniar)\nlibrary(simputation)\nlibrary(Hmisc)\nlibrary(mosaic)\nlibrary(patchwork)\nlibrary(visdat)\nlibrary(tidyverse) \n\ntheme_set(theme_bw())\n\n\n7.1.1 Data Load\n\nsmart_cle <- readRDS(\"data/smart_cle.Rds\")"
  },
  {
    "objectID": "singleimputation.html#selecting-some-variables-from-the-smart_cle-data",
    "href": "singleimputation.html#selecting-some-variables-from-the-smart_cle-data",
    "title": "7  Dealing with Missingness: Single Imputation",
    "section": "7.2 Selecting Some Variables from the smart_cle data",
    "text": "7.2 Selecting Some Variables from the smart_cle data\n\nsmart_cle1 <- smart_cle |> \n  select(SEQNO, physhealth, genhealth, bmi, \n         age_imp, female, race_eth, internet30, \n         smoke100, activity, drinks_wk, veg_day)\n\nThe smart_cle.Rds data file available on the Data and Code page of our website describes information on 99 variables for 1133 respondents to the BRFSS 2017, who live in the Cleveland-Elyria, OH, Metropolitan Statistical Area. The variables in the smart_cle1.csv file are listed below, along with the items that generate these responses.\n\n\n\n\n\n\n\nVariable\nDescription\n\n\n\n\nSEQNO\nrespondent identification number (all begin with 2016)\n\n\nphyshealth\nNow thinking about your physical health, which includes physical illness and injury, for how many days during the past 30 days was your physical health not good?\n\n\ngenhealth\nWould you say that in general, your health is … (five categories: Excellent, Very Good, Good, Fair or Poor)\n\n\nbmi\nBody mass index, in kg/m2\n\n\nage_imp\nAge, imputed, in years\n\n\nfemale\nSex, 1 = female, 0 = male\n\n\nrace_eth\nRace and Ethnicity, in five categories\n\n\ninternet30\nHave you used the internet in the past 30 days? (1 = yes, 0 = no)\n\n\nsmoke100\nHave you smoked at least 100 cigarettes in your life? (1 = yes, 0 = no)\n\n\nactivity\nPhysical activity (Highly Active, Active, Insufficiently Active, Inactive)\n\n\ndrinks_wk\nOn average, how many drinks of alcohol do you consume in a week?\n\n\nveg_day\nHow many servings of vegetables do you consume per day, on average?\n\n\n\n\nstr(smart_cle1)\n\ntibble [1,133 × 12] (S3: tbl_df/tbl/data.frame)\n $ SEQNO     : num [1:1133] 2.02e+09 2.02e+09 2.02e+09 2.02e+09 2.02e+09 ...\n $ physhealth: num [1:1133] 4 0 0 0 0 2 2 0 0 0 ...\n $ genhealth : Factor w/ 5 levels \"1_Excellent\",..: 1 1 3 3 3 2 3 2 4 1 ...\n $ bmi       : num [1:1133] NA 23.1 26.9 26.5 24.2 ...\n $ age_imp   : num [1:1133] 51 28 37 36 88 43 23 34 58 54 ...\n $ female    : num [1:1133] 1 1 1 1 0 0 0 0 0 1 ...\n $ race_eth  : Factor w/ 5 levels \"White non-Hispanic\",..: 1 1 3 1 1 1 1 3 2 1 ...\n $ internet30: num [1:1133] 1 1 0 1 1 1 1 1 1 1 ...\n $ smoke100  : num [1:1133] 1 0 0 1 1 1 0 0 0 1 ...\n $ activity  : Factor w/ 4 levels \"Highly_Active\",..: 4 4 3 1 1 NA 1 1 1 1 ...\n $ drinks_wk : num [1:1133] 0.7 0 0 4.67 0.93 0 2 0 0 0.47 ...\n $ veg_day   : num [1:1133] NA 3 4.06 2.07 1.31 NA 1.57 0.83 0.49 1.72 ..."
  },
  {
    "objectID": "singleimputation.html#smart_cle1-seeing-our-missing-data",
    "href": "singleimputation.html#smart_cle1-seeing-our-missing-data",
    "title": "7  Dealing with Missingness: Single Imputation",
    "section": "7.3 smart_cle1: Seeing our Missing Data",
    "text": "7.3 smart_cle1: Seeing our Missing Data\nThe naniar package provides several useful functions for summarizing missingness in our data set. Like all tidy data sets, our smart_cle1 tibble contains rows which describe observations, sometimes called cases, and also contains columns which describe variables.\nOverall, there are 1133 cases, and 1133 observations in our smart_cle1 tibble.\n\nWe can obtain a count of the number of missing cells in the entire tibble.\n\n\nsmart_cle1 |> n_miss()\n\n[1] 479\n\n\n\nWe can use the miss_var_summary function to get a sorted table of each variable by number missing.\n\n\nmiss_var_summary(smart_cle1) |> kable()\n\n\n\n\nvariable\nn_miss\npct_miss\n\n\n\n\nactivity\n109\n9.6204766\n\n\nveg_day\n101\n8.9143866\n\n\nbmi\n91\n8.0317741\n\n\ndrinks_wk\n66\n5.8252427\n\n\nsmoke100\n40\n3.5304501\n\n\nrace_eth\n26\n2.2947926\n\n\nphyshealth\n24\n2.1182701\n\n\nage_imp\n11\n0.9708738\n\n\ninternet30\n7\n0.6178288\n\n\ngenhealth\n4\n0.3530450\n\n\nSEQNO\n0\n0.0000000\n\n\nfemale\n0\n0.0000000\n\n\n\n\n\n\nOr we can use the miss_var_table function to tabulate the number of variables that have each observed level of missingness.\n\n\nmiss_var_table(smart_cle1) \n\n# A tibble: 11 × 3\n   n_miss_in_var n_vars pct_vars\n           <int>  <int>    <dbl>\n 1             0      2    16.7 \n 2             4      1     8.33\n 3             7      1     8.33\n 4            11      1     8.33\n 5            24      1     8.33\n 6            26      1     8.33\n 7            40      1     8.33\n 8            66      1     8.33\n 9            91      1     8.33\n10           101      1     8.33\n11           109      1     8.33\n\n\n\nOr we can get a count for a specific variable, like activity:\n\n\nsmart_cle1 |> select(activity) |> n_miss()\n\n[1] 109\n\n\n\nWe can also use prop_miss_case or pct_miss_case to specify the proportion (or percentage) of missing observations across an entire data set, or within a specific variable.\n\n\nprop_miss_case(smart_cle1)\n\n[1] 0.2127096\n\n\n\nsmart_cle1 |> select(activity) |> pct_miss_case()\n\n[1] 9.620477\n\n\n\nWe can also use prop_miss_var or pct_miss_var to specify the proportion (or percentage) of variables with missing observations across an entire data set.\n\n\nprop_miss_var(smart_cle1)\n\n[1] 0.8333333\n\npct_miss_var(smart_cle1)\n\n[1] 83.33333\n\n\n\nWe use miss_case_table to identify the number of missing values for each of the cases (rows) in our tibble.\n\n\nmiss_case_table(smart_cle1)\n\n# A tibble: 7 × 3\n  n_miss_in_case n_cases pct_cases\n           <int>   <int>     <dbl>\n1              0     892    78.7  \n2              1     129    11.4  \n3              2      51     4.50 \n4              3      22     1.94 \n5              4      21     1.85 \n6              5      10     0.883\n7              6       8     0.706\n\n\n\nUse miss_case_summary to specify individual observations and count their missing values.\n\n\nmiss_case_summary(smart_cle1)\n\n# A tibble: 1,133 × 3\n    case n_miss pct_miss\n   <int>  <int>    <dbl>\n 1    17      6     50  \n 2    42      6     50  \n 3   254      6     50  \n 4   425      6     50  \n 5   521      6     50  \n 6   729      6     50  \n 7   757      6     50  \n 8  1051      6     50  \n 9    89      5     41.7\n10    94      5     41.7\n# … with 1,123 more rows\n\n\nThe case numbers identified here are row numbers. Extract the data for case 17, for instance, with the slice function.\n\nsmart_cle1 |> slice(17)\n\n# A tibble: 1 × 12\n      SEQNO physh…¹ genhe…²   bmi age_imp female race_…³ inter…⁴ smoke…⁵ activ…⁶\n      <dbl>   <dbl> <fct>   <dbl>   <dbl>  <dbl> <fct>     <dbl>   <dbl> <fct>  \n1    2.02e9       0 1_Exce…    NA      50      0 White …      NA      NA <NA>   \n# … with 2 more variables: drinks_wk <dbl>, veg_day <dbl>, and abbreviated\n#   variable names ¹​physhealth, ²​genhealth, ³​race_eth, ⁴​internet30, ⁵​smoke100,\n#   ⁶​activity\n\n\n\n7.3.1 Plotting Missingness\nThe gg_miss_var function plots the number of missing observations in each variable in our data set.\n\ngg_miss_var(smart_cle1)\n\nWarning: The `guide` argument in `scale_*()` cannot be `FALSE`. This was deprecated in\nggplot2 3.3.4.\nℹ Please use \"none\" instead.\nℹ The deprecated feature was likely used in the naniar package.\n  Please report the issue at <https://github.com/njtierney/naniar/issues>.\n\n\n\n\n\nSo the most commonly missing variable is activity.\nTo get a general sense of the missingness in our data, we might use either the vis_dat or the vis_miss function from the visdat package.\n\nvis_miss(smart_cle1)\n\nWarning: `gather_()` was deprecated in tidyr 1.2.0.\nℹ Please use `gather()` instead.\nℹ The deprecated feature was likely used in the visdat package.\n  Please report the issue at <https://github.com/ropensci/visdat/issues>.\n\n\n\n\n\n\nvis_dat(smart_cle1)"
  },
  {
    "objectID": "singleimputation.html#missing-data-mechanisms",
    "href": "singleimputation.html#missing-data-mechanisms",
    "title": "7  Dealing with Missingness: Single Imputation",
    "section": "7.4 Missing-data mechanisms",
    "text": "7.4 Missing-data mechanisms\nMy source for this description of mechanisms is Chapter 25 of Gelman and Hill (2007), and that chapter is available at this link.\n\nMCAR = Missingness completely at random. A variable is missing completely at random if the probability of missingness is the same for all units, for example, if for each subject, we decide whether to collect the diabetes status by rolling a die and refusing to answer if a “6” shows up. If data are missing completely at random, then throwing out cases with missing data does not bias your inferences.\nMissingness that depends only on observed predictors. A more general assumption, called missing at random or MAR, is that the probability a variable is missing depends only on available information. Here, we would have to be willing to assume that the probability of nonresponse to diabetes depends only on the other, fully recorded variables in the data. It is often reasonable to model this process as a logistic regression, where the outcome variable equals 1 for observed cases and 0 for missing. When an outcome variable is missing at random, it is acceptable to exclude the missing cases (that is, to treat them as NA), as long as the regression controls for all the variables that affect the probability of missingness.\nMissingness that depends on unobserved predictors. Missingness is no longer “at random” if it depends on information that has not been recorded and this information also predicts the missing values. If a particular treatment causes discomfort, a patient is more likely to drop out of the study. This missingness is not at random (unless “discomfort” is measured and observed for all patients). If missingness is not at random, it must be explicitly modeled, or else you must accept some bias in your inferences.\nMissingness that depends on the missing value itself. Finally, a particularly difficult situation arises when the probability of missingness depends on the (potentially missing) variable itself. For example, suppose that people with higher earnings are less likely to reveal them.\n\nEssentially, situations 3 and 4 are referred to collectively as non-random missingness, and cause more trouble for us than 1 and 2."
  },
  {
    "objectID": "singleimputation.html#options-for-dealing-with-missingness",
    "href": "singleimputation.html#options-for-dealing-with-missingness",
    "title": "7  Dealing with Missingness: Single Imputation",
    "section": "7.5 Options for Dealing with Missingness",
    "text": "7.5 Options for Dealing with Missingness\nThere are several available methods for dealing with missing data that are MCAR or MAR, but they basically boil down to:\n\nComplete Case (or Available Case) analyses\nSingle Imputation\nMultiple Imputation"
  },
  {
    "objectID": "singleimputation.html#complete-case-and-available-case-analyses",
    "href": "singleimputation.html#complete-case-and-available-case-analyses",
    "title": "7  Dealing with Missingness: Single Imputation",
    "section": "7.6 Complete Case (and Available Case) analyses",
    "text": "7.6 Complete Case (and Available Case) analyses\nIn Complete Case analyses, rows containing NA values are omitted from the data before analyses commence. This is the default approach for many statistical software packages, and may introduce unpredictable bias and fail to include some useful, often hard-won information.\n\nA complete case analysis can be appropriate when the number of missing observations is not large, and the missing pattern is either MCAR (missing completely at random) or MAR (missing at random.)\nTwo problems arise with complete-case analysis:\n\nIf the units with missing values differ systematically from the completely observed cases, this could bias the complete-case analysis.\nIf many variables are included in a model, there may be very few complete cases, so that most of the data would be discarded for the sake of a straightforward analysis.\n\nA related approach is available-case analysis where different aspects of a problem are studied with different subsets of the data, perhaps identified on the basis of what is missing in them."
  },
  {
    "objectID": "singleimputation.html#single-imputation",
    "href": "singleimputation.html#single-imputation",
    "title": "7  Dealing with Missingness: Single Imputation",
    "section": "7.7 Single Imputation",
    "text": "7.7 Single Imputation\nIn single imputation analyses, NA values are estimated/replaced one time with one particular data value for the purpose of obtaining more complete samples, at the expense of creating some potential bias in the eventual conclusions or obtaining slightly less accurate estimates than would be available if there were no missing values in the data.\n\nA single imputation can be just a replacement with the mean or median (for a quantity) or the mode (for a categorical variable.) However, such an approach, though easy to understand, underestimates variance and ignores the relationship of missing values to other variables.\nSingle imputation can also be done using a variety of models to try to capture information about the NA values that are available in other variables within the data set.\nThe simputation package can help us execute single imputations using a wide variety of techniques, within the pipe approach used by the tidyverse. Another approach I have used in the past is the mice package, which can also perform single imputations."
  },
  {
    "objectID": "singleimputation.html#multiple-imputation",
    "href": "singleimputation.html#multiple-imputation",
    "title": "7  Dealing with Missingness: Single Imputation",
    "section": "7.8 Multiple Imputation",
    "text": "7.8 Multiple Imputation\nMultiple imputation, where NA values are repeatedly estimated/replaced with multiple data values, for the purpose of obtaining mode complete samples and capturing details of the variation inherent in the fact that the data have missingness, so as to obtain more accurate estimates than are possible with single imputation.\n\nWe’ll postpone the discussion of multiple imputation for a while."
  },
  {
    "objectID": "singleimputation.html#approach-1-building-a-complete-case-analysis-smart_cle1_cc",
    "href": "singleimputation.html#approach-1-building-a-complete-case-analysis-smart_cle1_cc",
    "title": "7  Dealing with Missingness: Single Imputation",
    "section": "7.9 Approach 1: Building a Complete Case Analysis: smart_cle1_cc",
    "text": "7.9 Approach 1: Building a Complete Case Analysis: smart_cle1_cc\nIn the 431 course, we usually dealt with missing data by restricting our analyses to respondents with complete data on all variables. Let’s start by doing that here. We’ll create a new tibble called smart_cle1_cc which includes all respondents with complete data on all of these variables.\n\nsmart_cle1_cc <- smart_cle1 |> \n  drop_na()\n\ndim(smart_cle1_cc)\n\n[1] 892  12\n\n\nOur smart_cle1_cc tibble now has many fewer observations than its predecessors, but all of the variables in this complete cases tibble have no missing observations.\n\n\n\nData Set\nRows\nColumns\nMissingness?\n\n\n\n\nsmart_cle\n1133\n99\nQuite a bit.\n\n\nsmart_cle1\n1133\n12\nQuite a bit.\n\n\nsmart_cle1_cc\n892\n12\nNone."
  },
  {
    "objectID": "singleimputation.html#approach-2-single-imputation-to-create-smart_cle1_sh",
    "href": "singleimputation.html#approach-2-single-imputation-to-create-smart_cle1_sh",
    "title": "7  Dealing with Missingness: Single Imputation",
    "section": "7.10 Approach 2: Single Imputation to create smart_cle1_sh",
    "text": "7.10 Approach 2: Single Imputation to create smart_cle1_sh\nNext, we’ll create a data set which has all of the rows in the original smart_cle1 tibble, but deals with missingness by imputing (estimating / filling in) new values for each of the missing values. To do this, we’ll make heavy use of the simputation package in R.\nThe simputation package is designed for single imputation work. Note that we’ll eventually adopt a multiple imputation strategy in some of our modeling work, and we’ll use some specialized tools to facilitate that later.\nTo begin, we’ll create a “shadow” in our tibble to track what we’ll need to impute.\n\nsmart_cle1_sh <- bind_shadow(smart_cle1)\n\nnames(smart_cle1_sh)\n\n [1] \"SEQNO\"         \"physhealth\"    \"genhealth\"     \"bmi\"          \n [5] \"age_imp\"       \"female\"        \"race_eth\"      \"internet30\"   \n [9] \"smoke100\"      \"activity\"      \"drinks_wk\"     \"veg_day\"      \n[13] \"SEQNO_NA\"      \"physhealth_NA\" \"genhealth_NA\"  \"bmi_NA\"       \n[17] \"age_imp_NA\"    \"female_NA\"     \"race_eth_NA\"   \"internet30_NA\"\n[21] \"smoke100_NA\"   \"activity_NA\"   \"drinks_wk_NA\"  \"veg_day_NA\"   \n\n\nNote that the bind_shadow() function doubles the number of variables in our tibble, specifically by creating a new variable for each that takes the value !NA or NA. For example, consider\n\nsmart_cle1_sh |> count(activity, activity_NA)\n\n# A tibble: 5 × 3\n  activity              activity_NA     n\n  <fct>                 <fct>       <int>\n1 Highly_Active         !NA           338\n2 Active                !NA           173\n3 Insufficiently_Active !NA           201\n4 Inactive              !NA           312\n5 <NA>                  NA            109\n\n\nThe activity_NA variable takes the value !NA (meaning not missing) when the value of the activity variable is known, and takes the value NA for observations where the activity variable is missing. This background tracking will be helpful to us when we try to assess the impact of imputation on some of our summaries.\n\n7.10.1 What Type of Missingness Do We Have?\nThere are three types of missingness that we might assume in any given setting: missing completely at random (MCAR), missing at random (MAR) and missing not at random (MNAR). Together, MCAR and MAR are sometimes called ignorable non-response, which essentially means that imputation provides a way to useful estimates. MNAR or missing NOT at random is sometimes called non-ignorable missingness, implying that even high-quality imputation may not be sufficient to provide useful information to us.\nMissing Completely at Random means that the missing data points are a random subset of the data. Essentially, there is nothing that makes some data more likely to be missing than others. If the data truly match the standard for MCAR, then a complete-case analysis will be about as good as an analysis after single or multiple imputation.\nMissing at Random means that there is a systematic relationship between the observed data and the missingness mechanism. Another way to say this is that the missing value is not related to the reason why it is missing, but is related to the other variables collected in the study. The implication is that the missingness can be accounted for by studying the variables with complete information. Imputation strategies can be very helpful here, incorporating what we know (or think we know) about the relationships between the results that are missing and the results that we see.\n\nWikipedia provides a nice example. If men are less likely to fill in a depression survey, but this has nothing to do with their level of depression after accounting for the fact that they are male, then the missingess can be assumed MAR.\nDetermining whether missingness is MAR or MNAR can be tricky. We’ll spend more time discussing this later.\n\nMissing NOT at Random means that the missing value is related to the reason why it is missing.\n\nContinuing the Wikipedia example, if men failed to fill in a depression survey because of their level of depression, then this would be MNAR.\nSingle imputation is most helpful in the MAR situation, although it is also appropriate when we assume MCAR.\nMultiple imputation will, similarly, be more helpful in MCAR and MAR situations than when data are missing NOT at random.\n\nIt’s worth noting that many people are unwilling to impute values for outcomes or key predictors in a modeling setting, but are happy to impute for less important covariates. For now, we’ll assume MCAR or MAR for all of the missingness in our smart_cle1 data, which will allow us to adopt a single imputation strategy.\n\n\n7.10.2 Single imputation into smart_cle1_sh\nWhich variables in smart_cle1_sh contain missing data?\n\nmiss_var_summary(smart_cle1_sh)\n\n# A tibble: 24 × 3\n   variable   n_miss pct_miss\n   <chr>       <int>    <dbl>\n 1 activity      109    9.62 \n 2 veg_day       101    8.91 \n 3 bmi            91    8.03 \n 4 drinks_wk      66    5.83 \n 5 smoke100       40    3.53 \n 6 race_eth       26    2.29 \n 7 physhealth     24    2.12 \n 8 age_imp        11    0.971\n 9 internet30      7    0.618\n10 genhealth       4    0.353\n# … with 14 more rows\n\n\nWe will impute these variables using several different strategies, all supported nicely by the simputation package.\nThese include imputation methods based solely on the distribution of the complete cases of the variable being imputed.\n\nimpute_median: impute the median value of all non-missing observations into the missing values for the variable\nimpute_rhd: random “hot deck” imputation involves drawing at random from the complete cases for that variable\n\nAlso available are imputation strategies that impute predicted values from models using other variables in the data set besides the one being imputed.\n\nimpute_pmm: imputation using predictive mean matching\nimpute_rlm: imputation using robust linear models\nimpute_cart: imputation using classification and regression trees\nimpute_knn: imputation using k-nearest neighbors methods\n\n\n\n7.10.3 Imputing Binary Categorical Variables\nHere, we’ll arbitrarily impute our 1/0 variables as follows:\n\nFor internet30 we’ll use the impute_rhd approach to draw a random observation from the existing set of 1s and 0s in the complete internet30 data.\nFor smoke100 we’ll use a method called predictive mean matching (impute_pmm) which takes the result from a model based on the (imputed) internet30 value and whether or not the subject is female, and converts it to the nearest value in the observed smoke100 data. This is a good approach for imputing discrete variables.\n\nThese are completely arbitrary choices, for demonstration purposes.\n\nset.seed(2020001)\n\nsmart_cle1_sh <- smart_cle1_sh |> data.frame() |>\n    impute_rhd(internet30 ~ 1) |>\n    impute_pmm(smoke100 ~ internet30 + female) |>\n  as_tibble()\n\nsmart_cle1_sh |> count(smoke100, smoke100_NA)\n\n# A tibble: 4 × 3\n  smoke100 smoke100_NA     n\n     <dbl> <fct>       <int>\n1        0 !NA           579\n2        0 NA             21\n3        1 !NA           514\n4        1 NA             19\n\nsmart_cle1_sh |> count(internet30, internet30_NA)\n\n# A tibble: 4 × 3\n  internet30 internet30_NA     n\n       <dbl> <fct>         <int>\n1          0 !NA             207\n2          0 NA                1\n3          1 !NA             919\n4          1 NA                6\n\n\nOther approaches that may be used with 1/0 variables include impute_knn and impute_pmm.\n\n\n7.10.4 Imputing Quantitative Variables\nWe’ll demonstrate a different approach for imputing each of the quantitative variables with missing observations. Again, we’re making purely arbitrary decisions here about what to include in each imputation. In practical work, we’d want to be a bit more thoughtful about this.\nNote that I’m choosing to use impute_pmm with the physhealth and age_imp variables. This is (in part) because I want my imputations to be integers, as the other observations are for those variables. impute_rhd would also accomplish this.\n\nset.seed(2020001)\nsmart_cle1_sh <- smart_cle1_sh |> data.frame() |>\n    impute_rhd(veg_day ~ 1) |>\n    impute_median(drinks_wk ~ 1) |>\n    impute_pmm(physhealth ~ drinks_wk + female + smoke100) |>\n    impute_pmm(age_imp ~ drinks_wk + physhealth) |>\n    impute_rlm(bmi ~ physhealth + smoke100) |>\n  as_tibble()\n\n\n\n7.10.5 Imputation Results\nLet’s plot a few of these results, so we can see what imputation has done to the distribution of these quantities.\n\nveg_day\n\n\nggplot(smart_cle1_sh, aes(x = veg_day_NA, y = veg_day)) +\n  geom_count() + \n  labs(title = \"Imputation Results for veg_day\")\n\n\n\n\n\nfavstats(veg_day ~ veg_day_NA, data = smart_cle1_sh)\n\n  veg_day_NA  min     Q1 median   Q3  max     mean       sd    n missing\n1        !NA 0.00 1.2675   1.72 2.42 7.49 1.912548 1.038403 1032       0\n2         NA 0.26 1.3400   1.86 2.72 5.97 2.085050 1.062316  101       0\n\n\n\ndrinks_wk for which we imputed the median value…\n\n\nggplot(smart_cle1_sh, aes(x = drinks_wk_NA, y = drinks_wk)) +\n  geom_count() + \n  labs(title = \"Imputation Results for drinks_wk\")\n\n\n\n\n\nsmart_cle1_sh |> filter(drinks_wk_NA == \"NA\") |>\n  tabyl(drinks_wk)\n\n drinks_wk  n percent\n      0.23 66       1\n\n\n\nphyshealth, a count between 0 and 30…\n\n\nggplot(smart_cle1_sh, \n       aes(x = physhealth, y = physhealth_NA)) +\n  geom_density_ridges() +\n  labs(title = \"Imputation Results for physhealth\")\n\nPicking joint bandwidth of 0.426\n\n\n\n\n\n\nsmart_cle1_sh |> filter(physhealth_NA == \"NA\") |>\n  tabyl(physhealth)\n\n physhealth  n    percent\n          3  1 0.04166667\n          4  2 0.08333333\n          5 13 0.54166667\n          6  8 0.33333333\n\n\n\nage_imp, in (integer) years\n\n\nggplot(smart_cle1_sh, \n       aes(x = age_imp, color = age_imp_NA)) +\n  geom_freqpoly(binwidth = 2) +\n  labs(title = \"Imputation Results for age_imp\")\n\n\n\n\n\nsmart_cle1_sh |> filter(age_imp_NA == \"NA\") |>\n  tabyl(age_imp)\n\n age_imp n    percent\n      48 1 0.09090909\n      57 7 0.63636364\n      58 1 0.09090909\n      61 1 0.09090909\n      63 1 0.09090909\n\n\n\nbmi or body mass index\n\n\nggplot(smart_cle1_sh, aes(x = bmi, fill = bmi_NA)) +\n  geom_histogram(bins = 30) + \n  labs(title = \"Histogram of BMI and imputed BMI\")\n\n\n\n\n\nfavstats(bmi ~ bmi_NA, data = smart_cle1_sh)\n\n  bmi_NA     min      Q1   median       Q3      max     mean        sd    n\n1    !NA 13.3000 24.1100 27.30000 31.68000 70.56000 28.40947 6.6289286 1042\n2     NA 27.0693 27.0693 27.50229 27.66574 30.75898 27.66057 0.8964101   91\n  missing\n1       0\n2       0\n\n\n\n\n7.10.6 Imputing Multi-Categorical Variables\nThe three multi-categorical variables we have left to impute are activity, race_eth and genhealth, and each is presented as a factor in R, rather than as a character variable.\nWe’ll arbitrarily decide to impute\n\nactivity and genhealth with a classification tree using physhealth, bmi and smoke100,\nand then impute race_eth with a random draw from the distribution of complete cases.\n\n\nset.seed(2020001)\nsmart_cle1_sh <- smart_cle1_sh |>\n  data.frame() |>\n    impute_cart(activity + genhealth ~ physhealth + bmi + smoke100) |>\n    impute_rhd(race_eth ~ 1) |>\n  as_tibble()\n\nLet’s check our results.\n\nsmart_cle1_sh |> count(activity_NA, activity)\n\n# A tibble: 6 × 3\n  activity_NA activity                  n\n  <fct>       <fct>                 <int>\n1 !NA         Highly_Active           338\n2 !NA         Active                  173\n3 !NA         Insufficiently_Active   201\n4 !NA         Inactive                312\n5 NA          Highly_Active            90\n6 NA          Inactive                 19\n\n\n\nsmart_cle1_sh |> count(race_eth_NA, race_eth)\n\n# A tibble: 9 × 3\n  race_eth_NA race_eth                     n\n  <fct>       <fct>                    <int>\n1 !NA         White non-Hispanic         805\n2 !NA         Black non-Hispanic         222\n3 !NA         Other race non-Hispanic     24\n4 !NA         Multiracial non-Hispanic    22\n5 !NA         Hispanic                    34\n6 NA          White non-Hispanic          19\n7 NA          Black non-Hispanic           4\n8 NA          Multiracial non-Hispanic     2\n9 NA          Hispanic                     1\n\n\n\nsmart_cle1_sh |> count(genhealth_NA, genhealth)\n\n# A tibble: 7 × 3\n  genhealth_NA genhealth       n\n  <fct>        <fct>       <int>\n1 !NA          1_Excellent   164\n2 !NA          2_VeryGood    383\n3 !NA          3_Good        364\n4 !NA          4_Fair        158\n5 !NA          5_Poor         60\n6 NA           2_VeryGood      3\n7 NA           3_Good          1\n\n\nAnd now, we should have no missing values in the data, at all.\n\nmiss_case_table(smart_cle1_sh)\n\n# A tibble: 1 × 3\n  n_miss_in_case n_cases pct_cases\n           <int>   <int>     <dbl>\n1              0    1133       100\n\n\n\n\n7.10.7 Saving the new tibbles\n\nsaveRDS(smart_cle1_cc, (\"data/smart_cle1_cc.Rds\"))\nsaveRDS(smart_cle1_sh, (\"data/smart_cle1_sh.Rds\"))\n\n\n\n\n\nGelman, Andrew, and Jennifer Hill. 2007. Data Analysis Using Regression and Multilevel/Hierarchical Models. New York: Cambridge University Press."
  },
  {
    "objectID": "datasummaries.html#r-setup-used-here",
    "href": "datasummaries.html#r-setup-used-here",
    "title": "8  Summarizing smart_cle1",
    "section": "8.1 R Setup Used Here",
    "text": "8.1 R Setup Used Here\n\nknitr::opts_chunk$set(comment = NA)\n\nlibrary(janitor) \nlibrary(broom)\nlibrary(Hmisc)\nlibrary(mosaic)\nlibrary(knitr)\nlibrary(tidyverse) \n\ntheme_set(theme_bw())\n\n\n8.1.1 Data Load\n\nsmart_cle1_sh <- read_rds(\"data/smart_cle1_sh.Rds\")\nsmart_cle1_cc <- read_rds(\"data/smart_cle1_cc.Rds\")"
  },
  {
    "objectID": "datasummaries.html#whats-in-these-data",
    "href": "datasummaries.html#whats-in-these-data",
    "title": "8  Summarizing smart_cle1",
    "section": "8.2 What’s in these data?",
    "text": "8.2 What’s in these data?\nThose files (_sh contains single imputations, and a shadow set of variables which have _NA at the end of their names, while _cc contains only the complete cases) describe information on the following variables from the BRFSS 2017, who live in the Cleveland-Elyria, OH, Metropolitan Statistical Area.\n\n\n\n\n\n\n\nVariable\nDescription\n\n\n\n\nSEQNO\nrespondent identification number (all begin with 2016)\n\n\nphyshealth\nNow thinking about your physical health, which includes physical illness and injury, for how many days during the past 30 days was your physical health not good?\n\n\ngenhealth\nWould you say that in general, your health is … (five categories: Excellent, Very Good, Good, Fair or Poor)\n\n\nbmi\nBody mass index, in kg/m2\n\n\nage_imp\nAge, imputed, in years\n\n\nfemale\nSex, 1 = female, 0 = male\n\n\nrace_eth\nRace and Ethnicity, in five categories\n\n\ninternet30\nHave you used the internet in the past 30 days? (1 = yes, 0 = no)\n\n\nsmoke100\nHave you smoked at least 100 cigarettes in your life? (1 = yes, 0 = no)\n\n\nactivity\nPhysical activity (Highly Active, Active, Insufficiently Active, Inactive)\n\n\ndrinks_wk\nOn average, how many drinks of alcohol do you consume in a week?\n\n\nveg_day\nHow many servings of vegetables do you consume per day, on average?"
  },
  {
    "objectID": "datasummaries.html#general-approaches-to-obtaining-numeric-summaries",
    "href": "datasummaries.html#general-approaches-to-obtaining-numeric-summaries",
    "title": "8  Summarizing smart_cle1",
    "section": "8.3 General Approaches to Obtaining Numeric Summaries",
    "text": "8.3 General Approaches to Obtaining Numeric Summaries\n\n8.3.1 summary for a data frame\nOf course, we can use the usual summary to get some basic information about the data.\n\nsummary(smart_cle1_cc)\n\n     SEQNO             physhealth           genhealth        bmi       \n Min.   :2.017e+09   Min.   : 0.000   1_Excellent:134   Min.   :13.30  \n 1st Qu.:2.017e+09   1st Qu.: 0.000   2_VeryGood :314   1st Qu.:24.23  \n Median :2.017e+09   Median : 0.000   3_Good     :284   Median :27.48  \n Mean   :2.017e+09   Mean   : 4.361   4_Fair     :114   Mean   :28.51  \n 3rd Qu.:2.017e+09   3rd Qu.: 3.000   5_Poor     : 46   3rd Qu.:31.82  \n Max.   :2.017e+09   Max.   :30.000                     Max.   :63.00  \n    age_imp          female                           race_eth  \n Min.   :18.00   Min.   :0.0000   White non-Hispanic      :661  \n 1st Qu.:43.00   1st Qu.:0.0000   Black non-Hispanic      :168  \n Median :58.00   Median :1.0000   Other race non-Hispanic : 20  \n Mean   :56.46   Mean   :0.5807   Multiracial non-Hispanic: 17  \n 3rd Qu.:69.00   3rd Qu.:1.0000   Hispanic                : 26  \n Max.   :95.00   Max.   :1.0000                                 \n   internet30       smoke100                       activity     drinks_wk     \n Min.   :0.000   Min.   :0.0000   Highly_Active        :308   Min.   : 0.000  \n 1st Qu.:1.000   1st Qu.:0.0000   Active               :155   1st Qu.: 0.000  \n Median :1.000   Median :0.0000   Insufficiently_Active:167   Median : 0.470  \n Mean   :0.833   Mean   :0.4787   Inactive             :262   Mean   : 2.727  \n 3rd Qu.:1.000   3rd Qu.:1.0000                               3rd Qu.: 2.850  \n Max.   :1.000   Max.   :1.0000                               Max.   :56.000  \n    veg_day     \n Min.   :0.000  \n 1st Qu.:1.280  \n Median :1.730  \n Mean   :1.919  \n 3rd Qu.:2.422  \n Max.   :7.300  \n\n\n\n\n8.3.2 The inspect function from the mosaic package\n\ninspect(smart_cle1_cc)\n\n\ncategorical variables:  \n       name  class levels   n missing\n1 genhealth factor      5 892       0\n2  race_eth factor      5 892       0\n3  activity factor      4 892       0\n                                   distribution\n1 2_VeryGood (35.2%), 3_Good (31.8%) ...       \n2 White non-Hispanic (74.1%) ...               \n3 Highly_Active (34.5%) ...                    \n\nquantitative variables:  \n        name   class       min         Q1       median           Q3\n1      SEQNO numeric 2.017e+09 2.0170e+09 2.017001e+09 2.017001e+09\n2 physhealth numeric 0.000e+00 0.0000e+00 0.000000e+00 3.000000e+00\n3        bmi numeric 1.330e+01 2.4235e+01 2.747500e+01 3.181500e+01\n4    age_imp numeric 1.800e+01 4.3000e+01 5.800000e+01 6.900000e+01\n5     female numeric 0.000e+00 0.0000e+00 1.000000e+00 1.000000e+00\n6 internet30 numeric 0.000e+00 1.0000e+00 1.000000e+00 1.000000e+00\n7   smoke100 numeric 0.000e+00 0.0000e+00 0.000000e+00 1.000000e+00\n8  drinks_wk numeric 0.000e+00 0.0000e+00 4.700000e-01 2.850000e+00\n9    veg_day numeric 0.000e+00 1.2800e+00 1.730000e+00 2.422500e+00\n           max         mean          sd   n missing\n1 2017001133.0 2.017001e+09 326.8928344 892       0\n2         30.0 4.360987e+00   8.8153373 892       0\n3         63.0 2.850905e+01   6.5057975 892       0\n4         95.0 5.645516e+01  18.0333027 892       0\n5          1.0 5.807175e-01   0.4937185 892       0\n6          1.0 8.329596e-01   0.3732212 892       0\n7          1.0 4.786996e-01   0.4998263 892       0\n8         56.0 2.726525e+00   5.7172011 892       0\n9          7.3 1.918643e+00   1.0262415 892       0\n\n\n\n\n8.3.3 The describe function in Hmisc\nThis provides some useful additional summaries, including a list of the lowest and highest values (which is very helpful when checking data.)\n\nsmart_cle1_cc |>\n  select(bmi, genhealth, female) |>\n  describe()\n\nselect(smart_cle1_cc, bmi, genhealth, female) \n\n 3  Variables      892  Observations\n--------------------------------------------------------------------------------\nbmi \n       n  missing distinct     Info     Mean      Gmd      .05      .10 \n     892        0      481        1    28.51    6.952    20.09    21.40 \n     .25      .50      .75      .90      .95 \n   24.23    27.48    31.81    36.78    41.09 \n\nlowest : 13.30 13.64 15.71 15.75 17.07, highest: 52.74 56.31 57.12 58.98 63.00\n--------------------------------------------------------------------------------\ngenhealth \n       n  missing distinct \n     892        0        5 \n\nlowest : 1_Excellent 2_VeryGood  3_Good      4_Fair      5_Poor     \nhighest: 1_Excellent 2_VeryGood  3_Good      4_Fair      5_Poor     \n                                                                      \nValue      1_Excellent  2_VeryGood      3_Good      4_Fair      5_Poor\nFrequency          134         314         284         114          46\nProportion       0.150       0.352       0.318       0.128       0.052\n--------------------------------------------------------------------------------\nfemale \n       n  missing distinct     Info      Sum     Mean      Gmd \n     892        0        2     0.73      518   0.5807   0.4875 \n\n--------------------------------------------------------------------------------\n\n\n\nThe Info measure is used for quantitative and binary variables. It is a relative information measure that increases towards 1 for variables with no ties, and is smaller for variables with many ties.\nThe Gmd is the Gini mean difference. It is a measure of spread (or dispersion), where larger values indicate greater spread in the distribution, like the standard deviation or the interquartile range. It is defined as the mean absolute difference between any pairs of observations.\n\nSee the Help file for describe in the Hmisc package for more details on these measures, and on the settings for describe."
  },
  {
    "objectID": "datasummaries.html#counting-as-exploratory-data-analysis",
    "href": "datasummaries.html#counting-as-exploratory-data-analysis",
    "title": "8  Summarizing smart_cle1",
    "section": "8.4 Counting as exploratory data analysis",
    "text": "8.4 Counting as exploratory data analysis\nCounting and/or tabulating things can be amazingly useful. Suppose we want to understand the genhealth values, after our single imputation.\n\nsmart_cle1_sh |> count(genhealth) |>\n  mutate(percent = 100*n / sum(n))\n\n# A tibble: 5 × 3\n  genhealth       n percent\n  <fct>       <int>   <dbl>\n1 1_Excellent   164   14.5 \n2 2_VeryGood    386   34.1 \n3 3_Good        365   32.2 \n4 4_Fair        158   13.9 \n5 5_Poor         60    5.30\n\n\nWe might use tabyl to do this job…\n\nsmart_cle1_sh |> \n  tabyl(genhealth) |>\n  adorn_pct_formatting(digits = 1) |>\n  kable()\n\n\n\n\ngenhealth\nn\npercent\n\n\n\n\n1_Excellent\n164\n14.5%\n\n\n2_VeryGood\n386\n34.1%\n\n\n3_Good\n365\n32.2%\n\n\n4_Fair\n158\n13.9%\n\n\n5_Poor\n60\n5.3%\n\n\n\n\n\n\n8.4.1 Did genhealth vary by smoking status?\n\nsmart_cle1_sh |> \n  count(genhealth, smoke100) |> \n  mutate(percent = 100*n / sum(n))\n\n# A tibble: 10 × 4\n   genhealth   smoke100     n percent\n   <fct>          <dbl> <int>   <dbl>\n 1 1_Excellent        0   105    9.27\n 2 1_Excellent        1    59    5.21\n 3 2_VeryGood         0   220   19.4 \n 4 2_VeryGood         1   166   14.7 \n 5 3_Good             0   184   16.2 \n 6 3_Good             1   181   16.0 \n 7 4_Fair             0    67    5.91\n 8 4_Fair             1    91    8.03\n 9 5_Poor             0    24    2.12\n10 5_Poor             1    36    3.18\n\n\nSuppose we want to find the percentage within each smoking status group. Here’s one approach…\n\nsmart_cle1_sh |>\n    count(smoke100, genhealth) |>\n    group_by(smoke100) |>\n    mutate(prob = 100*n / sum(n)) \n\n# A tibble: 10 × 4\n# Groups:   smoke100 [2]\n   smoke100 genhealth       n  prob\n      <dbl> <fct>       <int> <dbl>\n 1        0 1_Excellent   105 17.5 \n 2        0 2_VeryGood    220 36.7 \n 3        0 3_Good        184 30.7 \n 4        0 4_Fair         67 11.2 \n 5        0 5_Poor         24  4   \n 6        1 1_Excellent    59 11.1 \n 7        1 2_VeryGood    166 31.1 \n 8        1 3_Good        181 34.0 \n 9        1 4_Fair         91 17.1 \n10        1 5_Poor         36  6.75\n\n\nAnd here’s another …\n\nsmart_cle1_sh |>\n  tabyl(smoke100, genhealth) |>\n  adorn_totals(where = c(\"row\", \"col\")) |> \n  adorn_percentages(denominator = \"row\") |>\n  adorn_pct_formatting(digits = 1) |>\n  adorn_ns(position = \"front\")\n\n smoke100 1_Excellent  2_VeryGood      3_Good      4_Fair    5_Poor\n        0 105 (17.5%) 220 (36.7%) 184 (30.7%)  67 (11.2%) 24 (4.0%)\n        1  59 (11.1%) 166 (31.1%) 181 (34.0%)  91 (17.1%) 36 (6.8%)\n    Total 164 (14.5%) 386 (34.1%) 365 (32.2%) 158 (13.9%) 60 (5.3%)\n          Total\n   600 (100.0%)\n   533 (100.0%)\n 1,133 (100.0%)\n\n\n\n\n8.4.2 What’s the distribution of physhealth?\nWe can count quantitative variables with discrete sets of possible values, like physhealth, which is captured as an integer (that must fall between 0 and 30.)\n\nsmart_cle1_sh |> count(physhealth)\n\n# A tibble: 21 × 2\n   physhealth     n\n        <dbl> <int>\n 1          0   690\n 2          1    49\n 3          2    61\n 4          3    39\n 5          4    17\n 6          5    43\n 7          6    13\n 8          7    18\n 9          8     5\n10         10    32\n# … with 11 more rows\n\n\nOf course, a natural summary of a quantitative variable like this would be graphical.\n\nggplot(smart_cle1_sh, aes(physhealth)) +\n    geom_histogram(binwidth = 1, \n                   fill = \"dodgerblue\", col = \"white\") +\n    labs(title = \"Days with Poor Physical Health in the Past 30\",\n         subtitle = \"Most subjects are pretty healthy in this regard, but there are some 30s\")\n\n\n\n\n\n\n8.4.3 What’s the distribution of bmi?\nbmi is the body-mass index, an indicator of size (thickness, really.)\n\nggplot(smart_cle1_sh, aes(bmi)) +\n    geom_histogram(bins = 30, \n                   fill = \"firebrick\", col = \"white\") + \n    labs(title = paste0(\"Body-Mass Index for \", \n                        nrow(smart_cle1_sh), \n                        \" BRFSS respondents\"))\n\n\n\n\n\n\n8.4.4 How many of the respondents have a BMI below 30?\n\nsmart_cle1_sh |> count(bmi < 30) |> \n  mutate(proportion = n / sum(n))\n\n# A tibble: 2 × 3\n  `bmi < 30`     n proportion\n  <lgl>      <int>      <dbl>\n1 FALSE        330      0.291\n2 TRUE         803      0.709\n\n\n\n\n8.4.5 How many of the respondents with a BMI < 30 are highly active?\n\nsmart_cle1_sh |> \n  filter(bmi < 30) |> \n  tabyl(activity) |>\n  adorn_pct_formatting()\n\n              activity   n percent\n         Highly_Active 343   42.7%\n                Active 133   16.6%\n Insufficiently_Active 129   16.1%\n              Inactive 198   24.7%\n\n\n\n\n8.4.6 Is obesity associated with smoking history?\n\nsmart_cle1_sh |> count(smoke100, bmi < 30) |>\n    group_by(smoke100) |>\n    mutate(percent = 100*n/sum(n))\n\n# A tibble: 4 × 4\n# Groups:   smoke100 [2]\n  smoke100 `bmi < 30`     n percent\n     <dbl> <lgl>      <int>   <dbl>\n1        0 FALSE        163    27.2\n2        0 TRUE         437    72.8\n3        1 FALSE        167    31.3\n4        1 TRUE         366    68.7\n\n\n\n\n8.4.7 Comparing drinks_wk summaries by obesity status\nCan we compare the drinks_wk means, medians and 75th percentiles for respondents whose BMI is below 30 to the respondents whose BMI is not?\n\nsmart_cle1_sh |>\n    group_by(bmi < 30) |>\n    summarize(mean(drinks_wk), median(drinks_wk), \n              q75 = quantile(drinks_wk, 0.75))\n\n# A tibble: 2 × 4\n  `bmi < 30` `mean(drinks_wk)` `median(drinks_wk)`   q75\n  <lgl>                  <dbl>               <dbl> <dbl>\n1 FALSE                   1.67                0.23  1.17\n2 TRUE                    2.80                0.23  2.8"
  },
  {
    "objectID": "datasummaries.html#can-bmi-predict-physhealth",
    "href": "datasummaries.html#can-bmi-predict-physhealth",
    "title": "8  Summarizing smart_cle1",
    "section": "8.5 Can bmi predict physhealth?",
    "text": "8.5 Can bmi predict physhealth?\nWe’ll start with an effort to predict physhealth using bmi. A natural graph would be a scatterplot.\n\nggplot(data = smart_cle1_sh, aes(x = bmi, y = physhealth)) +\n    geom_point()\n\n\n\n\nA good question to ask ourselves here might be: “In what BMI range can we make a reasonable prediction of physhealth?”\nNow, we might take the plot above and add a simple linear model …\n\nggplot(data = smart_cle1_sh, aes(x = bmi, y = physhealth)) +\n    geom_point() +\n    geom_smooth(method = \"lm\", se = FALSE, col = \"red\")\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\nwhich shows the same least squares regression model that we can fit with the lm command.\n\n8.5.1 Fitting a Simple Regression Model\n\nmodel_A <- lm(physhealth ~ bmi, data = smart_cle1_sh)\n\nmodel_A\n\n\nCall:\nlm(formula = physhealth ~ bmi, data = smart_cle1_sh)\n\nCoefficients:\n(Intercept)          bmi  \n    -2.8121       0.2643  \n\nsummary(model_A)\n\n\nCall:\nlm(formula = physhealth ~ bmi, data = smart_cle1_sh)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-10.5258  -4.5943  -3.5608  -0.5106  29.2965 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) -2.81208    1.21672  -2.311    0.021 *  \nbmi          0.26433    0.04188   6.312 3.95e-10 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 8.968 on 1131 degrees of freedom\nMultiple R-squared:  0.03403,   Adjusted R-squared:  0.03317 \nF-statistic: 39.84 on 1 and 1131 DF,  p-value: 3.95e-10\n\nconfint(model_A, level = 0.95)\n\n                 2.5 %     97.5 %\n(Intercept) -5.1993624 -0.4247909\nbmi          0.1821599  0.3464915\n\n\nThe model coefficients can be obtained by printing the model object, and the summary function provides several useful descriptions of the model’s residuals, its statistical significance, and quality of fit.\n\n\n8.5.2 Model Summary for a Simple (One-Predictor) Regression\nThe fitted model predicts physhealth using a prediction equation we can read off from the model coefficient estimates. Specifically, we have:\n\ncoef(model_A)\n\n(Intercept)         bmi \n -2.8120766   0.2643257 \n\n\nso the equation is physhealth = -2.82 + 0.265 bmi.\nEach of the 1133 respondents included in the smart_cle1_sh data makes a contribution to this model.\n\n8.5.2.1 Residuals\nSuppose Harry is one of the people in that group, and Harry’s data is bmi = 20, and physhealth = 3.\n\nHarry’s observed value of physhealth is just the value we have in the data for them, in this case, observed physhealth = 3 for Harry.\nHarry’s fitted or predicted physhealth value is the result of calculating -2.82 + 0.265 bmi for Harry. So, if Harry’s BMI was 20, then Harry’s predicted physhealth value is -2.82 + 0.265 (20) = 2.48.\nThe residual for Harry is then his observed outcome minus his fitted outcome, so Harry has a residual of 3 - 2.48 = 0.52.\nGraphically, a residual represents vertical distance between the observed point and the fitted regression line.\nPoints above the regression line will have positive residuals, and points below the regression line will have negative residuals. Points on the line have zero residuals.\n\nThe residuals are summarized at the top of the summary output for linear model.\n\nsummary(model_A)\n\n\nCall:\nlm(formula = physhealth ~ bmi, data = smart_cle1_sh)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-10.5258  -4.5943  -3.5608  -0.5106  29.2965 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) -2.81208    1.21672  -2.311    0.021 *  \nbmi          0.26433    0.04188   6.312 3.95e-10 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 8.968 on 1131 degrees of freedom\nMultiple R-squared:  0.03403,   Adjusted R-squared:  0.03317 \nF-statistic: 39.84 on 1 and 1131 DF,  p-value: 3.95e-10\n\n\n\nThe mean residual will always be zero in an ordinary least squares model, but a five number summary of the residuals is provided by the summary, as is an estimated standard deviation of the residuals (called here the Residual standard error.)\nIn the smart_cle1_sh data, the minimum residual was -10.53, so for one subject, the observed value was 10.53 days smaller than the predicted value. This means that the prediction was 10.53 days too large for that subject.\nSimilarly, the maximum residual was 29.30 days, so for one subject the prediction was 29.30 days too small. Not a strong performance.\nIn a least squares model, the residuals are assumed to follow a Normal distribution, with mean zero, and standard deviation (for the smart_cle1_sh data) of about 9.0 days. We know this because the residual standard error is specified as 8.968 later in the linear model output. Thus, by the definition of a Normal distribution, we’d expect\nabout 68% of the residuals to be between -9 and +9 days,\nabout 95% of the residuals to be between -18 and +18 days,\nabout all (99.7%) of the residuals to be between -27 and +27 days.\n\n\n\n8.5.2.2 Coefficients section\nThe summary for a linear model shows Estimates, Standard Errors, t values and p values for each coefficient fit.\n\nsummary(model_A)\n\n\nCall:\nlm(formula = physhealth ~ bmi, data = smart_cle1_sh)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-10.5258  -4.5943  -3.5608  -0.5106  29.2965 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) -2.81208    1.21672  -2.311    0.021 *  \nbmi          0.26433    0.04188   6.312 3.95e-10 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 8.968 on 1131 degrees of freedom\nMultiple R-squared:  0.03403,   Adjusted R-squared:  0.03317 \nF-statistic: 39.84 on 1 and 1131 DF,  p-value: 3.95e-10\n\n\n\nThe Estimates are the point estimates of the intercept and slope of bmi in our model.\nIn this case, our estimated slope is 0.265, which implies that if Harry’s BMI is 20 and Sally’s BMI is 21, we predict that Sally’s physhealth will be 0.265 days larger than Harry’s.\nThe Standard Errors are also provided for each estimate. We can create rough 95% uncertainty intervals for these estimated coefficients by adding and subtracting two standard errors from each coefficient, or we can get a slightly more accurate answer with the confint function.\nHere, the 95% uncertainty interval for the slope of bmi is estimated to be (0.18, 0.35). This is a good measure of the uncertainty in the slope that is captured by our model. We are 95% confident in the process of building this interval, but this doesn’t mean we’re 95% sure that the true slope is actually in that interval.\n\nAlso available are a t value (just the Estimate divided by the Standard Error) and the appropriate p value for testing the null hypothesis that the true value of the coefficient is 0 against a two-tailed alternative.\n\nIf a slope coefficient is statistically detectably different from 0, this implies that 0 will not be part of the uncertainty interval obtained through confint.\nIf the slope was zero, it would suggest that bmi would add no predictive value to the model. But that’s unlikely here.\n\nIf the bmi slope coefficient is associated with a small p value, as in the case of our model_A, it suggests that the model including bmi is statistically detectably better at predicting physhealth than the model without bmi.\n\nWithout bmi our model_A would become an intercept-only model, in this case, which would predict the mean physhealth for everyone, regardless of any other information.\n\n\n\n8.5.2.3 Model Fit Summaries\n\nsummary(model_A)\n\n\nCall:\nlm(formula = physhealth ~ bmi, data = smart_cle1_sh)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-10.5258  -4.5943  -3.5608  -0.5106  29.2965 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) -2.81208    1.21672  -2.311    0.021 *  \nbmi          0.26433    0.04188   6.312 3.95e-10 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 8.968 on 1131 degrees of freedom\nMultiple R-squared:  0.03403,   Adjusted R-squared:  0.03317 \nF-statistic: 39.84 on 1 and 1131 DF,  p-value: 3.95e-10\n\n\nThe summary of a linear model also displays:\n\nThe residual standard error and associated degrees of freedom for the residuals.\nFor a simple (one-predictor) least regression like this, the residual degrees of freedom will be the sample size minus 2.\nThe multiple R-squared (or coefficient of determination)\nThis is interpreted as the proportion of variation in the outcome (physhealth) accounted for by the model, and will always fall between 0 and 1 as a result.\nOur model_A accounts for a mere 3.4% of the variation in physhealth.\nThe Adjusted R-squared value “adjusts” for the size of our model in terms of the number of coefficients included in the model.\nThe adjusted R-squared will always be smaller than the Multiple R-squared.\nWe still hope to find models with relatively large adjusted \\(R^2\\) values.\nIn particular, we hope to find models where the adjusted \\(R^2\\) isn’t substantially less than the Multiple R-squared.\nThe adjusted R-squared is usually a better estimate of likely performance of our model in new data than is the Multiple R-squared.\nThe adjusted R-squared result is no longer interpretable as a proportion of anything - in fact, it can fall below 0.\nWe can obtain the adjusted \\(R^2\\) from the raw \\(R^2\\), the number of observations N and the number of predictors p included in the model, as follows:\n\n\\[\nR^2_{adj} = 1 - \\frac{(1 - R^2)(N - 1)}{N - p - 1},\n\\]\n\nThe F statistic and p value from a global ANOVA test of the model.\n\nObtaining a statistically significant result here is usually pretty straightforward, since the comparison is between our model, and a model which simply predicts the mean value of the outcome for everyone.\nIn a simple (one-predictor) linear regression like this, the t statistic for the slope is just the square root of the F statistic, and the resulting p values for the slope’s t test and for the global F test will be identical.\n\nTo see the complete ANOVA F test for this model, we can run anova(model_A).\n\n\nanova(model_A)\n\nAnalysis of Variance Table\n\nResponse: physhealth\n            Df Sum Sq Mean Sq F value   Pr(>F)    \nbmi          1   3204  3204.4   39.84 3.95e-10 ***\nResiduals 1131  90968    80.4                     \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\n\n\n8.5.3 Using the broom package\nThe broom package has three functions of particular use in a linear regression model:\n\n8.5.3.1 The tidy function\ntidy builds a data frame/tibble containing information about the coefficients in the model, their standard errors, t statistics and p values.\n\ntidy(model_A)\n\n# A tibble: 2 × 5\n  term        estimate std.error statistic  p.value\n  <chr>          <dbl>     <dbl>     <dbl>    <dbl>\n1 (Intercept)   -2.81     1.22       -2.31 2.10e- 2\n2 bmi            0.264    0.0419      6.31 3.95e-10\n\n\nIt’s often useful to include other summaries in this tidying, for instance:\n\ntidy(model_A, conf.int = TRUE, conf.level = 0.9) |>\n  select(term, estimate, conf.low, conf.high)\n\n# A tibble: 2 × 4\n  term        estimate conf.low conf.high\n  <chr>          <dbl>    <dbl>     <dbl>\n1 (Intercept)   -2.81    -4.82     -0.809\n2 bmi            0.264    0.195     0.333\n\n\n\n\n8.5.3.2 The glance function\nglance` builds a data frame/tibble containing summary statistics about the model, including\n\nthe (raw) multiple \\(R^2\\) and adjusted R^2\nsigma which is the residual standard error\nthe F statistic, p.value model df and df.residual associated with the global ANOVA test, plus\nseveral statistics that will be useful in comparing models down the line:\nthe model’s log likelihood function value, logLik\nthe model’s Akaike’s Information Criterion value, AIC\nthe model’s Bayesian Information Criterion value, BIC\nand the model’s deviance statistic\n\n\nglance(model_A)\n\n# A tibble: 1 × 12\n  r.squared adj.r.squa…¹ sigma stati…²  p.value    df logLik   AIC   BIC devia…³\n      <dbl>        <dbl> <dbl>   <dbl>    <dbl> <dbl>  <dbl> <dbl> <dbl>   <dbl>\n1    0.0340       0.0332  8.97    39.8 3.95e-10     1 -4092. 8190. 8205.  90968.\n# … with 2 more variables: df.residual <int>, nobs <int>, and abbreviated\n#   variable names ¹​adj.r.squared, ²​statistic, ³​deviance\n\n\n\n\n8.5.3.3 The augment function\naugment builds a data frame/tibble which adds fitted values, residuals and other diagnostic summaries that describe each observation to the original data used to fit the model, and this includes\n\n.fitted and .resid, the fitted and residual values, in addition to\n.hat, the leverage value for this observation\n.cooksd, the Cook’s distance measure of influence for this observation\n.stdresid, the standardized residual (think of this as a z-score - a measure of the residual divided by its associated standard deviation .sigma)\nand se.fit which will help us generate prediction intervals for the model downstream\n\nNote that each of the new columns begins with . to avoid overwriting any data.\n\nhead(augment(model_A))\n\n# A tibble: 6 × 8\n  physhealth   bmi .fitted .resid     .hat .sigma    .cooksd .std.resid\n       <dbl> <dbl>   <dbl>  <dbl>    <dbl>  <dbl>      <dbl>      <dbl>\n1          4  27.9    4.57 -0.572 0.000886   8.97 0.00000181    -0.0638\n2          0  23.0    3.28 -3.28  0.00149    8.97 0.000100      -0.366 \n3          0  26.9    4.31 -4.31  0.000927   8.97 0.000107      -0.480 \n4          0  26.5    4.20 -4.20  0.000956   8.97 0.000105      -0.468 \n5          0  24.2    3.60 -3.60  0.00125    8.97 0.000101      -0.401 \n6          2  27.7    4.51 -2.51  0.000891   8.97 0.0000351     -0.281 \n\n\nFor more on the broom package, you may want to look at this vignette.\n\n\n\n8.5.4 How does the model do? (Residuals vs. Fitted Values)\n\nRemember that the \\(R^2\\) value was about 3.4%.\n\n\nplot(model_A, which = 1)\n\n\n\n\nThis is a plot of residuals vs. fitted values. The goal here is for this plot to look like a random scatter of points, perhaps like a “fuzzy football”, and that’s not what we have. Why?\nIf you prefer, here’s a ggplot2 version of a similar plot, now looking at standardized residuals instead of raw residuals, and adding a loess smooth and a linear fit to the result.\n\nggplot(augment(model_A), aes(x = .fitted, y = .std.resid)) +\n    geom_point() +\n    geom_smooth(method = \"lm\", se = FALSE, col = \"red\", linetype = \"dashed\") +\n    geom_smooth(method = \"loess\", se = FALSE, col = \"navy\") +\n    theme_bw()\n\n`geom_smooth()` using formula = 'y ~ x'\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\nThe problem we’re having here becomes, I think, a little more obvious if we look at what we’re predicting. Does physhealth look like a good candidate for a linear model?\n\nggplot(smart_cle1_sh, aes(x = physhealth)) +\n  geom_histogram(bins = 30, fill = \"dodgerblue\", \n                 color = \"royalblue\")\n\n\n\n\n\nsmart_cle1_sh |> count(physhealth == 0, physhealth == 30)\n\n# A tibble: 3 × 3\n  `physhealth == 0` `physhealth == 30`     n\n  <lgl>             <lgl>              <int>\n1 FALSE             FALSE                343\n2 FALSE             TRUE                 100\n3 TRUE              FALSE                690\n\n\nNo matter what model we fit, if we are predicting physhealth, and most of the data are values of 0 and 30, we have limited variation in our outcome, and so our linear model will be somewhat questionable just on that basis.\nA normal Q-Q plot of the standardized residuals for our model_A shows this problem, too.\n\nplot(model_A, which = 2)\n\n\n\n\nWe’re going to need a method to deal with this sort of outcome, that has both a floor and a ceiling. We’ll get there eventually, but linear regression alone doesn’t look promising.\nAll right, so that didn’t go anywhere great. We’ll try again, with a new outcome, in the next chapter."
  },
  {
    "objectID": "anovasmart.html#r-setup-used-here",
    "href": "anovasmart.html#r-setup-used-here",
    "title": "9  Analysis of Variance with SMART",
    "section": "9.1 R Setup Used Here",
    "text": "9.1 R Setup Used Here\n\nknitr::opts_chunk$set(comment = NA)\n\nlibrary(janitor)\nlibrary(broom)\nlibrary(knitr)\nlibrary(mosaic)\nlibrary(tidyverse) \n\ntheme_set(theme_bw())\n\n\n9.1.1 Data Load\n\nsmart_cle1_sh <- read_rds(\"data/smart_cle1_sh.Rds\")\n\nThe variables we’ll look at in this chapter are as follows.\n\n\n\n\n\n\n\nVariable\nDescription\n\n\n\n\nSEQNO\nrespondent identification number (all begin with 2016)\n\n\nbmi\nBody mass index, in kg/m2\n\n\nfemale\nSex, 1 = female, 0 = male\n\n\nsmoke100\nHave you smoked at least 100 cigarettes in your life? (1 = yes, 0 = no)\n\n\nactivity\nPhysical activity (Highly Active, Active, Insufficiently Active, Inactive)\n\n\ndrinks_wk\nOn average, how many drinks of alcohol do you consume in a week?\n\n\nphyshealth\nNow thinking about your physical health, which includes physical illness and injury, for how many days during the past 30 days was your physical health not good?"
  },
  {
    "objectID": "anovasmart.html#a-one-factor-analysis-of-variance",
    "href": "anovasmart.html#a-one-factor-analysis-of-variance",
    "title": "9  Analysis of Variance with SMART",
    "section": "9.2 A One-Factor Analysis of Variance",
    "text": "9.2 A One-Factor Analysis of Variance\nWe’ll be predicting body mass index, at first using a single factor as a predictor: the activity level.\n\n9.2.1 Can activity be used to predict bmi?\n\nggplot(smart_cle1_sh, aes(x = activity, y = bmi, \n                          fill = activity)) +\n  geom_violin(alpha = 0.3) +\n  geom_boxplot(width = 0.3, notch = TRUE) + \n  guides(fill = \"none\") +\n  coord_flip() +\n  labs(title = \"BMI as a function of Activity Level\",\n       subtitle = \"Subjects in the SMART CLE data\",\n       x = \"\", y = \"Body Mass Index\")\n\n\n\n\nHere’s a numerical summary of the distributions of bmi within each activity group.\n\nfavstats(bmi ~ activity, data = smart_cle1_sh)\n\n               activity   min      Q1   median     Q3   max     mean       sd\n1         Highly_Active 13.30 23.6275 26.99000 28.930 50.46 27.02253 5.217496\n2                Active 17.07 24.2400 27.06930 29.520 44.67 27.36157 5.151796\n3 Insufficiently_Active 17.49 25.0500 27.93776 32.180 49.98 29.04328 6.051823\n4              Inactive 13.64 25.2150 28.34000 33.775 70.56 30.15978 7.832675\n    n missing\n1 428       0\n2 173       0\n3 201       0\n4 331       0\n\n\n\n\n9.2.2 Should we transform bmi?\nThe analysis of variance is something of a misnomer. What we’re doing is using the variance to say something about population means. In light of the apparent right skew of the bmi results in each activity group, might it be a better choice to use a logarithmic transformation? We’ll use the natural logarithm here, which in R, is symbolized by log.\n\nggplot(smart_cle1_sh, aes(x = activity, y = log(bmi), \n                          fill = activity)) +\n  geom_violin(alpha = 0.3) +\n  geom_boxplot(width = 0.3, notch = TRUE) + \n  guides(fill = \"none\") +\n  coord_flip() +\n  labs(title = \"log(BMI) as a function of Activity Level\",\n       subtitle = \"Subjects in the SMART CLE data\",\n       x = \"\", y = \"log(Body Mass Index)\")\n\n\n\n\nThe logarithmic transformation yields distributions that look much more symmetric in each activity group, so we’ll proceed to build our regression model predicting log(bmi) using activity. Here’s the numerical summary of these logged results:\n\nfavstats(log(bmi) ~ activity, data = smart_cle1_sh)\n\n               activity      min       Q1   median       Q3      max     mean\n1         Highly_Active 2.587764 3.162411 3.295466 3.364879 3.921181 3.279246\n2                Active 2.837323 3.188004 3.298400 3.385068 3.799302 3.292032\n3 Insufficiently_Active 2.861629 3.220874 3.329979 3.471345 3.911623 3.348383\n4              Inactive 2.613007 3.227439 3.344274 3.519721 4.256463 3.376468\n         sd   n missing\n1 0.1851478 428       0\n2 0.1850568 173       0\n3 0.2007241 201       0\n4 0.2411196 331       0\n\n\n\n\n9.2.3 Building the ANOVA model\n\nmodel_5a <- lm(log(bmi) ~ activity, data = smart_cle1_sh)\n\nmodel_5a\n\n\nCall:\nlm(formula = log(bmi) ~ activity, data = smart_cle1_sh)\n\nCoefficients:\n                  (Intercept)                 activityActive  \n                      3.27925                        0.01279  \nactivityInsufficiently_Active               activityInactive  \n                      0.06914                        0.09722  \n\n\nThe activity data is categorical and there are four levels. The model equation is:\nlog(bmi) = 3.279 + 0.013 (activity = Active)\n                 + 0.069 (activity = Insufficiently Active)\n                 + 0.097 (activity = Inactive)\nwhere, for example, (activity = Active) is 1 if activity is Active, and 0 otherwise. The fourth level (Highly Active) is not shown here and is used as a baseline. Thus the model above can be interpreted as follows.\n\n\n\nactivity\nPredicted log(bmi)\nPredicted bmi\n\n\n\n\nHighly Active\n3.279\nexp(3.279) = 26.55\n\n\nActive\n3.279 + 0.013 = 3.292\nexp(3.292) = 26.90\n\n\nInsufficiently Active\n3.279 + 0.069 = 3.348\nexp(3.348) = 28.45\n\n\nInactive\n3.279 + 0.097 = 3.376\nexp(3.376) = 29.25\n\n\n\nThose predicted log(bmi) values should look familiar. They are just the means of log(bmi) in each group, but I’m sure you’ll also notice that the predicted bmi values are not exact matches for the observed means of bmi.\n\nsmart_cle1_sh |> group_by(activity) |>\n  summarise(mean(log(bmi)), mean(bmi))\n\n# A tibble: 4 × 3\n  activity              `mean(log(bmi))` `mean(bmi)`\n  <fct>                            <dbl>       <dbl>\n1 Highly_Active                     3.28        27.0\n2 Active                            3.29        27.4\n3 Insufficiently_Active             3.35        29.0\n4 Inactive                          3.38        30.2\n\n\n\n\n9.2.4 The ANOVA table\nNow, let’s press on to look at the ANOVA results for this model.\n\nanova(model_5a)\n\nAnalysis of Variance Table\n\nResponse: log(bmi)\n            Df Sum Sq Mean Sq F value    Pr(>F)    \nactivity     3  2.060 0.68652  16.225 2.496e-10 ***\nResiduals 1129 47.772 0.04231                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\nThe total variation in log(bmi), our outcome, is captured by the sums of squares here. SS(Total) = 2.058 + 47.770 = 49.828\nHere, the activity variable (with 4 levels, so 4-1 = 3 degrees of freedom) accounts for 4.13% (2.058 / 49.828) of the variation in log(bmi). Another way of saying this is that the model \\(R^2\\) or \\(\\eta^2\\) is 0.0413.\nThe variation accounted for by the activity categories meets the standard for a statistically detectable result, according to the ANOVA F test, although that’s not really important.\nThe square root of the Mean Square(Residuals) is the residual standard error, \\(\\sigma\\), we’ve seen in the past. MS(Residual) estimates the variance (0.0423), so the residual standard error is \\(\\sqrt{0.0423} \\approx 0.206\\).\n\n\n\n9.2.5 The Model Coefficients\nTo address the question of effect size for the various levels of activity on log(bmi), we could look directly at the regression model coefficients. For that, we might look at the model summary.\n\nsummary(model_5a)\n\n\nCall:\nlm(formula = log(bmi) ~ activity, data = smart_cle1_sh)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.76346 -0.12609 -0.00286  0.11055  0.88000 \n\nCoefficients:\n                              Estimate Std. Error t value Pr(>|t|)    \n(Intercept)                   3.279246   0.009943 329.806  < 2e-16 ***\nactivityActive                0.012785   0.018532   0.690     0.49    \nactivityInsufficiently_Active 0.069137   0.017589   3.931 8.99e-05 ***\nactivityInactive              0.097221   0.015056   6.457 1.58e-10 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.2057 on 1129 degrees of freedom\nMultiple R-squared:  0.04133,   Adjusted R-squared:  0.03878 \nF-statistic: 16.22 on 3 and 1129 DF,  p-value: 2.496e-10\n\n\nIf we want to see the confidence intervals around these estimates, we could use\n\nconfint(model_5a, conf.level = 0.95)\n\n                                    2.5 %     97.5 %\n(Intercept)                    3.25973769 3.29875522\nactivityActive                -0.02357630 0.04914707\nactivityInsufficiently_Active  0.03462572 0.10364764\nactivityInactive               0.06767944 0.12676300\n\n\nThe model suggests, based on these 1133 subjects, that (remember that the baseline category is Highly Active)\n\na 95% confidence (uncertainty) interval for the difference between Active and Highly Active subjects in log(BMI) ranges from -0.024 to 0.049\na 95% confidence (uncertainty) interval for the difference between Insufficiently Active and Highly Active subjects in log(BMI) ranges from 0.035 to 0.104\na 95% confidence (uncertainty) interval for the difference between Inactive and Highly Active subjects in log(BMI) ranges from 0.068 to 0.127\nthe model accounts for 4.13% of the variation in log(BMI), so that knowing the respondent’s activity level somewhat reduces the size of the prediction errors as compared to an intercept only model that would predict the overall mean log(BMI), regardless of activity level, for all subjects.\nfrom the summary of residuals, we see that one subject had a residual of 0.88 - that means they were predicted to have a log(BMI) 0.88 lower than their actual log(BMI) and one subject had a log(BMI) that is 0.76 larger than their actual log(BMI), at the extremes.\n\n\n\n9.2.6 Using tidy to explore the coefficients\nA better strategy for displaying the coefficients in any regression model is to use the tidy function from the broom package.\n\ntidy(model_5a, conf.int = TRUE, conf.level = 0.95) |>\n  kable(digits = 3)\n\n\n\n\n\n\n\n\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\nconf.low\nconf.high\n\n\n\n\n(Intercept)\n3.279\n0.010\n329.806\n0.00\n3.260\n3.299\n\n\nactivityActive\n0.013\n0.019\n0.690\n0.49\n-0.024\n0.049\n\n\nactivityInsufficiently_Active\n0.069\n0.018\n3.931\n0.00\n0.035\n0.104\n\n\nactivityInactive\n0.097\n0.015\n6.457\n0.00\n0.068\n0.127\n\n\n\n\n\n\n\n9.2.7 Using glance to summarize the model’s fit\n\nglance(model_5a) |> select(1:3) |> \n  kable(digits = c(4, 4, 3))\n\n\n\n\nr.squared\nadj.r.squared\nsigma\n\n\n\n\n0.0413\n0.0388\n0.206\n\n\n\n\n\n\nThe r.squared or \\(R^2\\) value is interpreted for a linear model as the percentage of variation in the outcome (here, log(bmi)) that is accounted for by the model.\nThe adj.r.squared or adjusted \\(R^2\\) value incorporates a small penalty for the number of predictors included in the model. Adjusted \\(R^2\\) is useful for models with more than one predictor, not simple regression models like this one. Like \\(R^2\\) and most of these other summaries, its primary value comes when making comparisons between models for the same outcome.\nThe sigma or \\(\\sigma\\) is the residual standard error. Doubling this value gives us a good idea of the range of errors made by the model (approximately 95% of the time if the normal distribution assumption for the residuals holds perfectly.)\n\n\nglance(model_5a) |> select(4:7) |>\n  kable(digits = c(2, 3, 0, 2))\n\n\n\n\nstatistic\np.value\ndf\nlogLik\n\n\n\n\n16.22\n0\n3\n185.99\n\n\n\n\n\n\nThe statistic and p.value shown here refer to the ANOVA F test and p value. They test the null hypothesis that the activity information is of no use in separating out the bmi data, or, equivalently, that the true \\(R^2\\) is 0.\nThe df indicates the model degrees of freedom, and in this case simply specifies the number of parameters fitted attributed to the model. Models that require more df for estimation require larger sample sizes.\nThe logLik is the log likelihood for the model. This is a function of the sample size, but we can compare the fit of multiple models by comparing this value across different models for the same outcome. You want to maximize the log-likelihood.\n\n\nglance(model_5a) |> select(8:9) |>\n  kable(digits = 2)\n\n\n\n\nAIC\nBIC\n\n\n\n\n-361.98\n-336.82\n\n\n\n\n\n\nThe AIC (or Akaike information criterion) and BIC (Bayes information criterion) are also used only to compare models. You want to minimize AIC and BIC in selecting a model. AIC and BIC are unique only up to a constant, so different packages or routines in R may give differing values, but in comparing two models - the difference in AIC (or BIC) should be consistent.\n\n\n\n9.2.8 Using augment to make predictions\nWe can obtain residuals and predicted (fitted) values for the points used to fit the model with augment from the broom package.\n\naugment(model_5a, se_fit = TRUE) |> \n  select(1:5) |> slice(1:4) |>\n  kable(digits = 3)\n\n\n\n\nlog(bmi)\nactivity\n.fitted\n.se.fit\n.resid\n\n\n\n\n3.330\nInactive\n3.376\n0.011\n-0.047\n\n\n3.138\nInactive\n3.376\n0.011\n-0.239\n\n\n3.293\nInsufficiently_Active\n3.348\n0.015\n-0.055\n\n\n3.278\nHighly_Active\n3.279\n0.010\n-0.002\n\n\n\n\n\n\nThe .fitted value is the predicted value of log(bmi) for this subject.\nThe .se.fit value shows the standard error associated with the fitted value.\nThe .resid is the residual value (observed - fitted log(bmi))\n\n\naugment(model_5a, se_fit = TRUE) |> \n  select(1:2, 6:9) |> slice(1:4) |>\n  kable(digits = 3)\n\n\n\n\nlog(bmi)\nactivity\n.hat\n.sigma\n.cooksd\n.std.resid\n\n\n\n\n3.330\nInactive\n0.003\n0.206\n0.000\n-0.227\n\n\n3.138\nInactive\n0.003\n0.206\n0.001\n-1.163\n\n\n3.293\nInsufficiently_Active\n0.005\n0.206\n0.000\n-0.269\n\n\n3.278\nHighly_Active\n0.002\n0.206\n0.000\n-0.008\n\n\n\n\n\n\nThe .hat value shows the leverage index associated with the observation (this is a function of the predictors - higher leveraged points have more unusual predictor values)\nThe .sigma value shows the estimate of the residual standard deviation if this observation were to be dropped from the model, and thus indexes how much of an outlier this observation’s residual is.\nThe .cooksd or Cook’s distance value shows the influence that the observation has on the model - it is one of a class of leave-one-out diagnostic measures. Larger values of Cook’s distance indicate more influential points.\nThe .std.resid shows the standardized residual (which is designed to have mean 0 and standard deviation 1, facilitating comparisons across models for differing outcomes)"
  },
  {
    "objectID": "anovasmart.html#a-two-factor-anova-without-interaction",
    "href": "anovasmart.html#a-two-factor-anova-without-interaction",
    "title": "9  Analysis of Variance with SMART",
    "section": "9.3 A Two-Factor ANOVA (without Interaction)",
    "text": "9.3 A Two-Factor ANOVA (without Interaction)\nLet’s add race_eth to the predictor set for log(BMI).\n\nmodel_5b <- lm(log(bmi) ~ activity + race_eth, data = smart_cle1_sh)\n\nanova(model_5b)\n\nAnalysis of Variance Table\n\nResponse: log(bmi)\n            Df Sum Sq Mean Sq F value    Pr(>F)    \nactivity     3  2.060 0.68652 16.5090 1.676e-10 ***\nrace_eth     4  0.989 0.24716  5.9435 9.843e-05 ***\nResiduals 1125 46.783 0.04158                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nNotice that the ANOVA model assesses these variables sequentially, so the SS(activity) = 2.058 is accounted for before we consider the SS(race_eth) = 0.990. Thus, in total, the model accounts for 2.058 + 0.990 = 3.048 of the sums of squares in log(bmi) in these data.\nIf we flip the order in the model, like this:\n\nlm(log(bmi) ~ race_eth + activity, data = smart_cle1_sh) |> \n  anova()\n\nAnalysis of Variance Table\n\nResponse: log(bmi)\n            Df Sum Sq Mean Sq F value    Pr(>F)    \nrace_eth     4  1.119 0.27981  6.7287 2.371e-05 ***\nactivity     3  1.929 0.64299 15.4620 7.332e-10 ***\nResiduals 1125 46.783 0.04158                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\nAfter flipping the order of the predictors, race_eth accounts for a larger Sum of Squares than it did previously, but activity accounts for a smaller amount, and the total between race_eth and activity remains the same, as 1.121 + 1.927 is still 3.048.\n\n\n9.3.1 Model Coefficients\nThe model coefficients are unchanged regardless of the order of the variables in our two-factor ANOVA model.\n\ntidy(model_5b, conf.int = TRUE, conf.level = 0.95) |>\n  select(term, estimate, std.error, conf.low, conf.high) |>\n  kable(digits = 3)\n\n\n\n\n\n\n\n\n\n\n\nterm\nestimate\nstd.error\nconf.low\nconf.high\n\n\n\n\n(Intercept)\n3.268\n0.010\n3.247\n3.288\n\n\nactivityActive\n0.012\n0.018\n-0.024\n0.048\n\n\nactivityInsufficiently_Active\n0.073\n0.018\n0.039\n0.108\n\n\nactivityInactive\n0.092\n0.015\n0.063\n0.122\n\n\nrace_ethBlack non-Hispanic\n0.066\n0.015\n0.036\n0.096\n\n\nrace_ethOther race non-Hispanic\n-0.086\n0.042\n-0.169\n-0.002\n\n\nrace_ethMultiracial non-Hispanic\n0.020\n0.042\n-0.063\n0.103\n\n\nrace_ethHispanic\n0.012\n0.035\n-0.057\n0.082\n\n\n\n\n\nThe model_5b equation is:\nlog(BMI) = 3.268\n      + 0.012 (activity = Active)\n      + 0.073 (activity = Insufficiently Active)\n      + 0.092 (activity = Inactive)\n      + 0.066 (race_eth = Black non-Hispanic)\n      - 0.086 (race_eth = Other race non-Hispanic)\n      + 0.020 (race_eth = Multiracial non-Hispanic)\n      + 0.012 (race_eth = Hispanic)\nand we can make predictions by filling in appropriate 1s and 0s for the indicator variables in parentheses.\nFor example, the predicted log(BMI) for a White Highly Active person is 3.268, as White and Highly Active are the baseline categories in our two factors.\nFor all other combinations, we can make predictions as follows:\n\nnew_dat = tibble(\n  race_eth = rep(c(\"White non-Hispanic\",\n                   \"Black non-Hispanic\",\n                   \"Other race non-Hispanic\",\n                   \"Multiracial non-Hispanic\",\n                   \"Hispanic\"), 4),\n  activity = c(rep(\"Highly_Active\", 5),\n               rep(\"Active\", 5),\n               rep(\"Insufficiently_Active\", 5),\n               rep(\"Inactive\", 5))\n  )\n\naugment(model_5b, newdata = new_dat)\n\n# A tibble: 20 × 3\n   race_eth                 activity              .fitted\n   <chr>                    <chr>                   <dbl>\n 1 White non-Hispanic       Highly_Active            3.27\n 2 Black non-Hispanic       Highly_Active            3.33\n 3 Other race non-Hispanic  Highly_Active            3.18\n 4 Multiracial non-Hispanic Highly_Active            3.29\n 5 Hispanic                 Highly_Active            3.28\n 6 White non-Hispanic       Active                   3.28\n 7 Black non-Hispanic       Active                   3.35\n 8 Other race non-Hispanic  Active                   3.19\n 9 Multiracial non-Hispanic Active                   3.30\n10 Hispanic                 Active                   3.29\n11 White non-Hispanic       Insufficiently_Active    3.34\n12 Black non-Hispanic       Insufficiently_Active    3.41\n13 Other race non-Hispanic  Insufficiently_Active    3.26\n14 Multiracial non-Hispanic Insufficiently_Active    3.36\n15 Hispanic                 Insufficiently_Active    3.35\n16 White non-Hispanic       Inactive                 3.36\n17 Black non-Hispanic       Inactive                 3.43\n18 Other race non-Hispanic  Inactive                 3.27\n19 Multiracial non-Hispanic Inactive                 3.38\n20 Hispanic                 Inactive                 3.37\n\n\n\naugment(model_5b, newdata = new_dat) |>\n  mutate(race_eth = fct_relevel(factor(race_eth),\n                                \"White non-Hispanic\",\n                                \"Black non-Hispanic\",\n                                \"Other race non-Hispanic\",\n                                \"Multiracial non-Hispanic\",\n                                \"Hispanic\"),\n         activity = fct_relevel(factor(activity),\n                                \"Highly_Active\",\n                                \"Active\",\n                                \"Insufficiently_Active\",\n                                \"Inactive\")) %>%\n  ggplot(., aes(x = activity, y = .fitted, \n                col = race_eth, group = race_eth)) +\n  geom_point(size = 2) + \n  geom_line() + \n  labs(title = \"Model 5b predictions for log(BMI)\",\n       subtitle = \"race_eth and activity, no interaction so lines are parallel\",\n       y = \"Model Predicted log(BMI)\",\n       x = \"\")\n\n\n\n\nThe lines joining the points for each race_eth category are parallel to each other. The groups always hold the same position relative to each other, regardless of their activity levels, and vice versa. There is no interaction in this model allowing the predicted effects of, say, activity on log(BMI) values to differ for the various race_eth groups. To do that, we’d have to fit the two-factor ANOVA model incorporating an interaction term."
  },
  {
    "objectID": "anovasmart.html#a-two-factor-anova-with-interaction",
    "href": "anovasmart.html#a-two-factor-anova-with-interaction",
    "title": "9  Analysis of Variance with SMART",
    "section": "9.4 A Two-Factor ANOVA (with Interaction)",
    "text": "9.4 A Two-Factor ANOVA (with Interaction)\nLet’s add the interaction of activity and race_eth (symbolized in R by activity * race_eth) to the model for log(BMI).\n\nmodel_5c <-  \n  lm(log(bmi) ~ activity * race_eth, data = smart_cle1_sh)\n\nanova(model_5c)\n\nAnalysis of Variance Table\n\nResponse: log(bmi)\n                    Df Sum Sq Mean Sq F value    Pr(>F)    \nactivity             3  2.060 0.68652 16.4468 1.839e-10 ***\nrace_eth             4  0.989 0.24716  5.9211 0.0001026 ***\nactivity:race_eth   12  0.324 0.02700  0.6469 0.8028368    \nResiduals         1113 46.459 0.04174                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nThe ANOVA model shows that the SS(interaction) = SS(activity:race_eth) is 0.324, and uses 12 degrees of freedom. The model including the interaction term now accounts for 2.058 + 0.990 + 0.324 = 3.372, which is 6.8% of the variation in log(BMI) overall (which is calculated as SS(Total) = 2.058 + 0.990 + 0.324 + 46.456 = 49.828.)\n\n9.4.1 Model Coefficients\nThe model coefficients now include additional product terms that incorporate indicator variables for both activity and race_eth. For each of the product terms to take effect, both their activity and race_eth status must yield a 1 in the indicator variables.\n\ntidy(model_5c, conf.int = TRUE, conf.level = 0.95) |>\n  select(term, estimate, std.error, conf.low, conf.high) |>\n  kable(digits = 3)\n\n\n\n\n\n\n\n\n\n\n\nterm\nestimate\nstd.error\nconf.low\nconf.high\n\n\n\n\n(Intercept)\n3.264\n0.011\n3.242\n3.287\n\n\nactivityActive\n0.021\n0.021\n-0.021\n0.062\n\n\nactivityInsufficiently_Active\n0.079\n0.020\n0.039\n0.118\n\n\nactivityInactive\n0.097\n0.018\n0.063\n0.132\n\n\nrace_ethBlack non-Hispanic\n0.062\n0.026\n0.011\n0.113\n\n\nrace_ethOther race non-Hispanic\n-0.070\n0.078\n-0.223\n0.083\n\n\nrace_ethMultiracial non-Hispanic\n0.067\n0.060\n-0.051\n0.185\n\n\nrace_ethHispanic\n0.110\n0.060\n-0.008\n0.228\n\n\nactivityActive:race_ethBlack non-Hispanic\n-0.001\n0.048\n-0.096\n0.094\n\n\nactivityInsufficiently_Active:race_ethBlack non-Hispanic\n0.005\n0.046\n-0.086\n0.096\n\n\nactivityInactive:race_ethBlack non-Hispanic\n0.008\n0.037\n-0.065\n0.080\n\n\nactivityActive:race_ethOther race non-Hispanic\n-0.065\n0.165\n-0.389\n0.259\n\n\nactivityInsufficiently_Active:race_ethOther race non-Hispanic\n-0.035\n0.101\n-0.233\n0.163\n\n\nactivityInactive:race_ethOther race non-Hispanic\n0.033\n0.129\n-0.221\n0.287\n\n\nactivityActive:race_ethMultiracial non-Hispanic\n-0.208\n0.134\n-0.470\n0.054\n\n\nactivityInsufficiently_Active:race_ethMultiracial non-Hispanic\n-0.050\n0.120\n-0.285\n0.184\n\n\nactivityInactive:race_ethMultiracial non-Hispanic\n-0.056\n0.110\n-0.272\n0.160\n\n\nactivityActive:race_ethHispanic\n-0.104\n0.096\n-0.291\n0.084\n\n\nactivityInsufficiently_Active:race_ethHispanic\n-0.240\n0.214\n-0.660\n0.179\n\n\nactivityInactive:race_ethHispanic\n-0.169\n0.082\n-0.331\n-0.008\n\n\n\n\n\nThe model_5c equation is:\nlog(BMI) = 3.264\n  + 0.021 (activity = Active)\n  + 0.079 (activity = Insufficiently Active)\n  + 0.097 (activity = Inactive)\n  + 0.062 (race_eth = Black non-Hispanic)\n  - 0.070 (race_eth = Other race non-Hispanic)\n  + 0.067 (race_eth = Multiracial non-Hispanic)\n  + 0.110 (race_eth = Hispanic)\n  - 0.002 (activity = Active)(race_eth = Black non-Hispanic)\n  + 0.005 (Insufficiently Active)(Black non-Hispanic)\n  + 0.008 (Inactive)(Black non-Hispanic)\n  - 0.065 (Active)(Other race non-Hispanic)\n  - 0.035 (Insufficiently Active)(Other race non-Hispanic)\n  + 0.033 (Inactive)(Other race non-Hispanic)\n  - 0.208 (Active)(Multiracial non-Hispanic)\n  - 0.050 (Insufficiently Active)(Multiracial non-Hispanic)\n  - 0.056 (Inactive)(Multiracial non-Hispanic)\n  - 0.104 (Active)(Hispanic)\n  - 0.240 (Insufficiently Active)(Hispanic)\n  - 0.169 (Inactive)(Hispanic)\n  \nand again, we can make predictions by filling in appropriate 1s and 0s for the indicator variables in parentheses.\nFor example, the predicted log(BMI) for a White Highly Active person is 3.264, as White and Highly Active are the baseline categories in our two factors.\nBut the predicted log(BMI) for a Hispanic Inactive person would be 3.264 + 0.097 + 0.110 - 0.169 = 3.302.\nAgain, we’ll plot the predicted log(BMI) predictions for each possible combination.\n\nnew_dat = tibble(\n  race_eth = rep(c(\"White non-Hispanic\",\n                   \"Black non-Hispanic\",\n                   \"Other race non-Hispanic\",\n                   \"Multiracial non-Hispanic\",\n                   \"Hispanic\"), 4),\n  activity = c(rep(\"Highly_Active\", 5),\n               rep(\"Active\", 5),\n               rep(\"Insufficiently_Active\", 5),\n               rep(\"Inactive\", 5))\n  )\n\naugment(model_5c, newdata = new_dat) |>\n  mutate(race_eth = fct_relevel(factor(race_eth),\n                                \"White non-Hispanic\",\n                                \"Black non-Hispanic\",\n                                \"Other race non-Hispanic\",\n                                \"Multiracial non-Hispanic\",\n                                \"Hispanic\"),\n         activity = fct_relevel(factor(activity),\n                                \"Highly_Active\",\n                                \"Active\",\n                                \"Insufficiently_Active\",\n                                \"Inactive\")) %>%\n  ggplot(., aes(x = activity, y = .fitted, \n                col = race_eth, group = race_eth)) +\n  geom_point(size = 2) + \n  geom_line() + \n  labs(title = \"Model 5c predictions for log(BMI)\",\n       subtitle = \"race_eth and activity, with interaction\",\n       y = \"Model Predicted log(BMI)\",\n       x = \"\")\n\n\n\n\nNote that the lines joining the points for each race_eth category are no longer parallel to each other. The race-ethnicity group relative positions on log(BMI) is now changing depending on the activity status.\n\n\n9.4.2 Is the interaction term necessary?\nWe can assess this in three ways, in order of importance:\n\nWith an interaction plot\nBy assessing the fraction of the variation in the outcome accounted for by the interaction\nBy assessing whether the interaction accounts for statistically detectable outcome variation\n\n\n9.4.2.1 The Interaction Plot\nA simple interaction plot is just a plot of the unadjusted outcome means, stratified by the two factors. For example, consider this plot for our two-factor ANOVA model. To obtain this plot, we first summarize the means within each group.\n\nsummaries_5 <- smart_cle1_sh |> \n  group_by(activity, race_eth) |>\n  summarize(n = n(), mean = mean(log(bmi)), \n            sd = sd(log(bmi)))\n\n`summarise()` has grouped output by 'activity'. You can override using the\n`.groups` argument.\n\nsummaries_5\n\n# A tibble: 20 × 5\n# Groups:   activity [4]\n   activity              race_eth                     n  mean      sd\n   <fct>                 <fct>                    <int> <dbl>   <dbl>\n 1 Highly_Active         White non-Hispanic         320  3.26  0.176 \n 2 Highly_Active         Black non-Hispanic          77  3.33  0.190 \n 3 Highly_Active         Other race non-Hispanic      7  3.19  0.198 \n 4 Highly_Active         Multiracial non-Hispanic    12  3.33  0.187 \n 5 Highly_Active         Hispanic                    12  3.37  0.296 \n 6 Active                White non-Hispanic         129  3.28  0.173 \n 7 Active                Black non-Hispanic          31  3.35  0.224 \n 8 Active                Other race non-Hispanic      2  3.15  0.0845\n 9 Active                Multiracial non-Hispanic     3  3.14  0.121 \n10 Active                Hispanic                     8  3.29  0.213 \n11 Insufficiently_Active White non-Hispanic         150  3.34  0.194 \n12 Insufficiently_Active Black non-Hispanic          35  3.41  0.213 \n13 Insufficiently_Active Other race non-Hispanic     11  3.24  0.137 \n14 Insufficiently_Active Multiracial non-Hispanic     4  3.36  0.374 \n15 Insufficiently_Active Hispanic                     1  3.21 NA     \n16 Inactive              White non-Hispanic         225  3.36  0.238 \n17 Inactive              Black non-Hispanic          83  3.43  0.247 \n18 Inactive              Other race non-Hispanic      4  3.32  0.238 \n19 Inactive              Multiracial non-Hispanic     5  3.37  0.129 \n20 Inactive              Hispanic                    14  3.30  0.264 \n\n\n\nggplot(summaries_5, aes(x = activity, y = mean, \n                        color = race_eth, \n                        group = race_eth)) +\n  geom_point(size = 3) +\n  geom_line() +\n  labs(title = \"Simple Interaction Plot for log(BMI)\",\n       subtitle = \"SMART CLE means by activity and race_eth\",\n       x = \"\", y = \"Mean of log(BMI)\")\n\n\n\n\nThe interaction plot suggests that there is a modest interaction here. The White non-Hispanic and Black non-Hispanic groups appear pretty parallel (and they are the two largest groups) and Other race non-Hispanic has a fairly similar pattern, but the other two groups (Hispanic and Multiracial non-Hispanic) bounce around quite a bit based on activity level.\nAn alternative would be to include a small “dodge” for each point and include error bars (means \\(\\pm\\) standard deviation) for each combination.\n\npd = position_dodge(0.2)\nggplot(summaries_5, aes(x = activity, y = mean, \n                        color = race_eth, \n                        group = race_eth)) +\n  geom_errorbar(aes(ymin = mean - sd,\n                    ymax = mean + sd),\n                width = 0.2, position = pd) +\n  geom_point(size = 3, position = pd) +\n  geom_line(position = pd) +\n  labs(title = \"Interaction Plot for log(BMI) with Error Bars\",\n       subtitle = \"SMART CLE means by activity and race_eth\",\n       x = \"\", y = \"Mean of log(BMI)\")\n\n\n\n\nHere, we see a warning flag because we have one combination (which turns out to be Insufficiently Active and Hispanic) with only one observation in it, so a standard deviation cannot be calculated. In general, I’ll stick with the simpler means plot most of the time.\n\n\n9.4.2.2 Does the interaction account for substantial variation?\nIn this case, we can look at the fraction of the overall sums of squares accounted for by the interaction.\n\nanova(model_5c)\n\nAnalysis of Variance Table\n\nResponse: log(bmi)\n                    Df Sum Sq Mean Sq F value    Pr(>F)    \nactivity             3  2.060 0.68652 16.4468 1.839e-10 ***\nrace_eth             4  0.989 0.24716  5.9211 0.0001026 ***\nactivity:race_eth   12  0.324 0.02700  0.6469 0.8028368    \nResiduals         1113 46.459 0.04174                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nHere we have\n\\[\n\\eta^2(Interaction) = \\frac{0.324}{2.058+0.990+0.324+46.456} = 0.0065\n\\]\nso the interaction accounts for 0.65% of the variation in bmi. That looks pretty modest.\n\n\n9.4.2.3 Does the interaction account for statistically detectable variation?\nWe can test this directly with the p value from the ANOVA table, which shows p = 0.803, which is far above any of our usual standards for a statistically detectable effect.\nOn the whole, I don’t think the interaction term is especially helpful in improving this model.\nIn the next chapter, we’ll look at two different examples of ANOVA models, now in more designed experiments. We’ll also add some additional details on how the analyses might proceed.\nWe’ll return to the SMART CLE data later in these Notes."
  },
  {
    "objectID": "anovaexamples.html#r-setup-used-here",
    "href": "anovaexamples.html#r-setup-used-here",
    "title": "10  Two-Way ANOVA and Interaction",
    "section": "10.1 R Setup Used Here",
    "text": "10.1 R Setup Used Here\n\nknitr::opts_chunk$set(comment = NA)\n\nlibrary(janitor)\nlibrary(broom)\nlibrary(ggridges)\nlibrary(glue)\nlibrary(gt)\nlibrary(mosaic)\nlibrary(patchwork)\nlibrary(tidyverse) \n\ntheme_set(theme_bw())\n\n\n10.1.1 Data Load\n\nbonding <- read_csv(\"data/bonding.csv\", show_col_types = FALSE) \ncortisol <- read_csv(\"data/cortisol.csv\", show_col_types = FALSE)"
  },
  {
    "objectID": "anovaexamples.html#the-bonding-data-a-designed-dental-experiment",
    "href": "anovaexamples.html#the-bonding-data-a-designed-dental-experiment",
    "title": "10  Two-Way ANOVA and Interaction",
    "section": "10.2 The bonding data: A Designed Dental Experiment",
    "text": "10.2 The bonding data: A Designed Dental Experiment\nThe bonding data describe a designed experiment into the properties of four different resin types (resin = A, B, C, D) and two different curing light sources (light = Halogen, LED) as they relate to the resulting bonding strength (measured in MPa1) on the surface of teeth. The source is Kim (2014).\nThe experiment involved making measurements of bonding strength under a total of 80 experimental setups, or runs, with 10 runs completed at each of the eight combinations of a light source and a resin type. The data are gathered in the bonding.csv file.\n\nbonding\n\n# A tibble: 80 × 4\n   runID light   resin strength\n   <chr> <chr>   <chr>    <dbl>\n 1 R101  LED     B         12.8\n 2 R102  Halogen B         22.2\n 3 R103  Halogen B         24.6\n 4 R104  LED     A         17  \n 5 R105  LED     C         32.2\n 6 R106  Halogen B         27.1\n 7 R107  LED     A         23.4\n 8 R108  Halogen A         23.5\n 9 R109  Halogen D         37.3\n10 R110  Halogen A         19.7\n# … with 70 more rows"
  },
  {
    "objectID": "anovaexamples.html#a-one-factor-analysis-of-variance",
    "href": "anovaexamples.html#a-one-factor-analysis-of-variance",
    "title": "10  Two-Way ANOVA and Interaction",
    "section": "10.3 A One-Factor Analysis of Variance",
    "text": "10.3 A One-Factor Analysis of Variance\nSuppose we are interested in the distribution of the strength values for the four different types of resin.\n\nbonding |> group_by(resin) |> \n  summarise(n = n(), mean(strength), median(strength))\n\n# A tibble: 4 × 4\n  resin     n `mean(strength)` `median(strength)`\n  <chr> <int>            <dbl>              <dbl>\n1 A        20             18.4               18.0\n2 B        20             22.2               22.7\n3 C        20             25.2               25.7\n4 D        20             32.1               35.3\n\n\nI’d begin serious work with a plot.\n\n10.3.1 Look at the Data!\n\nggplot(bonding, aes(x = resin, y = strength)) +\n    geom_violin(aes(fill = resin)) +\n    geom_boxplot(width = 0.2)\n\n\n\n\nAnother good plot for this purpose is a ridgeline plot.\n\nggplot(bonding, aes(x = strength, y = resin, fill = resin)) +\n    geom_density_ridges2() +\n    guides(fill = \"none\")\n\nPicking joint bandwidth of 3.09\n\n\n\n\n\n\n\n10.3.2 Table of Summary Statistics\nWith the small size of this experiment (n = 20 for each resin type), graphical summaries may not perform as well as they often do. We’ll also produce a quick table of summary statistics for strength within each resin type.\n\nfavstats(strength ~ resin, data = bonding)\n\n  resin  min     Q1 median    Q3  max   mean       sd  n missing\n1     A  9.3 15.725  17.95 20.40 28.0 18.415 4.805948 20       0\n2     B 11.8 18.450  22.70 25.75 35.2 22.230 6.748263 20       0\n3     C 14.5 20.650  25.70 30.70 34.5 25.155 6.326425 20       0\n4     D 17.3 21.825  35.30 40.15 47.2 32.075 9.735063 20       0\n\n\nSince the means and medians within each group are fairly close, and the distributions (with the possible exception of resin D) are reasonably well approximated by the Normal, I’ll fit an ANOVA model2.\n\nanova(lm(strength ~ resin, data = bonding))\n\nAnalysis of Variance Table\n\nResponse: strength\n          Df Sum Sq Mean Sq F value   Pr(>F)    \nresin      3 1999.7  666.57  13.107 5.52e-07 ***\nResiduals 76 3865.2   50.86                     \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nIt appears that the resin types have a significant association with mean strength of the bonds. Can we identify which resin types have generally higher or lower strength?\n\nt_bond <- TukeyHSD(aov(strength ~ resin, data = bonding), \n                ordered = TRUE, conf.level = 0.90)\n\ntidy(t_bond) |> \n  select(-c(term, null.value)) |>\n  mutate(across(.cols = -contrast, num, digits = 3)) |>\n  arrange(desc(estimate)) |>\n  gt() |>\n  tab_header(title = \"Comparing Mean Bond Strength across pairs of resin types\",\n             subtitle = \"90% Tukey HSD Confidence Intervals\") |>\n  tab_footnote(footnote = glue(nrow(bonding), \" teeth in bonding data\"))\n\nWarning: There was 1 warning in `mutate()`.\nℹ In argument: `across(.cols = -contrast, num, digits = 3)`.\nCaused by warning:\n! The `...` argument of `across()` is deprecated as of dplyr 1.1.0.\nSupply arguments directly to `.fns` through an anonymous function instead.\n\n  # Previously\n  across(a:b, mean, na.rm = TRUE)\n\n  # Now\n  across(a:b, \\(x) mean(x, na.rm = TRUE))\n\n\n\n\n\n\n  \n    \n      Comparing Mean Bond Strength across pairs of resin types\n    \n    \n      90% Tukey HSD Confidence Intervals\n    \n  \n  \n    \n      contrast\n      estimate\n      conf.low\n      conf.high\n      adj.p.value\n    \n  \n  \n    D-A\n13.660\n8.403\n18.917\n0.000\n    D-B\n9.845\n4.588\n15.102\n0.000\n    D-C\n6.920\n1.663\n12.177\n0.015\n    C-A\n6.740\n1.483\n11.997\n0.019\n    B-A\n3.815\n-1.442\n9.072\n0.335\n    C-B\n2.925\n-2.332\n8.182\n0.568\n  \n  \n  \n    \n       80 teeth in bonding data\n    \n  \n\n\n\ntidy(t_bond) |>\n  mutate(contrast = fct_reorder(contrast, estimate, .desc = TRUE)) %>%\n  ggplot(., aes(x = contrast, y = estimate, ymin = conf.low, ymax = conf.high)) +\n  geom_pointrange() +\n  geom_hline(yintercept = 0, col = \"red\", linetype = \"dashed\") +\n  geom_label(aes(label = round_half_up(estimate, 2))) +\n  labs(title = \"Comparing Mean Bond Strength across pairs of resin types\",\n       subtitle = \"90% Tukey HSD Confidence intervals\",\n       caption = glue(nrow(bonding), \" teeth in bonding data\"),\n       x = \"Pairwise Difference between resin types\",\n       y = \"Difference in Mean Bond Strength\")\n\n\n\n\nBased on these confidence intervals (which have a family-wise 90% confidence level), we see that D shows arger mean strength than A or B or C, and that C is also associated with larger mean strength than A."
  },
  {
    "objectID": "anovaexamples.html#a-two-way-anova-looking-at-two-factors",
    "href": "anovaexamples.html#a-two-way-anova-looking-at-two-factors",
    "title": "10  Two-Way ANOVA and Interaction",
    "section": "10.4 A Two-Way ANOVA: Looking at Two Factors",
    "text": "10.4 A Two-Way ANOVA: Looking at Two Factors\nNow, we’ll now add consideration of the light source into our study. We can look at the distribution of the strength values at the combinations of both light and resin, with a plot like this one.\n\nggplot(bonding, aes(x = resin, y = strength, color = light)) +\n    geom_point(size = 2, alpha = 0.5) +\n    facet_wrap(~ light) +\n    guides(color = \"none\") +\n    scale_color_manual(values = c(\"purple\", \"darkorange\")) +\n    theme_bw()"
  },
  {
    "objectID": "anovaexamples.html#a-means-plot-with-standard-deviations-to-check-for-interaction",
    "href": "anovaexamples.html#a-means-plot-with-standard-deviations-to-check-for-interaction",
    "title": "10  Two-Way ANOVA and Interaction",
    "section": "10.5 A Means Plot (with standard deviations) to check for interaction",
    "text": "10.5 A Means Plot (with standard deviations) to check for interaction\nSometimes, we’ll instead look at a plot simply of the means (and, often, the standard deviations) of strength at each combination of light and resin. We’ll start by building up a data set with the summaries we want to plot.\n\nbond.sum <- bonding |> \n    group_by(resin, light) |>\n    summarize(mean.str = mean(strength), sd.str = sd(strength))\n\n`summarise()` has grouped output by 'resin'. You can override using the\n`.groups` argument.\n\nbond.sum\n\n# A tibble: 8 × 4\n# Groups:   resin [4]\n  resin light   mean.str sd.str\n  <chr> <chr>      <dbl>  <dbl>\n1 A     Halogen     17.8   4.02\n2 A     LED         19.1   5.63\n3 B     Halogen     19.9   5.62\n4 B     LED         24.6   7.25\n5 C     Halogen     22.5   6.19\n6 C     LED         27.8   5.56\n7 D     Halogen     40.3   4.15\n8 D     LED         23.8   5.70\n\n\nNow, we’ll use this new data set to plot the means and standard deviations of strength at each combination of resin and light.\n\n## The error bars will overlap unless we adjust the position.\npd <- position_dodge(0.2) # move them .1 to the left and right\n\nggplot(bond.sum, aes(x = resin, y = mean.str, col = light)) +\n    geom_errorbar(aes(ymin = mean.str - sd.str, \n                      ymax = mean.str + sd.str),\n                  width = 0.2, position = pd) +\n    geom_point(size = 2, position = pd) + \n    geom_line(aes(group = light), position = pd) +\n    scale_color_manual(values = c(\"purple\", \"darkorange\")) +\n    theme_bw() +\n    labs(y = \"Bonding Strength (MPa)\", x = \"Resin Type\",\n         title = \"Observed Means (+/- SD) of Bonding Strength\")\n\n\n\n\nIs there evidence of a meaningful interaction between the resin type and the light source on the bonding strength in this plot?\n\nSure. A meaningful interaction just means that the strength associated with different resin types depends on the light source.\n\nWith LED light, it appears that resin C leads to the strongest bonding strength.\nWith Halogen light, though, it seems that resin D is substantially stronger.\n\nNote that the lines we see here connecting the light sources aren’t in parallel (as they would be if we had zero interaction between resin and light), but rather, they cross.\n\n\n10.5.1 Summarizing the data after grouping by resin and light\nWe might want to look at a numerical summary of the strengths within these groups, too.\n\nfavstats(strength ~ resin + light, data = bonding) |>\n    select(resin.light, median, mean, sd, n, missing)\n\n  resin.light median  mean       sd  n missing\n1   A.Halogen  18.35 17.77 4.024108 10       0\n2   B.Halogen  21.75 19.90 5.617631 10       0\n3   C.Halogen  21.30 22.54 6.191069 10       0\n4   D.Halogen  40.40 40.30 4.147556 10       0\n5       A.LED  17.80 19.06 5.625181 10       0\n6       B.LED  24.45 24.56 7.246792 10       0\n7       C.LED  28.45 27.77 5.564980 10       0\n8       D.LED  21.45 23.85 5.704043 10       0"
  },
  {
    "objectID": "anovaexamples.html#fitting-the-two-way-anova-model-with-interaction",
    "href": "anovaexamples.html#fitting-the-two-way-anova-model-with-interaction",
    "title": "10  Two-Way ANOVA and Interaction",
    "section": "10.6 Fitting the Two-Way ANOVA model with Interaction",
    "text": "10.6 Fitting the Two-Way ANOVA model with Interaction\n\nc3_m1 <- lm(strength ~ resin * light, data = bonding)\n\nsummary(c3_m1)\n\n\nCall:\nlm(formula = strength ~ resin * light, data = bonding)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-11.760  -3.663  -0.320   3.697  11.250 \n\nCoefficients:\n                Estimate Std. Error t value Pr(>|t|)    \n(Intercept)       17.770      1.771  10.033 2.57e-15 ***\nresinB             2.130      2.505   0.850   0.3979    \nresinC             4.770      2.505   1.904   0.0609 .  \nresinD            22.530      2.505   8.995 2.13e-13 ***\nlightLED           1.290      2.505   0.515   0.6081    \nresinB:lightLED    3.370      3.542   0.951   0.3446    \nresinC:lightLED    3.940      3.542   1.112   0.2697    \nresinD:lightLED  -17.740      3.542  -5.008 3.78e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 5.601 on 72 degrees of freedom\nMultiple R-squared:  0.6149,    Adjusted R-squared:  0.5775 \nF-statistic: 16.42 on 7 and 72 DF,  p-value: 9.801e-13\n\n\n\n10.6.1 The ANOVA table for our model\nIn a two-way ANOVA model, we begin by assessing the interaction term. If it’s important, then our best model is the model including the interaction. If it’s not important, we will often move on to consider a new model, fit without an interaction.\nThe ANOVA table is especially helpful in this case, because it lets us look specifically at the interaction effect.\n\nanova(c3_m1)\n\nAnalysis of Variance Table\n\nResponse: strength\n            Df  Sum Sq Mean Sq F value    Pr(>F)    \nresin        3 1999.72  666.57 21.2499 5.792e-10 ***\nlight        1   34.72   34.72  1.1067    0.2963    \nresin:light  3 1571.96  523.99 16.7043 2.457e-08 ***\nResiduals   72 2258.52   31.37                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\n\n10.6.2 Is the interaction important?\nIn this case, the interaction:\n\nis evident in the means plot, and\nis highly statistically significant, and\naccounts for a sizable fraction (27%) of the overall variation\n\n\\[\n\\eta^2_{interaction} = \\frac{\\mbox{SS(resin:light)}}{SS(Total)}\n= \\frac{1571.96}{1999.72 + 34.72 + 1571.96 + 2258.52} = 0.268\n\\]\nIf the interaction were either large or significant we would be inclined to keep it in the model. In this case, it’s both, so there’s no real reason to remove it.\n\n\n10.6.3 Interpreting the Interaction\nRecall the model equation, which is:\n\nc3_m1\n\n\nCall:\nlm(formula = strength ~ resin * light, data = bonding)\n\nCoefficients:\n    (Intercept)           resinB           resinC           resinD  \n          17.77             2.13             4.77            22.53  \n       lightLED  resinB:lightLED  resinC:lightLED  resinD:lightLED  \n           1.29             3.37             3.94           -17.74  \n\n\nSo, if light = Halogen, our equation is:\n\\[\nstrength = 17.77 + 2.13 resinB + 4.77 resinC + 22.53 resinD\n\\]\nAnd if light = LED, our equation is:\n\\[\nstrength = 19.06 + 5.50 resinB + 8.71 resinC + 4.79 resinD\n\\]\nNote that both the intercept and the slopes change as a result of the interaction. The model yields a different prediction for every possible combination of a resin type and a light source."
  },
  {
    "objectID": "anovaexamples.html#comparing-individual-combinations-of-resin-and-light",
    "href": "anovaexamples.html#comparing-individual-combinations-of-resin-and-light",
    "title": "10  Two-Way ANOVA and Interaction",
    "section": "10.7 Comparing Individual Combinations of resin and light",
    "text": "10.7 Comparing Individual Combinations of resin and light\nTo make comparisons between individual combinations of a resin type and a light source, using something like Tukey’s HSD approach for multiple comparisons, we first refit the model using the aov structure, rather than lm.\n\nc3m1_aov <- aov(strength ~ resin * light, data = bonding)\n\nsummary(c3m1_aov)\n\n            Df Sum Sq Mean Sq F value   Pr(>F)    \nresin        3 1999.7   666.6  21.250 5.79e-10 ***\nlight        1   34.7    34.7   1.107    0.296    \nresin:light  3 1572.0   524.0  16.704 2.46e-08 ***\nResiduals   72 2258.5    31.4                     \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nAnd now, we can obtain Tukey HSD comparisons (which will maintain an overall 90% family-wise confidence level) across the resin types, the light sources, and the combinations, with the TukeyHSD command. This approach is only completely appropriate if these comparisons are pre-planned, and if the design is balanced (as this is, with the same sample size for each combination of a light source and resin type.)\n\nTukeyHSD(c3m1_aov, conf.level = 0.9)\n\n  Tukey multiple comparisons of means\n    90% family-wise confidence level\n\nFit: aov(formula = strength ~ resin * light, data = bonding)\n\n$resin\n      diff        lwr       upr     p adj\nB-A  3.815 -0.3176052  7.947605 0.1461960\nC-A  6.740  2.6073948 10.872605 0.0016436\nD-A 13.660  9.5273948 17.792605 0.0000000\nC-B  2.925 -1.2076052  7.057605 0.3568373\nD-B  9.845  5.7123948 13.977605 0.0000026\nD-C  6.920  2.7873948 11.052605 0.0011731\n\n$light\n               diff       lwr       upr     p adj\nLED-Halogen -1.3175 -3.404306 0.7693065 0.2963128\n\n$`resin:light`\n                      diff         lwr        upr     p adj\nB:Halogen-A:Halogen   2.13  -4.9961962   9.256196 0.9893515\nC:Halogen-A:Halogen   4.77  -2.3561962  11.896196 0.5525230\nD:Halogen-A:Halogen  22.53  15.4038038  29.656196 0.0000000\nA:LED-A:Halogen       1.29  -5.8361962   8.416196 0.9995485\nB:LED-A:Halogen       6.79  -0.3361962  13.916196 0.1361092\nC:LED-A:Halogen      10.00   2.8738038  17.126196 0.0037074\nD:LED-A:Halogen       6.08  -1.0461962  13.206196 0.2443200\nC:Halogen-B:Halogen   2.64  -4.4861962   9.766196 0.9640100\nD:Halogen-B:Halogen  20.40  13.2738038  27.526196 0.0000000\nA:LED-B:Halogen      -0.84  -7.9661962   6.286196 0.9999747\nB:LED-B:Halogen       4.66  -2.4661962  11.786196 0.5818695\nC:LED-B:Halogen       7.87   0.7438038  14.996196 0.0473914\nD:LED-B:Halogen       3.95  -3.1761962  11.076196 0.7621860\nD:Halogen-C:Halogen  17.76  10.6338038  24.886196 0.0000000\nA:LED-C:Halogen      -3.48 -10.6061962   3.646196 0.8591455\nB:LED-C:Halogen       2.02  -5.1061962   9.146196 0.9922412\nC:LED-C:Halogen       5.23  -1.8961962  12.356196 0.4323859\nD:LED-C:Halogen       1.31  -5.8161962   8.436196 0.9995004\nA:LED-D:Halogen     -21.24 -28.3661962 -14.113804 0.0000000\nB:LED-D:Halogen     -15.74 -22.8661962  -8.613804 0.0000006\nC:LED-D:Halogen     -12.53 -19.6561962  -5.403804 0.0001014\nD:LED-D:Halogen     -16.45 -23.5761962  -9.323804 0.0000002\nB:LED-A:LED           5.50  -1.6261962  12.626196 0.3665620\nC:LED-A:LED           8.71   1.5838038  15.836196 0.0185285\nD:LED-A:LED           4.79  -2.3361962  11.916196 0.5471915\nC:LED-B:LED           3.21  -3.9161962  10.336196 0.9027236\nD:LED-B:LED          -0.71  -7.8361962   6.416196 0.9999920\nD:LED-C:LED          -3.92 -11.0461962   3.206196 0.7690762\n\n\nOne conclusion from this is that the combination of D and Halogen appears stronger than each of the other seven combinations."
  },
  {
    "objectID": "anovaexamples.html#the-bonding-model-without-interaction",
    "href": "anovaexamples.html#the-bonding-model-without-interaction",
    "title": "10  Two-Way ANOVA and Interaction",
    "section": "10.8 The bonding model without Interaction",
    "text": "10.8 The bonding model without Interaction\nIt seems incorrect in this situation to fit a model without the interaction term, but we’ll do so just so you can see what’s involved.\n\nc3_m2 <- lm(strength ~ resin + light, data = bonding)\n\nsummary(c3_m2)\n\n\nCall:\nlm(formula = strength ~ resin + light, data = bonding)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-14.1163  -4.9531   0.1187   4.4613  14.4663 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)   19.074      1.787  10.676  < 2e-16 ***\nresinB         3.815      2.260   1.688  0.09555 .  \nresinC         6.740      2.260   2.982  0.00386 ** \nresinD        13.660      2.260   6.044 5.39e-08 ***\nlightLED      -1.317      1.598  -0.824  0.41229    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 7.147 on 75 degrees of freedom\nMultiple R-squared:  0.3469,    Adjusted R-squared:  0.312 \nF-statistic: 9.958 on 4 and 75 DF,  p-value: 1.616e-06\n\n\nIn the no-interaction model, if light = Halogen, our equation is:\n\\[\nstrength = 19.07 + 3.82 resinB + 6.74 resinC + 13.66 resinD\n\\]\nAnd if light = LED, our equation is:\n\\[\nstrength = 17.75 + 3.82 resinB + 6.74 resinC + 13.66 resinD\n\\]\nSo, in the no-interaction model, only the intercept changes.\n\nanova(c3_m2)\n\nAnalysis of Variance Table\n\nResponse: strength\n          Df Sum Sq Mean Sq F value    Pr(>F)    \nresin      3 1999.7  666.57 13.0514 6.036e-07 ***\nlight      1   34.7   34.72  0.6797    0.4123    \nResiduals 75 3830.5   51.07                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nAnd, it appears, if we ignore the interaction, then resin type has a large impact on strength but light source doesn’t. This is clearer when we look at boxplots of the separated light and resin groups.\n\np1 <- ggplot(bonding, aes(x = light, y = strength)) + \n    geom_boxplot()\np2 <- ggplot(bonding, aes(x = resin, y = strength)) +\n    geom_boxplot()\n\np1 + p2"
  },
  {
    "objectID": "anovaexamples.html#cortisol-a-hypothetical-clinical-trial",
    "href": "anovaexamples.html#cortisol-a-hypothetical-clinical-trial",
    "title": "10  Two-Way ANOVA and Interaction",
    "section": "10.9 cortisol: A Hypothetical Clinical Trial",
    "text": "10.9 cortisol: A Hypothetical Clinical Trial\n156 adults who complained of problems with a high-stress lifestyle were enrolled in a hypothetical clinical trial of the effectiveness of a behavioral intervention designed to help reduce stress levels, as measured by salivary cortisol.\nThe subjects were randomly assigned to one of three intervention groups (usual care, low dose, and high dose.) The “low dose” subjects received a one-week intervention with a follow-up at week 5. The “high dose” subjects received a more intensive three-week intervention, with follow up at week 5.\nSince cortisol levels rise and fall with circadian rhythms, the cortisol measurements were taken just after rising for all subjects. These measurements were taken at baseline, and again at five weeks. The difference (baseline - week 5) in cortisol level (in micrograms / l) serves as the primary outcome.\n\n10.9.1 Codebook and Raw Data for cortisol\nThe data are gathered in the cortisol data set. Included are:\n\n\n\nVariable\nDescription\n\n\n\n\nsubject\nsubject identification code\n\n\ninterv\nintervention group (UC = usual care, Low, High)\n\n\nwaist\nwaist circumference at baseline (in inches)\n\n\nsex\nmale or female\n\n\ncort.1\nsalivary cortisol level (microg/l) week 1\n\n\ncort.5\nsalivary cortisol level (microg/l) week 5\n\n\n\n\ncortisol\n\n# A tibble: 156 × 6\n   subject interv waist sex   cort.1 cort.5\n   <chr>   <chr>  <dbl> <chr>  <dbl>  <dbl>\n 1 S1001   UC      48.3 M       13.4   13.3\n 2 S1002   Low     58.3 M       17.8   16.6\n 3 S1003   High    43   M       14.4   12.7\n 4 S1004   Low     44.9 M        9      9.8\n 5 S1005   High    46.1 M       14.2   14.2\n 6 S1006   UC      41.3 M       14.8   15.1\n 7 S1007   Low     51   F       13.7   16  \n 8 S1008   UC      42   F       17.3   18.7\n 9 S1009   Low     24.7 F       15.3   15.8\n10 S1010   Low     59.4 M       12.4   11.7\n# … with 146 more rows"
  },
  {
    "objectID": "anovaexamples.html#creating-a-factor-combining-sex-and-waist",
    "href": "anovaexamples.html#creating-a-factor-combining-sex-and-waist",
    "title": "10  Two-Way ANOVA and Interaction",
    "section": "10.10 Creating a factor combining sex and waist",
    "text": "10.10 Creating a factor combining sex and waist\nNext, we’ll put the waist and sex data in the cortisol example together. We want to build a second categorical variable (called fat_est) combining this information, to indicate “healthy” vs. “unhealthy” levels of fat around the waist.\n\nMale subjects whose waist circumference is 40 inches or more, and\nFemale subjects whose waist circumference is 35 inches or more, will fall in the “unhealthy” group.\n\n\ncortisol <- cortisol |>\n    mutate(\n        fat_est = factor(case_when(\n            sex == \"M\" & waist >= 40 ~ \"unhealthy\",\n            sex == \"F\" & waist >= 35 ~ \"unhealthy\",\n            TRUE                     ~ \"healthy\")),\n        cort_diff = cort.1 - cort.5)\n\nsummary(cortisol)\n\n   subject             interv              waist           sex           \n Length:156         Length:156         Min.   :20.80   Length:156        \n Class :character   Class :character   1st Qu.:33.27   Class :character  \n Mode  :character   Mode  :character   Median :40.35   Mode  :character  \n                                       Mean   :40.42                     \n                                       3rd Qu.:47.77                     \n                                       Max.   :59.90                     \n     cort.1           cort.5          fat_est      cort_diff      \n Min.   : 6.000   Min.   : 4.2   healthy  : 56   Min.   :-2.3000  \n 1st Qu.: 9.675   1st Qu.: 9.6   unhealthy:100   1st Qu.:-0.5000  \n Median :12.400   Median :12.6                   Median : 0.2000  \n Mean   :12.686   Mean   :12.4                   Mean   : 0.2821  \n 3rd Qu.:16.025   3rd Qu.:15.7                   3rd Qu.: 1.2000  \n Max.   :19.000   Max.   :19.7                   Max.   : 2.0000"
  },
  {
    "objectID": "anovaexamples.html#a-means-plot-for-the-cortisol-trial-with-standard-errors",
    "href": "anovaexamples.html#a-means-plot-for-the-cortisol-trial-with-standard-errors",
    "title": "10  Two-Way ANOVA and Interaction",
    "section": "10.11 A Means Plot for the cortisol trial (with standard errors)",
    "text": "10.11 A Means Plot for the cortisol trial (with standard errors)\nAgain, we’ll start by building up a data set with the summaries we want to plot.\n\ncort.sum <- cortisol |> \n    group_by(interv, fat_est) |>\n    summarize(mean.cort = mean(cort_diff), \n              se.cort = sd(cort_diff)/sqrt(n()))\n\n`summarise()` has grouped output by 'interv'. You can override using the\n`.groups` argument.\n\ncort.sum\n\n# A tibble: 6 × 4\n# Groups:   interv [3]\n  interv fat_est   mean.cort se.cort\n  <chr>  <fct>         <dbl>   <dbl>\n1 High   healthy       0.695   0.217\n2 High   unhealthy     0.352   0.150\n3 Low    healthy       0.5     0.182\n4 Low    unhealthy     0.327   0.190\n5 UC     healthy       0.347   0.225\n6 UC     unhealthy    -0.226   0.155\n\n\nNow, we’ll use this new data set to plot the means and standard errors.\n\n## The error bars will overlap unless we adjust the position.\npd <- position_dodge(0.2) # move them .1 to the left and right\n\nggplot(cort.sum, aes(x = interv, y = mean.cort, col = fat_est)) +\n    geom_errorbar(aes(ymin = mean.cort - se.cort, \n                      ymax = mean.cort + se.cort),\n                  width = 0.2, position = pd) +\n    geom_point(size = 2, position = pd) + \n    geom_line(aes(group = fat_est), position = pd) +\n    scale_color_manual(values = c(\"royalblue\", \"darkred\")) +\n    theme_bw() +\n    labs(y = \"Salivary Cortisol Level\", x = \"Intervention Group\",\n         title = \"Observed Means (+/- SE) of Salivary Cortisol\")"
  },
  {
    "objectID": "anovaexamples.html#a-two-way-anova-model-for-cortisol-with-interaction",
    "href": "anovaexamples.html#a-two-way-anova-model-for-cortisol-with-interaction",
    "title": "10  Two-Way ANOVA and Interaction",
    "section": "10.12 A Two-Way ANOVA model for cortisol with Interaction",
    "text": "10.12 A Two-Way ANOVA model for cortisol with Interaction\n\nc3_m3 <- lm(cort_diff ~ interv * fat_est, data = cortisol)\n\nanova(c3_m3)\n\nAnalysis of Variance Table\n\nResponse: cort_diff\n                Df  Sum Sq Mean Sq F value  Pr(>F)  \ninterv           2   7.847  3.9235  4.4698 0.01301 *\nfat_est          1   4.614  4.6139  5.2564 0.02326 *\ninterv:fat_est   2   0.943  0.4715  0.5371 0.58554  \nResiduals      150 131.666  0.8778                  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nDoes it seem like we need the interaction term in this case?\n\nsummary(c3_m3)\n\n\nCall:\nlm(formula = cort_diff ~ interv * fat_est, data = cortisol)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-2.62727 -0.75702  0.08636  0.84848  2.12647 \n\nCoefficients:\n                           Estimate Std. Error t value Pr(>|t|)   \n(Intercept)                  0.6950     0.2095   3.317  0.00114 **\nintervLow                   -0.1950     0.3001  -0.650  0.51689   \nintervUC                    -0.3479     0.3091  -1.126  0.26206   \nfat_estunhealthy            -0.3435     0.2655  -1.294  0.19774   \nintervLow:fat_estunhealthy   0.1708     0.3785   0.451  0.65256   \nintervUC:fat_estunhealthy   -0.2300     0.3846  -0.598  0.55068   \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.9369 on 150 degrees of freedom\nMultiple R-squared:  0.0924,    Adjusted R-squared:  0.06214 \nF-statistic: 3.054 on 5 and 150 DF,  p-value: 0.01179\n\n\n\n10.12.1 Notes on this Question\nWhen we’re evaluating a two-factor ANOVA model with an interaction, we are choosing between models with either:\n\njust one factor\nboth factors, but only as main effects\nboth factors and an interaction between them\n\nBut we don’t get to pick models that include any other combination of terms. For this two-way ANOVA, then, our choices are:\n\na model with `interv only\na model with `fat_est only\na model with both interv and fat_est but not their interaction\na model with interv and fat_est and their interaction\n\nThose are the only modeling options available to us.\nFirst, consider the ANOVA table, repeated below…\n\nanova(c3_m3)\n\nAnalysis of Variance Table\n\nResponse: cort_diff\n                Df  Sum Sq Mean Sq F value  Pr(>F)  \ninterv           2   7.847  3.9235  4.4698 0.01301 *\nfat_est          1   4.614  4.6139  5.2564 0.02326 *\ninterv:fat_est   2   0.943  0.4715  0.5371 0.58554  \nResiduals      150 131.666  0.8778                  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nThe conclusions here are as follows:\n\nThe interaction effect (interv:fat_est) has a large p value (0.58554) and assesses whether the two interaction terms (product terms) included in the model add detectable predictive value to the main effects model that includes only interv and fat_est alone. You are right to say that this ANOVA is sequential, which means that the p value for the interaction effect is looking at the additional effect of the interaction once we already have the main effects interv and fat_est included.\nThe interv and fat_est terms aren’t usually evaluated with hypothesis tests or interpreted in the ANOVA for this setting, since if we intend to include the interaction term (as this model does) we also need these main effects. If we wanted to look at those terms individually in a model without the interaction, then we’d want to fit that model (without the interaction term) to do so.\n\nNext, let’s look at the summary of the c3_m3 model, specifically the coefficients…\n\nsummary(c3_m3)\n\n\nCall:\nlm(formula = cort_diff ~ interv * fat_est, data = cortisol)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-2.62727 -0.75702  0.08636  0.84848  2.12647 \n\nCoefficients:\n                           Estimate Std. Error t value Pr(>|t|)   \n(Intercept)                  0.6950     0.2095   3.317  0.00114 **\nintervLow                   -0.1950     0.3001  -0.650  0.51689   \nintervUC                    -0.3479     0.3091  -1.126  0.26206   \nfat_estunhealthy            -0.3435     0.2655  -1.294  0.19774   \nintervLow:fat_estunhealthy   0.1708     0.3785   0.451  0.65256   \nintervUC:fat_estunhealthy   -0.2300     0.3846  -0.598  0.55068   \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.9369 on 150 degrees of freedom\nMultiple R-squared:  0.0924,    Adjusted R-squared:  0.06214 \nF-statistic: 3.054 on 5 and 150 DF,  p-value: 0.01179\n\n\nSo here, we see two p values associated with the interaction terms (the two product terms at the bottom of the regression) but these aren’t especially helpful, because we’re either going to include the interaction (in which case both of these terms will be in the model) or we’re not going to include the interaction (in which case neither of these terms will be in the model.)\nSo the p values provided here aren’t very helpful - like all such p values for t tests, they are looking at the value of the term in their row as the last predictor in to the model, essentially comparing the full model to the model without that specific component, but none of those tests enable us to decide which of the 4 available model choices is our best fit.\nNow, let’s consider the reason why, for example, the p value for fat_est in the summary() which is looking at comparing the following models …\n\na model including interv (which has 2 coefficients to account for its 3 categories), fat_est (which has 1 coefficient to account for its 2 categories), and the interv*fat_est interaction terms (which are 2 terms)\na model including interv and the interv*fat_est interaction (but somehow not the main effect of fat_est, which actually makes no sense: if we include the interaction we always include the main effect)\n\nto the p value for fat_est in the ANOVA which is looking at comparing\n\nthe model with interv alone to\nthe model with interv and fat_est as main effects, but no interaction\n\nOnly the ANOVA p value is therefore in any way useful, and it suggests that once you have the main effect of interv, adding fat_est’s main effect adds statistically detectable value (p = 0.023)"
  },
  {
    "objectID": "anovaexamples.html#a-two-way-anova-model-for-cortisol-without-interaction",
    "href": "anovaexamples.html#a-two-way-anova-model-for-cortisol-without-interaction",
    "title": "10  Two-Way ANOVA and Interaction",
    "section": "10.13 A Two-Way ANOVA model for cortisol without Interaction",
    "text": "10.13 A Two-Way ANOVA model for cortisol without Interaction\n\n10.13.1 The Graph\n\np1 <- ggplot(cortisol, aes(x = interv, y = cort_diff)) + \n    geom_boxplot()\np2 <- ggplot(cortisol, aes(x = fat_est, y = cort_diff)) +\n    geom_boxplot()\n\np1 + p2\n\n\n\n\n\n\n10.13.2 The ANOVA Model\n\nc3_m4 <- lm(cort_diff ~ interv + fat_est, data = cortisol)\n\nanova(c3_m4)\n\nAnalysis of Variance Table\n\nResponse: cort_diff\n           Df  Sum Sq Mean Sq F value  Pr(>F)  \ninterv      2   7.847  3.9235  4.4972 0.01266 *\nfat_est     1   4.614  4.6139  5.2886 0.02283 *\nResiduals 152 132.609  0.8724                  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nHow do these results compare to those we saw in the model with interaction?\n\n\n10.13.3 The Regression Summary\n\nsummary(c3_m4)\n\n\nCall:\nlm(formula = cort_diff ~ interv + fat_est, data = cortisol)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-2.55929 -0.74527  0.05457  0.86456  2.05489 \n\nCoefficients:\n                 Estimate Std. Error t value Pr(>|t|)    \n(Intercept)       0.70452    0.16093   4.378 2.22e-05 ***\nintervLow        -0.08645    0.18232  -0.474  0.63606    \nintervUC         -0.50063    0.18334  -2.731  0.00707 ** \nfat_estunhealthy -0.35878    0.15601  -2.300  0.02283 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.934 on 152 degrees of freedom\nMultiple R-squared:  0.0859,    Adjusted R-squared:  0.06785 \nF-statistic: 4.761 on 3 and 152 DF,  p-value: 0.00335\n\n\n\n\n10.13.4 Tukey HSD Comparisons\nWithout the interaction term, we can make direct comparisons between levels of the intervention, and between levels of the fat_est variable. This is probably best done here in a Tukey HSD comparison.\n\nTukeyHSD(aov(cort_diff ~ interv + fat_est, data = cortisol), conf.level = 0.9)\n\n  Tukey multiple comparisons of means\n    90% family-wise confidence level\n\nFit: aov(formula = cort_diff ~ interv + fat_est, data = cortisol)\n\n$interv\n                diff        lwr         upr     p adj\nLow-High -0.09074746 -0.4677566  0.28626166 0.8724916\nUC-High  -0.51642619 -0.8952964 -0.13755598 0.0150150\nUC-Low   -0.42567873 -0.8063312 -0.04502625 0.0570728\n\n$fat_est\n                        diff        lwr       upr     p adj\nunhealthy-healthy -0.3582443 -0.6162415 -0.100247 0.0229266\n\n\nWhat conclusions can we draw here?\n\n\n\n\nKim, Hae-Young. 2014. “Statistical Notes for Clinical Researchers: Two-Way Analysis of Variance (ANOVA) - Exploring Possible Interaction Between Factors.” Restorative Dentistry & Endodontics 39(2): 143–47. https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3978106/."
  },
  {
    "objectID": "ancova_emphysema.html#r-setup-used-here",
    "href": "ancova_emphysema.html#r-setup-used-here",
    "title": "11  Analysis of Covariance",
    "section": "11.1 R Setup Used Here",
    "text": "11.1 R Setup Used Here\n\nknitr::opts_chunk$set(comment = NA)\n\nlibrary(janitor)\nlibrary(broom)\nlibrary(mosaic)\nlibrary(tidyverse) \n\ntheme_set(theme_bw())\n\n\n11.1.1 Data Load\n\nemphysema <- read_csv(\"data/emphysema.csv\", show_col_types = FALSE)"
  },
  {
    "objectID": "ancova_emphysema.html#an-emphysema-study",
    "href": "ancova_emphysema.html#an-emphysema-study",
    "title": "11  Analysis of Covariance",
    "section": "11.2 An Emphysema Study",
    "text": "11.2 An Emphysema Study\nMy source for this example is Riffenburgh (2006), section 18.4. Serum theophylline levels (in mg/dl) were measured in 16 patients with emphysema at baseline, then 5 days later (at the end of a course of antibiotics) and then at 10 days after baseline. Clinicians anticipate that the antibiotic will increase the theophylline level. The data are stored in the emphysema.csv data file, and note that the age for patient 5 is not available.\n\n11.2.1 Codebook\n\n\n\nVariable\nDescription\n\n\n\n\npatient\nID code\n\n\nage\npatient’s age in years\n\n\nsex\npatient’s sex (F or M)\n\n\nst_base\npatient’s serum theophylline at baseline (mg/dl)\n\n\nst_day5\npatient’s serum theophylline at day 5 (mg/dl)\n\n\nst_day10\npatient’s serum theophylline at day 10 (mg/dl)\n\n\n\nWe’re going to look at the change from baseline to day 5 as our outcome of interest, since the clinical expectation is that the antibiotic (azithromycin) will increase theophylline levels.\n\nemphysema <- emphysema |> \n    mutate(st_delta = st_day5 - st_base)\n\nemphysema\n\n# A tibble: 16 × 7\n   patient   age sex   st_base st_day5 st_day10 st_delta\n     <dbl> <dbl> <chr>   <dbl>   <dbl>    <dbl>    <dbl>\n 1       1    61 F        14.1     2.3     10.3  -11.8  \n 2       2    70 F         7.2     5.4      7.3   -1.8  \n 3       3    65 M        14.2    11.9     11.3   -2.3  \n 4       4    65 M        10.3    10.7     13.8    0.400\n 5       5    NA M         9.9    10.7     11.7    0.800\n 6       6    76 M         5.2     6.8      4.2    1.6  \n 7       7    72 M        10.4    14.6     14.1    4.2  \n 8       8    69 F        10.5     7.2      5.4   -3.3  \n 9       9    66 M         5       5        5.1    0    \n10      10    62 M         8.6     8.1      7.4   -0.5  \n11      11    65 F        16.6    14.9     13     -1.70 \n12      12    71 M        16.4    18.6     17.1    2.20 \n13      13    51 F        12.2    11       12.3   -1.2  \n14      14    71 M         6.6     3.7      4.5   -2.9  \n15      15    64 F        15.4    15.2     13.6   -0.200\n16      16    50 M        10.2    10.8     11.2    0.600"
  },
  {
    "objectID": "ancova_emphysema.html#does-sex-affect-the-mean-change-in-theophylline",
    "href": "ancova_emphysema.html#does-sex-affect-the-mean-change-in-theophylline",
    "title": "11  Analysis of Covariance",
    "section": "11.3 Does sex affect the mean change in theophylline?",
    "text": "11.3 Does sex affect the mean change in theophylline?\n\nfavstats(~ st_delta, data = emphysema)\n\n   min     Q1 median   Q3 max     mean       sd  n missing\n -11.8 -1.925  -0.35 0.65 4.2 -0.99375 3.484149 16       0\n\n\n\nfavstats(st_delta ~ sex, data = emphysema)\n\n  sex   min     Q1 median     Q3  max      mean       sd  n missing\n1   F -11.8 -2.925  -1.75 -1.325 -0.2 -3.333333 4.267864  6       0\n2   M  -2.9 -0.375   0.50  1.400  4.2  0.410000 2.067446 10       0\n\n\nOverall, the mean change in theophylline during the course of the antibiotic is -0.99, but this is -3.33 for female patients and 0.41 for male patients.\nA one-way ANOVA model looks like this:\n\nanova(lm(st_delta ~ sex, data = emphysema))\n\nAnalysis of Variance Table\n\nResponse: st_delta\n          Df  Sum Sq Mean Sq F value  Pr(>F)  \nsex        1  52.547  52.547  5.6789 0.03189 *\nResiduals 14 129.542   9.253                  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nThe ANOVA F test finds a fairly large difference between the mean st_delta among males and the mean st_delta among females. But is there more to the story?"
  },
  {
    "objectID": "ancova_emphysema.html#is-there-an-association-between-age-and-sex-in-this-study",
    "href": "ancova_emphysema.html#is-there-an-association-between-age-and-sex-in-this-study",
    "title": "11  Analysis of Covariance",
    "section": "11.4 Is there an association between age and sex in this study?",
    "text": "11.4 Is there an association between age and sex in this study?\n\nfavstats(age ~ sex, data = emphysema)\n\n  sex min    Q1 median Q3 max     mean       sd n missing\n1   F  51 61.75   64.5 68  70 63.33333 6.889606 6       0\n2   M  50 65.00   66.0 71  76 66.44444 7.568208 9       1\n\n\nBut we note that the male patients are also older than the female patients, on average (mean age for males is 66.4, for females 63.3)\n\nDoes the fact that male patients are older affect change in theophylline level?\nAnd how should we deal with the one missing age value (in a male patient)?"
  },
  {
    "objectID": "ancova_emphysema.html#adding-a-quantitative-covariate-age-to-the-model",
    "href": "ancova_emphysema.html#adding-a-quantitative-covariate-age-to-the-model",
    "title": "11  Analysis of Covariance",
    "section": "11.5 Adding a quantitative covariate, age, to the model",
    "text": "11.5 Adding a quantitative covariate, age, to the model\nWe could fit an ANOVA model to predict st_delta using sex and age directly, but only if we categorized age into two or more groups. Because age is not categorical, we cannot include it in an ANOVA. But if age is an influence, and we don’t adjust for it, it may well bias the outcome of our initial ANOVA. With a quantitative variable like age, we will need a method called ANCOVA, for analysis of covariance.\n\n11.5.1 The ANCOVA model\nANCOVA in this case is just an ANOVA model with our outcome (st_delta) adjusted for a continuous covariate, called age. For the moment, we’ll ignore the one subject with missing age and simply fit the regression model with sex and age.\n\nsummary(lm(st_delta ~ sex + age, data = emphysema))\n\n\nCall:\nlm(formula = st_delta ~ sex + age, data = emphysema)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-8.3352 -0.4789  0.6948  1.5580  3.5202 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)  \n(Intercept) -6.90266    7.92948  -0.871   0.4011  \nsexM         3.52466    1.75815   2.005   0.0681 .\nage          0.05636    0.12343   0.457   0.6561  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.255 on 12 degrees of freedom\n  (1 observation deleted due to missingness)\nMultiple R-squared:  0.2882,    Adjusted R-squared:  0.1696 \nF-statistic:  2.43 on 2 and 12 DF,  p-value: 0.13\n\n\nThis model assumes that the slope of the regression line between st_delta and age is the same for both sexes.\nNote that the model yields st_delta = -6.9 + 3.52 (sex = male) + 0.056 age, or\n\nst_delta = -6.9 + 0.056 age for female patients, and\nst_delta = (-6.9 + 3.52) + 0.056 age = -3.38 + 0.056 age for male patients.\n\nNote that we can test this assumption of equal slopes by fitting an alternative model (with a product term between sex and age) that doesn’t require the assumption, and we’ll do that later.\n\n\n11.5.2 The ANCOVA Table\nFirst, though, we’ll look at the ANCOVA table.\n\nanova(lm(st_delta ~ sex + age, data = emphysema))\n\nAnalysis of Variance Table\n\nResponse: st_delta\n          Df  Sum Sq Mean Sq F value  Pr(>F)  \nsex        1  49.284  49.284  4.6507 0.05203 .\nage        1   2.209   2.209  0.2085 0.65612  \nResiduals 12 127.164  10.597                  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nWhen we tested sex without accounting for age, we found a p value of 0.032, which is less than our usual cutpoint of 0.05. But when we adjusted for age, we find that sex’s p value rises, even though age doesn’t seem to have a particularly strong influence on st_delta by itself, according to the ANCOVA table."
  },
  {
    "objectID": "ancova_emphysema.html#rerunning-the-ancova-model-after-simple-imputation",
    "href": "ancova_emphysema.html#rerunning-the-ancova-model-after-simple-imputation",
    "title": "11  Analysis of Covariance",
    "section": "11.6 Rerunning the ANCOVA model after simple imputation",
    "text": "11.6 Rerunning the ANCOVA model after simple imputation\nWe could have imputed the missing age value for patient 5, rather than just deleting that patient. Suppose we do the simplest potentially reasonable thing to do: insert the mean age in where the NA value currently exists.\n\nemph_imp <- replace_na(emphysema, list(age = mean(emphysema$age, na.rm = TRUE)))\n\nemph_imp\n\n# A tibble: 16 × 7\n   patient   age sex   st_base st_day5 st_day10 st_delta\n     <dbl> <dbl> <chr>   <dbl>   <dbl>    <dbl>    <dbl>\n 1       1  61   F        14.1     2.3     10.3  -11.8  \n 2       2  70   F         7.2     5.4      7.3   -1.8  \n 3       3  65   M        14.2    11.9     11.3   -2.3  \n 4       4  65   M        10.3    10.7     13.8    0.400\n 5       5  65.2 M         9.9    10.7     11.7    0.800\n 6       6  76   M         5.2     6.8      4.2    1.6  \n 7       7  72   M        10.4    14.6     14.1    4.2  \n 8       8  69   F        10.5     7.2      5.4   -3.3  \n 9       9  66   M         5       5        5.1    0    \n10      10  62   M         8.6     8.1      7.4   -0.5  \n11      11  65   F        16.6    14.9     13     -1.70 \n12      12  71   M        16.4    18.6     17.1    2.20 \n13      13  51   F        12.2    11       12.3   -1.2  \n14      14  71   M         6.6     3.7      4.5   -2.9  \n15      15  64   F        15.4    15.2     13.6   -0.200\n16      16  50   M        10.2    10.8     11.2    0.600\n\n\nMore on simple imputation and missing data is coming soon.\nFor now, we can rerun the ANCOVA model on this new data set, after imputation…\n\nanova(lm(st_delta ~ sex + age, data = emph_imp))\n\nAnalysis of Variance Table\n\nResponse: st_delta\n          Df  Sum Sq Mean Sq F value  Pr(>F)  \nsex        1  52.547  52.547  5.3623 0.03755 *\nage        1   2.151   2.151  0.2195 0.64721  \nResiduals 13 127.392   9.799                  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nWhen we do this, we see that now the sex variable returns to a p value below 0.05. Our complete case analysis (which omitted patient 5) gives us a different result than the ANCOVA based on the data after mean imputation."
  },
  {
    "objectID": "ancova_emphysema.html#looking-at-a-factor-covariate-interaction",
    "href": "ancova_emphysema.html#looking-at-a-factor-covariate-interaction",
    "title": "11  Analysis of Covariance",
    "section": "11.7 Looking at a factor-covariate interaction",
    "text": "11.7 Looking at a factor-covariate interaction\nLet’s run a model including the interaction (product) term between age and sex, which implies that the slope of age on our outcome (st_delta) depends on the patient’s sex. We’ll use the imputed data again. Here is the new ANCOVA table, which suggests that the interaction of age and sex is small (because it accounts for only a small amount of the total Sum of Squares) with a p value of 0.91.\n\nanova(lm(st_delta ~ sex * age, data = emph_imp))\n\nAnalysis of Variance Table\n\nResponse: st_delta\n          Df  Sum Sq Mean Sq F value  Pr(>F)  \nsex        1  52.547  52.547  4.9549 0.04594 *\nage        1   2.151   2.151  0.2028 0.66051  \nsex:age    1   0.130   0.130  0.0123 0.91355  \nResiduals 12 127.261  10.605                  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nSince the interaction term isn’t accounting for much variation, we probably don’t need it here. But let’s look at its interpretation anyway, just to fix ideas. To do that, we’ll need the coefficients from the underlying regression model.\n\ntidy(lm(st_delta ~ sex * age, data = emph_imp))\n\n# A tibble: 4 × 5\n  term        estimate std.error statistic p.value\n  <chr>          <dbl>     <dbl>     <dbl>   <dbl>\n1 (Intercept)  -5.65      13.5      -0.420   0.682\n2 sexM          1.72      16.8       0.102   0.920\n3 age           0.0365     0.211     0.173   0.866\n4 sexM:age      0.0289     0.260     0.111   0.914\n\n\nOur ANCOVA model for st_delta incorporating the age x sex product term is -5.65 + 1.72 (sex = M) + 0.037 age + 0.029 (sex = M)(age). So that means:\n\nour model for females is st_delta = -5.65 + 0.037 age\nour model for males is st_delta = (-5.65 + 1.72) + (0.037 + 0.029) age, or -3.93 + 0.066 age\n\nbut, again, our conclusion from the ANCOVA table is that this increase in complexity (letting both the slope and intercept vary by sex) doesn’t add much in the way of predictive value for our st_delta outcome."
  },
  {
    "objectID": "ancova_emphysema.html#centering-the-covariate-to-facilitate-ancova-interpretation",
    "href": "ancova_emphysema.html#centering-the-covariate-to-facilitate-ancova-interpretation",
    "title": "11  Analysis of Covariance",
    "section": "11.8 Centering the Covariate to Facilitate ANCOVA Interpretation",
    "text": "11.8 Centering the Covariate to Facilitate ANCOVA Interpretation\nWhen developing an ANCOVA model, we will often center or even center and rescale the covariate to facilitate interpretation of the product term. In this case, let’s center age and rescale it by dividing by two standard deviations.\n\nfavstats(~ age, data = emph_imp)\n\n min   Q1 median    Q3 max mean       sd  n missing\n  50 63.5   65.1 70.25  76 65.2 6.978061 16       0\n\n\nNote that in our imputed data, the mean age is 65.2 and the standard deviation of age is 7 years.\nSo we build the rescaled age variable that I’ll call age_z, and then use it to refit our model.\n\nemph_imp <- emph_imp |>\n    mutate(age_z = (age - mean(age))/ (2 * sd(age)))\n\nanova(lm(st_delta ~ sex * age_z, data = emph_imp))\n\nAnalysis of Variance Table\n\nResponse: st_delta\n          Df  Sum Sq Mean Sq F value  Pr(>F)  \nsex        1  52.547  52.547  4.9549 0.04594 *\nage_z      1   2.151   2.151  0.2028 0.66051  \nsex:age_z  1   0.130   0.130  0.0123 0.91355  \nResiduals 12 127.261  10.605                  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\ntidy(lm(st_delta ~ sex * age_z, data = emph_imp))\n\n# A tibble: 4 × 5\n  term        estimate std.error statistic p.value\n  <chr>          <dbl>     <dbl>     <dbl>   <dbl>\n1 (Intercept)   -3.27       1.39    -2.35   0.0364\n2 sexM           3.60       1.74     2.08   0.0601\n3 age_z          0.510      2.95     0.173  0.866 \n4 sexM:age_z     0.403      3.63     0.111  0.914 \n\n\nComparing the two models, we have:\n\n(unscaled): st_delta = -5.65 + 1.72 (sex = M) + 0.037 age + 0.029 (sex = M) x (age)\n(rescaled): st_delta = -3.27 + 3.60 (sex = M) + 0.510 rescaled age_z + 0.402 (sex = M) x (rescaled age_z)\n\nIn essence, the rescaled model on age_z is:\n\nst_delta = -3.27 + 0.510 age_z for female subjects, and\nst_delta = (-3.27 + 3.60) + (0.510 + 0.402) age_z = 0.33 + 0.912 age_z for male subjects\n\nInterpreting the centered, rescaled model, we have:\n\nno change in the ANOVA results or R-squared or residual standard deviation compared to the uncentered, unscaled model, but\nthe intercept (-3.27) now represents the st_delta for a female of average age,\nthe sex slope (3.60) represents the (male - female) difference in predicted st_delta for a person of average age,\nthe age_z slope (0.510) represents the difference in predicted st_delta for a female one standard deviation older than the mean age as compared to a female one standard deviation younger than the mean age, and\nthe product term’s slope (0.402) represents the male - female difference in the slope of age_z, so that if you add the age_z slope (0.510) and the interaction slope (0.402) you see the difference in predicted st_delta for a male one standard deviation older than the mean age as compared to a male one standard deviation younger than the mean age.\n\n\n\n\n\nRiffenburgh, Robert H. 2006. Statistics in Medicine. Second Edition. Burlington, MA: Elsevier Academic Press."
  },
  {
    "objectID": "ancova_smart.html#r-setup-used-here",
    "href": "ancova_smart.html#r-setup-used-here",
    "title": "12  Analysis of Covariance with the SMART data",
    "section": "12.1 R Setup Used Here",
    "text": "12.1 R Setup Used Here\n\nknitr::opts_chunk$set(comment = NA)\n\nlibrary(janitor)\nlibrary(broom)\nlibrary(mosaic)\nlibrary(tidyverse) \n\ntheme_set(theme_bw())\n\n\n12.1.1 Data Load\n\nsmart_cle1_sh <- readRDS(\"data/smart_cle1_sh.Rds\")"
  },
  {
    "objectID": "ancova_smart.html#a-new-small-study-predicting-bmi",
    "href": "ancova_smart.html#a-new-small-study-predicting-bmi",
    "title": "12  Analysis of Covariance with the SMART data",
    "section": "12.2 A New Small Study: Predicting BMI",
    "text": "12.2 A New Small Study: Predicting BMI\nWe’ll begin by investigating the problem of predicting bmi, at first with just three regression inputs: smoke100, female and physhealth, in our smart_cle1_sh data set.\n\nThe outcome of interest is bmi.\nInputs to the regression model are:\n\nsmoke100 = 1 if the subject has smoked 100 cigarettes in their lifetime\nfemale = 1 if the subject is female, and 0 if they are male\nphyshealth = number of poor physical health days in past 30 (treated as quantitative)\n\n\n\n12.2.1 Does smoke100 predict bmi well?\n\n12.2.1.1 Graphical Assessment\n\nggplot(smart_cle1_sh, aes(x = smoke100, y = bmi)) +\n    geom_point()\n\n\n\n\nNot so helpful. We should probably specify that smoke100 is a factor, and try another plotting approach.\n\nggplot(smart_cle1_sh, aes(x = factor(smoke100), y = bmi)) +\n    geom_boxplot()\n\n\n\n\nThe median BMI looks a little higher for those who have smoked 100 cigarettes. Let’s see if a model reflects that."
  },
  {
    "objectID": "ancova_smart.html#mod1-a-simple-t-test-model",
    "href": "ancova_smart.html#mod1-a-simple-t-test-model",
    "title": "12  Analysis of Covariance with the SMART data",
    "section": "12.3 mod1: A simple t-test model",
    "text": "12.3 mod1: A simple t-test model\n\nmod1 <- lm(bmi ~ smoke100, data = smart_cle1_sh)\nmod1\n\n\nCall:\nlm(formula = bmi ~ smoke100, data = smart_cle1_sh)\n\nCoefficients:\n(Intercept)     smoke100  \n    27.9390       0.8722  \n\nsummary(mod1)\n\n\nCall:\nlm(formula = bmi ~ smoke100, data = smart_cle1_sh)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-15.511  -4.019  -0.870   2.841  41.749 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  27.9390     0.2594 107.722   <2e-16 ***\nsmoke100      0.8722     0.3781   2.307   0.0213 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 6.353 on 1131 degrees of freedom\nMultiple R-squared:  0.004682,  Adjusted R-squared:  0.003802 \nF-statistic: 5.321 on 1 and 1131 DF,  p-value: 0.02125\n\nconfint(mod1)\n\n                 2.5 %    97.5 %\n(Intercept) 27.4301016 28.447875\nsmoke100     0.1303021  1.614195\n\ntidy(mod1, conf.int = TRUE, conf.level = 0.95)\n\n# A tibble: 2 × 7\n  term        estimate std.error statistic p.value conf.low conf.high\n  <chr>          <dbl>     <dbl>     <dbl>   <dbl>    <dbl>     <dbl>\n1 (Intercept)   27.9       0.259    108.    0        27.4       28.4 \n2 smoke100       0.872     0.378      2.31  0.0213    0.130      1.61\n\nglance(mod1)\n\n# A tibble: 1 × 12\n  r.squ…¹ adj.r…² sigma stati…³ p.value    df logLik   AIC   BIC devia…⁴ df.re…⁵\n    <dbl>   <dbl> <dbl>   <dbl>   <dbl> <dbl>  <dbl> <dbl> <dbl>   <dbl>   <int>\n1 0.00468 0.00380  6.35    5.32  0.0213     1 -3702. 7409. 7424.  45649.    1131\n# … with 1 more variable: nobs <int>, and abbreviated variable names\n#   ¹​r.squared, ²​adj.r.squared, ³​statistic, ⁴​deviance, ⁵​df.residual\n\n\nThe model suggests, based on these 1133 subjects, that\n\nour best prediction for non-smokers is BMI = 27.94 kg/m2, and\nour best prediction for those who have smoked 100 cigarettes is BMI = 27.94 + 0.87 = 28.81 kg/m2.\nthe mean difference between smokers and non-smokers is +0.87 kg/m2 in BMI\na 95% confidence (uncertainty) interval for that mean smoker - non-smoker difference in BMI ranges from 0.13 to 1.61\nthe model accounts for 0.47% of the variation in BMI, so that knowing the respondent’s tobacco status does very little to reduce the size of the prediction errors as compared to an intercept only model that would predict the overall mean BMI for each of our subjects.\nthe model makes some enormous errors, with one subject being predicted to have a BMI 42 points lower than his/her actual BMI.\n\nNote that this simple regression model just gives us the t-test.\n\nt.test(bmi ~ smoke100, var.equal = TRUE, data = smart_cle1_sh)\n\n\n    Two Sample t-test\n\ndata:  bmi by smoke100\nt = -2.3066, df = 1131, p-value = 0.02125\nalternative hypothesis: true difference in means between group 0 and group 1 is not equal to 0\n95 percent confidence interval:\n -1.6141946 -0.1303021\nsample estimates:\nmean in group 0 mean in group 1 \n       27.93899        28.81124"
  },
  {
    "objectID": "ancova_smart.html#mod2-adding-another-predictor-two-way-anova-without-interaction",
    "href": "ancova_smart.html#mod2-adding-another-predictor-two-way-anova-without-interaction",
    "title": "12  Analysis of Covariance with the SMART data",
    "section": "12.4 mod2: Adding another predictor (two-way ANOVA without interaction)",
    "text": "12.4 mod2: Adding another predictor (two-way ANOVA without interaction)\nWhen we add in the information about female (sex) to our original model, we might first picture the data. We could look at separate histograms,\n\nggplot(smart_cle1_sh, aes(x = bmi)) +\n    geom_histogram(bins = 30) +\n    facet_grid(female ~ smoke100, labeller = label_both)\n\n\n\n\nor maybe boxplots?\n\nggplot(smart_cle1_sh, aes(x = factor(female), y = bmi)) +\n    geom_boxplot() +\n    facet_wrap(~ smoke100, labeller = label_both)\n\n\n\n\n\nggplot(smart_cle1_sh, aes(x = female, y = bmi))+\n    geom_point(size = 3, alpha = 0.2) +\n    theme_bw() +\n    facet_wrap(~ smoke100, labeller = label_both)\n\n\n\n\nOK. Let’s try fitting a model.\n\nmod2 <- lm(bmi ~ female + smoke100, data = smart_cle1_sh)\nmod2\n\n\nCall:\nlm(formula = bmi ~ female + smoke100, data = smart_cle1_sh)\n\nCoefficients:\n(Intercept)       female     smoke100  \n    28.0265      -0.1342       0.8555  \n\n\nThis new model predicts only four predicted values:\n\nbmi = 28.0265 if the subject is male and has not smoked 100 cigarettes (so female = 0 and smoke100 = 0)\nbmi = 28.0265 - 0.1342 = 27.8923 if the subject is female and has not smoked 100 cigarettes (female = 1 and smoke100 = 0)\nbmi = 28.0265 + 0.8555 = 28.8820 if the subject is male and has smoked 100 cigarettes (so female = 0 and smoke100 = 1), and, finally\nbmi = 28.0265 - 0.1342 + 0.8555 = 28.7478 if the subject is female and has smoked 100 cigarettes (so both female and smoke100 = 1).\n\nAnother way to put this is that for those who have not smoked 100 cigarettes, the model is:\n\nbmi = 28.0265 - 0.1342 female\n\nand for those who have smoked 100 cigarettes, the model is:\n\nbmi = 28.8820 - 0.1342 female\n\nOnly the intercept of the bmi-female model changes depending on smoke100.\n\nsummary(mod2)\n\n\nCall:\nlm(formula = bmi ~ female + smoke100, data = smart_cle1_sh)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-15.448  -3.972  -0.823   2.774  41.678 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  28.0265     0.3620  77.411   <2e-16 ***\nfemale       -0.1342     0.3875  -0.346   0.7291    \nsmoke100      0.8555     0.3814   2.243   0.0251 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 6.356 on 1130 degrees of freedom\nMultiple R-squared:  0.004788,  Adjusted R-squared:  0.003027 \nF-statistic: 2.718 on 2 and 1130 DF,  p-value: 0.06642\n\nconfint(mod2)\n\n                 2.5 %     97.5 %\n(Intercept) 27.3161140 28.7368281\nfemale      -0.8944773  0.6259881\nsmoke100     0.1072974  1.6037825\n\n\nThe slopes of both female and smoke100 have confidence intervals that are completely below zero, indicating that both female sex and smoke100 appear to be associated with reductions in bmi.\nThe \\(R^2\\) value suggests that 0.4788% of the variation in bmi is accounted for by this ANOVA model.\nIn fact, this regression (on two binary indicator variables) is simply a two-way ANOVA model without an interaction term.\n\nanova(mod2)\n\nAnalysis of Variance Table\n\nResponse: bmi\n            Df Sum Sq Mean Sq F value  Pr(>F)  \nfemale       1     16  16.301  0.4036 0.52538  \nsmoke100     1    203 203.296  5.0330 0.02506 *\nResiduals 1130  45644  40.393                  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1"
  },
  {
    "objectID": "ancova_smart.html#mod3-adding-the-interaction-term-two-way-anova-with-interaction",
    "href": "ancova_smart.html#mod3-adding-the-interaction-term-two-way-anova-with-interaction",
    "title": "12  Analysis of Covariance with the SMART data",
    "section": "12.5 mod3: Adding the interaction term (Two-way ANOVA with interaction)",
    "text": "12.5 mod3: Adding the interaction term (Two-way ANOVA with interaction)\nSuppose we want to let the effect of female vary depending on the smoke100 status. Then we need to incorporate an interaction term in our model.\n\nmod3 <- lm(bmi ~ female * smoke100, data = smart_cle1_sh)\nmod3\n\n\nCall:\nlm(formula = bmi ~ female * smoke100, data = smart_cle1_sh)\n\nCoefficients:\n    (Intercept)           female         smoke100  female:smoke100  \n        28.2690          -0.5064           0.4119           0.7536  \n\n\nSo, for example, for a male who has smoked 100 cigarettes, this model predicts\n\nbmi = 28.269 - 0.506 (0) + 0.412 (1) + 0.754 (0)(1) = 28.269 + 0.412 = 28.681\n\nAnd for a female who has smoked 100 cigarettes, the model predicts\n\nbmi = 28.269 - 0.506 (1) + 0.412 (1) + 0.754 (1)(1) = 28.269 - 0.506 + 0.412 + 0.754 = 28.929\n\nFor those who have not smoked 100 cigarettes, the model is:\n\nbmi = 28.269 - 0.506 female\n\nBut for those who have smoked 100 cigarettes, the model is:\n\nbmi = (28.269 + 0.412) + (-0.506 + 0.754) female, or ,,,\nbmi = 28.681 - 0.248 female\n\nNow, both the slope and the intercept of the bmi-female model change depending on smoke100.\n\nsummary(mod3)\n\n\nCall:\nlm(formula = bmi ~ female * smoke100, data = smart_cle1_sh)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-15.628  -3.938  -0.829   2.759  41.879 \n\nCoefficients:\n                Estimate Std. Error t value Pr(>|t|)    \n(Intercept)      28.2690     0.4396  64.301   <2e-16 ***\nfemale           -0.5064     0.5446  -0.930    0.353    \nsmoke100          0.4119     0.5946   0.693    0.489    \nfemale:smoke100   0.7536     0.7750   0.972    0.331    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 6.356 on 1129 degrees of freedom\nMultiple R-squared:  0.005621,  Adjusted R-squared:  0.002979 \nF-statistic: 2.127 on 3 and 1129 DF,  p-value: 0.09507\n\nconfint(mod3)\n\n                     2.5 %     97.5 %\n(Intercept)     27.4063783 29.1315563\nfemale          -1.5749026  0.5621793\nsmoke100        -0.7547605  1.5786121\nfemale:smoke100 -0.7670239  2.2742178\n\n\nIn fact, this regression (on two binary indicator variables and a product term) is simply a two-way ANOVA model with an interaction term.\n\nanova(mod3)\n\nAnalysis of Variance Table\n\nResponse: bmi\n                  Df Sum Sq Mean Sq F value  Pr(>F)  \nfemale             1     16  16.301  0.4035 0.52539  \nsmoke100           1    203 203.296  5.0327 0.02507 *\nfemale:smoke100    1     38  38.194  0.9455 0.33107  \nResiduals       1129  45606  40.395                  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nThe interaction term doesn’t change very much here. Its uncertainty interval includes zero, and the overall model still accounts for less than 1% of the variation in bmi."
  },
  {
    "objectID": "ancova_smart.html#mod4-using-smoke100-and-physhealth-in-a-model-for-bmi",
    "href": "ancova_smart.html#mod4-using-smoke100-and-physhealth-in-a-model-for-bmi",
    "title": "12  Analysis of Covariance with the SMART data",
    "section": "12.6 mod4: Using smoke100 and physhealth in a model for bmi",
    "text": "12.6 mod4: Using smoke100 and physhealth in a model for bmi\n\nggplot(smart_cle1_sh, aes(x = physhealth, y = bmi, color = factor(smoke100))) +\n    geom_point() + \n    guides(col = \"none\") +\n    geom_smooth(method = \"lm\", se = FALSE) +\n    facet_wrap(~ smoke100, labeller = label_both) \n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\nDoes the difference in slopes of bmi and physhealth for those who have and haven’t smoked at least 100 cigarettes appear to be substantial and important?\n\nmod4 <- lm(bmi ~ smoke100 * physhealth, data = smart_cle1_sh)\n\nsummary(mod4)\n\n\nCall:\nlm(formula = bmi ~ smoke100 * physhealth, data = smart_cle1_sh)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-19.011  -3.811  -0.559   2.609  38.249 \n\nCoefficients:\n                    Estimate Std. Error t value Pr(>|t|)    \n(Intercept)         27.49077    0.28369  96.904  < 2e-16 ***\nsmoke100             0.57017    0.41911   1.360 0.173965    \nphyshealth           0.10840    0.02995   3.619 0.000308 ***\nsmoke100:physhealth  0.03326    0.04093   0.813 0.416561    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 6.252 on 1129 degrees of freedom\nMultiple R-squared:  0.03782,   Adjusted R-squared:  0.03526 \nF-statistic: 14.79 on 3 and 1129 DF,  p-value: 1.886e-09\n\n\nDoes it seem as though the addition of physhealth has improved our model substantially over a model with smoke100 alone (which, you recall, was mod1)?\nSince the mod4 model contains the mod1 model’s predictors as a subset and the outcome is the same for each model, we consider the models nested and have some extra tools available to compare them.\n\nI might start by looking at the basic summaries for each model.\n\n\nglance(mod4)\n\n# A tibble: 1 × 12\n  r.squ…¹ adj.r…² sigma stati…³ p.value    df logLik   AIC   BIC devia…⁴ df.re…⁵\n    <dbl>   <dbl> <dbl>   <dbl>   <dbl> <dbl>  <dbl> <dbl> <dbl>   <dbl>   <int>\n1  0.0378  0.0353  6.25    14.8 1.89e-9     3 -3682. 7375. 7400.  44129.    1129\n# … with 1 more variable: nobs <int>, and abbreviated variable names\n#   ¹​r.squared, ²​adj.r.squared, ³​statistic, ⁴​deviance, ⁵​df.residual\n\n\n\nglance(mod1)\n\n# A tibble: 1 × 12\n  r.squ…¹ adj.r…² sigma stati…³ p.value    df logLik   AIC   BIC devia…⁴ df.re…⁵\n    <dbl>   <dbl> <dbl>   <dbl>   <dbl> <dbl>  <dbl> <dbl> <dbl>   <dbl>   <int>\n1 0.00468 0.00380  6.35    5.32  0.0213     1 -3702. 7409. 7424.  45649.    1131\n# … with 1 more variable: nobs <int>, and abbreviated variable names\n#   ¹​r.squared, ²​adj.r.squared, ³​statistic, ⁴​deviance, ⁵​df.residual\n\n\n\nThe \\(R^2\\) is much larger for the model with physhealth, but still very tiny.\nSmaller AIC and smaller BIC statistics are more desirable. Here, there’s little to choose from, so mod4 looks better, too.\nWe might also consider a significance test by looking at an ANOVA model comparison. This is only appropriate because mod1 is nested in mod4.\n\n\nanova(mod4, mod1)\n\nAnalysis of Variance Table\n\nModel 1: bmi ~ smoke100 * physhealth\nModel 2: bmi ~ smoke100\n  Res.Df   RSS Df Sum of Sq     F    Pr(>F)    \n1   1129 44129                                 \n2   1131 45649 -2   -1519.7 19.44 5.005e-09 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nThe addition of the physhealth term appears to be an improvement, not that that means very much."
  },
  {
    "objectID": "ancova_smart.html#making-predictions-with-a-linear-regression-model",
    "href": "ancova_smart.html#making-predictions-with-a-linear-regression-model",
    "title": "12  Analysis of Covariance with the SMART data",
    "section": "12.7 Making Predictions with a Linear Regression Model",
    "text": "12.7 Making Predictions with a Linear Regression Model\nRecall model 4, which yields predictions for body mass index on the basis of the main effects of having smoked (smoke100) and days of poor physical health (physhealth) and their interaction.\n\nmod4\n\n\nCall:\nlm(formula = bmi ~ smoke100 * physhealth, data = smart_cle1_sh)\n\nCoefficients:\n        (Intercept)             smoke100           physhealth  \n           27.49077              0.57017              0.10840  \nsmoke100:physhealth  \n            0.03326  \n\n\n\n12.7.1 Fitting an Individual Prediction and 95% Prediction Interval\nWhat do we predict for the bmi of a subject who has smoked at least 100 cigarettes in their life and had 8 poor physical health days in the past 30?\n\nnew1 <- tibble(smoke100 = 1, physhealth = 8)\npredict(mod4, newdata = new1, interval = \"prediction\", level = 0.95)\n\n       fit      lwr      upr\n1 29.19423 16.91508 41.47338\n\n\nThe predicted bmi for this new subject is shown above. The prediction interval shows the bounds of a 95% uncertainty interval for a predicted bmi for an individual smoker1 who has 8 days of poor physical health out of the past 30. From the predict function applied to a linear model, we can get the prediction intervals for any new data points in this manner.\n\n\n12.7.2 Confidence Interval for an Average Prediction\n\nWhat do we predict for the average body mass index of a population of subjects who are smokers and have physhealth = 8?\n\n\npredict(mod4, newdata = new1, interval = \"confidence\", level = 0.95)\n\n       fit      lwr      upr\n1 29.19423 28.64267 29.74579\n\n\n\nHow does this result compare to the prediction interval?\n\n\n\n12.7.3 Fitting Multiple Individual Predictions to New Data\n\nHow does our prediction change for a respondent if they instead have 7, or 9 poor physical health days? What if they have or don’t have a history of smoking?\n\n\nc8_new2 <- tibble(subjectid = 1001:1006, smoke100 = c(1, 1, 1, 0, 0, 0), physhealth = c(7, 8, 9, 7, 8, 9))\npred2 <- predict(mod4, newdata = c8_new2, interval = \"prediction\", level = 0.95) |> as_tibble()\n\nresult2 <- bind_cols(c8_new2, pred2)\nresult2\n\n# A tibble: 6 × 6\n  subjectid smoke100 physhealth   fit   lwr   upr\n      <int>    <dbl>      <dbl> <dbl> <dbl> <dbl>\n1      1001        1          7  29.1  16.8  41.3\n2      1002        1          8  29.2  16.9  41.5\n3      1003        1          9  29.3  17.1  41.6\n4      1004        0          7  28.2  16.0  40.5\n5      1005        0          8  28.4  16.1  40.6\n6      1006        0          9  28.5  16.2  40.7\n\n\nThe result2 tibble contains predictions for each scenario.\n\nWhich has a bigger impact on these predictions and prediction intervals? A one category change in smoke100 or a one hour change in physhealth?"
  },
  {
    "objectID": "ancova_smart.html#centering-the-model",
    "href": "ancova_smart.html#centering-the-model",
    "title": "12  Analysis of Covariance with the SMART data",
    "section": "12.8 Centering the model",
    "text": "12.8 Centering the model\nOur model mod4 has four predictors (the constant, physhealth, smoke100 and their interaction) but just two inputs (smoke100 and physhealth.) If we center the quantitative input physhealth before building the model, we get a more interpretable interaction term.\n\nsmart_cle1_sh_c <- smart_cle1_sh |>\n    mutate(physhealth_c = physhealth - mean(physhealth))\n\nmod4_c <- lm(bmi ~ smoke100 * physhealth_c, data = smart_cle1_sh_c)\n\nsummary(mod4_c)\n\n\nCall:\nlm(formula = bmi ~ smoke100 * physhealth_c, data = smart_cle1_sh_c)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-19.011  -3.811  -0.559   2.609  38.249 \n\nCoefficients:\n                      Estimate Std. Error t value Pr(>|t|)    \n(Intercept)           27.99821    0.25576 109.471  < 2e-16 ***\nsmoke100               0.72589    0.37288   1.947 0.051818 .  \nphyshealth_c           0.10840    0.02995   3.619 0.000308 ***\nsmoke100:physhealth_c  0.03326    0.04093   0.813 0.416561    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 6.252 on 1129 degrees of freedom\nMultiple R-squared:  0.03782,   Adjusted R-squared:  0.03526 \nF-statistic: 14.79 on 3 and 1129 DF,  p-value: 1.886e-09\n\n\nWhat has changed as compared to the original mod4?\n\nOur original model was bmi = 27.5 + 0.57 smoke100 + 0.11 physhealth - 0.03 smoke100 x physhealth\nOur new model is bmi = 28.0 + 0.73 smoke100 + 0.11 centered physhealth + 0.03 smoke100 x centered physhealth.\n\nSo our new model on centered data is:\n\n28.0 + 0.11 centered physhealth_c for non-smokers, and\n(28.0 + 0.73) + (0.11 - 0.03) centered physhealth_c, or 28.73 + 0.08 centered physhealth_c for smokers.\n\nIn our new (centered physhealth_c) model,\n\nthe main effect of smoke100 now corresponds to a predictive difference (smoker - non-smoker) in bmi with physhealth at its mean value, 4.68 days,\nthe intercept term is now the predicted bmi for a non-smoker with an average physhealth, and\nthe product term corresponds to the change in the slope of centered physhealth_c on bmi for a smoker rather than a non-smoker, while\nthe residual standard deviation and the R-squared values remain unchanged from the model before centering.\n\n\n12.8.1 Plot of Model 4 on Centered physhealth: mod4_c\n\nggplot(smart_cle1_sh_c, aes(x = physhealth_c, y = bmi, group = smoke100, col = factor(smoke100))) +\n    geom_point(alpha = 0.5, size = 2) +\n    geom_smooth(method = \"lm\", se = FALSE, formula = y ~ x) +\n    guides(color = \"none\") +\n    labs(x = \"Poor Physical Health Days, centered\", y = \"Body Mass Index\",\n         title = \"Model `mod4` on centered data\") +\n    facet_wrap(~ smoke100, labeller = label_both)"
  },
  {
    "objectID": "ancova_smart.html#rescaling-an-input-by-subtracting-the-mean-and-dividing-by-2-standard-deviations",
    "href": "ancova_smart.html#rescaling-an-input-by-subtracting-the-mean-and-dividing-by-2-standard-deviations",
    "title": "12  Analysis of Covariance with the SMART data",
    "section": "12.9 Rescaling an input by subtracting the mean and dividing by 2 standard deviations",
    "text": "12.9 Rescaling an input by subtracting the mean and dividing by 2 standard deviations\nCentering helped us interpret the main effects in the regression, but it still leaves a scaling problem.\n\nThe smoke100 coefficient estimate is much larger than that of physhealth, but this is misleading, considering that we are comparing the complete change in one variable (smoking or not) to a 1-day change in physhealth.\nGelman and Hill (2007) recommend all continuous predictors be scaled by dividing by 2 standard deviations, so that:\n\na 1-unit change in the rescaled predictor corresponds to a change from 1 standard deviation below the mean, to 1 standard deviation above.\nan unscaled binary (1/0) predictor with 50% probability of occurring will be exactly comparable to a rescaled continuous predictor done in this way.\n\n\n\nsmart_cle1_sh_rescale <- smart_cle1_sh |>\n    mutate(physhealth_z = (physhealth - mean(physhealth))/(2*sd(physhealth)))\n\n\n12.9.1 Refitting model mod4 to the rescaled data\n\nmod4_z <- lm(bmi ~ smoke100 * physhealth_z, data = smart_cle1_sh_rescale)\n\nsummary(mod4_z)\n\n\nCall:\nlm(formula = bmi ~ smoke100 * physhealth_z, data = smart_cle1_sh_rescale)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-19.011  -3.811  -0.559   2.609  38.249 \n\nCoefficients:\n                      Estimate Std. Error t value Pr(>|t|)    \n(Intercept)            27.9982     0.2558 109.471  < 2e-16 ***\nsmoke100                0.7259     0.3729   1.947 0.051818 .  \nphyshealth_z            1.9774     0.5463   3.619 0.000308 ***\nsmoke100:physhealth_z   0.6068     0.7467   0.813 0.416561    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 6.252 on 1129 degrees of freedom\nMultiple R-squared:  0.03782,   Adjusted R-squared:  0.03526 \nF-statistic: 14.79 on 3 and 1129 DF,  p-value: 1.886e-09\n\n\n\n\n12.9.2 Interpreting the model on rescaled data\nWhat has changed as compared to the original mod4?\n\nOur original model was bmi = 27.5 + 0.57 smoke100 + 0.11 physhealth + 0.03 smoke100 x physhealth\nOur model on centered physhealth was bmi = 28.0 + 0.73 smoke100 + 0.11 centered physhealth + 0.03 smoke100 x centered physhealth.\nOur new model on rescaled physhealth is bmi = 28.0 + 0.73 smoke100 + 1.98 rescaled physhealth + 0.61 smoke100 x rescaled physhealth\n\nSo our rescaled model is:\n\n28.0 + 1.98 rescaled physhealth_z for non-smokers, and\n(28.0 + 0.73) + (1.98 + 0.61) rescaled physhealth_z, or 28.73 + 2.59 rescaled physhealth_z for smokers.\n\nIn this new rescaled (physhealth_z) model, then,\n\nthe main effect of smoke100, 0.73, still corresponds to a predictive difference (smoker - non-smoker) in bmi with physhealth at its mean value, 4.68 days,\nthe intercept term is still the predicted bmi for a non-smoker with an average physhealth count, and\nthe residual standard deviation and the R-squared values remain unchanged,\n\nas before, but now we also have that:\n\nthe coefficient of physhealth_z indicates the predictive difference in bmi associated with a change in physhealth of 2 standard deviations (from one standard deviation below the mean of 4.68 to one standard deviation above 4.68.)\n\nSince the standard deviation of physhealth is 9.12 (see below), this covers a massive range of potential values of physhealth from 0 all the way up to 4.68 + 2(9.12) = 22.92 days.\n\n\n\nfavstats(~ physhealth, data = smart_cle1_sh)\n\n min Q1 median Q3 max     mean       sd    n missing\n   0  0      0  4  30 4.681377 9.120899 1133       0\n\n\n\nthe coefficient of the product term (0.61) corresponds to the change in the coefficient of physhealth_z for smokers as compared to non-smokers.\n\n\n\n12.9.3 Plot of model on rescaled data\n\nggplot(smart_cle1_sh_rescale, aes(x = physhealth_z, y = bmi, \n                              group = smoke100, col = factor(smoke100))) +\n    geom_point(alpha = 0.5) +\n    geom_smooth(method = \"lm\", se = FALSE, size = 1.5) +\n    scale_color_discrete(name = \"Is subject a smoker?\") +\n    labs(x = \"Poor Physical Health Days, standardized (2 sd)\", y = \"Body Mass Index\",\n         title = \"Model `mod4_z` on rescaled data\") \n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\nThere’s very little difference here."
  },
  {
    "objectID": "ancova_smart.html#mod5-what-if-we-add-more-variables",
    "href": "ancova_smart.html#mod5-what-if-we-add-more-variables",
    "title": "12  Analysis of Covariance with the SMART data",
    "section": "12.10 mod5: What if we add more variables?",
    "text": "12.10 mod5: What if we add more variables?\nWe can boost our \\(R^2\\) a bit, to nearly 5%, by adding in two new variables, related to whether or not the subject (in the past 30 days) used the internet, and the average number of alcoholic drinks per week consumed by ths subject.\n\nmod5 <- lm(bmi ~ smoke100 + female +physhealth + internet30 + drinks_wk,\n         data = smart_cle1_sh)\nsummary(mod5)\n\n\nCall:\nlm(formula = bmi ~ smoke100 + female + physhealth + internet30 + \n    drinks_wk, data = smart_cle1_sh)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-18.358  -3.846  -0.657   2.534  38.049 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 27.52400    0.56076  49.083  < 2e-16 ***\nsmoke100     0.82654    0.37739   2.190  0.02872 *  \nfemale      -0.43272    0.38510  -1.124  0.26140    \nphyshealth   0.12469    0.02074   6.012 2.47e-09 ***\ninternet30   0.44287    0.48830   0.907  0.36462    \ndrinks_wk   -0.10193    0.03352  -3.041  0.00241 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 6.231 on 1127 degrees of freedom\nMultiple R-squared:  0.04582,   Adjusted R-squared:  0.04159 \nF-statistic: 10.82 on 5 and 1127 DF,  p-value: 3.48e-10\n\n\n\nHere’s the ANOVA for this model. What can we study with this?\n\n\nanova(mod5)\n\nAnalysis of Variance Table\n\nResponse: bmi\n             Df Sum Sq Mean Sq F value    Pr(>F)    \nsmoke100      1    215  214.75  5.5304  0.018861 *  \nfemale        1      5    4.85  0.1249  0.723878    \nphyshealth    1   1508 1508.08 38.8372 6.497e-10 ***\ninternet30    1     15   14.69  0.3783  0.538650    \ndrinks_wk     1    359  359.05  9.2466  0.002414 ** \nResiduals  1127  43762   38.83                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\nConsider the revised output below. Now what can we study?\n\n\nanova(lm(bmi ~ smoke100 + internet30 + drinks_wk + female + physhealth,\n         data = smart_cle1_sh))\n\nAnalysis of Variance Table\n\nResponse: bmi\n             Df Sum Sq Mean Sq F value    Pr(>F)    \nsmoke100      1    215  214.75  5.5304 0.0188606 *  \ninternet30    1      8    7.81  0.2010 0.6539723    \ndrinks_wk     1    444  443.79 11.4288 0.0007479 ***\nfemale        1     32   31.58  0.8132 0.3673566    \nphyshealth    1   1403 1403.49 36.1438 2.472e-09 ***\nResiduals  1127  43762   38.83                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\nWhat does the output below let us conclude?\n\n\nanova(lm(bmi ~ smoke100 + internet30 + drinks_wk + female + physhealth, \n         data = smart_cle1_sh),\n      lm(bmi ~ smoke100 + female + drinks_wk, \n         data = smart_cle1_sh))\n\nAnalysis of Variance Table\n\nModel 1: bmi ~ smoke100 + internet30 + drinks_wk + female + physhealth\nModel 2: bmi ~ smoke100 + female + drinks_wk\n  Res.Df   RSS Df Sum of Sq      F    Pr(>F)    \n1   1127 43762                                  \n2   1129 45166 -2   -1403.7 18.075 1.877e-08 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\nWhat does it mean for the models to be “nested”?"
  },
  {
    "objectID": "ancova_smart.html#mod6-would-adding-self-reported-health-help",
    "href": "ancova_smart.html#mod6-would-adding-self-reported-health-help",
    "title": "12  Analysis of Covariance with the SMART data",
    "section": "12.11 mod6: Would adding self-reported health help?",
    "text": "12.11 mod6: Would adding self-reported health help?\nAnd we can do even a bit better than that by adding in a multi-categorical measure: self-reported general health.\n\nmod6 <- lm(bmi ~ female + smoke100 + physhealth + internet30 + drinks_wk + genhealth,\n         data = smart_cle1_sh)\nsummary(mod6)\n\n\nCall:\nlm(formula = bmi ~ female + smoke100 + physhealth + internet30 + \n    drinks_wk + genhealth, data = smart_cle1_sh)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-19.216  -3.659  -0.736   2.669  36.810 \n\nCoefficients:\n                    Estimate Std. Error t value Pr(>|t|)    \n(Intercept)         25.20736    0.71106  35.450  < 2e-16 ***\nfemale              -0.31949    0.37667  -0.848   0.3965    \nsmoke100             0.45866    0.37214   1.232   0.2180    \nphyshealth           0.04353    0.02506   1.737   0.0827 .  \ninternet30           0.93270    0.48273   1.932   0.0536 .  \ndrinks_wk           -0.07712    0.03294  -2.341   0.0194 *  \ngenhealth2_VeryGood  1.21169    0.56838   2.132   0.0332 *  \ngenhealth3_Good      3.22783    0.58009   5.564 3.29e-08 ***\ngenhealth4_Fair      4.14497    0.73284   5.656 1.96e-08 ***\ngenhealth5_Poor      5.86335    1.09253   5.367 9.73e-08 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 6.089 on 1123 degrees of freedom\nMultiple R-squared:  0.09206,   Adjusted R-squared:  0.08478 \nF-statistic: 12.65 on 9 and 1123 DF,  p-value: < 2.2e-16\n\n\n\nIf Harry and Marty have the same values of female, smoke100, physhealth, internet30 and drinks_wk, but Harry rates his health as Good, and Marty rates his as Fair, then what is the difference in the predictions? Who is predicted to have a larger BMI, and by how much?\nWhat does this normal probability plot of the residuals suggest?\n\n\nplot(mod6, which = 2)"
  },
  {
    "objectID": "ancova_smart.html#key-regression-assumptions-for-building-effective-prediction-models",
    "href": "ancova_smart.html#key-regression-assumptions-for-building-effective-prediction-models",
    "title": "12  Analysis of Covariance with the SMART data",
    "section": "12.12 Key Regression Assumptions for Building Effective Prediction Models",
    "text": "12.12 Key Regression Assumptions for Building Effective Prediction Models\n\nValidity - the data you are analyzing should map to the research question you are trying to answer.\n\nThe outcome should accurately reflect the phenomenon of interest.\nThe model should include all relevant predictors. (It can be difficult to decide which predictors are necessary, and what to do with predictors that have large standard errors.)\nThe model should generalize to all of the cases to which it will be applied.\nCan the available data answer our question reliably?\n\nAdditivity and linearity - most important assumption of a regression model is that its deterministic component is a linear function of the predictors. We often think about transformations in this setting.\nIndependence of errors - errors from the prediction line are independent of each other\nEqual variance of errors - if this is violated, we can more efficiently estimate parameters using weighted least squares approaches, where each point is weighted inversely proportional to its variance, but this doesn’t affect the coefficients much, if at all.\nNormality of errors - not generally important for estimating the regression line\n\n\n12.12.1 Checking Assumptions in model mod6\n\nHow does the assumption of linearity behind this model look?\n\n\nplot(mod6, which = 1)\n\n\n\n\nWe see no strong signs of serious non-linearity here. There’s no obvious curve in the plot, for example. We may have a problem with increasing variance as we move to the right.\n\nWhat can we conclude from the plot below?\n\n\nplot(mod6, which = 5)\n\n\n\n\nThis plot can help us identify points with large standardized residuals, large leverage values, and large influence on the model (as indicated by large values of Cook’s distance.) In this case, I see no signs of any points used in the model with especially large influence, although there are some poorly fitted points (with especially large standardized residuals.)\nWe might want to identify the point listed here as 961, which appears to have an enormous standardized residual. To do so, we can use the slice function from dplyr.\n\nsmart_cle1_sh |> slice(961) |> select(SEQNO)\n\n# A tibble: 1 × 1\n       SEQNO\n       <dbl>\n1 2017000961\n\n\nNow we know exactly which subject we’re talking about.\n\nWhat other residual plots are available with plot and how do we interpret them?\n\n\nplot(mod6, which = 2)\n\n\n\n\nThis plot is simply a Normal Q-Q plot of the standardized residuals from our model. We’re looking here for serious problems with the assumption of Normality.\n\nplot(mod6, which = 3)\n\n\n\n\nThis is a scale-location plot, designed to help us see non-constant variance in the residuals as we move across the fitted values as a linear trend, rather than as a fan shape, by plotting the square root of the residuals on the vertical axis.\n\nplot(mod6, which = 4)\n\n\n\n\nFinally, this is an index plot of the Cook’s distance values, allowing us to identify points that are particularly large. Remember that a value of 0.5 (or perhaps even 1.0) is a reasonable boundary for a substantially influential point.\n\n\n\n\nGelman, Andrew, and Jennifer Hill. 2007. Data Analysis Using Regression and Multilevel/Hierarchical Models. New York: Cambridge University Press."
  },
  {
    "objectID": "nonlinearity.html#r-setup-used-here",
    "href": "nonlinearity.html#r-setup-used-here",
    "title": "13  Adding Non-linear Terms to a Linear Regression",
    "section": "13.1 R Setup Used Here",
    "text": "13.1 R Setup Used Here\n\nknitr::opts_chunk$set(comment = NA)\n\nlibrary(janitor)\nlibrary(broom)\nlibrary(car)\nlibrary(Hmisc)\nlibrary(mosaic)\nlibrary(rms)\nlibrary(patchwork)\nlibrary(tidyverse) \n\ntheme_set(theme_bw())\n\n\n13.1.1 Data Load\n\npollution <- read_csv(\"data/pollution.csv\", show_col_types = FALSE)"
  },
  {
    "objectID": "nonlinearity.html#the-pollution-data",
    "href": "nonlinearity.html#the-pollution-data",
    "title": "13  Adding Non-linear Terms to a Linear Regression",
    "section": "13.2 The pollution data",
    "text": "13.2 The pollution data\nConsider the pollution data set, which contain 15 independent variables and a measure of mortality, describing 60 US metropolitan areas in 1959-1961. The data come from McDonald and Schwing (1973), and are available at http://www4.stat.ncsu.edu/~boos/var.select/pollution.html and our web site.\n\npollution\n\n# A tibble: 60 × 16\n      x1    x2    x3    x4    x5    x6    x7    x8    x9   x10   x11   x12   x13\n   <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl>\n 1    13    49    68   7    3.36  12.2  90.7  2702   3    51.9   9.7   105    32\n 2    28    32    81   7    3.27  12.1  81    3665   7.5  51.6  13.2     4     2\n 3    10    55    70   7.3  3.11  12.1  88.9  3033   5.9  51    14     144    66\n 4    43    32    74  10.1  3.38   9.5  79.2  3214   2.9  43.7  12      11     7\n 5    25    12    73   9.2  3.28  12.1  83.1  2095   2    51.9   9.8    20    11\n 6    35    46    85   7.1  3.22  11.8  79.9  1441  14.8  51.2  16.1     1     1\n 7    60    67    82  10    2.98  11.5  88.6  4657  13.5  47.3  22.4     3     1\n 8    11    53    68   9.2  2.99  12.1  90.6  4700   7.8  48.9  12.3   648   319\n 9    31    24    72   9    3.37  10.9  82.8  3226   5.1  45.2  12.3     5     3\n10    15    30    73   8.2  3.15  12.2  84.2  4824   4.7  53.1  12.7    17     8\n# … with 50 more rows, and 3 more variables: x14 <dbl>, x15 <dbl>, y <dbl>\n\n\nHere’s a codebook:\n\n\n\n\n\n\n\nVariable\nDescription\n\n\n\n\ny\nTotal Age Adjusted Mortality Rate\n\n\nx1\nMean annual precipitation in inches\n\n\nx2\nMean January temperature in degrees Fahrenheit\n\n\nx3\nMean July temperature in degrees Fahrenheit\n\n\nx4\nPercent of 1960 SMSA population that is 65 years of age or over\n\n\nx5\nPopulation per household, 1960 SMSA\n\n\nx6\nMedian school years completed for those over 25 in 1960 SMSA\n\n\nx7\nPercent of housing units that are found with facilities\n\n\nx8\nPopulation per square mile in urbanized area in 1960\n\n\nx9\nPercent of 1960 urbanized area population that is non-white\n\n\nx10\nPercent employment in white-collar occupations in 1960 urbanized area\n\n\nx11\nPercent of families with income under 3; 000 in 1960 urbanized area\n\n\nx12\nRelative population potential of hydrocarbons, HC\n\n\nx13\nRelative pollution potential of oxides of nitrogen, NOx\n\n\nx14\nRelative pollution potential of sulfur dioxide, SO2\n\n\nx15\nPercent relative humidity, annual average at 1 p.m."
  },
  {
    "objectID": "nonlinearity.html#fitting-a-straight-line-model-to-predict-y-from-x2",
    "href": "nonlinearity.html#fitting-a-straight-line-model-to-predict-y-from-x2",
    "title": "13  Adding Non-linear Terms to a Linear Regression",
    "section": "13.3 Fitting a straight line model to predict y from x2",
    "text": "13.3 Fitting a straight line model to predict y from x2\nConsider the relationship between y, the age-adjusted mortality rate, and x2, the mean January temperature, across these 60 areas. I’ll include both a linear model (in blue) and a loess smooth (in red.) Does the relationship appear to be linear?\n\nggplot(pollution, aes(x = x2, y = y)) +\n    geom_point() +\n    geom_smooth(method = \"lm\", col = \"blue\", se = F) +\n    geom_smooth(method = \"loess\", col = \"red\", se = F)\n\n`geom_smooth()` using formula = 'y ~ x'\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\nSuppose we plot the residuals that emerge from the linear model shown in blue, above. Do we see a curve in a plot of residuals against fitted values?\n\nplot(lm(y ~ x2, data = pollution), which = 1)"
  },
  {
    "objectID": "nonlinearity.html#quadratic-polynomial-model-to-predict-y-using-x2",
    "href": "nonlinearity.html#quadratic-polynomial-model-to-predict-y-using-x2",
    "title": "13  Adding Non-linear Terms to a Linear Regression",
    "section": "13.4 Quadratic polynomial model to predict y using x2",
    "text": "13.4 Quadratic polynomial model to predict y using x2\nA polynomial in the variable x of degree D is a linear combination of the powers of x up to D.\nFor example:\n\nLinear: \\(y = \\beta_0 + \\beta_1 x\\)\nQuadratic: \\(y = \\beta_0 + \\beta_1 x + \\beta_2 x^2\\)\nCubic: \\(y = \\beta_0 + \\beta_1 x + \\beta_2 x^2 + \\beta_3 x^3\\)\nQuartic: \\(y = \\beta_0 + \\beta_1 x + \\beta_2 x^2 + \\beta_3 x^3 + \\beta_4 x^4\\)\nQuintic: \\(y = \\beta_0 + \\beta_1 x + \\beta_2 x^2 + \\beta_3 x^3 + \\beta_4 x^4 + \\beta_5 x^5\\)\n\nFitting such a model creates a polynomial regression.\n\n13.4.1 The raw quadratic model\nLet’s look at a quadratic model which predicts y using x2 and the square of x2, so that our model is of the form:\n\\[\ny = \\beta_0 + \\beta_1 x_2 + \\beta_2 x_2^2 + error\n\\]\nThere are several ways to fit this exact model.\n\nOne approach is to calculate the square of x2 within our pollution data set, and then feed both x2 and x2squared to lm.\nAnother approach uses the I function within our lm to specify the use of both x2 and its square.\nYet another approach uses the poly function within our lm, which can be used to specify raw models including x2 and x2squared.\n\n\npollution <- pollution |>\n    mutate(x2squared = x2^2)\n\nmod2a <- lm(y ~ x2 + x2squared, data = pollution)\nmod2b <- lm(y ~ x2 + I(x2^2), data = pollution)\nmod2c <- lm(y ~ poly(x2, degree = 2, raw = TRUE), data = pollution)\n\nEach of these approaches produces the same model, as they are just different ways of expressing the same idea.\n\nsummary(mod2a)\n\n\nCall:\nlm(formula = y ~ x2 + x2squared, data = pollution)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-148.977  -38.651    6.889   35.312  189.346 \n\nCoefficients:\n             Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 785.77449   79.54086   9.879 5.87e-14 ***\nx2            8.87640    4.27394   2.077   0.0423 *  \nx2squared    -0.11704    0.05429  -2.156   0.0353 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 60.83 on 57 degrees of freedom\nMultiple R-squared:  0.07623,   Adjusted R-squared:  0.04382 \nF-statistic: 2.352 on 2 and 57 DF,  p-value: 0.1044\n\n\nAnd if we plot the fitted values for this mod2 using whatever approach you like, we get exactly the same result.\n\nmod2a_aug <- augment(mod2a, pollution)\n\nggplot(mod2a_aug, aes(x = x2, y = y)) +\n    geom_point() +\n    geom_line(aes(x = x2, y = .fitted), col = \"red\") +\n    labs(title = \"Model 2a: Quadratic fit using x2 and x2^2\")\n\n\n\n\n\nmod2b_aug <- augment(mod2b, pollution)\n\nmod2c_aug <- augment(mod2c, pollution)\n\np1 <- ggplot(mod2b_aug, aes(x = x2, y = y)) +\n    geom_point() +\n    geom_line(aes(x = x2, y = .fitted), col = \"red\") +\n    labs(title = \"Model 2b: Quadratic fit\")\n\np2 <- ggplot(mod2c_aug, aes(x = x2, y = y)) +\n    geom_point() +\n    geom_line(aes(x = x2, y = .fitted), col = \"blue\") +\n    labs(title = \"Model 2c: Quadratic fit\")\n\np1 + p2\n\n\n\n\n\n\n13.4.2 Raw quadratic fit after centering x2\nSometimes, we’ll center (and perhaps rescale, too) the x2 variable before including it in a quadratic fit like this.\n\npollution <- pollution |>\n    mutate(x2_c = x2 - mean(x2))\n\nmod2d <- lm(y ~ x2_c + I(x2_c^2), data = pollution)\n\nsummary(mod2d)\n\n\nCall:\nlm(formula = y ~ x2_c + I(x2_c^2), data = pollution)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-148.977  -38.651    6.889   35.312  189.346 \n\nCoefficients:\n             Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 952.25941    9.59896  99.204   <2e-16 ***\nx2_c          0.92163    0.93237   0.988   0.3271    \nI(x2_c^2)    -0.11704    0.05429  -2.156   0.0353 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 60.83 on 57 degrees of freedom\nMultiple R-squared:  0.07623,   Adjusted R-squared:  0.04382 \nF-statistic: 2.352 on 2 and 57 DF,  p-value: 0.1044\n\n\nNote that this model looks very different, with the exception of the second order quadratic term. But, it produces the same fitted values as the models we fit previously.\n\nmod2d_aug <- augment(mod2d, pollution)\n\nggplot(mod2d_aug, aes(x = x2, y = y)) +\n    geom_point() +\n    geom_line(aes(x = x2, y = .fitted), col = \"red\") +\n    labs(title = \"Model 2d: Quadratic fit using centered x2 and x2^2\")\n\n\n\n\nOr, if you don’t believe me yet, look at the four sets of fitted values another way.\n\nfavstats(~ .fitted, data = mod2a_aug)\n\n      min       Q1  median       Q3     max     mean       sd  n missing\n 855.1041 936.7155 945.597 950.2883 954.073 940.3585 17.17507 60       0\n\nfavstats(~ .fitted, data = mod2b_aug)\n\n      min       Q1  median       Q3     max     mean       sd  n missing\n 855.1041 936.7155 945.597 950.2883 954.073 940.3585 17.17507 60       0\n\nfavstats(~ .fitted, data = mod2c_aug)\n\n      min       Q1  median       Q3     max     mean       sd  n missing\n 855.1041 936.7155 945.597 950.2883 954.073 940.3585 17.17507 60       0\n\nfavstats(~ .fitted, data = mod2d_aug)\n\n      min       Q1  median       Q3     max     mean       sd  n missing\n 855.1041 936.7155 945.597 950.2883 954.073 940.3585 17.17507 60       0"
  },
  {
    "objectID": "nonlinearity.html#orthogonal-polynomials",
    "href": "nonlinearity.html#orthogonal-polynomials",
    "title": "13  Adding Non-linear Terms to a Linear Regression",
    "section": "13.5 Orthogonal Polynomials",
    "text": "13.5 Orthogonal Polynomials\nNow, let’s fit an orthogonal polynomial of degree 2 to predict y using x2.\n\nmod2_orth <- lm(y ~ poly(x2, 2), data = pollution)\n\nsummary(mod2_orth)\n\n\nCall:\nlm(formula = y ~ poly(x2, 2), data = pollution)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-148.977  -38.651    6.889   35.312  189.346 \n\nCoefficients:\n             Estimate Std. Error t value Pr(>|t|)    \n(Intercept)   940.358      7.853 119.746   <2e-16 ***\npoly(x2, 2)1  -14.345     60.829  -0.236   0.8144    \npoly(x2, 2)2 -131.142     60.829  -2.156   0.0353 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 60.83 on 57 degrees of freedom\nMultiple R-squared:  0.07623,   Adjusted R-squared:  0.04382 \nF-statistic: 2.352 on 2 and 57 DF,  p-value: 0.1044\n\n\nNow this looks very different in the equation, but, again, we can see that this produces exactly the same fitted values as our previous models, and the same model fit summaries. Is it, in fact, the same model? Here, we’ll plot the fitted Model 2a in a red line, and this new Model 2 with Orthogonal Polynomials as blue points.\n\nmod2orth_aug <- augment(mod2_orth, pollution)\n\nggplot(mod2orth_aug, aes(x = x2, y = y)) +\n    geom_point() +\n    geom_point(aes(x = x2, y = .fitted), \n               col = \"blue\", size = 2) +\n    geom_line(data = mod2a_aug, aes(x = x2, y = .fitted),\n              col = \"red\") +\n    labs(title = \"Model 2 with Orthogonal Polynomial, degree 2\")\n\n\n\n\nYes, it is again the same model in terms of the predictions it makes for y.\nBy default, with raw = FALSE, the poly() function within a linear model computes what is called an orthogonal polynomial. An orthogonal polynomial sets up a model design matrix using the coding we’ve seen previously: x2 and x2^2 in our case, and then scales those columns so that each column is orthogonal to the previous ones. This eliminates the collinearity (correlation between predictors) and lets our t tests tell us whether the addition of any particular polynomial term improves the fit of the model over the lower orders.\nWould the addition of a cubic term help us much in predicting y from x2?\n\nmod3 <- lm(y ~ poly(x2, 3), data = pollution)\nsummary(mod3)\n\n\nCall:\nlm(formula = y ~ poly(x2, 3), data = pollution)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-146.262  -39.679    5.569   35.984  191.536 \n\nCoefficients:\n             Estimate Std. Error t value Pr(>|t|)    \n(Intercept)   940.358      7.917 118.772   <2e-16 ***\npoly(x2, 3)1  -14.345     61.328  -0.234   0.8159    \npoly(x2, 3)2 -131.142     61.328  -2.138   0.0369 *  \npoly(x2, 3)3   16.918     61.328   0.276   0.7837    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 61.33 on 56 degrees of freedom\nMultiple R-squared:  0.07748,   Adjusted R-squared:  0.02806 \nF-statistic: 1.568 on 3 and 56 DF,  p-value: 0.2073\n\n\nIt doesn’t appear that the cubic term adds much here, if anything. The p value is not significant for the third degree polynomial, the summaries of fit quality aren’t much improved, and as we can see from the plot below, the predictions don’t actually change all that much.\n\nmod3_aug <- augment(mod3, pollution)\n\nggplot(mod3_aug, aes(x = x2, y = y)) +\n    geom_point() +\n    geom_line(aes(x = x2, y = .fitted), \n              col = \"blue\") +\n    geom_line(data = mod2orth_aug, aes(x = x2, y = .fitted),\n              col = \"red\") +\n    labs(title = \"Quadratic (red) vs. Cubic (blue) Polynomial Fits\")"
  },
  {
    "objectID": "nonlinearity.html#fit-a-cubic-polynomial-to-predict-y-from-x3",
    "href": "nonlinearity.html#fit-a-cubic-polynomial-to-predict-y-from-x3",
    "title": "13  Adding Non-linear Terms to a Linear Regression",
    "section": "13.6 Fit a cubic polynomial to predict y from x3",
    "text": "13.6 Fit a cubic polynomial to predict y from x3\nWhat if we consider another predictor instead? Let’s look at x3, the Mean July temperature in degrees Fahrenheit. Here is the loess smooth.\n\nggplot(pollution, aes(x = x3, y = y)) +\n    geom_point() +\n    geom_smooth(method = \"loess\")\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\nThat looks pretty curvy - perhaps we need a more complex polynomial. We’ll consider a linear model (mod4_L), a quadratic fit (mod4_Q) and a polynomial of degree 3: a cubic fit (mod_4C)\n\nmod4_L <- lm(y ~ x3, data = pollution)\nsummary(mod4_L)\n\n\nCall:\nlm(formula = y ~ x3, data = pollution)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-139.813  -34.341    4.271   38.197  149.587 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  670.529    123.140   5.445  1.1e-06 ***\nx3             3.618      1.648   2.196   0.0321 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 60.29 on 58 degrees of freedom\nMultiple R-squared:  0.07674,   Adjusted R-squared:  0.06082 \nF-statistic: 4.821 on 1 and 58 DF,  p-value: 0.03213\n\nmod4_Q <- lm(y ~ poly(x3, 2), data = pollution)\nsummary(mod4_Q)\n\n\nCall:\nlm(formula = y ~ poly(x3, 2), data = pollution)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-132.004  -42.184    4.069   47.126  157.396 \n\nCoefficients:\n             Estimate Std. Error t value Pr(>|t|)    \n(Intercept)   940.358      7.553 124.503   <2e-16 ***\npoly(x3, 2)1  132.364     58.504   2.262   0.0275 *  \npoly(x3, 2)2 -125.270     58.504  -2.141   0.0365 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 58.5 on 57 degrees of freedom\nMultiple R-squared:  0.1455,    Adjusted R-squared:  0.1155 \nF-statistic: 4.852 on 2 and 57 DF,  p-value: 0.01133\n\nmod4_C <- lm(y ~ poly(x3, 3), data = pollution)\nsummary(mod4_C)\n\n\nCall:\nlm(formula = y ~ poly(x3, 3), data = pollution)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-148.004  -29.998    1.441   34.579  141.396 \n\nCoefficients:\n             Estimate Std. Error t value Pr(>|t|)    \n(Intercept)   940.358      7.065 133.095  < 2e-16 ***\npoly(x3, 3)1  132.364     54.728   2.419  0.01886 *  \npoly(x3, 3)2 -125.270     54.728  -2.289  0.02588 *  \npoly(x3, 3)3 -165.439     54.728  -3.023  0.00377 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 54.73 on 56 degrees of freedom\nMultiple R-squared:  0.2654,    Adjusted R-squared:  0.226 \nF-statistic: 6.742 on 3 and 56 DF,  p-value: 0.0005799\n\n\nIt looks like the cubic polynomial term is of some real importance here. Do the linear, quadratic and cubic model fitted values look different?\n\nmod4_L_aug <- augment(mod4_L, pollution)\n\nmod4_Q_aug <- augment(mod4_Q, pollution)\n\nmod4_C_aug <- augment(mod4_C, pollution)\n\nggplot(pollution, aes(x = x3, y = y)) +\n    geom_point() +\n    geom_line(data = mod4_L_aug, aes(x = x3, y = .fitted), \n              col = \"blue\", size = 1.25) +\n    geom_line(data = mod4_Q_aug, aes(x = x3, y = .fitted),\n              col = \"black\", size = 1.25) +\n    geom_line(data = mod4_C_aug, aes(x = x3, y = .fitted),\n              col = \"red\", size = 1.25) +\n    geom_text(x = 66, y = 930, label = \"Linear Fit\", col = \"blue\") +\n    geom_text(x = 64, y = 820, label = \"Quadratic Fit\", col = \"black\") +\n    geom_text(x = 83, y = 900, label = \"Cubic Fit\", col = \"red\") +\n    labs(title = \"Linear, Quadratic and Cubic Fits predicting y with x3\") +\n    theme_bw()\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead."
  },
  {
    "objectID": "nonlinearity.html#fitting-a-restricted-cubic-spline-in-a-linear-regression",
    "href": "nonlinearity.html#fitting-a-restricted-cubic-spline-in-a-linear-regression",
    "title": "13  Adding Non-linear Terms to a Linear Regression",
    "section": "13.7 Fitting a restricted cubic spline in a linear regression",
    "text": "13.7 Fitting a restricted cubic spline in a linear regression\n\nA linear spline is a continuous function formed by connecting points (called knots of the spline) by line segments.\nA restricted cubic spline is a way to build highly complicated curves into a regression equation in a fairly easily structured way.\nA restricted cubic spline is a series of polynomial functions joined together at the knots.\n\nSuch a spline gives us a way to flexibly account for non-linearity without over-fitting the model.\nRestricted cubic splines can fit many different types of non-linearities.\nSpecifying the number of knots is all you need to do in R to get a reasonable result from a restricted cubic spline.\n\n\nThe most common choices are 3, 4, or 5 knots. Each additional knot adds to the non-linearity, and spends an additional degree of freedom:\n\n3 Knots, 2 degrees of freedom, allows the curve to “bend” once.\n4 Knots, 3 degrees of freedom, lets the curve “bend” twice.\n5 Knots, 4 degrees of freedom, lets the curve “bend” three times.\n\nFor most applications, three to five knots strike a nice balance between complicating the model needlessly and fitting data pleasingly. Let’s consider a restricted cubic spline model for our y based on x3 again, but now with:\n\nin mod5a, 3 knots,\nin mod5b, 4 knots, and\nin mod5c, 5 knots\n\n\nmod5a_rcs <- lm(y ~ rcs(x3, 3), data = pollution)\nmod5b_rcs <- lm(y ~ rcs(x3, 4), data = pollution)\nmod5c_rcs <- lm(y ~ rcs(x3, 5), data = pollution)\n\nHere, for instance, is the summary of the 5-knot model:\n\nsummary(mod5c_rcs)\n\n\nCall:\nlm(formula = y ~ rcs(x3, 5), data = pollution)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-141.522  -32.009    1.674   31.971  147.878 \n\nCoefficients:\n                Estimate Std. Error t value Pr(>|t|)\n(Intercept)      468.113    396.319   1.181    0.243\nrcs(x3, 5)x3       6.447      5.749   1.121    0.267\nrcs(x3, 5)x3'    -25.633     46.810  -0.548    0.586\nrcs(x3, 5)x3''   323.137    293.065   1.103    0.275\nrcs(x3, 5)x3''' -612.578    396.270  -1.546    0.128\n\nResidual standard error: 54.35 on 55 degrees of freedom\nMultiple R-squared:  0.2883,    Adjusted R-squared:  0.2366 \nF-statistic: 5.571 on 4 and 55 DF,  p-value: 0.0007734\n\n\nWe’ll begin by storing the fitted values from these three models and other summaries, for plotting.\n\nmod5a_aug <- augment(mod5a_rcs, pollution)\n\nmod5b_aug <- augment(mod5b_rcs, pollution)\n\nmod5c_aug <- augment(mod5c_rcs, pollution)\n\n\np2 <- ggplot(pollution, aes(x = x3, y = y)) +\n    geom_point() +\n    geom_smooth(method = \"loess\", col = \"purple\", se = F) +\n    labs(title = \"Loess Smooth\") +\n    theme_bw()\n\np3 <- ggplot(mod5a_aug, aes(x = x3, y = y)) +\n    geom_point() +\n    geom_line(aes(x = x3, y = .fitted), \n              col = \"blue\", size = 1.25) +\n    labs(title = \"RCS, 3 knots\") +\n    theme_bw()\n\np4 <- ggplot(mod5b_aug, aes(x = x3, y = y)) +\n    geom_point() +\n    geom_line(aes(x = x3, y = .fitted), \n              col = \"black\", size = 1.25) +\n    labs(title = \"RCS, 4 knots\") +\n    theme_bw()\n\np5 <- ggplot(mod5c_aug, aes(x = x3, y = y)) +\n    geom_point() +\n    geom_line(aes(x = x3, y = .fitted), \n              col = \"red\", size = 1.25) +\n    labs(title = \"RCS, 5 knots\") +\n    theme_bw()\n\n(p2 + p3) / (p4 + p5)\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\nDoes it seem like the fit improves markedly (perhaps approaching the loess smooth result) as we increase the number of knots?\n\nanova(mod5a_rcs, mod5b_rcs, mod5c_rcs)\n\nAnalysis of Variance Table\n\nModel 1: y ~ rcs(x3, 3)\nModel 2: y ~ rcs(x3, 4)\nModel 3: y ~ rcs(x3, 5)\n  Res.Df    RSS Df Sum of Sq      F   Pr(>F)   \n1     57 194935                                \n2     56 171448  1   23486.9 7.9503 0.006672 **\n3     55 162481  1    8967.2 3.0354 0.087057 . \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nBased on an ANOVA comparison, the fourth knot adds significant predictive value (p = 0.0067), but the fifth knot is borderline (p = 0.0871). From the glance function in the broom package, we can also look at some key summaries.\n\nglance(mod5a_rcs)\n\n# A tibble: 1 × 12\n  r.squ…¹ adj.r…² sigma stati…³ p.value    df logLik   AIC   BIC devia…⁴ df.re…⁵\n    <dbl>   <dbl> <dbl>   <dbl>   <dbl> <dbl>  <dbl> <dbl> <dbl>   <dbl>   <int>\n1   0.146   0.116  58.5    4.88  0.0111     2  -328.  663.  672. 194935.      57\n# … with 1 more variable: nobs <int>, and abbreviated variable names\n#   ¹​r.squared, ²​adj.r.squared, ³​statistic, ⁴​deviance, ⁵​df.residual\n\nglance(mod5b_rcs)\n\n# A tibble: 1 × 12\n  r.squ…¹ adj.r…² sigma stati…³ p.value    df logLik   AIC   BIC devia…⁴ df.re…⁵\n    <dbl>   <dbl> <dbl>   <dbl>   <dbl> <dbl>  <dbl> <dbl> <dbl>   <dbl>   <int>\n1   0.249   0.209  55.3    6.19 0.00104     3  -324.  658.  668. 171448.      56\n# … with 1 more variable: nobs <int>, and abbreviated variable names\n#   ¹​r.squared, ²​adj.r.squared, ³​statistic, ⁴​deviance, ⁵​df.residual\n\nglance(mod5c_rcs)\n\n# A tibble: 1 × 12\n  r.squ…¹ adj.r…² sigma stati…³ p.value    df logLik   AIC   BIC devia…⁴ df.re…⁵\n    <dbl>   <dbl> <dbl>   <dbl>   <dbl> <dbl>  <dbl> <dbl> <dbl>   <dbl>   <int>\n1   0.288   0.237  54.4    5.57 7.73e-4     4  -322.  657.  669. 162481.      55\n# … with 1 more variable: nobs <int>, and abbreviated variable names\n#   ¹​r.squared, ²​adj.r.squared, ³​statistic, ⁴​deviance, ⁵​df.residual\n\n\n\n\n\nModel\nKnots\n\\(R^2\\)\nAdj. \\(R^2\\)\nAIC\nBIC\n\n\n\n\n5a\n3\n0.146\n0.116\n663.4\n671.8\n\n\n5b\n4\n0.249\n0.209\n657.7\n668.2\n\n\n5c\n5\n0.288\n0.237\n656.5\n669.1\n\n\n\nWithin our sample, the five-knot RCS outperforms the 3- and 4-knot versions on adjusted \\(R^2\\) and AIC (barely) and does a little worse than the 4-knot RCS on BIC.\nOf course, we could also use the cross-validation methods we’ve developed for other linear regressions to assess predictive capacity of these models. I’ll skip that for now.\nTo see the values of x3 where the splines place their knots, we can use the attributes function.\n\nattributes(rcs(pollution$x3, 5))\n\n$dim\n[1] 60  4\n\n$dimnames\n$dimnames[[1]]\nNULL\n\n$dimnames[[2]]\n[1] \"pollution\"    \"pollution'\"   \"pollution''\"  \"pollution'''\"\n\n\n$class\n[1] \"rms\"\n\n$name\n[1] \"pollution\"\n\n$label\n[1] \"pollution\"\n\n$assume\n[1] \"rcspline\"\n\n$assume.code\n[1] 4\n\n$parms\n[1] 68 72 74 77 82\n\n$nonlinear\n[1] FALSE  TRUE  TRUE  TRUE\n\n$colnames\n[1] \"pollution\"    \"pollution'\"   \"pollution''\"  \"pollution'''\"\n\n\nThe knots in this particular 5-knot spline are placed by the computer at 68, 72, 74, 77 and 82, it seems.\nThere are two kinds of Multivariate Regression Models\n\n[Prediction] Those that are built so that we can make accurate predictions.\n[Explanatory] Those that are built to help understand underlying phenomena.\n\nWhile those two notions overlap considerably, they do imply different things about how we strategize about model-building and model assessment. Harrell’s primary concern is effective use of the available data for prediction - this implies some things that will be different from what we’ve seen in the past.\nHarrell refers to multivariable regression modeling strategy as the process of spending degrees of freedom. The main job in strategizing about multivariate modeling is to\n\nDecide the number of degrees of freedom that can be spent\nDecide where to spend them\nSpend them, wisely.\n\nWhat this means is essentially linked to making decisions about predictor complexity, both in terms of how many predictors will be included in the regression model, and about how we’ll include those predictors."
  },
  {
    "objectID": "nonlinearity.html#spending-degrees-of-freedom",
    "href": "nonlinearity.html#spending-degrees-of-freedom",
    "title": "13  Adding Non-linear Terms to a Linear Regression",
    "section": "13.8 “Spending” Degrees of Freedom",
    "text": "13.8 “Spending” Degrees of Freedom\n\n“Spending” df includes\n\nfitting parameter estimates in models, or\nexamining figures built using the outcome variable Y that tell you how to model the predictors.\n\n\nIf you use a scatterplot of Y vs. X or the residuals of the Y-X regression model vs. X to decide whether a linear model is appropriate, then how many degrees of freedom have you actually spent?\nGrambsch and O’Brien conclude that if you wish to preserve the key statistical properties of the various estimation and fitting procedures used in building a model, you can’t retrieve these degrees of freedom once they have been spent.\n\n13.8.1 Overfitting and Limits on the # of Predictors\nSuppose you have a total sample size of \\(n\\) observations, then you really shouldn’t be thinking about estimating more than \\(n / 15\\) regression coefficients, at the most.\n\nIf \\(k\\) is the number of parameters in a full model containing all candidate predictors for a stepwise analysis, then \\(k\\) should be no greater than \\(n / 15\\).\n\\(k\\) should include all variables screened for association with the response, including interaction terms.\nSometimes I hold myself to a tougher standard, or \\(n / 50\\) predictors, at maximum.\n\nSo if you have 97 observations in your data, then you can probably just barely justify the use of a stepwise analysis using the main effects alone of 5 candidate variables (with one additional DF for the intercept term) using the \\(n/15\\) limit.\nHarrell (2001) also mentions that if you have a narrowly distributed predictor, without a lot of variation to work with, then an even larger sample size \\(n\\) should be required. See Vittinghoff et al. (2012), Section 10.3 for more details.\n\n\n13.8.2 The Importance of Collinearity\n\nCollinearity denotes correlation between predictors high enough to degrade the precision of the regression coefficient estimates substantially for some or all of the correlated predictors\n\n\nVittinghoff et al. (2012), section 10.4.1\nCan one predictor in a model be predicted well using the other predictors in the model?\n\nStrong correlations (for instance, \\(r \\geq 0.8\\)) are especially troublesome.\n\nEffects of collinearity\n\ndecreases precision, in the sense of increasing the standard errors of the parameter estimates\ndecreases power\nincreases the difficulty of interpreting individual predictor effects\noverall F test is significant, but individual t tests may not be\n\n\nSuppose we want to assess whether variable \\(X_j\\) is collinear with the other predictors in a model. We run a regression predicting \\(X_j\\) using the other predictors, and obtain the \\(R^2\\). The VIF is defined as 1 / (1 - this \\(R^2\\)), and we usually interpret VIFs above 5 as indicating a serious multicollinearity problem (i.e. \\(R^2\\) values for this predictor of 0.8 and above would thus concern us.)\n\nvif(lm(y ~ x1 + x2 + x3 + x4 + x5 + x6, data = pollution))\n\n      x1       x2       x3       x4       x5       x6 \n2.238862 2.058731 2.153044 4.174448 3.447399 1.792996 \n\n\nOccasionally, you’ll see the inverse of VIF reported, and this is called tolerance.\n\ntolerance = 1 / VIF\n\n\n\n13.8.3 Collinearity in an Explanatory Model\n\nWhen we are attempting to identify multiple independent predictors (the explanatory model approach), then we will need to choose between collinear variables\n\noptions suggested by Vittinghoff et al. (2012), p. 422, include choosing on the basis of plausibility as a causal factor,\nchoosing the variable that has higher data quality (is measured more accurately or has fewer missing values.)\nOften, we choose to include a variable that is statistically significant as a predictor, and drop others, should we be so lucky.\n\nLarger effects, especially if they are associated with predictors that have minimal correlation with the other predictors under study, cause less trouble in terms of potential violation of the \\(n/15\\) rule for what constitutes a reasonable number of predictors.\n\n\n\n13.8.4 Collinearity in a Prediction Model\n\nIf we are primarily building a prediction model for which inference on the individual predictors is not of interest, then it is totally reasonable to use both predictors in the model, if doing so reduces prediction error.\n\nCollinearity doesn’t affect predictions in our model development sample.\nCollinearity doesn’t affect predictions on new data so long as the new data have similar relationships between predictors.\nIf our key predictor is correlated strongly with a confounder and if the predictor remains an important part of the model after adjustment for the confounder, then this suggests a meaningful independent effect.\n\nIf the effects of the predictor are clearly confounded by the adjustment variable, we again have a clear result.\nIf neither appears to add meaningful predictive value after adjustment, the data may be inadequate.\n\nIf the collinearity is between adjustment variables, but doesn’t involve the key predictor, then inclusion of the collinear variables is unlikely to cause substantial problems."
  },
  {
    "objectID": "nonlinearity.html#spending-df-on-non-linearity-the-spearman-plot",
    "href": "nonlinearity.html#spending-df-on-non-linearity-the-spearman-plot",
    "title": "13  Adding Non-linear Terms to a Linear Regression",
    "section": "13.9 Spending DF on Non-Linearity: The Spearman Plot",
    "text": "13.9 Spending DF on Non-Linearity: The Spearman Plot\nWe need a flexible approach to assessing non-linearity and fitting models with non-linear predictors. This will lead us to a measure of what Harrell (2001) calls potential predictive punch which hides the true form of the regression from the analyst so as to preserve statistical properties, but that lets us make sensible decisions about whether a predictor should be included in a model, and the number of parameters (degrees of freedom, essentially) we are willing to devote to it.\nWhat if we want to consider where best to spend our degrees of freedom on non-linear predictor terms, like interactions, polynomial functions or curved splines to represent our input data? The approach we’ll find useful in the largest variety of settings is a combination of\n\na rank correlation assessment of potential predictive punch (using a Spearman \\(\\rho^2\\) plot, available in the Hmisc package), followed by\nthe application of restricted cubic splines to fit and assess models.\n\nLet’s try such a plot for our fifteen predictors:\n\nsp2 <- spearman2(y ~ x1 + x2 + x3 + x4 + x5 + x6 + x7 +\n                   x8 + x9 + x10 + x11 + x12 + x13 +\n                   x14 + x15, data = pollution)\n\nplot(sp2)\n\n\n\n\nThe variable with the largest adjusted squared Spearman \\(\\rho\\) statistic in this setting is x9, followed by x6 and x14. With only 60 observations, we might well want to restrict ourselves to a very small model. What the Spearman plot suggests is that we focus any non-linear terms on x9 first, and then perhaps x6 and x14 as they have some potential predictive power. It may or may not work out that the non-linear terms are productive.\n\n13.9.1 Fitting a Big Model to the pollution data\nSo, one possible model built in reaction this plot might be to fit:\n\na restricted cubic spline with 5 knots on x9,\na restricted cubic spline with 3 knots on x6,\na quadratic polynomial on x14, and\na linear fit to x1 and x13\n\nThat’s way more degrees of freedom (4 for x9, 2 for x6, 2 for x14 and 1 each for x1 and x13 makes a total of 10 without the intercept term) than we can really justify with a sample of 60 observations. But let’s see what happens.\n\nmod_big <- lm(y ~ rcs(x9, 5) + rcs(x6, 3) + poly(x14, 2) + \n                  x1 + x13, data = pollution)\n\nanova(mod_big)\n\nAnalysis of Variance Table\n\nResponse: y\n             Df Sum Sq Mean Sq F value    Pr(>F)    \nrcs(x9, 5)    4 100164 25040.9 17.8482 4.229e-09 ***\nrcs(x6, 3)    2  38306 19152.8 13.6513 1.939e-05 ***\npoly(x14, 2)  2  15595  7797.7  5.5579  0.006677 ** \nx1            1   4787  4787.3  3.4122  0.070759 .  \nx13           1    712   711.9  0.5074  0.479635    \nResiduals    49  68747  1403.0                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nThis anova suggests that we have at least some predictive value in each spline (x9 and x6) and some additional value in x14, although it’s not as clear that the linear terms (x1 and x13) did much good.\n\n\n13.9.2 Limitations of lm for fitting complex linear regression models\nWe can certainly assess this big, complex model using lm in comparison to other models:\n\nwith in-sample summary statistics like adjusted \\(R^2\\), AIC and BIC,\nwe can assess its assumptions with residual plots, and\nwe can also compare out-of-sample predictive quality through cross-validation,\n\nBut to really delve into the details of how well this complex model works, and to help plot what is actually being fit, we’ll probably want to fit the model using an alternative method for fitting linear models, called ols, from the rms package developed by Frank Harrell and colleagues. That’s where we’re heading, in Chapter 14.\n\n\n\n\nHarrell, Frank E. 2001. Regression Modeling Strategies. New York: Springer.\n\n\nMcDonald, Gary C., and Richard C. Schwing. 1973. “Instabilities of Regression Estimates Relating Air Pollution to Mortality.” Technometrics 15 (3): 463–81.\n\n\nVittinghoff, Eric, David V. Glidden, Stephen C. Shiboski, and Charles E. McCulloch. 2012. Regression Methods in Biostatistics: Linear, Logistic, Survival, and Repeated Measures Models. Second Edition. Springer-Verlag, Inc. http://www.biostat.ucsf.edu/vgsm/."
  },
  {
    "objectID": "olsfitting.html#r-setup-used-here",
    "href": "olsfitting.html#r-setup-used-here",
    "title": "14  Using ols to fit linear models",
    "section": "14.1 R Setup Used Here",
    "text": "14.1 R Setup Used Here\n\nknitr::opts_chunk$set(comment = NA)\n\nlibrary(janitor)\nlibrary(Hmisc)\nlibrary(rms)\nlibrary(tidyverse) \n\ntheme_set(theme_bw())\n\n\n14.1.1 Data Load\n\npollution <- read_csv(\"data/pollution.csv\", show_col_types = FALSE)"
  },
  {
    "objectID": "olsfitting.html#fitting-a-model-with-ols",
    "href": "olsfitting.html#fitting-a-model-with-ols",
    "title": "14  Using ols to fit linear models",
    "section": "14.2 Fitting a model with ols",
    "text": "14.2 Fitting a model with ols\nWe will use the datadist approach when fitting a linear model with ols from the rms package, so as to store additional important elements of the model fit.\n\nd <- datadist(pollution)\noptions(datadist = \"d\")\n\nNext, we’ll fit the model using ols and place its results in newmod.\n\nnewmod <- ols(y ~ rcs(x9, 5) + rcs(x6, 3) + pol(x14, 2) + \n                  x1 + x13, \n              data = pollution, x = TRUE, y = TRUE)\nnewmod\n\nLinear Regression Model\n \n ols(formula = y ~ rcs(x9, 5) + rcs(x6, 3) + pol(x14, 2) + x1 + \n     x13, data = pollution, x = TRUE, y = TRUE)\n \n                  Model Likelihood    Discrimination    \n                        Ratio Test           Indexes    \n Obs       60    LR chi2     72.02    R2       0.699    \n sigma37.4566    d.f.           10    R2 adj   0.637    \n d.f.      49    Pr(> chi2) 0.0000    g       58.961    \n \n Residuals\n \n     Min      1Q  Median      3Q     Max \n -86.189 -18.554  -1.799  18.645 104.307 \n \n \n           Coef      S.E.     t     Pr(>|t|)\n Intercept  796.2658 162.3269  4.91 <0.0001 \n x9          -2.6328   6.3504 -0.41 0.6803  \n x9'        121.4651 124.4827  0.98 0.3340  \n x9''      -219.8025 227.6775 -0.97 0.3391  \n x9'''      151.5700 171.3867  0.88 0.3808  \n x6           7.6817  15.5230  0.49 0.6229  \n x6'        -29.4388  18.0531 -1.63 0.1094  \n x14          0.5652   0.2547  2.22 0.0311  \n x14^2       -0.0010   0.0010 -0.96 0.3407  \n x1           1.0717   0.7317  1.46 0.1494  \n x13         -0.1028   0.1443 -0.71 0.4796  \n \n\n\nSome of the advantages and disadvantages of fitting linear regression models with ols or lm will reveal themselves over time. For now, one advantage for ols is that the entire variance-covariance matrix is saved. Most of the time, there will be some value to considering both ols and lm approaches.\nMost of this output should be familiar, but a few pieces are different.\n\n14.2.1 The Model Likelihood Ratio Test\nThe Model Likelihood Ratio Test compares newmod to the null model with only an intercept term. It is a goodness-of-fit test that we’ll use in several types of model settings this semester.\n\nIn many settings, the logarithm of the likelihood ratio, multiplied by -2, yields a value which can be compared to a \\(\\chi^2\\) distribution. So here, the value 72.02 is -2(log likelihood), and is compared to a \\(\\chi^2\\) distribution with 10 degrees of freedom. We reject the null hypothesis that newmod is no better than the null model, and conclude instead that at least one of these predictors adds some value.\n\nFor ols, interpret the model likelihood ratio test like the global (ANOVA) F test in lm.\nThe likelihood function is the probability of observing our data under the specified model.\nWe can compare two nested models by evaluating the difference in their likelihood ratios and degrees of freedom, then comparing the result to a \\(\\chi^2\\) distribution.\n\n\n\n\n14.2.2 The g statistic\nThe g statistic is new and is referred to as the g-index. it’s based on Gini’s mean difference and is purported to be a robust and highly efficient measure of variation.\n\nHere, g = 58.961, which implies that if you randomly select two of the 60 areas included in the model, the average difference in predicted y (Age-Adjusted Mortality Rate) using this model will be 58.961.\n\nTechnically, g is Gini’s mean difference of the predicted values."
  },
  {
    "objectID": "olsfitting.html#anova-for-an-ols-model",
    "href": "olsfitting.html#anova-for-an-ols-model",
    "title": "14  Using ols to fit linear models",
    "section": "14.3 ANOVA for an ols model",
    "text": "14.3 ANOVA for an ols model\nOne advantage of the ols approach is that when you apply an anova to it, it separates out the linear and non-linear components of restricted cubic splines and polynomial terms (as well as product terms, if your model includes them.)\n\nanova(newmod)\n\n                Analysis of Variance          Response: y \n\n Factor          d.f. Partial SS  MS         F     P     \n x9               4    35219.7647  8804.9412  6.28 0.0004\n  Nonlinear       3     1339.3081   446.4360  0.32 0.8121\n x6               2     9367.6008  4683.8004  3.34 0.0437\n  Nonlinear       1     3730.7388  3730.7388  2.66 0.1094\n x14              2    18679.6957  9339.8478  6.66 0.0028\n  Nonlinear       1     1298.7625  1298.7625  0.93 0.3407\n x1               1     3009.1829  3009.1829  2.14 0.1494\n x13              1      711.9108   711.9108  0.51 0.4796\n TOTAL NONLINEAR  5     6656.1824  1331.2365  0.95 0.4582\n REGRESSION      10   159563.8285 15956.3829 11.37 <.0001\n ERROR           49    68746.8004  1402.9959             \n\n\nUnlike the anova approach in lm, in ols ANOVA, partial F tests are presented - each predictor is assessed as “last predictor in” much like the usual t tests in lm. In essence, the partial sums of squares and F tests here describe the marginal impact of removing each covariate from newmod.\nWe conclude that the non-linear parts of x9 and x6 and x14 combined don’t seem to add much value, but that overall, x9, x6 and x14 seem to be valuable. So it must be the linear parts of those variables within our model that are doing most of the predictive work."
  },
  {
    "objectID": "olsfitting.html#effect-estimates",
    "href": "olsfitting.html#effect-estimates",
    "title": "14  Using ols to fit linear models",
    "section": "14.4 Effect Estimates",
    "text": "14.4 Effect Estimates\nA particularly useful thing to get out of the ols approach that is not as easily available in lm (without recoding or standardizing our predictors) is a summary of the effects of each predictor in an interesting scale.\n\nsummary(newmod)\n\n             Effects              Response : y \n\n Factor Low   High  Diff. Effect   S.E.    Lower 0.95 Upper 0.95\n x9      4.95 15.65 10.70  40.4060 14.0790  12.1120   68.6990   \n x6     10.40 11.50  1.10 -18.2930  8.1499 -34.6710   -1.9153   \n x14    11.00 69.00 58.00  28.3480 10.6480   6.9503   49.7460   \n x1     32.75 43.25 10.50  11.2520  7.6833  -4.1878   26.6930   \n x13     4.00 23.75 19.75  -2.0303  2.8502  -7.7579    3.6973   \n\n\nThis “effects summary” shows the effect on y of moving from the 25th to the 75th percentile of each variable (along with a standard error and 95% confidence interval) while holding the other variable at the level specified at the bottom of the output.\nThe most useful way to look at this sort of analysis is often a plot.\n\nplot(summary(newmod))\n\n\n\n\nFor x9 note from the summary above that the 25th percentile is 4.95 and the 75th is 15.65. Our conclusion is that the estimated effect of moving x9 from 4.95 to 15.65 is an increase of 40.4 on y, with a 95% CI of (12.1, 68.7).\nFor a categorical variable, the low level is shown first and then the high level.\nThe plot shows the point estimate (arrow head) and then the 90% (narrowest bar), 95% (middle bar) and 99% (widest bar in lightest color) confidence intervals for each predictor’s effect.\n\nIt’s easier to distinguish this in the x9 plot than the one for x13.\nRemember that what is being compared is the first value to the second value’s impact on the outcome, with other predictors held constant.\n\n\n14.4.1 Simultaneous Confidence Intervals\nThese confidence intervals make no effort to deal with the multiple comparisons problem, but just fit individual 95% (or whatever level you choose) confidence intervals for each predictor. The natural alternative is to make an adjustment for multiple comparisons in fitting the confidence intervals, so that the set of (in this case, five - one for each predictor) confidence intervals for effect sizes has a family-wise 95% confidence level. You’ll note that the effect estimates and standard errors are unchanged from those shown above, but the confidence limits are a bit wider.\n\nsummary(newmod, conf.type=c('simultaneous'))\n\n             Effects              Response : y \n\n Factor Low   High  Diff. Effect   S.E.    Lower 0.95 Upper 0.95\n x9      4.95 15.65 10.70  40.4060 14.0790   3.11970  77.6920   \n x6     10.40 11.50  1.10 -18.2930  8.1499 -39.87600   3.2899   \n x14    11.00 69.00 58.00  28.3480 10.6480   0.14962  56.5470   \n x1     32.75 43.25 10.50  11.2520  7.6833  -9.09500  31.6000   \n x13     4.00 23.75 19.75  -2.0303  2.8502  -9.57830   5.5177   \n\n\nRemember that if you’re looking for the usual lm summary for an ols object, use summary.lm."
  },
  {
    "objectID": "olsfitting.html#the-predict-function-for-an-ols-model",
    "href": "olsfitting.html#the-predict-function-for-an-ols-model",
    "title": "14  Using ols to fit linear models",
    "section": "14.5 The Predict function for an ols model",
    "text": "14.5 The Predict function for an ols model\nThe Predict function is very flexible, and can be used to produce individual or simultaneous confidence limits.\n\nPredict(newmod, x9 = 12, x6 = 12, x14 = 40, x1 = 40, x13 = 20) \n\n  x9 x6 x14 x1 x13     yhat    lower   upper\n1 12 12  40 40  20 923.0982 893.0984 953.098\n\nResponse variable (y): y \n\nLimits are 0.95 confidence limits\n\n# individual limits\n\nPredict(newmod, x9 = 5:15) # individual limits\n\n   x9    x6 x14 x1 x13     yhat    lower    upper\n1   5 11.05  30 38   9 913.7392 889.4802 937.9983\n2   6 11.05  30 38   9 916.3490 892.0082 940.6897\n3   7 11.05  30 38   9 921.3093 898.9657 943.6529\n4   8 11.05  30 38   9 927.6464 907.0355 948.2574\n5   9 11.05  30 38   9 934.3853 913.3761 955.3946\n6  10 11.05  30 38   9 940.5510 917.8371 963.2648\n7  11 11.05  30 38   9 945.2225 921.9971 968.4479\n8  12 11.05  30 38   9 948.2885 926.4576 970.1194\n9  13 11.05  30 38   9 950.2608 930.3003 970.2213\n10 14 11.05  30 38   9 951.6671 932.2370 971.0971\n11 15 11.05  30 38   9 953.0342 932.1662 973.9021\n\nResponse variable (y): y \n\nAdjust to: x6=11.05 x14=30 x1=38 x13=9  \n\nLimits are 0.95 confidence limits\n\nPredict(newmod, x9 = 5:15, conf.type = 'simult')\n\n   x9    x6 x14 x1 x13     yhat    lower    upper\n1   5 11.05  30 38   9 913.7392 882.3582 945.1203\n2   6 11.05  30 38   9 916.3490 884.8623 947.8356\n3   7 11.05  30 38   9 921.3093 892.4061 950.2126\n4   8 11.05  30 38   9 927.6464 900.9846 954.3083\n5   9 11.05  30 38   9 934.3853 907.2082 961.5625\n6  10 11.05  30 38   9 940.5510 911.1688 969.9331\n7  11 11.05  30 38   9 945.2225 915.1786 975.2664\n8  12 11.05  30 38   9 948.2885 920.0485 976.5285\n9  13 11.05  30 38   9 950.2608 924.4403 976.0812\n10 14 11.05  30 38   9 951.6671 926.5328 976.8014\n11 15 11.05  30 38   9 953.0342 926.0398 980.0285\n\nResponse variable (y): y \n\nAdjust to: x6=11.05 x14=30 x1=38 x13=9  \n\nLimits are 0.95 confidence limits\n\n\nThe plot below shows the individual effects in newmod in five subpanels, using the default approach of displaying the same range of values as are seen in the data. Note that each panel shows point and interval estimates of the effects, and spot the straight lines in x1 and x13, the single bends in x14 and x6 and the wiggles in x9, corresponding to the amount of non-linearity specified in the model.\n\nggplot(Predict(newmod))"
  },
  {
    "objectID": "olsfitting.html#checking-influence-via-dfbeta",
    "href": "olsfitting.html#checking-influence-via-dfbeta",
    "title": "14  Using ols to fit linear models",
    "section": "14.6 Checking Influence via dfbeta",
    "text": "14.6 Checking Influence via dfbeta\nFor an ols object, we have several tools for looking at residuals. The most interesting to me is which.influence which is reliant on the notion of dfbeta.\n\nDFBETA is estimated for each observation in the data, and each coefficient in the model.\nThe DFBETA is the difference in the estimated coefficient caused by deleting the observation, scaled by the coefficient’s standard error estimated with the observation deleted.\nThe which.influence command applied to an ols model produces a list of all of the predictors estimated by the model, including the intercept.\n\nFor each predictor, the command lists all observations (by row number) that, if removed from the model, would cause the estimated coefficient (the “beta”) for that predictor to change by at least some particular cutoff.\nThe default is that the DFBETA for that predictor is 0.2 or more.\n\n\n\nwhich.influence(newmod)\n\n$Intercept\n[1]  1  4  7 20 50 55 60\n\n$x9\n[1]  1 15 38 39 50 51 52 53 58\n\n$x6\n[1]  2  4  7 16 20 36 50 55 60\n\n$x14\n[1]  6  7 27 42 50 56 58 60\n\n$x1\n[1]  1  7 10 27 52 60\n\n$x13\n[1]  7  8 60\n\n\nThe implication here, for instance, is that if we drop row 15 from our data frame, and refit the model, this will have a meaningful impact on the estimate of x9 but not on the other coefficients. But if we drop, say, row 60, we will affect the estimates of the intercept, x6, x14, x1, and x13.\n\n14.6.1 Using the residuals command for dfbetas\nTo see the dfbeta values, standardized according to the approach I used above, you can use the following code (I’ll use head to just show the first few rows of results) to get a matrix of the results.\n\nhead(residuals(newmod, type = \"dfbetas\"))\n\n              [,1]          [,2]          [,3]          [,4]          [,5]\n[1,] -0.2142788779  0.2464314961 -0.1638633763  0.1420748132 -0.0837601846\n[2,] -0.1609522082 -0.1651907163  0.0813708161 -0.0571478560 -0.0104199085\n[3,]  0.0002978421  0.0003616715 -0.0003743893  0.0003648207 -0.0003104531\n[4,] -0.6689504955  0.1060165855 -0.0582613403  0.0567350766 -0.0514665792\n[5,] -0.0185374830  0.0941987899 -0.0712140473  0.0651961744 -0.0473578767\n[6,] -0.0733189060  0.0441321361 -0.0775996337  0.0686460863 -0.0310662575\n              [,6]          [,7]          [,8]          [,9]         [,10]\n[1,]  0.0885057227 -0.1236601786  0.1976364429 -0.1948087617  0.4602900927\n[2,]  0.1761716451 -0.2487893025  0.1331560976 -0.1425694930  0.0532802632\n[3,] -0.0001793501  0.0001379045 -0.0002413065  0.0001584916 -0.0008536018\n[4,]  0.6480133104 -0.4398587906  0.1281557908 -0.0762194326 -0.1230959028\n[5,] -0.0027596047 -0.0242159160 -0.0125205909  0.0030015343  0.0456581256\n[6,]  0.0598043946 -0.1228968480  0.2222504825 -0.1842672286  0.0425024727\n             [,11]\n[1,]  1.476010e-01\n[2,]  1.561944e-01\n[3,] -8.851148e-05\n[4,] -9.553782e-02\n[5,]  4.318623e-02\n[6,]  5.491929e-02\n\n\n\n\n14.6.2 Using the residuals command for other summaries\nThe residuals command will also let you get ordinary residuals, leverage values and dffits values, which are the normalized differences in predicted values when observations are omitted. See ?residuals.ols for more details.\n\ntemp <- data.frame(area = 1:60)\ntemp$residual <- residuals(newmod, type = \"ordinary\")\ntemp$leverage <- residuals(newmod, type = \"hat\")\ntemp$dffits <- residuals(newmod, type = \"dffits\")\ntemp <- as_tibble(temp)\n\nggplot(temp, aes(x = area, y = dffits)) +\n    geom_point() +\n    geom_line()\n\n\n\n\nIt appears that point 60 has the largest (positive) dffits value. Recall that point 60 seemed influential on several predictors and the intercept term. Point 7 has the smallest (or largest negative) dffits, and also appears to have been influential on several predictors and the intercept.\n\nwhich.max(temp$dffits)\n\n[1] 60\n\nwhich.min(temp$dffits)\n\n[1] 7"
  },
  {
    "objectID": "olsfitting.html#model-validation-and-correcting-for-optimism",
    "href": "olsfitting.html#model-validation-and-correcting-for-optimism",
    "title": "14  Using ols to fit linear models",
    "section": "14.7 Model Validation and Correcting for Optimism",
    "text": "14.7 Model Validation and Correcting for Optimism\nIn 431, we learned about splitting our regression models into training samples and test samples, performing variable selection work on the training sample to identify two or three candidate models (perhaps via a stepwise approach), and then comparing the predictions made by those models in a test sample.\nAt the final project presentations, I mentioned (to many folks) that there was a way to automate this process a bit in 432, that would provide some ways to get the machine to split the data for you multiple times, and then average over the results, using a bootstrap approach. This is it.\nThe validate function allows us to perform cross-validation of our models for some summary statistics (and then correct those statistics for optimism in describing likely predictive accuracy) in an easy way.\nvalidate develops:\n\nResampling validation with or without backward elimination of variables\nEstimates of the optimism in measures of predictive accuracy\nEstimates of the intercept and slope of a calibration model\n\n\\[\n(\\mbox{observed y}) = Intercept + Slope (\\mbox{predicted y})\n\\]\nwith the following code…\n\nset.seed(432002); validate(newmod, method = \"boot\", B = 40)\n\n          index.orig training      test  optimism index.corrected  n\nR-square      0.6989   0.7589    0.6176    0.1413          0.5576 40\nMSE        1145.7800 899.5893 1454.9828 -555.3935       1701.1735 40\ng            58.9614  60.5120   56.0360    4.4759         54.4855 40\nIntercept     0.0000   0.0000   72.3650  -72.3650         72.3650 40\nSlope         1.0000   1.0000    0.9217    0.0783          0.9217 40\n\n\nSo, for R-square we see that our original estimate was 0.6989\n\nOur estimated R-square across n = 40 training samples was 0.7589, but in the resulting tests, the average R-square was only 0.6176\nThis suggests an optimism of 0.7589 - 0.6176 = 0.1413.\nWe then apply that optimism to obtain a new estimate of \\(R^2\\) corrected for overfitting, at 0.5576, which is probably a better estimate of what our results might look like in new data that were similar to (but not the same as) the data we used in building newmod than our initial estimate of 0.6989\n\nWe also obtain optimism-corrected estimates of the mean squared error (square of the residual standard deviation), the g index, and the intercept and slope of the calibration model. The “corrected” slope is a shrinkage factor that takes overfitting into account."
  },
  {
    "objectID": "olsfitting.html#building-a-nomogram-for-our-model",
    "href": "olsfitting.html#building-a-nomogram-for-our-model",
    "title": "14  Using ols to fit linear models",
    "section": "14.8 Building a Nomogram for Our Model",
    "text": "14.8 Building a Nomogram for Our Model\nAnother nice feature of an ols model object is that we can picture the model with a nomogram easily. Here is model newmod.\n\nplot(nomogram(newmod))\n\n\n\n\nFor this model, we can use this plot to predict y as follows:\n\nfind our values of x9 on the appropriate line\ndraw a vertical line up to the points line to count the points associated with our subject\nrepeat the process to obtain the points associated with x6, x14, x1, and x13. Sum the points.\ndraw a vertical line down from that number in the Total Points line to estimate y (the Linear Predictor) = Age-Adjusted Mortality Rate.\n\nThe impact of the non-linearity is seen in the x6 results, for example, which turn around from 9-10 to 11-12. We also see non-linearity’s effects in the scales of the non-linear terms in terms of points awarded.\nAn area with a combination of predictor values leading to a total of 100 points, for instance, would lead to a prediction of a Mortality Rate near 905. An area with a total of 140 points would have a predicted Mortality Rate of 955, roughly.\n\n\n\n\nHarrell, Frank E. 2001. Regression Modeling Strategies. New York: Springer."
  },
  {
    "objectID": "prostate1.html#r-setup-used-here",
    "href": "prostate1.html#r-setup-used-here",
    "title": "15  A Model for Prostate Cancer",
    "section": "15.1 R Setup Used Here",
    "text": "15.1 R Setup Used Here\n\nknitr::opts_chunk$set(comment = NA)\n\nlibrary(broom)\nlibrary(rms)\nlibrary(tidyverse) \n\ntheme_set(theme_bw())\n\n\n15.1.1 Data Load\n\nprost <- read_csv(\"data/prost.csv\", show_col_types = FALSE)"
  },
  {
    "objectID": "prostate1.html#data-load-and-background",
    "href": "prostate1.html#data-load-and-background",
    "title": "15  A Model for Prostate Cancer",
    "section": "15.2 Data Load and Background",
    "text": "15.2 Data Load and Background\nThe data in prost.csv is derived from Stamey et al. (1989) who examined the relationship between the level of prostate-specific antigen and a number of clinical measures in 97 men who were about to receive a radical prostatectomy. The prost data, as I’ll name it in R, contains 97 rows and 11 columns.\n\nprost\n\n# A tibble: 97 × 10\n   subject   lpsa lcavol lweight   age bph      svi   lcp gleason pgg45\n     <dbl>  <dbl>  <dbl>   <dbl> <dbl> <chr>  <dbl> <dbl> <chr>   <dbl>\n 1       1 -0.431 -0.580    2.77    50 Low        0 -1.39 6           0\n 2       2 -0.163 -0.994    3.32    58 Low        0 -1.39 6           0\n 3       3 -0.163 -0.511    2.69    74 Low        0 -1.39 7          20\n 4       4 -0.163 -1.20     3.28    58 Low        0 -1.39 6           0\n 5       5  0.372  0.751    3.43    62 Low        0 -1.39 6           0\n 6       6  0.765 -1.05     3.23    50 Low        0 -1.39 6           0\n 7       7  0.765  0.737    3.47    64 Medium     0 -1.39 6           0\n 8       8  0.854  0.693    3.54    58 High       0 -1.39 6           0\n 9       9  1.05  -0.777    3.54    47 Low        0 -1.39 6           0\n10      10  1.05   0.223    3.24    63 Low        0 -1.39 6           0\n# … with 87 more rows\n\n\nNote that a related prost data frame is also available as part of several R packages, including the faraway package, but there is an error in the lweight data for subject 32 in those presentations. The value of lweight for subject 32 should not be 6.1, corresponding to a prostate that is 449 grams in size, but instead the lweight value should be 3.804438, corresponding to a 44.9 gram prostate1.\nI’ve also changed the gleason and bph variables from their presentation in other settings, to let me teach some additional details."
  },
  {
    "objectID": "prostate1.html#code-book",
    "href": "prostate1.html#code-book",
    "title": "15  A Model for Prostate Cancer",
    "section": "15.3 Code Book",
    "text": "15.3 Code Book\n\n\n\nVariable\nDescription\n\n\n\n\nsubject\nsubject number (1 to 97)\n\n\nlpsa\nlog(prostate specific antigen in ng/ml), our outcome\n\n\nlcavol\nlog(cancer volume in cm3)\n\n\nlweight\nlog(prostate weight, in g)\n\n\nage\nage\n\n\nbph\nbenign prostatic hyperplasia amount (Low, Medium, or High)\n\n\nsvi\nseminal vesicle invasion (1 = yes, 0 = no)\n\n\nlcp\nlog(capsular penetration, in cm)\n\n\ngleason\ncombined Gleason score (6, 7, or > 7 here)\n\n\npgg45\npercentage Gleason scores 4 or 5\n\n\n\nNotes:\n\nin general, higher levels of PSA are stronger indicators of prostate cancer. An old standard (established almost exclusively with testing in white males, and definitely flawed) suggested that values below 4 were normal, and above 4 needed further testing. A PSA of 4 corresponds to an lpsa of 1.39.\nall logarithms are natural (base e) logarithms, obtained in R with the function log()\nall variables other than subject and lpsa are candidate predictors\nthe gleason variable captures the highest combined Gleason score[^Scores range (in these data) from 6 (a well-differentiated, or low-grade cancer) to 9 (a high-grade cancer), although the maximum possible score is 10. 6 is the lowest score used for cancerous prostates. As this combination value increases, the rate at which the cancer grows and spreads should increase. This score refers to the combined Gleason grade, which is based on the sum of two areas (each scored 1-5) that make up most of the cancer.] in a biopsy, and higher scores indicate more aggressive cancer cells. It’s stored here as 6, 7, or > 7.\nthe pgg45 variable captures the percentage of individual Gleason scores[^The 1-5 scale for individual biopsies are defined so that 1 indicates something that looks like normal prostate tissue, and 5 indicates that the cells and their growth patterns look very abnormal. In this study, the percentage of 4s and 5s shown in the data appears to be based on 5-20 individual scores in most subjects.] that are 4 or 5, on a 1-5 scale, where higher scores indicate more abnormal cells."
  },
  {
    "objectID": "prostate1.html#additions-for-later-use",
    "href": "prostate1.html#additions-for-later-use",
    "title": "15  A Model for Prostate Cancer",
    "section": "15.4 Additions for Later Use",
    "text": "15.4 Additions for Later Use\nThe code below adds to the prost tibble:\n\na factor version of the svi variable, called svi_f, with levels No and Yes,\na factor version of gleason called gleason_f, with the levels ordered > 7, 7, and finally 6,\na factor version of bph called bph_f, with levels ordered Low, Medium, High,\na centered version of lcavol called lcavol_c,\nexponentiated cavol and psa results derived from the natural logarithms lcavol and lpsa.\n\n\nprost <- prost |>\n    mutate(svi_f = fct_recode(factor(svi), \"No\" = \"0\", \"Yes\" = \"1\"),\n           gleason_f = fct_relevel(gleason, c(\"> 7\", \"7\", \"6\")),\n           bph_f = fct_relevel(bph, c(\"Low\", \"Medium\", \"High\")),\n           lcavol_c = lcavol - mean(lcavol),\n           cavol = exp(lcavol),\n           psa = exp(lpsa))\n\nglimpse(prost)\n\nRows: 97\nColumns: 16\n$ subject   <dbl> 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 1…\n$ lpsa      <dbl> -0.4307829, -0.1625189, -0.1625189, -0.1625189, 0.3715636, 0…\n$ lcavol    <dbl> -0.5798185, -0.9942523, -0.5108256, -1.2039728, 0.7514161, -…\n$ lweight   <dbl> 2.769459, 3.319626, 2.691243, 3.282789, 3.432373, 3.228826, …\n$ age       <dbl> 50, 58, 74, 58, 62, 50, 64, 58, 47, 63, 65, 63, 63, 67, 57, …\n$ bph       <chr> \"Low\", \"Low\", \"Low\", \"Low\", \"Low\", \"Low\", \"Medium\", \"High\", …\n$ svi       <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ lcp       <dbl> -1.3862944, -1.3862944, -1.3862944, -1.3862944, -1.3862944, …\n$ gleason   <chr> \"6\", \"6\", \"7\", \"6\", \"6\", \"6\", \"6\", \"6\", \"6\", \"6\", \"6\", \"6\", …\n$ pgg45     <dbl> 0, 0, 20, 0, 0, 0, 0, 0, 0, 0, 0, 0, 30, 5, 5, 0, 30, 0, 0, …\n$ svi_f     <fct> No, No, No, No, No, No, No, No, No, No, No, No, No, No, No, …\n$ gleason_f <fct> 6, 6, 7, 6, 6, 6, 6, 6, 6, 6, 6, 6, 7, 7, 7, 6, 7, 6, 6, 6, …\n$ bph_f     <fct> Low, Low, Low, Low, Low, Low, Medium, High, Low, Low, Low, M…\n$ lcavol_c  <dbl> -1.9298281, -2.3442619, -1.8608352, -2.5539824, -0.5985935, …\n$ cavol     <dbl> 0.56, 0.37, 0.60, 0.30, 2.12, 0.35, 2.09, 2.00, 0.46, 1.25, …\n$ psa       <dbl> 0.65, 0.85, 0.85, 0.85, 1.45, 2.15, 2.15, 2.35, 2.85, 2.85, …"
  },
  {
    "objectID": "prostate1.html#fitting-and-evaluating-a-two-predictor-model",
    "href": "prostate1.html#fitting-and-evaluating-a-two-predictor-model",
    "title": "15  A Model for Prostate Cancer",
    "section": "15.5 Fitting and Evaluating a Two-Predictor Model",
    "text": "15.5 Fitting and Evaluating a Two-Predictor Model\nTo begin, let’s use two predictors (lcavol and svi) and their interaction in a linear regression model that predicts lpsa. I’ll call this model prost_A\nEarlier, we centered the lcavol values to facilitate interpretation of the terms. I’ll use that centered version (called lcavol_c) of the quantitative predictor, and the 1/0 version of the svi variable[^We could certainly use the factor version of svi here, but it won’t change the model in any meaningful way. There’s no distinction in model fitting via lm between a 0/1 numeric variable and a No/Yes factor variable. The factor version of this information will be useful elsewhere, for instance in plotting the model.].\n\nprost_A <- lm(lpsa ~ lcavol_c * svi, data = prost)\nsummary(prost_A)\n\n\nCall:\nlm(formula = lpsa ~ lcavol_c * svi, data = prost)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-1.6305 -0.5007  0.1266  0.4886  1.6847 \n\nCoefficients:\n             Estimate Std. Error t value Pr(>|t|)    \n(Intercept)   2.33134    0.09128  25.540  < 2e-16 ***\nlcavol_c      0.58640    0.08207   7.145 1.98e-10 ***\nsvi           0.60132    0.35833   1.678   0.0967 .  \nlcavol_c:svi  0.06479    0.26614   0.243   0.8082    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.7595 on 93 degrees of freedom\nMultiple R-squared:  0.5806,    Adjusted R-squared:  0.5671 \nF-statistic: 42.92 on 3 and 93 DF,  p-value: < 2.2e-16\n\n\n\n15.5.1 Using tidy\nIt can be very useful to build a data frame of the model’s results. We can use the tidy function in the broom package to do so.\n\ntidy(prost_A)\n\n# A tibble: 4 × 5\n  term         estimate std.error statistic  p.value\n  <chr>           <dbl>     <dbl>     <dbl>    <dbl>\n1 (Intercept)    2.33      0.0913    25.5   8.25e-44\n2 lcavol_c       0.586     0.0821     7.15  1.98e-10\n3 svi            0.601     0.358      1.68  9.67e- 2\n4 lcavol_c:svi   0.0648    0.266      0.243 8.08e- 1\n\n\nThis makes it much easier to pull out individual elements of the model fit.\nFor example, to specify the coefficient for svi, rounded to three decimal places, I could use\ntidy(prost_A) |> filter(term == \"svi\") |> select(estimate) |> round(3)\n\nThe result is 0.601.\nIf you look at the Markdown file, you’ll see that the number shown in the bullet point above this one was generated using inline R code, and the function specified above.\n\n\n\n15.5.2 Interpretation\n\nThe intercept, 2.33, for the model is the predicted value of lpsa when lcavol is at its average and there is no seminal vesicle invasion (e.g. svi = 0).\nThe coefficient for lcavol_c, 0.59, is the predicted change in lpsa associated with a one unit increase in lcavol (or lcavol_c) when there is no seminal vesicle invasion.\nThe coefficient for svi, 0.6, is the predicted change in lpsa associated with having no svi to having an svi while the lcavol remains at its average.\nThe coefficient for lcavol_c:svi, the product term, which is 0.06, is the difference in the slope of lcavol_c for a subject with svi as compared to one with no svi."
  },
  {
    "objectID": "prostate1.html#exploring-model-prost_a",
    "href": "prostate1.html#exploring-model-prost_a",
    "title": "15  A Model for Prostate Cancer",
    "section": "15.6 Exploring Model prost_A",
    "text": "15.6 Exploring Model prost_A\nThe glance function from the broom package builds a nice one-row summary for the model.\n\nglance(prost_A)\n\n# A tibble: 1 × 12\n  r.squared adj.r.squa…¹ sigma stati…²  p.value    df logLik   AIC   BIC devia…³\n      <dbl>        <dbl> <dbl>   <dbl>    <dbl> <dbl>  <dbl> <dbl> <dbl>   <dbl>\n1     0.581        0.567 0.759    42.9 1.68e-17     3  -109.  228.  241.    53.6\n# … with 2 more variables: df.residual <int>, nobs <int>, and abbreviated\n#   variable names ¹​adj.r.squared, ²​statistic, ³​deviance\n\n\nThis summary includes, in order,\n\nthe model \\(R^2\\), adjusted \\(R^2\\) and \\(\\hat{\\sigma}\\), the residual standard deviation,\nthe ANOVA F statistic and associated p value,\nthe number of degrees of freedom used by the model, and its log-likelihood ratio\nthe model’s AIC (Akaike Information Criterion) and BIC (Bayesian Information Criterion)\nthe model’s deviance statistic and residual degrees of freedom\n\n\n15.6.1 summary for Model prost_A\nIf necessary, we can also run summary on this prost_A object to pick up some additional summaries. Since the svi variable is binary, the interaction term is, too, so the t test here and the F test in the ANOVA yield the same result.\n\nsummary(prost_A)\n\n\nCall:\nlm(formula = lpsa ~ lcavol_c * svi, data = prost)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-1.6305 -0.5007  0.1266  0.4886  1.6847 \n\nCoefficients:\n             Estimate Std. Error t value Pr(>|t|)    \n(Intercept)   2.33134    0.09128  25.540  < 2e-16 ***\nlcavol_c      0.58640    0.08207   7.145 1.98e-10 ***\nsvi           0.60132    0.35833   1.678   0.0967 .  \nlcavol_c:svi  0.06479    0.26614   0.243   0.8082    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.7595 on 93 degrees of freedom\nMultiple R-squared:  0.5806,    Adjusted R-squared:  0.5671 \nF-statistic: 42.92 on 3 and 93 DF,  p-value: < 2.2e-16\n\n\nIf you’ve forgotten the details of the pieces of this summary, review the Part C Notes from 431.\n\n\n15.6.2 Adjusted \\(R^2\\)\n\\(R^2\\) is greedy.\n\n\\(R^2\\) will always suggest that we make our models as big as possible, often including variables of dubious predictive value.\nAs a result, there are various methods for penalizing \\(R^2\\) so that we wind up with smaller models.\nThe adjusted \\(R^2\\) is often a useful way to compare multiple models for the same response.\n\n\\(R^2_{adj} = 1 - \\frac{(1-R^2)(n - 1)}{n - k}\\), where \\(n\\) = the number of observations and \\(k\\) is the number of coefficients estimated by the regression (including the intercept and any slopes).\nSo, in this case, \\(R^2_{adj} = 1 - \\frac{(1 - 0.5806)(97 - 1)}{97 - 4} = 0.5671\\)\nThe adjusted \\(R^2\\) value is not, technically, a proportion of anything, but it is comparable across models for the same outcome.\nThe adjusted \\(R^2\\) will always be less than the (unadjusted) \\(R^2\\).\n\n\n\n\n15.6.3 Coefficient Confidence Intervals\nHere are the 90% confidence intervals for the coefficients in Model A. Adjust the level to get different intervals.\n\nconfint(prost_A, level = 0.90)\n\n                     5 %      95 %\n(Intercept)   2.17968697 2.4830012\nlcavol_c      0.45004577 0.7227462\nsvi           0.00599401 1.1966454\nlcavol_c:svi -0.37737623 0.5069622\n\n\nWhat can we conclude from this about the utility of the interaction term?\n\n\n15.6.4 ANOVA for Model prost_A\nThe interaction term appears unnecessary. We might wind up fitting the model without it. A complete ANOVA test is available, including a p value, if you want it.\n\nanova(prost_A)\n\nAnalysis of Variance Table\n\nResponse: lpsa\n             Df Sum Sq Mean Sq  F value    Pr(>F)    \nlcavol_c      1 69.003  69.003 119.6289 < 2.2e-16 ***\nsvi           1  5.237   5.237   9.0801  0.003329 ** \nlcavol_c:svi  1  0.034   0.034   0.0593  0.808191    \nResiduals    93 53.643   0.577                       \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nNote that the anova approach for a lm object is sequential. The first row shows the impact of lcavol_c as compared to a model with no predictors (just an intercept). The second row shows the impact of adding svi to a model that already contains lcavol_c. The third row shows the impact of adding the interaction (product) term to the model with the two main effects. So the order in which the variables are added to the regression model matters for this ANOVA. The F tests here describe the incremental impact of each covariate in turn.\n\n\n15.6.5 Residuals, Fitted Values and Standard Errors with augment\nThe augment function in the broom package builds a data frame including the data used in the model, along with predictions (fitted values), residuals and other useful information.\n\nprost_A_aug <- augment(prost_A)\nsummary(prost_A_aug)\n\n      lpsa            lcavol_c             svi            .fitted      \n Min.   :-0.4308   Min.   :-2.69708   Min.   :0.0000   Min.   :0.7498  \n 1st Qu.: 1.7317   1st Qu.:-0.83719   1st Qu.:0.0000   1st Qu.:1.8404  \n Median : 2.5915   Median : 0.09691   Median :0.0000   Median :2.3950  \n Mean   : 2.4784   Mean   : 0.00000   Mean   :0.2165   Mean   :2.4784  \n 3rd Qu.: 3.0564   3rd Qu.: 0.77703   3rd Qu.:0.0000   3rd Qu.:3.0709  \n Max.   : 5.5829   Max.   : 2.47099   Max.   :1.0000   Max.   :4.5417  \n     .resid             .hat             .sigma          .cooksd         \n Min.   :-1.6305   Min.   :0.01316   Min.   :0.7423   Min.   :0.0000069  \n 1st Qu.:-0.5007   1st Qu.:0.01562   1st Qu.:0.7569   1st Qu.:0.0007837  \n Median : 0.1266   Median :0.02498   Median :0.7617   Median :0.0034699  \n Mean   : 0.0000   Mean   :0.04124   Mean   :0.7595   Mean   :0.0111314  \n 3rd Qu.: 0.4886   3rd Qu.:0.04939   3rd Qu.:0.7631   3rd Qu.:0.0103533  \n Max.   : 1.6847   Max.   :0.24627   Max.   :0.7636   Max.   :0.1341093  \n   .std.resid       \n Min.   :-2.194508  \n 1st Qu.:-0.687945  \n Median : 0.168917  \n Mean   : 0.001249  \n 3rd Qu.: 0.653612  \n Max.   : 2.261830  \n\n\nElements shown here include:\n\n.fitted Fitted values of model (or predicted values)\n.se.fit Standard errors of fitted values\n.resid Residuals (observed - fitted values)\n.hat Diagonal of the hat matrix (these indicate leverage - points with high leverage indicate unusual combinations of predictors - values more than 2-3 times the mean leverage are worth some study - leverage is always between 0 and 1, and measures the amount by which the predicted value would change if the observation’s y value was increased by one unit - a point with leverage 1 would cause the line to follow that point perfectly)\n.sigma Estimate of residual standard deviation when corresponding observation is dropped from model\n.cooksd Cook’s distance, which helps identify influential points (values of Cook’s d > 0.5 may be influential, values > 1.0 almost certainly are - an influential point changes the fit substantially when it is removed from the data)\n.std.resid Standardized residuals (values above 2 in absolute value are worth some study - treat these as normal deviates [Z scores], essentially)\n\nSee ?augment.lm in R for more details.\n\n\n15.6.6 Making Predictions with prost_A\nSuppose we want to predict the lpsa for a patient with cancer volume equal to this group’s mean, for both a patient with and without seminal vesicle invasion, and in each case, we want to use a 90% prediction interval?\n\nnewdata <- data.frame(lcavol_c = c(0,0), svi = c(0,1))\npredict(prost_A, newdata, interval = \"prediction\", level = 0.90)\n\n       fit      lwr      upr\n1 2.331344 1.060462 3.602226\n2 2.932664 1.545742 4.319586\n\n\nSince the predicted value in fit refers to the natural logarithm of PSA, to make the predictions in terms of PSA, we would need to exponentiate. The code below will accomplish that task.\n\npred <- predict(prost_A, newdata, interval = \"prediction\", level = 0.90)\nexp(pred)\n\n       fit      lwr      upr\n1 10.29177 2.887706 36.67978\n2 18.77758 4.691450 75.15750"
  },
  {
    "objectID": "prostate1.html#plotting-model-prost_a",
    "href": "prostate1.html#plotting-model-prost_a",
    "title": "15  A Model for Prostate Cancer",
    "section": "15.7 Plotting Model prost_A",
    "text": "15.7 Plotting Model prost_A\n\n15.7.0.1 Plot logs conventionally\nHere, we’ll use ggplot2 to plot the logarithms of the variables as they came to us, on a conventional coordinate scale. Note that the lines are nearly parallel. What does this suggest about our Model A?\n\nggplot(prost, aes(x = lcavol, y = lpsa, group = svi_f, color = svi_f)) +\n    geom_point() +\n    geom_smooth(method = \"lm\", formula = y ~ x, se = FALSE) + \n    scale_color_discrete(name = \"Seminal Vesicle Invasion?\") +\n    theme_bw() +\n    labs(x = \"Log (cancer volume, cc)\", \n         y = \"Log (Prostate Specific Antigen, ng/ml)\", \n         title = \"Two Predictor Model prost_A, including Interaction\")\n\n\n\n\n\n\n15.7.0.2 Plot on log-log scale\nAnother approach (which might be easier in some settings) would be to plot the raw values of Cancer Volume and PSA, but use logarithmic axes, again using the natural (base e) logarithm, as follows. If we use the default choice with `trans = “log”, we’ll find a need to select some useful break points for the grid, as I’ve done in what follows.\n\nggplot(prost, aes(x = cavol, y = psa, group = svi_f, color = svi_f)) +\n    geom_point() +\n    geom_smooth(method = \"lm\", formula = y ~ x, se = FALSE) + \n    scale_color_discrete(name = \"Seminal Vesicle Invasion?\") +\n    scale_x_continuous(trans = \"log\", \n                       breaks = c(0.5, 1, 2, 5, 10, 25, 50)) +\n    scale_y_continuous(trans = \"log\", \n                       breaks = c(1, 2, 4, 10, 25, 50, 100, 200)) +\n    theme_bw() +\n    labs(x = \"Cancer volume, in cubic centimeters\", \n         y = \"Prostate Specific Antigen, in ng/ml\", \n         title = \"Two Predictor Model prost_A, including Interaction\")\n\n\n\n\nI’ve used the break point of 4 on the Y axis because of the old rule suggesting further testing for asymptomatic men with PSA of 4 or higher, but the other break points are arbitrary - they seemed to work for me, and used round numbers.\n\n\n15.7.1 Residual Plots of prost_A\n\nplot(prost_A, which = 1)\n\n\n\n\n\nplot(prost_A, which = 5)\n\n\n\n\nIn our next Chapter, we’ll see how well this model can be validated.\n\n\n\n\nStamey, J. N. Kabalin, T. A. et al. 1989. “Prostate Specific Antigen in the Diagnosis and Treatment of Adenocarcinoma of the Prostate: II. Radical Prostatectomy Treated Patients.” Journal of Urology 141(5): 1076–83. https://www.ncbi.nlm.nih.gov/pubmed/2468795."
  },
  {
    "objectID": "prostate2.html#r-setup-used-here",
    "href": "prostate2.html#r-setup-used-here",
    "title": "16  Validating our Prostate Cancer Model",
    "section": "16.1 R Setup Used Here",
    "text": "16.1 R Setup Used Here\n\nknitr::opts_chunk$set(comment = NA)\n\nlibrary(broom)\nlibrary(rsample)\nlibrary(yardstick)\nlibrary(caret)\nlibrary(tidyverse) \n\ntheme_set(theme_bw())\n\n\n16.1.1 Data Load\n\nprost <- read_csv(\"data/prost.csv\", show_col_types = FALSE) \n\nWe’ll repeat the data cleaning and model-fitting from our previous chapter."
  },
  {
    "objectID": "prostate2.html#data-cleaning",
    "href": "prostate2.html#data-cleaning",
    "title": "16  Validating our Prostate Cancer Model",
    "section": "16.2 Data Cleaning",
    "text": "16.2 Data Cleaning\n\nprost <- prost |>\n    mutate(svi_f = fct_recode(factor(svi), \"No\" = \"0\", \"Yes\" = \"1\"),\n           gleason_f = fct_relevel(gleason, c(\"> 7\", \"7\", \"6\")),\n           bph_f = fct_relevel(bph, c(\"Low\", \"Medium\", \"High\")),\n           lcavol_c = lcavol - mean(lcavol),\n           cavol = exp(lcavol),\n           psa = exp(lpsa))"
  },
  {
    "objectID": "prostate2.html#fitting-the-prosta-model",
    "href": "prostate2.html#fitting-the-prosta-model",
    "title": "16  Validating our Prostate Cancer Model",
    "section": "16.3 Fitting the prostA model",
    "text": "16.3 Fitting the prostA model\n\nprost_A <- lm(lpsa ~ lcavol_c * svi, data = prost)"
  },
  {
    "objectID": "prostate2.html#split-validation-of-model-prost_a",
    "href": "prostate2.html#split-validation-of-model-prost_a",
    "title": "16  Validating our Prostate Cancer Model",
    "section": "16.4 Split Validation of Model prost_A",
    "text": "16.4 Split Validation of Model prost_A\nSuppose we want to evaluate whether our model prost_A predicts effectively in new data.\nWe’ll first demonstrate a validation split approach (used, for instance, in 431) which splits our sample into a separate training (perhaps 70% of the data) and test (perhaps 30% of the data) samples, and then:\n\nfit the model in the training sample,\nuse the resulting model to make predictions for lpsa in the test sample, and\nevaluate the quality of those predictions, perhaps by comparing the results to what we’d get using a different model.\n\nOur goal will be to cross-validate model prost_A, which, you’ll recall, uses lcavol_c, svi and their interaction, to predict lpsa in the prost data.\nWe’ll start by identifying a random sample of 70% of our prost data in a training sample (which we’ll call prost_train, and leave the rest as our test sample, called prost_test. To do this, we’ll use functions from the rsample package.\n\nset.seed(432432)\n\nprost_split <- initial_split(prost, prop = 0.7)\n\nprost_train <- training(prost_split)\nprost_test <- testing(prost_split)\n\n\nDon’t forget to pre-specify the random seed, for replicability, as I’ve done here.\n\nLet’s verify that we now have the samples we expect…\n\ndim(prost_train)\n\n[1] 67 16\n\ndim(prost_test)\n\n[1] 30 16\n\n\nOK. Next, we’ll run the prost_A model in the training sample.\n\nprost_A_train <- lm(lpsa ~ lcavol_c * svi, data = prost_train)\n\nprost_A_train\n\n\nCall:\nlm(formula = lpsa ~ lcavol_c * svi, data = prost_train)\n\nCoefficients:\n (Intercept)      lcavol_c           svi  lcavol_c:svi  \n      2.2900        0.6922        1.1317       -0.4269  \n\n\nThen we’ll use the coefficients from this model to obtain predicted lpsa values in the test sample.\n\nprost_A_test_aug <- augment(prost_A, newdata = prost_test)\n\nNow, we can use the functions from the yardstick package to obtain several key summaries of fit quality for our model. These summary statistics are:\n\nthe RMSE or root mean squared error, which measures the average difference (i.e. prediction error) between the observed known outcome values and the values predicted by the model by first squaring all of the errors, averaging them, and then taking the square root of the result. The lower the RMSE, the better the model.\nthe Rsquared or \\(R^2\\), which is just the square of the Pearson correlation coefficient relating the predicted and observed values, so we’d like this to be as large as possible, and\nthe MAE or mean absolute error, which is a bit less sensitive to outliers than the RMSE, because it measures the average prediction error by taking the absolute value of each error, and then grabbing the average of those values. The lower the MAE, the better the model.\n\nThese statistics are more helpful, generally, for comparing multiple models to each other, than for making final decisions on their own. The yardstick package provides individual functions to summarize performance, as follows.\n\nrmse(data = prost_A_test_aug, truth = lpsa, estimate = .fitted)\n\n# A tibble: 1 × 3\n  .metric .estimator .estimate\n  <chr>   <chr>          <dbl>\n1 rmse    standard       0.813\n\nrsq(data = prost_A_test_aug, truth = lpsa, estimate = .fitted)\n\n# A tibble: 1 × 3\n  .metric .estimator .estimate\n  <chr>   <chr>          <dbl>\n1 rsq     standard       0.515\n\nmae(data = prost_A_test_aug, truth = lpsa, estimate = .fitted)\n\n# A tibble: 1 × 3\n  .metric .estimator .estimate\n  <chr>   <chr>          <dbl>\n1 mae     standard       0.672"
  },
  {
    "objectID": "prostate2.html#v-fold-cross-validation-approach-for-model-prosta",
    "href": "prostate2.html#v-fold-cross-validation-approach-for-model-prosta",
    "title": "16  Validating our Prostate Cancer Model",
    "section": "16.5 V-fold Cross-Validation Approach for model prostA",
    "text": "16.5 V-fold Cross-Validation Approach for model prostA\n\nV-fold cross-validation (also known as k-fold cross-validation) randomly splits the data into V groups of roughly equal size (called “folds”). A resample of the analysis data consists of V-1 of the folds while the assessment set contains the final fold. In basic V-fold cross-validation (i.e. no repeats), the number of resamples is equal to V.\n\n\nhttps://rsample.tidymodels.org/reference/vfold_cv.html\n\nThe idea of, for instance, 5-fold cross-validation in this case is to create five different subgroups (or folds) of the data, and then select 4 of the folds to be used as a model training sample, leaving the remaining fold as the model testing sample. We then repeat this over each of the five possible selections of testing sample, and summarize the results. This is very straightforward using the caret package, so we’ll demonstrate that approach here.\nFirst, we use the trainControl() function from caret to set up five-fold cross-validation.\n\nset.seed(432432)\nctrl <- trainControl(method = \"cv\", number = 5)\n\nNext, we train our model on these five folds:\n\npros_model <- train(lpsa ~ lcavol_c * svi, data = prost, \n               method = \"lm\", trControl = ctrl)\n\nNow, we can view a summary of the k-fold cross-validation\n\npros_model\n\nLinear Regression \n\n97 samples\n 2 predictor\n\nNo pre-processing\nResampling: Cross-Validated (5 fold) \nSummary of sample sizes: 79, 78, 77, 77, 77 \nResampling results:\n\n  RMSE       Rsquared   MAE      \n  0.7777655  0.5946201  0.6411997\n\nTuning parameter 'intercept' was held constant at a value of TRUE\n\n\n\nNo pre-processing means we didn’t scale the data before fitting models.\nWe used 5-fold cross-validation\nThe sample size for these training sets was between 77 and 79 for each pass.\nThe validated root mean squared error (averaged across the five resamplings) was 0.7778\nThe cross-validated R-squared is 0.595\nThe cross-validated mean absolute error is 0.641\n\nTo examine the final fitted model, we have:\n\npros_model$finalModel\n\n\nCall:\nlm(formula = .outcome ~ ., data = dat)\n\nCoefficients:\n   (Intercept)        lcavol_c             svi  `lcavol_c:svi`  \n       2.33134         0.58640         0.60132         0.06479  \n\n\nThis model can be presented using all of our usual tools from the broom package.\n\ntidy(pros_model$finalModel)\n\n# A tibble: 4 × 5\n  term           estimate std.error statistic  p.value\n  <chr>             <dbl>     <dbl>     <dbl>    <dbl>\n1 (Intercept)      2.33      0.0913    25.5   8.25e-44\n2 lcavol_c         0.586     0.0821     7.15  1.98e-10\n3 svi              0.601     0.358      1.68  9.67e- 2\n4 `lcavol_c:svi`   0.0648    0.266      0.243 8.08e- 1\n\n\n\nglance(pros_model$finalModel)\n\n# A tibble: 1 × 12\n  r.squared adj.r.squa…¹ sigma stati…²  p.value    df logLik   AIC   BIC devia…³\n      <dbl>        <dbl> <dbl>   <dbl>    <dbl> <dbl>  <dbl> <dbl> <dbl>   <dbl>\n1     0.581        0.567 0.759    42.9 1.68e-17     3  -109.  228.  241.    53.6\n# … with 2 more variables: df.residual <int>, nobs <int>, and abbreviated\n#   variable names ¹​adj.r.squared, ²​statistic, ³​deviance\n\n\nWe can also review the model predictions made within each fold:\n\npros_model$resample\n\n       RMSE  Rsquared       MAE Resample\n1 0.7447413 0.7030717 0.6124787    Fold1\n2 0.8208461 0.3071658 0.6699472    Fold2\n3 0.6510721 0.7944721 0.5589196    Fold3\n4 0.8514246 0.4731454 0.6885641    Fold4\n5 0.8207437 0.6952452 0.6760887    Fold5"
  },
  {
    "objectID": "multiple_imp.html#r-setup-used-here",
    "href": "multiple_imp.html#r-setup-used-here",
    "title": "17  Multiple Imputation and Linear Regression",
    "section": "17.1 R Setup Used Here",
    "text": "17.1 R Setup Used Here\n\nknitr::opts_chunk$set(comment = NA)\n\nlibrary(janitor)\nlibrary(broom)\nlibrary(car)\nlibrary(knitr)\nlibrary(mosaic)\nlibrary(mice)\nlibrary(rms)\nlibrary(naniar)\nlibrary(tidyverse) \n\ntheme_set(theme_bw())"
  },
  {
    "objectID": "multiple_imp.html#data-load",
    "href": "multiple_imp.html#data-load",
    "title": "17  Multiple Imputation and Linear Regression",
    "section": "17.2 Data Load",
    "text": "17.2 Data Load\nIn this chapter, we’ll return to the smart_ohio file based on data from BRFSS 2017 that we built and cleaned back in Chapter 6.\n\nsmart_ohio <- readRDS(\"data/smart_ohio.Rds\")"
  },
  {
    "objectID": "multiple_imp.html#developing-a-smart_16-data-set",
    "href": "multiple_imp.html#developing-a-smart_16-data-set",
    "title": "17  Multiple Imputation and Linear Regression",
    "section": "17.3 Developing a smart_16 data set",
    "text": "17.3 Developing a smart_16 data set\nWe’re going to look at a selection of variables from this tibble, among subjects who have been told they have diabetes, and who also provided a response to our physhealth (Number of Days Physical Health Not Good) variable, which asks “Now thinking about your physical health, which includes physical illness and injury, for how many days during the past 30 days was your physical health not good?” We’ll build two models. In this chapter, we’ll look at a linear model for physhealth and in the next chapter, we’ll look at a logistic regression describing whether or not the subject’s physhealth response was at least 1.\n\nsmart_16 <- smart_ohio |>\n    filter(dm_status == \"Diabetes\") |>\n    filter(complete.cases(physhealth)) |>\n    mutate(bad_phys = ifelse(physhealth > 0, 1, 0),\n           comor = hx_mi + hx_chd + hx_stroke + hx_asthma +\n               hx_skinc + hx_otherc + hx_copd + hx_arthr) |>\n    select(SEQNO, mmsa, physhealth, bad_phys, age_imp, smoke100,\n           comor, hx_depress, bmi, activity)\n\nThe variables included in this smart_16 tibble are:\n\n\n\n\n\n\n\nVariable\nDescription\n\n\n\n\nSEQNO\nrespondent identification number (all begin with 2016)\n\n\nmmsa\n\n\n\nphyshealth\nNow thinking about your physical health, which includes physical illness and injury, for how many days during the past 30 days was your physical health not good?\n\n\nbad_phys\nIs physhealth 1 or more?\n\n\nage_imp\nAge in years (imputed from age categories)\n\n\nsmoke100\nHave you smoked at least 100 cigarettes in your life? (1 = yes, 0 = no)\n\n\nhx_depress\nHas a doctor, nurse, or other health professional ever told you that you have a depressive disorder, including depression, major depression, dysthymia, or minor depression?\n\n\nbmi\nBody mass index, in kg/m2\n\n\nactivity\nPhysical activity (Highly Active, Active, Insufficiently Active, Inactive)\n\n\ncomor\nSum of 8 potential groups of comorbidities (see below)\n\n\n\nThe comor variable is the sum of the following 8 variables, each of which is measured on a 1 = Yes, 0 = No scale, and begin with “Has a doctor, nurse, or other health professional ever told you that you had …”\n\nhx_mi: a heart attack, also called a myocardial infarction?\nhx_chd: angina or coronary heart disease?\nhx_stroke: a stroke?\nhx_asthma: asthma?\nhx_skinc: skin cancer?\nhx_otherc: any other types of cancer?\nhx_copd: Chronic Obstructive Pulmonary Disease or COPD, emphysema or chronic bronchitis?\nhx_arthr: some form of arthritis, rheumatoid arthritis, gout, lupus, or fibromyalgia?\n\n\nsmart_16 |> tabyl(comor)\n\n comor   n     percent valid_percent\n     0 224 0.211920530   0.221782178\n     1 315 0.298013245   0.311881188\n     2 228 0.215704825   0.225742574\n     3 130 0.122989593   0.128712871\n     4  72 0.068117313   0.071287129\n     5  29 0.027436140   0.028712871\n     6   9 0.008514664   0.008910891\n     7   3 0.002838221   0.002970297\n    NA  47 0.044465468            NA\n\n\n\n17.3.1 Any missing values?\nWe have 1057 observations (rows) in the smart_16 data set, of whom 860 have complete data on all variables.\n\ndim(smart_16)\n\n[1] 1057   10\n\nn_case_complete(smart_16)\n\n[1] 860\n\n\nWhich variables are missing?\n\nmiss_var_summary(smart_16)\n\n# A tibble: 10 × 3\n   variable   n_miss pct_miss\n   <chr>       <int>    <dbl>\n 1 activity       85    8.04 \n 2 bmi            84    7.95 \n 3 comor          47    4.45 \n 4 smoke100       24    2.27 \n 5 age_imp        12    1.14 \n 6 hx_depress      3    0.284\n 7 SEQNO           0    0    \n 8 mmsa            0    0    \n 9 physhealth      0    0    \n10 bad_phys        0    0    \n\n\nNote that our outcomes (physhealth and the derived bad_phys) have no missing values here, by design. We will be performing multiple imputation to account appropriately for missingness in the predictors with missing values."
  },
  {
    "objectID": "multiple_imp.html#obtaining-a-simple-imputation-with-mice",
    "href": "multiple_imp.html#obtaining-a-simple-imputation-with-mice",
    "title": "17  Multiple Imputation and Linear Regression",
    "section": "17.4 Obtaining a Simple Imputation with mice",
    "text": "17.4 Obtaining a Simple Imputation with mice\nThe mice package provides several approaches we can use for imputation in building models of all kinds. Here, we’ll use it just to obtain a single set of imputed results that we can apply to “complete” our data for the purposes of thinking about (a) transforming our outcome and (b) considering the addition of non-linear predictor terms.\n\n# requires library(mice)\n\nset.seed(432)\n\n# create small data set including only variables to\n# be used in building the imputation model\n\nsm16 <- smart_16 |> \n    select(physhealth, activity, age_imp, bmi, comor, \n           hx_depress, smoke100)\n\nsmart_16_mice1 <- mice(sm16, m = 1)\n\n\n iter imp variable\n  1   1  activity  age_imp  bmi  comor  hx_depress  smoke100\n  2   1  activity  age_imp  bmi  comor  hx_depress  smoke100\n  3   1  activity  age_imp  bmi  comor  hx_depress  smoke100\n  4   1  activity  age_imp  bmi  comor  hx_depress  smoke100\n  5   1  activity  age_imp  bmi  comor  hx_depress  smoke100\n\nsmart_16_imp1 <- mice::complete(smart_16_mice1)\n\nn_case_miss(smart_16_imp1)\n\n[1] 0\n\n\nAnd now we’ll use this completed smart_16_imp1 data set (the product of just a single imputation) to help us address the next two issues."
  },
  {
    "objectID": "multiple_imp.html#linear-regression-considering-a-transformation-of-the-outcome",
    "href": "multiple_imp.html#linear-regression-considering-a-transformation-of-the-outcome",
    "title": "17  Multiple Imputation and Linear Regression",
    "section": "17.5 Linear Regression: Considering a Transformation of the Outcome",
    "text": "17.5 Linear Regression: Considering a Transformation of the Outcome\nA plausible strategy here would be to try to identify an outcome transformation only after some accounting for missing predictor values, perhaps through a simple imputation approach. However, to keep things simple here, I’ll just use the complete cases in this section.\nRecall that our outcome here, physhealth can take the value 0, and is thus not strictly positive.\n\nfavstats(~ physhealth, data = smart_16_imp1)\n\n min Q1 median Q3 max     mean       sd    n missing\n   0  0      2 20  30 9.227058 11.92676 1057       0\n\n\nSo, if we want to investigate a potential transformation with a Box-Cox plot, we’ll have to add a small value to each physhealth value. We’ll add 1, so that the range of potential values is now from 1-31.\n\nsmart_16_imp1 <- smart_16_imp1 |>\n  mutate(phplus1 = physhealth + 1)\n\ntest_model <- lm(phplus1 ~ age_imp + comor + smoke100 + \n                   hx_depress + bmi + activity, data = smart_16_imp1)\n\nboxCox(test_model)\n\n\n\n\nIt looks like the logarithm is a reasonable transformation in this setting. So we’ll create a new outcome, that is the natural logarithm of (physhealth + 1), which we’ll call phys_tr to remind us that a transformation is involved that we’ll eventually need to back out of to make predictions. We’ll build this new variable in both our original smart_16 data set and in the simply imputed data set we’re using for just these early stages.\n\nsmart_16_imp1 <- smart_16_imp1 |>\n    mutate(phys_tr = log(physhealth + 1))\n\nsmart_16 <- smart_16 |>\n    mutate(phys_tr = log(physhealth + 1))\n\nSo we have phys_tr = log(physhealth + 1)\n\nwhere we are referring above to the natural (base \\(e\\) logarithm).\n\nWe can also specify our back-transformation to the original physhealth values from our new phys_tr as physhealth = exp(phys_tr) - 1."
  },
  {
    "objectID": "multiple_imp.html#linear-regression-considering-non-linearity-in-the-predictors",
    "href": "multiple_imp.html#linear-regression-considering-non-linearity-in-the-predictors",
    "title": "17  Multiple Imputation and Linear Regression",
    "section": "17.6 Linear Regression: Considering Non-Linearity in the Predictors",
    "text": "17.6 Linear Regression: Considering Non-Linearity in the Predictors\nConsider the following Spearman \\(\\rho^2\\) plot.\n\nplot(spearman2(phys_tr ~ age_imp + comor + smoke100 + \n           hx_depress + bmi + activity, data = smart_16_imp1))\n\n\n\n\nAfter our single imputation, we have the same N value in all rows of this plot, which is what we want to see. It appears that in considering potential non-linear terms, comor and hx_depress and perhaps activity are worthy of increased attention. I’ll make a couple of arbitrary choices, to add a raw cubic polynomial to represent the comor information, and we’ll add an interaction term between hx_depress and activity."
  },
  {
    "objectID": "multiple_imp.html#main-effects-linear-regression-with-lm-on-the-complete-cases",
    "href": "multiple_imp.html#main-effects-linear-regression-with-lm-on-the-complete-cases",
    "title": "17  Multiple Imputation and Linear Regression",
    "section": "17.7 “Main Effects” Linear Regression with lm on the Complete Cases",
    "text": "17.7 “Main Effects” Linear Regression with lm on the Complete Cases\nRecall that we have 860 complete cases in our smart_16 data, out of a total of 1057 observations in total. A model using only the complete cases should thus drop the remaining 197 subjects. Let’s see if a main effects only model for our newly transformed phys_tr outcome does in fact do this.\n\nm_1cc <- \n    lm(phys_tr ~ age_imp + comor + smoke100 + \n           hx_depress + bmi + activity, data = smart_16)\n\nsummary(m_1cc)\n\n\nCall:\nlm(formula = phys_tr ~ age_imp + comor + smoke100 + hx_depress + \n    bmi + activity, data = smart_16)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-3.0801 -1.0389 -0.2918  1.1029  2.8478 \n\nCoefficients:\n                               Estimate Std. Error t value Pr(>|t|)    \n(Intercept)                    0.581959   0.370847   1.569  0.11696    \nage_imp                       -0.007043   0.003813  -1.847  0.06511 .  \ncomor                          0.301773   0.033105   9.116  < 2e-16 ***\nsmoke100                       0.099038   0.090280   1.097  0.27295    \nhx_depress                     0.471949   0.104232   4.528 6.81e-06 ***\nbmi                            0.016375   0.006295   2.601  0.00945 ** \nactivityActive                -0.229927   0.154912  -1.484  0.13812    \nactivityInsufficiently_Active -0.116998   0.139440  -0.839  0.40168    \nactivityInactive               0.256118   0.115266   2.222  0.02655 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.303 on 851 degrees of freedom\n  (197 observations deleted due to missingness)\nMultiple R-squared:  0.1806,    Adjusted R-squared:  0.1729 \nF-statistic: 23.45 on 8 and 851 DF,  p-value: < 2.2e-16\n\n\nNote that the appropriate number of observations are listed as “deleted due to missingness.”\n\n17.7.1 Quality of Fit Statistics\n\nglance(m_1cc) |>\n    select(r.squared, adj.r.squared, sigma, AIC, BIC) |>\n    kable(digits = c(3, 3, 2, 1, 1))\n\n\n\n\nr.squared\nadj.r.squared\nsigma\nAIC\nBIC\n\n\n\n\n0.181\n0.173\n1.3\n2906.3\n2953.8\n\n\n\n\n\n\n\n17.7.2 Interpreting Effect Sizes\n\ntidy(m_1cc, conf.int = TRUE) |>\n    select(term, estimate, std.error, conf.low, conf.high) |>\n    kable(digits = 3)\n\n\n\n\n\n\n\n\n\n\n\nterm\nestimate\nstd.error\nconf.low\nconf.high\n\n\n\n\n(Intercept)\n0.582\n0.371\n-0.146\n1.310\n\n\nage_imp\n-0.007\n0.004\n-0.015\n0.000\n\n\ncomor\n0.302\n0.033\n0.237\n0.367\n\n\nsmoke100\n0.099\n0.090\n-0.078\n0.276\n\n\nhx_depress\n0.472\n0.104\n0.267\n0.677\n\n\nbmi\n0.016\n0.006\n0.004\n0.029\n\n\nactivityActive\n-0.230\n0.155\n-0.534\n0.074\n\n\nactivityInsufficiently_Active\n-0.117\n0.139\n-0.391\n0.157\n\n\nactivityInactive\n0.256\n0.115\n0.030\n0.482\n\n\n\n\n\nWe’ll interpret three of the predictors here to demonstrate ideas: comor, hx_depress and activity.\n\n\n\n\nIf we have two subjects with the same values of age_imp, smoke100, hx_depress, bmi, and activity, but Harry has a comor score that is one point higher than Sally’s, then the model predicts that Harry’s transformed outcome (specifically the natural logarithm of (his physhealth days + 1)) will be 0.302 higher than Sally’s, with a 95% confidence interval around that estimate ranging from (round(a$conf.low,3), round(a$conf.high,3)).\n\n\n\n\n\nIf we have two subjects with the same values of age_imp, comor, smoke100, bmi, and activity, but Harry has a history of depression (hx_depress = 1) while Sally does not have such a history (so Sally’s hx_depress = 0), then the model predicts that Harry’s transformed outcome (specifically the natural logarithm of (his physhealth days + 1)) will be 0.472 higher than Sally’s, with a 95% confidence interval around that estimate ranging from (round(a$conf.low,3), round(a$conf.high,3)).\n\n\n\n\n\nThe activity variable has four categories as indicated in the table below. The model uses the “Highly_Active” category as the reference group.\n\n\nsmart_16_imp1 |> tabyl(activity)\n\n              activity   n   percent\n         Highly_Active 252 0.2384106\n                Active 135 0.1277200\n Insufficiently_Active 193 0.1825922\n              Inactive 477 0.4512772\n\n\n\n\n\n\nFrom the tidied set of coefficients, we can describe the activity effects as follows.\n\nIf Sally is “Highly Active” and Harry is “Active” but they otherwise have the same values of all predictors, then our prediction is that Harry’s transformed outcome (specifically the natural logarithm of (his physhealth days + 1)) will be 0.23 lower than Sally’s, with a 95% confidence interval around that estimate ranging from (round(a$conf.low,3), round(a$conf.high,3)).\nIf instead Harry is “Insufficiently Active” but nothing else changes, then our prediction is that Harry’s transformed outcome will be 0.117 lower than Sally’s, with a 95% confidence interval around that estimate ranging from (round(a2$conf.low,3), round(a2$conf.high,3)).\nIf instead Harry is “Inactive” but nothing else changes, then our prediction is that Harry’s transformed outcome will be -0.117 higher than Sally’s, with a 95% confidence interval around that estimate ranging from (round(a2$conf.low,3), round(a2$conf.high,3)).\n\n\n\n\n17.7.3 Making Predictions with the Model\nLet’s describe two subjects, and use this model (and the ones that follow) to predict their physhealth values.\n\nSheena is age 50, has 2 comorbidities, has smoked 100 cigarettes in her life, has no history of depression, a BMI of 25, and is Highly Active.\nJacob is age 65, has 4 comorbidities, has never smoked, has a history of depression, a BMI of 32 and is Inactive.\n\nWe’ll first build predictions for Sheena and Jacob (with 95% prediction intervals) for phys_tr.\n\nnew2 <- tibble(\n    name = c(\"Sheena\", \"Jacob\"),\n    age_imp = c(50, 65),\n    comor = c(2, 4),\n    smoke100 = c(1, 0),\n    hx_depress = c(0, 1),\n    bmi = c(25, 32),\n    activity = c(\"Highly_Active\", \"Inactive\")\n)\n\npreds_m_1cc <- predict(m_1cc, newdata = new2, \n                       interval = \"prediction\")\n\npreds_m_1cc\n\n       fit      lwr      upr\n1 1.341778 -1.22937 3.912925\n2 2.583336  0.01399 5.152681\n\n\nThe model makes predictions for our transformed outcome, phys_tr. Now, we need to back-transform the predictions and the confidence intervals to build predictions for physhealth.\n\npreds_m_1cc <- preds_m_1cc |>\n    tbl_df() |>\n    mutate(names = c(\"Sheena\", \"Jacob\"),\n           pred_physhealth = exp(fit) - 1,\n           conf_low = exp(lwr) - 1,\n           conf_high = exp(upr) - 1) |>\n    select(names, pred_physhealth, conf_low, conf_high, \n           everything())\n\nWarning: `tbl_df()` was deprecated in dplyr 1.0.0.\nℹ Please use `tibble::as_tibble()` instead.\n\npreds_m_1cc |> kable(digits = 3)\n\n\n\n\nnames\npred_physhealth\nconf_low\nconf_high\nfit\nlwr\nupr\n\n\n\n\nSheena\n2.826\n-0.708\n49.045\n1.342\n-1.229\n3.913\n\n\nJacob\n12.241\n0.014\n171.894\n2.583\n0.014\n5.153"
  },
  {
    "objectID": "multiple_imp.html#augmented-linear-regression-with-lm-on-the-complete-cases",
    "href": "multiple_imp.html#augmented-linear-regression-with-lm-on-the-complete-cases",
    "title": "17  Multiple Imputation and Linear Regression",
    "section": "17.8 “Augmented” Linear Regression with lm on the Complete Cases",
    "text": "17.8 “Augmented” Linear Regression with lm on the Complete Cases\nNow, we’ll add the non-linear terms we discussed earlier. We’ll add a (raw) cubic polynomial to represent the comor information, and we’ll add an interaction term between hx_depress and activity.\n\nm_2cc <- \n    lm(phys_tr ~ age_imp + pol(comor, 3) + smoke100 + \n           bmi + hx_depress*activity, data = smart_16)\n\nsummary(m_2cc)\n\n\nCall:\nlm(formula = phys_tr ~ age_imp + pol(comor, 3) + smoke100 + bmi + \n    hx_depress * activity, data = smart_16)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-2.907 -1.063 -0.267  1.143  2.924 \n\nCoefficients:\n                                          Estimate Std. Error t value Pr(>|t|)\n(Intercept)                               0.514823   0.376203   1.368  0.17153\nage_imp                                  -0.008100   0.003865  -2.096  0.03640\npol(comor, 3)comor                        0.634274   0.160630   3.949 8.51e-05\npol(comor, 3)comor^2                     -0.130626   0.073525  -1.777  0.07599\npol(comor, 3)comor^3                      0.012508   0.008977   1.393  0.16386\nsmoke100                                  0.089345   0.090336   0.989  0.32294\nbmi                                       0.015203   0.006315   2.408  0.01627\nhx_depress                                0.647054   0.229696   2.817  0.00496\nactivityActive                           -0.202196   0.172300  -1.174  0.24092\nactivityInsufficiently_Active            -0.005815   0.166221  -0.035  0.97210\nactivityInactive                          0.290380   0.132198   2.197  0.02832\nhx_depress:activityActive                -0.124836   0.395415  -0.316  0.75230\nhx_depress:activityInsufficiently_Active -0.376355   0.310160  -1.213  0.22531\nhx_depress:activityInactive              -0.172952   0.267427  -0.647  0.51798\n                                            \n(Intercept)                                 \nage_imp                                  *  \npol(comor, 3)comor                       ***\npol(comor, 3)comor^2                     .  \npol(comor, 3)comor^3                        \nsmoke100                                    \nbmi                                      *  \nhx_depress                               ** \nactivityActive                              \nactivityInsufficiently_Active               \nactivityInactive                         *  \nhx_depress:activityActive                   \nhx_depress:activityInsufficiently_Active    \nhx_depress:activityInactive                 \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.301 on 846 degrees of freedom\n  (197 observations deleted due to missingness)\nMultiple R-squared:  0.187, Adjusted R-squared:  0.1745 \nF-statistic: 14.97 on 13 and 846 DF,  p-value: < 2.2e-16\n\n\nNote again that the appropriate number of observations are listed as “deleted due to missingness.”\n\n17.8.1 Quality of Fit Statistics\n\nglance(m_2cc) |>\n    select(r.squared, adj.r.squared, sigma, AIC, BIC) |>\n    kable(digits = c(3, 3, 2, 1, 1))\n\n\n\n\nr.squared\nadj.r.squared\nsigma\nAIC\nBIC\n\n\n\n\n0.187\n0.175\n1.3\n2909.5\n2980.9\n\n\n\n\n\n\n\n17.8.2 ANOVA assessing the impact of the non-linear terms\n\nanova(m_1cc, m_2cc)\n\nAnalysis of Variance Table\n\nModel 1: phys_tr ~ age_imp + comor + smoke100 + hx_depress + bmi + activity\nModel 2: phys_tr ~ age_imp + pol(comor, 3) + smoke100 + bmi + hx_depress * \n    activity\n  Res.Df    RSS Df Sum of Sq      F Pr(>F)\n1    851 1444.0                           \n2    846 1432.8  5    11.265 1.3303  0.249\n\n\nThe difference between the models doesn’t meet the standard for statistical detectabilty at our usual \\(\\alpha\\) levels.\n\n\n17.8.3 Interpreting Effect Sizes\n\ntidy(m_2cc, conf.int = TRUE) |>\n    select(term, estimate, std.error, conf.low, conf.high) |>\n    kable(digits = 3)\n\n\n\n\n\n\n\n\n\n\n\nterm\nestimate\nstd.error\nconf.low\nconf.high\n\n\n\n\n(Intercept)\n0.515\n0.376\n-0.224\n1.253\n\n\nage_imp\n-0.008\n0.004\n-0.016\n-0.001\n\n\npol(comor, 3)comor\n0.634\n0.161\n0.319\n0.950\n\n\npol(comor, 3)comor^2\n-0.131\n0.074\n-0.275\n0.014\n\n\npol(comor, 3)comor^3\n0.013\n0.009\n-0.005\n0.030\n\n\nsmoke100\n0.089\n0.090\n-0.088\n0.267\n\n\nbmi\n0.015\n0.006\n0.003\n0.028\n\n\nhx_depress\n0.647\n0.230\n0.196\n1.098\n\n\nactivityActive\n-0.202\n0.172\n-0.540\n0.136\n\n\nactivityInsufficiently_Active\n-0.006\n0.166\n-0.332\n0.320\n\n\nactivityInactive\n0.290\n0.132\n0.031\n0.550\n\n\nhx_depress:activityActive\n-0.125\n0.395\n-0.901\n0.651\n\n\nhx_depress:activityInsufficiently_Active\n-0.376\n0.310\n-0.985\n0.232\n\n\nhx_depress:activityInactive\n-0.173\n0.267\n-0.698\n0.352\n\n\n\n\n\nLet’s focus first on interpreting the interaction terms between hx_depress and activity.\nAssume first that we have a set of subjects with the same values of age_imp, smoke100, bmi, and comor.\n\nArnold has hx_depress = 1 and is Inactive\nBetty has hx_depress = 1 and is Insufficiently Active\nCarlos has hx_depress = 1 and is Active\nDebbie has hx_depress = 1 and is Highly Active\nEamon has hx_depress = 0 and is Inactive\nFlorence has hx_depress = 0 and is Insufficiently Active\nGarry has hx_depress = 0 and is Active\nHarry has hx_depress = 0 and is Highly Active\n\nSo the model, essentially can be used to compare each of the first seven people on that list to Harry (who has the reference levels of both hx_depress and activity.) Let’s compare Arnold to Harry.\nFor instance, as compared to Harry, Arnold is expected to have a transformed outcome (specifically the natural logarithm of (his physhealth days + 1)) that is:\n\n\n\n\n0.647 higher because Arnold’s hx_depress = 1, and\n0.29 higher still because Arnold’s activity is “Inactive”, and\n0.173 lower because of the combination (see the `hx_depress:activityInactive” row)\n\nSo, in total, we expect Arnold’s transformed outcome to be 0.647 + 0.29 + (-0.173), or 0.764 higher than Harry’s.\nIf we want to compare Arnold to, for instance, Betty, we first calculate Betty’s difference from Harry, and then compare the two differences.\nAs compared to Harry, Betty is expected to have a transformed outcome (specifically the natural logarithm of (her physhealth days + 1)) that is:\n\n\n\n\n0.647 higher because Betty’s hx_depress = 1, and\n0.006 lower still because Betty’s activity is “Insufficiently Active”, and\n0.376 lower because of the combination (see the `hx_depress:activityInsufficiently_Active” row)\n\nSo, in total, we expect Betty’s transformed outcome to be 0.647 + (-0.006) + (-0.376), or 0.265 higher than Harry’s.\nAnd thus we can compare Betty and Arnold directly.\n\nArnold is predicted to have an outcome that is 0.764 higher than Harry’s.\nBetty is predicted to have an outcome that is 0.265 higher than Harry’s.\nAnd so Arnold’s predicted outcome (phys_tr) is 0.499 larger than Betty’s.\n\nNow, suppose we want to look at our cubic polynomial in comor.\n\nSuppose Harry and Sally have the same values for all other predictors in the model, but Harry has 1 comorbidity where Sally has none. Then the three terms in the model related to comor will be 1 for Harry and 0 for Sally, and the interpretation becomes pretty straightforward.\nBut suppose instead that nothing has changed except Harry has 2 comorbidities and Sally has just 1. The size of the impact of this Harry - Sally difference is far larger in this situation, because the comor variable enters the model in a non-linear way. This is an area where fitting the model using ols can be helpful because of the ability to generate plots (of effects, nomograms, etc.) that can show this non-linearity in a clear way.\n\nSuppose for instance, that Harry and Sally share the following values for the other predictors: each is age 40, has never smoked, has no history of depression, a BMI of 30 and is Highly Active.\n\nNow, if Harry has 1 comorbidity and Sally has none, the predicted phys_tr values for Harry and Sally are as indicated below.\n\n\nhands1 <- tibble(\n    name = c(\"Harry\", \"Sally\"),\n    age_imp = c(40, 40),\n    comor = c(1, 0),\n    smoke100 = c(0, 0),\n    hx_depress = c(0, 0),\n    bmi = c(30, 30),\n    activity = c(\"Highly_Active\", \"Highly_Active\")\n)\n\npredict(m_2cc, newdata = hands1)\n\n        1         2 \n1.1630840 0.6469282 \n\n\nBut if Harry has 2 comorbidities and Sally 1, the predictions are:\n\nhands2 <- tibble(\n    name = c(\"Harry\", \"Sally\"),\n    age_imp = c(40, 40),\n    comor = c(2, 1), # only thing that changes\n    smoke100 = c(0, 0),\n    hx_depress = c(0, 0),\n    bmi = c(30, 30),\n    activity = c(\"Highly_Active\", \"Highly_Active\")\n)\n\npredict(m_2cc, newdata = hands2)\n\n       1        2 \n1.493035 1.163084 \n\n\nNote that the difference in predictions between Harry and Sally is much smaller now than it was previously.\n\n\n17.8.4 Making Predictions with the Model\nAs before, we’ll use the new model to predict physhealth values for Sheena and Jacob.\n\nSheena is age 50, has 2 comorbidities, has smoked 100 cigarettes in her life, has no history of depression, a BMI of 25, and is Highly Active.\nJacob is age 65, has 4 comorbidities, has never smoked, has a history of depression, a BMI of 32 and is Inactive.\n\nWe’ll first build predictions for Sheena and Jacob (with 95% prediction intervals) for phys_tr.\n\nnew2 <- tibble(\n    name = c(\"Sheena\", \"Jacob\"),\n    age_imp = c(50, 65),\n    comor = c(2, 4),\n    smoke100 = c(1, 0),\n    hx_depress = c(0, 1),\n    bmi = c(25, 32),\n    activity = c(\"Highly_Active\", \"Inactive\")\n)\n\npreds_m_2cc <- predict(m_2cc, newdata = new2, \n                       interval = \"prediction\")\n\npreds_m_2cc\n\n       fit         lwr      upr\n1 1.425362 -1.14707613 3.997801\n2 2.486907 -0.08635658 5.060171\n\n\nNow, we need to back-transform the predictions and the confidence intervals that describe phys_tr to build predictions for physhealth.\n\npreds_m_2cc <- preds_m_2cc |>\n    tbl_df() |>\n    mutate(names = c(\"Sheena\", \"Jacob\"),\n           pred_physhealth = exp(fit) - 1,\n           conf_low = exp(lwr) - 1,\n           conf_high = exp(upr) - 1) |>\n    select(names, pred_physhealth, conf_low, conf_high, \n           everything())\n\nWarning: `tbl_df()` was deprecated in dplyr 1.0.0.\nℹ Please use `tibble::as_tibble()` instead.\n\npreds_m_2cc |> kable(digits = 3)\n\n\n\n\nnames\npred_physhealth\nconf_low\nconf_high\nfit\nlwr\nupr\n\n\n\n\nSheena\n3.159\n-0.682\n53.478\n1.425\n-1.147\n3.998\n\n\nJacob\n11.024\n-0.083\n156.617\n2.487\n-0.086\n5.060"
  },
  {
    "objectID": "multiple_imp.html#using-mice-to-perform-multiple-imputation",
    "href": "multiple_imp.html#using-mice-to-perform-multiple-imputation",
    "title": "17  Multiple Imputation and Linear Regression",
    "section": "17.9 Using mice to perform Multiple Imputation",
    "text": "17.9 Using mice to perform Multiple Imputation\nLet’s focus on the main effects model, and look at the impact of performing multiple imputation to account for the missing data. Recall that in our smart_16 data, the most “missingness” is shown in the activity variable, which is still missing less than 10% of the time. So we’ll try a set of 10 imputations, using the default settings in the mice package.\n\n# requires library(mice)\n\nset.seed(432)\n\n# create small data set including only variables to\n# be used in building the imputation model\n\nsm16 <- smart_16 |> \n    select(physhealth, phys_tr, activity, age_imp, bmi, comor, \n           hx_depress, smoke100)\n\nsmart_16_mice10 <- mice(sm16, m = 10)\n\n\n iter imp variable\n  1   1  activity  age_imp  bmi  comor  hx_depress  smoke100\n  1   2  activity  age_imp  bmi  comor  hx_depress  smoke100\n  1   3  activity  age_imp  bmi  comor  hx_depress  smoke100\n  1   4  activity  age_imp  bmi  comor  hx_depress  smoke100\n  1   5  activity  age_imp  bmi  comor  hx_depress  smoke100\n  1   6  activity  age_imp  bmi  comor  hx_depress  smoke100\n  1   7  activity  age_imp  bmi  comor  hx_depress  smoke100\n  1   8  activity  age_imp  bmi  comor  hx_depress  smoke100\n  1   9  activity  age_imp  bmi  comor  hx_depress  smoke100\n  1   10  activity  age_imp  bmi  comor  hx_depress  smoke100\n  2   1  activity  age_imp  bmi  comor  hx_depress  smoke100\n  2   2  activity  age_imp  bmi  comor  hx_depress  smoke100\n  2   3  activity  age_imp  bmi  comor  hx_depress  smoke100\n  2   4  activity  age_imp  bmi  comor  hx_depress  smoke100\n  2   5  activity  age_imp  bmi  comor  hx_depress  smoke100\n  2   6  activity  age_imp  bmi  comor  hx_depress  smoke100\n  2   7  activity  age_imp  bmi  comor  hx_depress  smoke100\n  2   8  activity  age_imp  bmi  comor  hx_depress  smoke100\n  2   9  activity  age_imp  bmi  comor  hx_depress  smoke100\n  2   10  activity  age_imp  bmi  comor  hx_depress  smoke100\n  3   1  activity  age_imp  bmi  comor  hx_depress  smoke100\n  3   2  activity  age_imp  bmi  comor  hx_depress  smoke100\n  3   3  activity  age_imp  bmi  comor  hx_depress  smoke100\n  3   4  activity  age_imp  bmi  comor  hx_depress  smoke100\n  3   5  activity  age_imp  bmi  comor  hx_depress  smoke100\n  3   6  activity  age_imp  bmi  comor  hx_depress  smoke100\n  3   7  activity  age_imp  bmi  comor  hx_depress  smoke100\n  3   8  activity  age_imp  bmi  comor  hx_depress  smoke100\n  3   9  activity  age_imp  bmi  comor  hx_depress  smoke100\n  3   10  activity  age_imp  bmi  comor  hx_depress  smoke100\n  4   1  activity  age_imp  bmi  comor  hx_depress  smoke100\n  4   2  activity  age_imp  bmi  comor  hx_depress  smoke100\n  4   3  activity  age_imp  bmi  comor  hx_depress  smoke100\n  4   4  activity  age_imp  bmi  comor  hx_depress  smoke100\n  4   5  activity  age_imp  bmi  comor  hx_depress  smoke100\n  4   6  activity  age_imp  bmi  comor  hx_depress  smoke100\n  4   7  activity  age_imp  bmi  comor  hx_depress  smoke100\n  4   8  activity  age_imp  bmi  comor  hx_depress  smoke100\n  4   9  activity  age_imp  bmi  comor  hx_depress  smoke100\n  4   10  activity  age_imp  bmi  comor  hx_depress  smoke100\n  5   1  activity  age_imp  bmi  comor  hx_depress  smoke100\n  5   2  activity  age_imp  bmi  comor  hx_depress  smoke100\n  5   3  activity  age_imp  bmi  comor  hx_depress  smoke100\n  5   4  activity  age_imp  bmi  comor  hx_depress  smoke100\n  5   5  activity  age_imp  bmi  comor  hx_depress  smoke100\n  5   6  activity  age_imp  bmi  comor  hx_depress  smoke100\n  5   7  activity  age_imp  bmi  comor  hx_depress  smoke100\n  5   8  activity  age_imp  bmi  comor  hx_depress  smoke100\n  5   9  activity  age_imp  bmi  comor  hx_depress  smoke100\n  5   10  activity  age_imp  bmi  comor  hx_depress  smoke100\n\nsummary(smart_16_mice10)\n\nClass: mids\nNumber of multiple imputations:  10 \nImputation methods:\nphyshealth    phys_tr   activity    age_imp        bmi      comor hx_depress \n        \"\"         \"\"  \"polyreg\"      \"pmm\"      \"pmm\"      \"pmm\"      \"pmm\" \n  smoke100 \n     \"pmm\" \nPredictorMatrix:\n           physhealth phys_tr activity age_imp bmi comor hx_depress smoke100\nphyshealth          0       1        1       1   1     1          1        1\nphys_tr             1       0        1       1   1     1          1        1\nactivity            1       1        0       1   1     1          1        1\nage_imp             1       1        1       0   1     1          1        1\nbmi                 1       1        1       1   0     1          1        1\ncomor               1       1        1       1   1     0          1        1"
  },
  {
    "objectID": "multiple_imp.html#running-the-linear-regression-in-lm-with-multiple-imputation",
    "href": "multiple_imp.html#running-the-linear-regression-in-lm-with-multiple-imputation",
    "title": "17  Multiple Imputation and Linear Regression",
    "section": "17.10 Running the Linear Regression in lm with Multiple Imputation",
    "text": "17.10 Running the Linear Regression in lm with Multiple Imputation\nNext, we’ll run the linear model (main effects) on each of the 10 imputed data sets.\n\nm10_mods <- \n    with(smart_16_mice10, lm(phys_tr ~ age_imp + comor + \n                                 smoke100 + hx_depress + \n                                 bmi + activity))\n\nsummary(m10_mods)\n\n# A tibble: 90 × 6\n   term                          estimate std.error statistic  p.value  nobs\n   <chr>                            <dbl>     <dbl>     <dbl>    <dbl> <int>\n 1 (Intercept)                    0.317     0.326       0.971 3.32e- 1  1057\n 2 age_imp                       -0.00489   0.00334    -1.47  1.43e- 1  1057\n 3 comor                          0.313     0.0295     10.6   4.72e-25  1057\n 4 smoke100                       0.135     0.0799      1.69  9.22e- 2  1057\n 5 hx_depress                     0.500     0.0929      5.38  9.14e- 8  1057\n 6 bmi                            0.0187    0.00564     3.31  9.64e- 4  1057\n 7 activityActive                -0.202     0.138      -1.46  1.44e- 1  1057\n 8 activityInsufficiently_Active -0.0695    0.124      -0.561 5.75e- 1  1057\n 9 activityInactive               0.262     0.103       2.54  1.11e- 2  1057\n10 (Intercept)                    0.363     0.332       1.10  2.74e- 1  1057\n# … with 80 more rows\n\n\nThen, we’ll pool results across the 10 imputations\n\nm10_pool <- pool(m10_mods)\nsummary(m10_pool, conf.int = TRUE) |>\n    select(-statistic, -df) |>\n    kable(digits = 3)\n\n\n\n\n\n\n\n\n\n\n\n\nterm\nestimate\nstd.error\np.value\n2.5 %\n97.5 %\n\n\n\n\n(Intercept)\n0.444\n0.342\n0.194\n-0.227\n1.114\n\n\nage_imp\n-0.005\n0.003\n0.128\n-0.012\n0.002\n\n\ncomor\n0.309\n0.031\n0.000\n0.249\n0.369\n\n\nsmoke100\n0.114\n0.083\n0.171\n-0.049\n0.278\n\n\nhx_depress\n0.512\n0.094\n0.000\n0.327\n0.696\n\n\nbmi\n0.016\n0.006\n0.009\n0.004\n0.027\n\n\nactivityActive\n-0.204\n0.140\n0.146\n-0.479\n0.071\n\n\nactivityInsufficiently_Active\n-0.044\n0.129\n0.735\n-0.298\n0.210\n\n\nactivityInactive\n0.260\n0.106\n0.014\n0.052\n0.469\n\n\n\n\n\nAnd we can compare these results to the complete case analysis we completed earlier.\n\ntidy(m_1cc, conf.int = TRUE) |>\n    select(term, estimate, std.error, p.value, conf.low, conf.high) |>\n    kable(digits = 3)\n\n\n\n\n\n\n\n\n\n\n\n\nterm\nestimate\nstd.error\np.value\nconf.low\nconf.high\n\n\n\n\n(Intercept)\n0.582\n0.371\n0.117\n-0.146\n1.310\n\n\nage_imp\n-0.007\n0.004\n0.065\n-0.015\n0.000\n\n\ncomor\n0.302\n0.033\n0.000\n0.237\n0.367\n\n\nsmoke100\n0.099\n0.090\n0.273\n-0.078\n0.276\n\n\nhx_depress\n0.472\n0.104\n0.000\n0.267\n0.677\n\n\nbmi\n0.016\n0.006\n0.009\n0.004\n0.029\n\n\nactivityActive\n-0.230\n0.155\n0.138\n-0.534\n0.074\n\n\nactivityInsufficiently_Active\n-0.117\n0.139\n0.402\n-0.391\n0.157\n\n\nactivityInactive\n0.256\n0.115\n0.027\n0.030\n0.482\n\n\n\n\n\nNote that there are some sizeable differences here, although nothing enormous.\nIf we want the pooled \\(R^2\\) or pooled adjusted \\(R^2\\) after imputation, R will provide it (and a 95% confidence interval around the estimate) with …\n\npool.r.squared(m10_mods)\n\n          est     lo 95     hi 95        fmi\nR^2 0.1912561 0.1482819 0.2369623 0.08061427\n\n\n\npool.r.squared(m10_mods, adjusted = TRUE)\n\n              est     lo 95     hi 95        fmi\nadj R^2 0.1850807 0.1425132 0.2305277 0.08312639\n\n\nWe can see the fraction of missing information about each coefficient due to non-response (fmi) and other details with the following code…\n\nm10_pool\n\nClass: mipo    m = 10 \n                           term  m    estimate         ubar            b\n1                   (Intercept) 10  0.44377168 1.078194e-01 8.002900e-03\n2                       age_imp 10 -0.00522810 1.123668e-05 4.824290e-07\n3                         comor 10  0.30871888 8.801039e-04 5.466082e-05\n4                      smoke100 10  0.11415718 6.474388e-03 4.130673e-04\n5                    hx_depress 10  0.51155722 8.669413e-03 1.582684e-04\n6                           bmi 10  0.01576150 3.182381e-05 3.425191e-06\n7                activityActive 10 -0.20412627 1.914250e-02 4.851040e-04\n8 activityInsufficiently_Active 10 -0.04383739 1.565925e-02 9.855183e-04\n9              activityInactive 10  0.26046070 1.069627e-02 5.023887e-04\n             t dfcom       df        riv     lambda        fmi\n1 1.166226e-01  1048 599.8172 0.08164758 0.07548446 0.07855177\n2 1.176735e-05  1048 814.9042 0.04722675 0.04509697 0.04743197\n3 9.402308e-04  1048 677.6361 0.06831796 0.06394909 0.06669961\n4 6.928762e-03  1048 666.2487 0.07018023 0.06557795 0.06837041\n5 8.843508e-03  1048 982.0512 0.02008154 0.01968621 0.02167660\n6 3.559152e-05  1048 432.0874 0.11839279 0.10585976 0.10996992\n7 1.967611e-02  1048 939.5064 0.02787591 0.02711992 0.02918437\n8 1.674332e-02  1048 672.0473 0.06922874 0.06474643 0.06751736\n9 1.124890e-02  1048 785.1905 0.05166545 0.04912727 0.05154007"
  },
  {
    "objectID": "multiple_imp.html#fit-the-multiple-imputation-model-with-aregimpute",
    "href": "multiple_imp.html#fit-the-multiple-imputation-model-with-aregimpute",
    "title": "17  Multiple Imputation and Linear Regression",
    "section": "17.11 Fit the Multiple Imputation Model with aregImpute",
    "text": "17.11 Fit the Multiple Imputation Model with aregImpute\nHere, we’ll use aregImpute to deal with missing values through multiple imputation, and use the ols function in the rms package to fit the model.\nThe first step is to fit the multiple imputation model. We’ll use n.impute = 10 imputations, with B = 10 bootstrap samples for the preditive mean matching, and fit both linear models and models with restricted cubic splines with 3 knots (nk = c(0, 3)) allowing the target variable to have a non-linear transformation when nk is 3, via tlinear = FALSE.\n\nset.seed(43201602)\ndd <- datadist(smart_16)\noptions(datadist = \"dd\")\n\nfit16_imp <- \n    aregImpute(~ phys_tr + age_imp + comor + smoke100 + \n                   hx_depress + bmi + activity,\n               nk = c(0, 3), tlinear = FALSE, \n               data = smart_16, B = 10, n.impute = 10)\n\nIteration 1 \nIteration 2 \nIteration 3 \nIteration 4 \nIteration 5 \nIteration 6 \nIteration 7 \nIteration 8 \nIteration 9 \nIteration 10 \nIteration 11 \nIteration 12 \nIteration 13 \n\n\nHere are the results of that imputation model.\n\nfit16_imp\n\n\nMultiple Imputation using Bootstrap and PMM\n\naregImpute(formula = ~phys_tr + age_imp + comor + smoke100 + \n    hx_depress + bmi + activity, data = smart_16, n.impute = 10, \n    nk = c(0, 3), tlinear = FALSE, B = 10)\n\nn: 1057     p: 7    Imputations: 10     nk: 0 \n\nNumber of NAs:\n   phys_tr    age_imp      comor   smoke100 hx_depress        bmi   activity \n         0         12         47         24          3         84         85 \n\n           type d.f.\nphys_tr       s    1\nage_imp       s    1\ncomor         s    1\nsmoke100      l    1\nhx_depress    l    1\nbmi           s    1\nactivity      c    3\n\nR-squares for Predicting Non-Missing Values for Each Variable\nUsing Last Imputations of Predictors\n   age_imp      comor   smoke100 hx_depress        bmi   activity \n     0.224      0.206      0.059      0.167      0.169      0.057 \n\nResampling results for determining the complexity of imputation models\n\nVariable being imputed: age_imp \n                                          nk=0   nk=3\nBootstrap bias-corrected R^2             0.186  0.215\n10-fold cross-validated  R^2             0.211  0.215\nBootstrap bias-corrected mean   |error|  9.108 10.894\n10-fold cross-validated  mean   |error| 65.169 10.919\nBootstrap bias-corrected median |error|  7.290  8.784\n10-fold cross-validated  median |error| 66.006  8.613\n\nVariable being imputed: comor \n                                         nk=0  nk=3\nBootstrap bias-corrected R^2            0.183 0.182\n10-fold cross-validated  R^2            0.184 0.193\nBootstrap bias-corrected mean   |error| 0.987 1.184\n10-fold cross-validated  mean   |error| 1.759 1.171\nBootstrap bias-corrected median |error| 0.828 0.910\n10-fold cross-validated  median |error| 1.574 0.892\n\nVariable being imputed: smoke100 \n                                          nk=0   nk=3\nBootstrap bias-corrected R^2            0.0224 0.0187\n10-fold cross-validated  R^2            0.0358 0.0217\nBootstrap bias-corrected mean   |error| 0.4853 0.4866\n10-fold cross-validated  mean   |error| 0.9462 0.9561\nBootstrap bias-corrected median |error| 0.4788 0.4772\n10-fold cross-validated  median |error| 0.8479 0.8706\n\nVariable being imputed: hx_depress \n                                         nk=0  nk=3\nBootstrap bias-corrected R^2            0.157 0.138\n10-fold cross-validated  R^2            0.147 0.148\nBootstrap bias-corrected mean   |error| 0.355 0.360\n10-fold cross-validated  mean   |error| 0.801 0.783\nBootstrap bias-corrected median |error| 0.333 0.337\n10-fold cross-validated  median |error| 0.711 0.673\n\nVariable being imputed: bmi \n                                          nk=0  nk=3\nBootstrap bias-corrected R^2             0.125 0.122\n10-fold cross-validated  R^2             0.134 0.133\nBootstrap bias-corrected mean   |error|  5.221 6.822\n10-fold cross-validated  mean   |error| 32.458 6.884\nBootstrap bias-corrected median |error|  4.178 5.782\n10-fold cross-validated  median |error| 31.330 5.932\n\nVariable being imputed: activity \n                                          nk=0   nk=3\nBootstrap bias-corrected R^2            0.0312 0.0275\n10-fold cross-validated  R^2            0.0450 0.0402\nBootstrap bias-corrected mean   |error| 1.8774 1.8884\n10-fold cross-validated  mean   |error| 1.1121 1.1043\nBootstrap bias-corrected median |error| 2.0000 2.0000\n10-fold cross-validated  median |error| 1.0000 1.0000\n\n\n\npar(mfrow = c(3,2))\nplot(fit16_imp)\n\n\n\npar(mfrow = c(1,1))\n\nThe plot helps us see where the imputations are happening."
  },
  {
    "objectID": "multiple_imp.html#fit-linear-regression-using-ols-and-fit.mult.impute",
    "href": "multiple_imp.html#fit-linear-regression-using-ols-and-fit.mult.impute",
    "title": "17  Multiple Imputation and Linear Regression",
    "section": "17.12 Fit Linear Regression using ols and fit.mult.impute",
    "text": "17.12 Fit Linear Regression using ols and fit.mult.impute\n\nm16_imp <- \n    fit.mult.impute(phys_tr ~ age_imp + comor + smoke100 +\n                        hx_depress + bmi + activity,\n                    fitter = ols, xtrans = fit16_imp,\n                    data = smart_16, x = TRUE, y = TRUE)\n\n\nVariance Inflation Factors Due to Imputation:\n\n                     Intercept                        age_imp \n                          1.03                           1.01 \n                         comor                       smoke100 \n                          1.03                           1.06 \n                    hx_depress                            bmi \n                          1.02                           1.06 \n               activity=Active activity=Insufficiently_Active \n                          1.19                           1.14 \n             activity=Inactive \n                          1.23 \n\nRate of Missing Information:\n\n                     Intercept                        age_imp \n                          0.03                           0.01 \n                         comor                       smoke100 \n                          0.03                           0.06 \n                    hx_depress                            bmi \n                          0.02                           0.06 \n               activity=Active activity=Insufficiently_Active \n                          0.16                           0.13 \n             activity=Inactive \n                          0.19 \n\nd.f. for t-distribution for Tests of Single Coefficients:\n\n                     Intercept                        age_imp \n                       8176.67                       45410.80 \n                         comor                       smoke100 \n                      13030.27                        2670.64 \n                    hx_depress                            bmi \n                      28199.30                        2935.89 \n               activity=Active activity=Insufficiently_Active \n                        354.62                         571.56 \n             activity=Inactive \n                        258.42 \n\nThe following fit components were averaged over the 10 model fits:\n\n  fitted.values stats linear.predictors \n\n\n\n17.12.1 Summaries and Coefficients\nHere are the results:\n\nm16_imp\n\nLinear Regression Model\n \n fit.mult.impute(formula = phys_tr ~ age_imp + comor + smoke100 + \n     hx_depress + bmi + activity, fitter = ols, xtrans = fit16_imp, \n     data = smart_16, x = TRUE, y = TRUE)\n \n                 Model Likelihood    Discrimination    \n                       Ratio Test           Indexes    \n Obs    1057    LR chi2    219.94    R2       0.188    \n sigma1.2881    d.f.            8    R2 adj   0.182    \n d.f.   1048    Pr(> chi2) 0.0000    g        0.687    \n \n Residuals\n \n     Min      1Q  Median      3Q     Max \n -3.0621 -1.0327 -0.2878  1.1104  2.8018 \n \n \n                                Coef    S.E.   t     Pr(>|t|)\n Intercept                       0.4052 0.3352  1.21 0.2271  \n age_imp                        -0.0049 0.0034 -1.46 0.1437  \n comor                           0.3078 0.0302 10.20 <0.0001 \n smoke100                        0.1259 0.0830  1.52 0.1296  \n hx_depress                      0.5120 0.0940  5.45 <0.0001 \n bmi                             0.0164 0.0058  2.83 0.0048  \n activity=Active                -0.1773 0.1513 -1.17 0.2416  \n activity=Insufficiently_Active -0.0396 0.1342 -0.30 0.7680  \n activity=Inactive               0.2401 0.1144  2.10 0.0360  \n \n\n\n\n\n17.12.2 Effect Sizes\nWe can plot and summarize the effect sizes using the usual ols tools:\n\nsummary(m16_imp)\n\n             Effects              Response : phys_tr \n\n Factor                                    Low   High  Diff. Effect    S.E.    \n age_imp                                   57.00 73.00 16.00 -0.079163 0.054107\n comor                                      1.00  2.00  1.00  0.307790 0.030171\n smoke100                                   0.00  1.00  1.00  0.125890 0.083000\n hx_depress                                 0.00  1.00  1.00  0.511980 0.094007\n bmi                                       27.29 36.65  9.36  0.153530 0.054322\n activity - Highly_Active:Inactive          4.00  1.00    NA -0.240070 0.114350\n activity - Active:Inactive                 4.00  2.00    NA -0.417320 0.137640\n activity - Insufficiently_Active:Inactive  4.00  3.00    NA -0.279650 0.115000\n Lower 0.95 Upper 0.95\n -0.185330   0.027008 \n  0.248590   0.366990 \n -0.036973   0.288760 \n  0.327520   0.696450 \n  0.046932   0.260120 \n -0.464450  -0.015686 \n -0.687400  -0.147250 \n -0.505310  -0.054002 \n\nplot(summary(m16_imp))\n\n\n\n\n\n\n17.12.3 Making Predictions with this Model\nOnce again, let’s make predictions for our two subjects, and use this model (and the ones that follow) to predict their physhealth values.\n\nSheena is age 50, has 2 comorbidities, has smoked 100 cigarettes in her life, has no history of depression, a BMI of 25, and is Highly Active.\nJacob is age 65, has 4 comorbidities, has never smoked, has a history of depression, a BMI of 32 and is Inactive.\n\n\nnew2 <- tibble(\n    name = c(\"Sheena\", \"Jacob\"),\n    age_imp = c(50, 65),\n    comor = c(2, 4),\n    smoke100 = c(1, 0),\n    hx_depress = c(0, 1),\n    bmi = c(25, 32),\n    activity = c(\"Highly_Active\", \"Inactive\")\n)\n\npreds_m_16imp <- predict(m16_imp, \n                         newdata = data.frame(new2))\n\npreds_m_16imp\n\n       1        2 \n1.309306 2.591649 \n\n\n\npreds_m_16imp <- preds_m_16imp |>\n    tbl_df() |>\n    mutate(names = c(\"Sheena\", \"Jacob\"),\n           pred_physhealth = exp(value) - 1) |>\n    select(names, pred_physhealth)\n\nWarning: `tbl_df()` was deprecated in dplyr 1.0.0.\nℹ Please use `tibble::as_tibble()` instead.\n\npreds_m_16imp |> kable(digits = 3)\n\n\n\n\nnames\npred_physhealth\n\n\n\n\nSheena\n2.704\n\n\nJacob\n12.352\n\n\n\n\n\n\n\n17.12.4 Nomogram\nWe can also develop a nomogram, if we like. As a special touch, we’ll add a prediction at the bottom which back-transforms out of the predicted phys_tr back to the physhealth days.\n\nplot(nomogram(m16_imp, \n              fun = list(function(x) exp(x) - 1),\n              funlabel = \"Predicted physhealth days\",\n              fun.at = seq(0, 30, 3)))\n\n\n\n\nWe can see the big role of comor and hx_depress in this model.\n\n\n17.12.5 Validating Summary Statistics\nWe can cross-validate summary measures, like \\(R^2\\)…\n\nvalidate(m16_imp)\n\n          index.orig training   test optimism index.corrected  n\nR-square      0.1867   0.1984 0.1793   0.0192          0.1676 40\nMSE           1.6472   1.6168 1.6623  -0.0455          1.6927 40\ng             0.6876   0.7031 0.6749   0.0282          0.6594 40\nIntercept     0.0000   0.0000 0.0677  -0.0677          0.0677 40\nSlope         1.0000   1.0000 0.9636   0.0364          0.9636 40"
  },
  {
    "objectID": "tableone.html#r-setup-used-here",
    "href": "tableone.html#r-setup-used-here",
    "title": "18  Building Table 1",
    "section": "18.1 R Setup Used Here",
    "text": "18.1 R Setup Used Here\n\nknitr::opts_chunk$set(comment = NA)\n\nlibrary(tableone)\nlibrary(knitr)\nlibrary(tidyverse) \n\ntheme_set(theme_bw())\n\nMany scientific articles involve direct comparison of results from various exposures, perhaps treatments. In 431, we studied numerous methods, including various sorts of hypothesis tests, confidence intervals, and descriptive summaries, which can help us to understand and compare outcomes in such a setting. One common approach is to present what’s often called Table 1. Table 1 provides a summary of the characteristics of a sample, or of groups of samples, which is most commonly used to help understand the nature of the data being compared."
  },
  {
    "objectID": "tableone.html#data-load",
    "href": "tableone.html#data-load",
    "title": "18  Building Table 1",
    "section": "18.2 Data Load",
    "text": "18.2 Data Load\n\nfakestroke <- read_csv(\"data/fakestroke.csv\", show_col_types = FALSE)\nbloodbrain <- read_csv(\"data/bloodbrain.csv\", show_col_types = FALSE)"
  },
  {
    "objectID": "tableone.html#two-examples-from-the-new-england-journal-of-medicine",
    "href": "tableone.html#two-examples-from-the-new-england-journal-of-medicine",
    "title": "18  Building Table 1",
    "section": "18.3 Two examples from the New England Journal of Medicine",
    "text": "18.3 Two examples from the New England Journal of Medicine\n\n18.3.1 A simple Table 1\nTable 1 is especially common in the context of clinical research. Consider the excerpt below, from a January 2015 article in the New England Journal of Medicine (Tolaney et al. 2015).\n\n\n\n\n\nThis (partial) table reports baseline characteristics on age group, sex and race, describing 406 patients with HER2-positive1 invasive breast cancer that began the protocol therapy. Age, sex and race (along with severity of illness) are the most commonly identified characteristics in a Table 1.\nIn addition to the measures shown in this excerpt, the full Table also includes detailed information on the primary tumor for each patient, including its size, nodal status and histologic grade. Footnotes tell us that the percentages shown are subject to rounding, and may not total 100, and that the race information was self-reported.\n\n\n18.3.2 A group comparison\nA more typical Table 1 involves a group comparison, for example in this excerpt from Roy et al. (2008). This Table 1 describes a multi-center randomized clinical trial comparing two different approaches to caring for patients with heart failure and atrial fibrillation2.\n\n\n\n\n\nThe article provides percentages, means and standard deviations across groups, but note that it does not provide p values for the comparison of baseline characteristics. This is a common feature of NEJM reports on randomized clinical trials, where we anticipate that the two groups will be well matched at baseline. Note that the patients in this study were randomly assigned to either the rhythm-control group or to the rate-control group, using blocked randomization stratified by study center."
  },
  {
    "objectID": "tableone.html#the-mr-clean-trial",
    "href": "tableone.html#the-mr-clean-trial",
    "title": "18  Building Table 1",
    "section": "18.4 The MR CLEAN trial",
    "text": "18.4 The MR CLEAN trial\nBerkhemer et al. (2015) reported on the MR CLEAN trial, involving 500 patients with acute ischemic stroke caused by a proximal intracranial arterial occlusion. The trial was conducted at 16 medical centers in the Netherlands, where 233 were randomly assigned to the intervention (intraarterial treatment plus usual care) and 267 to control (usual care alone.) The primary outcome was the modified Rankin scale score at 90 days; this categorical scale measures functional outcome, with scores ranging from 0 (no symptoms) to 6 (death). The fundamental conclusion of Berkhemer et al. (2015) was that in patients with acute ischemic stroke caused by a proximal intracranial occlusion of the anterior circulation, intraarterial treatment administered within 6 hours after stroke onset was effective and safe.\nHere’s the Table 1 from Berkhemer et al. (2015).\n\n\n\n\n\nThe Table was accompanied by the following notes."
  },
  {
    "objectID": "tableone.html#simulated-fakestroke-data",
    "href": "tableone.html#simulated-fakestroke-data",
    "title": "18  Building Table 1",
    "section": "18.5 Simulated fakestroke data",
    "text": "18.5 Simulated fakestroke data\nConsider the simulated data, available on our Data and Code website in the fakestroke.csv file, which I built to let us mirror the Table 1 for MR CLEAN (Berkhemer et al. 2015). The fakestroke.csv file contains the following 18 variables for 500 patients.\n\n\n\n\n\n\n\nVariable\nDescription\n\n\n\n\nstudyid\nStudy ID # (z001 through z500)\n\n\ntrt\nTreatment group (Intervention or Control)\n\n\nage\nAge in years\n\n\nsex\nMale or Female\n\n\nnihss\nNIH Stroke Scale Score (can range from 0-42; higher scores indicate more severe neurological deficits)\n\n\nlocation\nStroke Location - Left or Right Hemisphere\n\n\nhx.isch\nHistory of Ischemic Stroke (Yes/No)\n\n\nafib\nAtrial Fibrillation (1 = Yes, 0 = No)\n\n\ndm\nDiabetes Mellitus (1 = Yes, 0 = No)\n\n\nmrankin\nPre-stroke modified Rankin scale score (0, 1, 2 or > 2) indicating functional disability - complete range is 0 (no symptoms) to 6 (death)\n\n\nsbp\nSystolic blood pressure, in mm Hg\n\n\niv.altep\nTreatment with IV alteplase (Yes/No)\n\n\ntime.iv\nTime from stroke onset to start of IV alteplase (minutes) if iv.altep=Yes\n\n\naspects\nAlberta Stroke Program Early Computed Tomography score, which measures extent of stroke from 0 - 10; higher scores indicate fewer early ischemic changes\n\n\nia.occlus\nIntracranial arterial occlusion, based on vessel imaging - five categories3\n\n\nextra.ica\nExtracranial ICA occlusion (1 = Yes, 0 = No)\n\n\ntime.rand\nTime from stroke onset to study randomization, in minutes\n\n\ntime.punc\nTime from stroke onset to groin puncture, in minutes (only if Intervention)\n\n\n\nHere’s a quick look at the simulated data in fakestroke.\n\nfakestroke\n\n# A tibble: 500 × 18\n   studyid trt         age sex   nihss locat…¹ hx.isch  afib    dm mrankin   sbp\n   <chr>   <chr>     <dbl> <chr> <dbl> <chr>   <chr>   <dbl> <dbl> <chr>   <dbl>\n 1 z001    Control      53 Male     21 Right   No          0     0 2         127\n 2 z002    Interven…    51 Male     23 Left    No          1     0 0         137\n 3 z003    Control      68 Fema…    11 Right   No          0     0 0         138\n 4 z004    Control      28 Male     22 Left    No          0     0 0         122\n 5 z005    Control      91 Male     24 Right   No          0     0 0         162\n 6 z006    Control      34 Fema…    18 Left    No          0     0 2         166\n 7 z007    Interven…    75 Male     25 Right   No          0     0 0         140\n 8 z008    Control      89 Fema…    18 Right   No          0     0 0         157\n 9 z009    Control      75 Male     25 Left    No          1     0 2         129\n10 z010    Interven…    26 Fema…    27 Right   No          0     0 0         143\n# … with 490 more rows, 7 more variables: iv.altep <chr>, time.iv <dbl>,\n#   aspects <dbl>, ia.occlus <chr>, extra.ica <dbl>, time.rand <dbl>,\n#   time.punc <dbl>, and abbreviated variable name ¹​location"
  },
  {
    "objectID": "tableone.html#building-table-1-for-fakestroke-attempt-1",
    "href": "tableone.html#building-table-1-for-fakestroke-attempt-1",
    "title": "18  Building Table 1",
    "section": "18.6 Building Table 1 for fakestroke: Attempt 1",
    "text": "18.6 Building Table 1 for fakestroke: Attempt 1\nOur goal, then, is to take the data in fakestroke.csv and use it to generate a Table 1 for the study that compares the 233 patients in the Intervention group to the 267 patients in the Control group, on all of the other variables (except study ID #) available. I’ll use the tableone package of functions available in R to help me complete this task. We’ll make a first attempt, using the CreateTableOne function in the tableone package. To use the function, we’ll need to specify:\n\nthe vars or variables we want to place in the rows of our Table 1 (which will include just about everything in the fakestroke data except the studyid code and the trt variable for which we have other plans, and the time.punc which applies only to subjects in the Intervention group.)\n\nA useful trick here is to use the dput function, specifically something like dput(names(fakestroke)) can be used to generate a list of all of the variables included in the fakestroke tibble, and then this can be copied and pasted into the vars specification, saving some typing.\n\nthe strata which indicates the levels want to use in the columns of our Table 1 (for us, that’s trt)\n\n\nfs.vars <- c(\"age\", \"sex\", \"nihss\", \"location\", \n          \"hx.isch\", \"afib\", \"dm\", \"mrankin\", \"sbp\",\n          \"iv.altep\", \"time.iv\", \"aspects\", \n          \"ia.occlus\", \"extra.ica\", \"time.rand\")\n\nfs.trt <- c(\"trt\")\n\natt1 <- CreateTableOne(data = fakestroke, \n                       vars = fs.vars, \n                       strata = fs.trt)\nprint(att1)\n\n                       Stratified by trt\n                        Control        Intervention   p      test\n  n                        267            233                    \n  age (mean (SD))        65.38 (16.10)  63.93 (18.09)  0.343     \n  sex = Male (%)           157 (58.8)     135 (57.9)   0.917     \n  nihss (mean (SD))      18.08 (4.32)   17.97 (5.04)   0.787     \n  location = Right (%)     114 (42.7)     117 (50.2)   0.111     \n  hx.isch = Yes (%)         25 ( 9.4)      29 (12.4)   0.335     \n  afib (mean (SD))        0.26 (0.44)    0.28 (0.45)   0.534     \n  dm (mean (SD))          0.13 (0.33)    0.12 (0.33)   0.923     \n  mrankin (%)                                          0.922     \n     > 2                    11 ( 4.1)      10 ( 4.3)             \n     0                     214 (80.1)     190 (81.5)             \n     1                      29 (10.9)      21 ( 9.0)             \n     2                      13 ( 4.9)      12 ( 5.2)             \n  sbp (mean (SD))       145.00 (24.40) 146.03 (26.00)  0.647     \n  iv.altep = Yes (%)       242 (90.6)     203 (87.1)   0.267     \n  time.iv (mean (SD))    87.96 (26.01)  98.22 (45.48)  0.003     \n  aspects (mean (SD))     8.65 (1.47)    8.35 (1.64)   0.033     \n  ia.occlus (%)                                        0.795     \n     A1 or A2                2 ( 0.8)       1 ( 0.4)             \n     ICA with M1            75 (28.2)      59 (25.3)             \n     Intracranial ICA        3 ( 1.1)       1 ( 0.4)             \n     M1                    165 (62.0)     154 (66.1)             \n     M2                     21 ( 7.9)      18 ( 7.7)             \n  extra.ica (mean (SD))   0.26 (0.44)    0.32 (0.47)   0.150     \n  time.rand (mean (SD)) 213.88 (70.29) 202.51 (57.33)  0.051     \n\n\n\n18.6.1 Some of this is very useful, and other parts need to be fixed.\n\nThe 1/0 variables (afib, dm, extra.ica) might be better if they were treated as the factors they are, and reported as the Yes/No variables are reported, with counts and percentages rather than with means and standard deviations.\nIn some cases, we may prefer to re-order the levels of the categorical (factor) variables, particularly the mrankin variable, but also the ia.occlus variable. It would also be more typical to put the Intervention group to the left and the Control group to the right, so we may need to adjust our trt variable’s levels accordingly.\nFor each of the quantitative variables (age, nihss, sbp, time.iv, aspects, extra.ica, time.rand and time.punc) we should make a decision whether a summary with mean and standard deviation is appropriate, or whether we should instead summarize with, say, the median and quartiles. A mean and standard deviation really only yields an appropriate summary when the data are least approximately Normally distributed. This will make the p values a bit more reasonable, too. The test column in the first attempt will soon have something useful to tell us.\nIf we’d left in the time.punc variable, we’d get some warnings, having to do with the fact that time.punc is only relevant to patients in the Intervention group.\n\n\n\n18.6.2 fakestroke Cleaning Up Categorical Variables\nLet’s specify each of the categorical variables as categorical explicitly. This helps the CreateTableOne function treat them appropriately, and display them with counts and percentages. This includes all of the 1/0, Yes/No and multi-categorical variables.\n\nfs.factorvars <- c(\"sex\", \"location\", \"hx.isch\", \"afib\", \"dm\", \n                   \"mrankin\", \"iv.altep\", \"ia.occlus\", \"extra.ica\")\n\nThen we simply add a factorVars = fs.factorvars call to the CreateTableOne function.\nWe also want to re-order some of those categorical variables, so that the levels are more useful to us. Specifically, we want to:\n\nplace Intervention before Control in the trt variable,\nreorder the mrankin scale as 0, 1, 2, > 2, and\nrearrange the ia.occlus variable to the order4 presented in Berkhemer et al. (2015).\n\nTo accomplish this, we’ll use the fct_relevel function from the forcats package (loaded with the rest of the core tidyverse packages) to reorder our levels manually.\n\nfakestroke <- fakestroke %>%\n    mutate(trt = fct_relevel(trt, \"Intervention\", \"Control\"),\n           mrankin = fct_relevel(mrankin, \"0\", \"1\", \"2\", \"> 2\"),\n           ia.occlus = fct_relevel(ia.occlus, \"Intracranial ICA\", \n                                   \"ICA with M1\", \"M1\", \"M2\", \n                                   \"A1 or A2\")\n           )"
  },
  {
    "objectID": "tableone.html#fakestroke-table-1-attempt-2",
    "href": "tableone.html#fakestroke-table-1-attempt-2",
    "title": "18  Building Table 1",
    "section": "18.7 fakestroke Table 1: Attempt 2",
    "text": "18.7 fakestroke Table 1: Attempt 2\n\natt2 <- CreateTableOne(data = fakestroke, \n                       vars = fs.vars,\n                       factorVars = fs.factorvars,\n                       strata = fs.trt)\nprint(att2)\n\n                       Stratified by trt\n                        Intervention   Control        p      test\n  n                        233            267                    \n  age (mean (SD))        63.93 (18.09)  65.38 (16.10)  0.343     \n  sex = Male (%)           135 (57.9)     157 (58.8)   0.917     \n  nihss (mean (SD))      17.97 (5.04)   18.08 (4.32)   0.787     \n  location = Right (%)     117 (50.2)     114 (42.7)   0.111     \n  hx.isch = Yes (%)         29 (12.4)      25 ( 9.4)   0.335     \n  afib = 1 (%)              66 (28.3)      69 (25.8)   0.601     \n  dm = 1 (%)                29 (12.4)      34 (12.7)   1.000     \n  mrankin (%)                                          0.922     \n     0                     190 (81.5)     214 (80.1)             \n     1                      21 ( 9.0)      29 (10.9)             \n     2                      12 ( 5.2)      13 ( 4.9)             \n     > 2                    10 ( 4.3)      11 ( 4.1)             \n  sbp (mean (SD))       146.03 (26.00) 145.00 (24.40)  0.647     \n  iv.altep = Yes (%)       203 (87.1)     242 (90.6)   0.267     \n  time.iv (mean (SD))    98.22 (45.48)  87.96 (26.01)  0.003     \n  aspects (mean (SD))     8.35 (1.64)    8.65 (1.47)   0.033     \n  ia.occlus (%)                                        0.795     \n     Intracranial ICA        1 ( 0.4)       3 ( 1.1)             \n     ICA with M1            59 (25.3)      75 (28.2)             \n     M1                    154 (66.1)     165 (62.0)             \n     M2                     18 ( 7.7)      21 ( 7.9)             \n     A1 or A2                1 ( 0.4)       2 ( 0.8)             \n  extra.ica = 1 (%)         75 (32.2)      70 (26.3)   0.179     \n  time.rand (mean (SD)) 202.51 (57.33) 213.88 (70.29)  0.051     \n\n\nThe categorical data presentation looks much improved.\n\n18.7.1 What summaries should we show?\nNow, we’ll move on to the issue of making a decision about what type of summary to show for the quantitative variables. Since the fakestroke data are just simulated and only match the summary statistics of the original results, not the details, we’ll adopt the decisions made by Berkhemer et al. (2015), which were to use medians and interquartile ranges to summarize the distributions of all of the continuous variables except systolic blood pressure.\n\nSpecifying certain quantitative variables as non-normal causes R to show them with medians and the 25th and 75th percentiles, rather than means and standard deviations, and also causes those variables to be tested using non-parametric tests, like the Wilcoxon signed rank test, rather than the t test. The test column indicates this with the word nonnorm.\n\nIn real data situations, what should we do? The answer is to look at the data. I would not make the decision as to which approach to take without first plotting (perhaps in a histogram or a Normal Q-Q plot) the observed distributions in each of the two samples, so that I could make a sound decision about whether Normality was a reasonable assumption. If the means and medians are meaningfully different from each other, this is especially important.\nTo be honest, though, if the variable in question is a relatively unimportant covariate and the p values for the two approaches are nearly the same, I’d say that further investigation is rarely important,\n\nSpecifying exact tests for certain categorical variables (we’ll try this for the location and mrankin variables) can be done, and these changes will be noted in the test column, as well.\n\nIn real data situations, I would rarely be concerned about this issue, and often choose Pearson (approximate) options across the board. This is reasonable so long as the number of subjects falling in each category is reasonably large, say above 10. If not, then an exact test may be a tiny improvement.\nParaphrasing Rosenbaum (2017), having an exact rather than an approximate test result is about as valuable as having a nice crease in your trousers.\n\n\nTo finish our Table 1, then, we need to specify which variables should be treated as non-Normal in the print statement - notice that we don’t need to redo the CreateTableOne for this change.\n\nprint(att2, \n      nonnormal = c(\"age\", \"nihss\", \"time.iv\", \"aspects\", \"time.rand\"),\n      exact = c(\"location\", \"mrankin\"))\n\n                          Stratified by trt\n                           Intervention            Control                \n  n                           233                     267                 \n  age (median [IQR])        65.80 [54.50, 76.00]    65.70 [55.75, 76.20]  \n  sex = Male (%)              135 (57.9)              157 (58.8)          \n  nihss (median [IQR])      17.00 [14.00, 21.00]    18.00 [14.00, 22.00]  \n  location = Right (%)        117 (50.2)              114 (42.7)          \n  hx.isch = Yes (%)            29 (12.4)               25 ( 9.4)          \n  afib = 1 (%)                 66 (28.3)               69 (25.8)          \n  dm = 1 (%)                   29 (12.4)               34 (12.7)          \n  mrankin (%)                                                             \n     0                        190 (81.5)              214 (80.1)          \n     1                         21 ( 9.0)               29 (10.9)          \n     2                         12 ( 5.2)               13 ( 4.9)          \n     > 2                       10 ( 4.3)               11 ( 4.1)          \n  sbp (mean (SD))          146.03 (26.00)          145.00 (24.40)         \n  iv.altep = Yes (%)          203 (87.1)              242 (90.6)          \n  time.iv (median [IQR])    85.00 [67.00, 110.00]   87.00 [65.00, 116.00] \n  aspects (median [IQR])     9.00 [7.00, 10.00]      9.00 [8.00, 10.00]   \n  ia.occlus (%)                                                           \n     Intracranial ICA           1 ( 0.4)                3 ( 1.1)          \n     ICA with M1               59 (25.3)               75 (28.2)          \n     M1                       154 (66.1)              165 (62.0)          \n     M2                        18 ( 7.7)               21 ( 7.9)          \n     A1 or A2                   1 ( 0.4)                2 ( 0.8)          \n  extra.ica = 1 (%)            75 (32.2)               70 (26.3)          \n  time.rand (median [IQR]) 204.00 [152.00, 249.50] 196.00 [149.00, 266.00]\n                          Stratified by trt\n                           p      test   \n  n                                      \n  age (median [IQR])        0.579 nonnorm\n  sex = Male (%)            0.917        \n  nihss (median [IQR])      0.453 nonnorm\n  location = Right (%)      0.106 exact  \n  hx.isch = Yes (%)         0.335        \n  afib = 1 (%)              0.601        \n  dm = 1 (%)                1.000        \n  mrankin (%)               0.917 exact  \n     0                                   \n     1                                   \n     2                                   \n     > 2                                 \n  sbp (mean (SD))           0.647        \n  iv.altep = Yes (%)        0.267        \n  time.iv (median [IQR])    0.596 nonnorm\n  aspects (median [IQR])    0.075 nonnorm\n  ia.occlus (%)             0.795        \n     Intracranial ICA                    \n     ICA with M1                         \n     M1                                  \n     M2                                  \n     A1 or A2                            \n  extra.ica = 1 (%)         0.179        \n  time.rand (median [IQR])  0.251 nonnorm"
  },
  {
    "objectID": "tableone.html#obtaining-a-more-detailed-summary",
    "href": "tableone.html#obtaining-a-more-detailed-summary",
    "title": "18  Building Table 1",
    "section": "18.8 Obtaining a more detailed Summary",
    "text": "18.8 Obtaining a more detailed Summary\nIf this was a real data set, we’d want to get a more detailed description of the data to make decisions about things like potentially collapsing categories of a variable, or whether or not a normal distribution was useful for a particular continuous variable, etc. You can do this with the summary command applied to a created Table 1, which shows, among other things, the effect of changing from normal to non-normal p values for continuous variables, and from approximate to “exact” p values for categorical factors.\nAgain, as noted above, in a real data situation, we’d want to plot the quantitative variables (within each group) to make a smart decision about whether a t test or Wilcoxon approach is more appropriate.\nNote in the summary below that we have some missing values here. Often, we’ll present this information within the Table 1, as well.\n\nsummary(att2)\n\n\n     ### Summary of continuous variables ###\n\ntrt: Intervention\n            n miss p.miss mean sd median p25 p75 min max  skew  kurt\nage       233    0    0.0   64 18     66  54  76  23  96 -0.34 -0.52\nnihss     233    0    0.0   18  5     17  14  21  10  28  0.48 -0.74\nsbp       233    0    0.0  146 26    146 129 164  78 214 -0.07 -0.22\ntime.iv   233   30   12.9   98 45     85  67 110  42 218  1.03  0.08\naspects   233    0    0.0    8  2      9   7  10   5  10 -0.56 -0.98\ntime.rand 233    2    0.9  203 57    204 152 250 100 300  0.01 -1.16\n------------------------------------------------------------ \ntrt: Control\n            n miss p.miss mean sd median p25 p75 min max   skew  kurt\nage       267    0    0.0   65 16     66  56  76  24  94 -0.296 -0.28\nnihss     267    0    0.0   18  4     18  14  22  11  25  0.017 -1.24\nsbp       267    1    0.4  145 24    145 128 161  82 231  0.156  0.08\ntime.iv   267   25    9.4   88 26     87  65 116  44 130  0.001 -1.32\naspects   267    4    1.5    9  1      9   8  10   5  10 -1.071  0.36\ntime.rand 267    0    0.0  214 70    196 149 266 120 360  0.508 -0.93\n\np-values\n              pNormal pNonNormal\nage       0.342813660 0.57856976\nnihss     0.787487252 0.45311695\nsbp       0.647157646 0.51346132\ntime.iv   0.003073372 0.59641104\naspects   0.032662901 0.07464683\ntime.rand 0.050803672 0.25134327\n\nStandardize mean differences\n              1 vs 2\nage       0.08478764\nnihss     0.02405390\nsbp       0.04100833\ntime.iv   0.27691223\naspects   0.19210662\ntime.rand 0.17720957\n\n=======================================================================================\n\n     ### Summary of categorical variables ### \n\ntrt: Intervention\n       var   n miss p.miss            level freq percent cum.percent\n       sex 233    0    0.0           Female   98    42.1        42.1\n                                       Male  135    57.9       100.0\n                                                                    \n  location 233    0    0.0             Left  116    49.8        49.8\n                                      Right  117    50.2       100.0\n                                                                    \n   hx.isch 233    0    0.0               No  204    87.6        87.6\n                                        Yes   29    12.4       100.0\n                                                                    \n      afib 233    0    0.0                0  167    71.7        71.7\n                                          1   66    28.3       100.0\n                                                                    \n        dm 233    0    0.0                0  204    87.6        87.6\n                                          1   29    12.4       100.0\n                                                                    \n   mrankin 233    0    0.0                0  190    81.5        81.5\n                                          1   21     9.0        90.6\n                                          2   12     5.2        95.7\n                                        > 2   10     4.3       100.0\n                                                                    \n  iv.altep 233    0    0.0               No   30    12.9        12.9\n                                        Yes  203    87.1       100.0\n                                                                    \n ia.occlus 233    0    0.0 Intracranial ICA    1     0.4         0.4\n                                ICA with M1   59    25.3        25.8\n                                         M1  154    66.1        91.8\n                                         M2   18     7.7        99.6\n                                   A1 or A2    1     0.4       100.0\n                                                                    \n extra.ica 233    0    0.0                0  158    67.8        67.8\n                                          1   75    32.2       100.0\n                                                                    \n------------------------------------------------------------ \ntrt: Control\n       var   n miss p.miss            level freq percent cum.percent\n       sex 267    0    0.0           Female  110    41.2        41.2\n                                       Male  157    58.8       100.0\n                                                                    \n  location 267    0    0.0             Left  153    57.3        57.3\n                                      Right  114    42.7       100.0\n                                                                    \n   hx.isch 267    0    0.0               No  242    90.6        90.6\n                                        Yes   25     9.4       100.0\n                                                                    \n      afib 267    0    0.0                0  198    74.2        74.2\n                                          1   69    25.8       100.0\n                                                                    \n        dm 267    0    0.0                0  233    87.3        87.3\n                                          1   34    12.7       100.0\n                                                                    \n   mrankin 267    0    0.0                0  214    80.1        80.1\n                                          1   29    10.9        91.0\n                                          2   13     4.9        95.9\n                                        > 2   11     4.1       100.0\n                                                                    \n  iv.altep 267    0    0.0               No   25     9.4         9.4\n                                        Yes  242    90.6       100.0\n                                                                    \n ia.occlus 267    1    0.4 Intracranial ICA    3     1.1         1.1\n                                ICA with M1   75    28.2        29.3\n                                         M1  165    62.0        91.4\n                                         M2   21     7.9        99.2\n                                   A1 or A2    2     0.8       100.0\n                                                                    \n extra.ica 267    1    0.4                0  196    73.7        73.7\n                                          1   70    26.3       100.0\n                                                                    \n\np-values\n            pApprox    pExact\nsex       0.9171387 0.8561188\nlocation  0.1113553 0.1056020\nhx.isch   0.3352617 0.3124683\nafib      0.6009691 0.5460206\ndm        1.0000000 1.0000000\nmrankin   0.9224798 0.9173657\niv.altep  0.2674968 0.2518374\nia.occlus 0.7945580 0.8189090\nextra.ica 0.1793385 0.1667574\n\nStandardize mean differences\n               1 vs 2\nsex       0.017479025\nlocation  0.151168444\nhx.isch   0.099032275\nafib      0.055906317\ndm        0.008673478\nmrankin   0.062543164\niv.altep  0.111897009\nia.occlus 0.117394890\nextra.ica 0.129370206\n\n\nIn this case, I have simulated the data to mirror the results in the published Table 1 for this study. In no way have I captured the full range of the real data, or any of the relationships in that data, so it’s more important here to see what’s available in the analysis, rather than to interpret it closely in the clinical context."
  },
  {
    "objectID": "tableone.html#exporting-the-completed-table-1-from-r-to-excel-or-word",
    "href": "tableone.html#exporting-the-completed-table-1-from-r-to-excel-or-word",
    "title": "18  Building Table 1",
    "section": "18.9 Exporting the Completed Table 1 from R to Excel or Word",
    "text": "18.9 Exporting the Completed Table 1 from R to Excel or Word\nOnce you’ve built the table and are generally satisfied with it, you’ll probably want to be able to drop it into Excel or Word for final cleanup.\n\n18.9.1 Approach A: Save and open in Excel\nOne option is to save the Table 1 to a .csv file within our data subfolder (note that the data folder must already exist), which you can then open directly in Excel. This is the approach I generally use. Note the addition of some quote, noSpaces and printToggle selections here.\n\nfs.table1save <- print(att2, \n      nonnormal = c(\"age\", \"nihss\", \"time.iv\", \"aspects\", \"time.rand\"),\n      exact = c(\"location\", \"mrankin\"),\n      quote = FALSE, noSpaces = TRUE, printToggle = FALSE)\n\nwrite.csv(fs.table1save, file = \"data/fs-table1.csv\")\n\nWhen I then open the fs-table1.csv file in Excel, it looks like this:\n\n\n\n\n\nAnd from here, I can either drop it directly into Word, or present it as is, or start tweaking it to meet formatting needs.\n\n\n18.9.2 Approach B: Produce the Table so you can cut and paste it\n\nprint(att2, \n      nonnormal = c(\"age\", \"nihss\", \"time.iv\", \"aspects\", \"time.rand\"),\n      exact = c(\"location\", \"mrankin\"),\n      quote = TRUE, noSpaces = TRUE)\n\nThis will look like a mess by itself, but if you:\n\ncopy and paste that mess into Excel\nselect Text to Columns from the Data menu\nselect Delimited, then Space and select Treat consecutive delimiters as one\n\nyou should get something usable again.\nOr, in Word,\n\ninsert the text\nselect the text with your mouse\nselect Insert … Table … Convert Text to Table\nplace a quotation mark in the “Other” area under Separate text at …\n\nAfter dropping blank columns, the result looks pretty good."
  },
  {
    "objectID": "tableone.html#a-controlled-biological-experiment---the-blood-brain-barrier",
    "href": "tableone.html#a-controlled-biological-experiment---the-blood-brain-barrier",
    "title": "18  Building Table 1",
    "section": "18.10 A Controlled Biological Experiment - The Blood-Brain Barrier",
    "text": "18.10 A Controlled Biological Experiment - The Blood-Brain Barrier\nMy source for the data and the following explanatory paragraph is page 307 from Ramsey and Schafer (2002). The original data come from Barnett et al. (1995).\n\nThe human brain (and that of rats, coincidentally) is protected from the bacteria and toxins that course through the bloodstream by something called the blood-brain barrier. After a method of disrupting the barrier was developed, researchers tested this new mechanism, as follows. A series of 34 rats were inoculated with human lung cancer cells to induce brain tumors. After 9-11 days they were infused with either the barrier disruption (BD) solution or, as a control, a normal saline (NS) solution. Fifteen minutes later, the rats received a standard dose of a particular therapeutic antibody (L6-F(ab’)2. The key measure of the effectiveness of transmission across the brain-blood barrier is the ratio of the antibody concentration in the brain tumor to the antibody concentration in normal tissue outside the brain. The rats were then sacrificed, and the amounts of antibody in the brain tumor and in normal tissue from the liver were measured. The study’s primary objective is to determine whether the antibody concentration in the tumor increased when the blood-barrier disruption infusion was given, and if so, by how much?"
  },
  {
    "objectID": "tableone.html#the-bloodbrain.csv-file",
    "href": "tableone.html#the-bloodbrain.csv-file",
    "title": "18  Building Table 1",
    "section": "18.11 The bloodbrain.csv file",
    "text": "18.11 The bloodbrain.csv file\nConsider the data, available on our Data and Code website in the bloodbrain.csv file, which includes the following variables:\n\n\n\n\n\n\n\nVariable\nDescription\n\n\n\n\ncase\nidentification number for the rat (1 - 34)\n\n\nbrain\nan outcome: Brain tumor antibody count (per gram)\n\n\nliver\nan outcome: Liver antibody count (per gram)\n\n\ntlratio\nan outcome: tumor / liver concentration ratio\n\n\nsolution\nthe treatment: BD (barrier disruption) or NS (normal saline)\n\n\nsactime\na design variable: Sacrifice time (hours; either 0.5, 3, 24 or 72)\n\n\npostin\ncovariate: Days post-inoculation of lung cancer cells (9, 10 or 11)\n\n\nsex\ncovariate: M or F\n\n\nwt.init\ncovariate: Initial weight (grams)\n\n\nwt.loss\ncovariate: Weight loss (grams)\n\n\nwt.tumor\ncovariate: Tumor weight (10-4 grams)\n\n\n\nAnd here’s what the data look like in R.\n\nbloodbrain\n\n# A tibble: 34 × 11\n    case  brain   liver tlratio solution sactime postin sex   wt.init wt.loss\n   <dbl>  <dbl>   <dbl>   <dbl> <chr>      <dbl>  <dbl> <chr>   <dbl>   <dbl>\n 1     1  41081 1456164  0.0282 BD           0.5     10 F         239     5.9\n 2     2  44286 1602171  0.0276 BD           0.5     10 F         225     4  \n 3     3 102926 1601936  0.0642 BD           0.5     10 F         224    -4.9\n 4     4  25927 1776411  0.0146 BD           0.5     10 F         184     9.8\n 5     5  42643 1351184  0.0316 BD           0.5     10 F         250     6  \n 6     6  31342 1790863  0.0175 NS           0.5     10 F         196     7.7\n 7     7  22815 1633386  0.0140 NS           0.5     10 F         200     0.5\n 8     8  16629 1618757  0.0103 NS           0.5     10 F         273     4  \n 9     9  22315 1567602  0.0142 NS           0.5     10 F         216     2.8\n10    10  77961 1060057  0.0735 BD           3       10 F         267     2.6\n# … with 24 more rows, and 1 more variable: wt.tumor <dbl>"
  },
  {
    "objectID": "tableone.html#a-table-1-for-bloodbrain",
    "href": "tableone.html#a-table-1-for-bloodbrain",
    "title": "18  Building Table 1",
    "section": "18.12 A Table 1 for bloodbrain",
    "text": "18.12 A Table 1 for bloodbrain\nBarnett et al. (1995) did not provide a Table 1 for these data, so let’s build one to compare the two solutions (BD vs. NS) on the covariates and outcomes, plus the natural logarithm of the tumor/liver concentration ratio (tlratio). We’ll opt to treat the sacrifice time (sactime) and the days post-inoculation of lung cancer cells (postin) as categorical rather than quantitative variables.\n\nbloodbrain <- bloodbrain %>%\n    mutate(logTL = log(tlratio))\n\ndput(names(bloodbrain))\n\nc(\"case\", \"brain\", \"liver\", \"tlratio\", \"solution\", \"sactime\", \n\"postin\", \"sex\", \"wt.init\", \"wt.loss\", \"wt.tumor\", \"logTL\")\n\n\nOK - there’s the list of variables we’ll need. I’ll put the outcomes at the bottom of the table.\n\nbb.vars <- c(\"sactime\", \"postin\", \"sex\", \"wt.init\", \"wt.loss\", \n             \"wt.tumor\", \"brain\", \"liver\", \"tlratio\", \"logTL\")\n\nbb.factors <- c(\"sactime\", \"sex\", \"postin\")\n\nbb.att1 <- CreateTableOne(data = bloodbrain,\n                          vars = bb.vars,\n                          factorVars = bb.factors,\n                          strata = c(\"solution\"))\nsummary(bb.att1)\n\n\n     ### Summary of continuous variables ###\n\nsolution: BD\n          n miss p.miss   mean    sd median    p25   p75    min   max  skew\nwt.init  17    0      0    243 3e+01  2e+02  2e+02 3e+02  2e+02 3e+02 -0.39\nwt.loss  17    0      0      3 5e+00  4e+00  1e+00 6e+00 -5e+00 1e+01 -0.10\nwt.tumor 17    0      0    157 8e+01  2e+02  1e+02 2e+02  2e+01 4e+02  0.53\nbrain    17    0      0  56043 3e+04  5e+04  4e+04 8e+04  6e+03 1e+05  0.29\nliver    17    0      0 672577 7e+05  6e+05  2e+04 1e+06  2e+03 2e+06  0.35\ntlratio  17    0      0      2 3e+00  1e-01  6e-02 3e+00  1e-02 9e+00  1.58\nlogTL    17    0      0     -1 2e+00 -2e+00 -3e+00 1e+00 -4e+00 2e+00  0.08\n         kurt\nwt.init   0.7\nwt.loss   0.2\nwt.tumor  1.0\nbrain    -0.6\nliver    -1.7\ntlratio   1.7\nlogTL    -1.7\n------------------------------------------------------------ \nsolution: NS\n          n miss p.miss   mean    sd median    p25    p75    min   max  skew\nwt.init  17    0      0    240 3e+01  2e+02  2e+02  3e+02  2e+02 3e+02  0.33\nwt.loss  17    0      0      4 4e+00  3e+00  2e+00  7e+00 -4e+00 1e+01 -0.09\nwt.tumor 17    0      0    209 1e+02  2e+02  2e+02  3e+02  3e+01 5e+02  0.63\nbrain    17    0      0  23887 1e+04  2e+04  1e+04  3e+04  1e+03 5e+04  0.30\nliver    17    0      0 664975 7e+05  7e+05  2e+04  1e+06  9e+02 2e+06  0.40\ntlratio  17    0      0      1 2e+00  5e-02  3e-02  9e-01  1e-02 7e+00  2.27\nlogTL    17    0      0     -2 2e+00 -3e+00 -3e+00 -7e-02 -5e+00 2e+00  0.27\n          kurt\nwt.init  -0.48\nwt.loss   0.08\nwt.tumor  0.77\nbrain    -0.35\nliver    -1.56\ntlratio   4.84\nlogTL    -1.61\n\np-values\n             pNormal  pNonNormal\nwt.init  0.807308940 0.641940278\nwt.loss  0.683756156 0.876749808\nwt.tumor 0.151510151 0.190482094\nbrain    0.001027678 0.002579901\nliver    0.974853609 0.904045603\ntlratio  0.320501715 0.221425879\nlogTL    0.351633525 0.221425879\n\nStandardize mean differences\n             1 vs 2\nwt.init  0.08435244\nwt.loss  0.14099823\nwt.tumor 0.50397184\nbrain    1.23884159\nliver    0.01089667\ntlratio  0.34611465\nlogTL    0.32420504\n\n=======================================================================================\n\n     ### Summary of categorical variables ### \n\nsolution: BD\n     var  n miss p.miss level freq percent cum.percent\n sactime 17    0    0.0   0.5    5    29.4        29.4\n                            3    4    23.5        52.9\n                           24    4    23.5        76.5\n                           72    4    23.5       100.0\n                                                      \n  postin 17    0    0.0     9    1     5.9         5.9\n                           10   14    82.4        88.2\n                           11    2    11.8       100.0\n                                                      \n     sex 17    0    0.0     F   13    76.5        76.5\n                            M    4    23.5       100.0\n                                                      \n------------------------------------------------------------ \nsolution: NS\n     var  n miss p.miss level freq percent cum.percent\n sactime 17    0    0.0   0.5    4    23.5        23.5\n                            3    5    29.4        52.9\n                           24    4    23.5        76.5\n                           72    4    23.5       100.0\n                                                      \n  postin 17    0    0.0     9    2    11.8        11.8\n                           10   13    76.5        88.2\n                           11    2    11.8       100.0\n                                                      \n     sex 17    0    0.0     F   13    76.5        76.5\n                            M    4    23.5       100.0\n                                                      \n\np-values\n          pApprox pExact\nsactime 0.9739246      1\npostin  0.8309504      1\nsex     1.0000000      1\n\nStandardize mean differences\n           1 vs 2\nsactime 0.1622214\npostin  0.2098877\nsex     0.0000000\n\n\nNote that, in this particular case, the decisions we make about normality vs. non-normality (for quantitative variables) and the decisions we make about approximate vs. exact testing (for categorical variables) won’t actually change the implications of the p values. Each approach gives similar results for each variable. Of course, that’s not always true.\n\n18.12.1 Generate final Table 1 for bloodbrain\nI’ll choose to treat tlratio and its logarithm as non-Normal, but otherwise, use t tests, but admittedly, that’s an arbitrary decision, really.\n\nprint(bb.att1, nonnormal = c(\"tlratio\", \"logTL\"))\n\n                        Stratified by solution\n                         BD                      NS                      \n  n                             17                      17               \n  sactime (%)                                                            \n     0.5                         5 (29.4)                4 (23.5)        \n     3                           4 (23.5)                5 (29.4)        \n     24                          4 (23.5)                4 (23.5)        \n     72                          4 (23.5)                4 (23.5)        \n  postin (%)                                                             \n     9                           1 ( 5.9)                2 (11.8)        \n     10                         14 (82.4)               13 (76.5)        \n     11                          2 (11.8)                2 (11.8)        \n  sex = M (%)                    4 (23.5)                4 (23.5)        \n  wt.init (mean (SD))       242.82 (27.23)          240.47 (28.54)       \n  wt.loss (mean (SD))         3.34 (4.68)             3.94 (3.88)        \n  wt.tumor (mean (SD))      157.29 (84.00)          208.53 (116.68)      \n  brain (mean (SD))       56043.41 (33675.40)     23887.18 (14610.53)    \n  liver (mean (SD))      672577.35 (694479.58)   664975.47 (700773.13)   \n  tlratio (median [IQR])      0.12 [0.06, 2.84]       0.05 [0.03, 0.94]  \n  logTL (median [IQR])       -2.10 [-2.74, 1.04]     -2.95 [-3.41, -0.07]\n                        Stratified by solution\n                         p      test   \n  n                                    \n  sactime (%)             0.974        \n     0.5                               \n     3                                 \n     24                                \n     72                                \n  postin (%)              0.831        \n     9                                 \n     10                                \n     11                                \n  sex = M (%)             1.000        \n  wt.init (mean (SD))     0.807        \n  wt.loss (mean (SD))     0.684        \n  wt.tumor (mean (SD))    0.152        \n  brain (mean (SD))       0.001        \n  liver (mean (SD))       0.975        \n  tlratio (median [IQR])  0.221 nonnorm\n  logTL (median [IQR])    0.221 nonnorm\n\n\nOr, we can get an Excel-readable version placed in a data subfolder, using\n\nbb.t1 <- print(bb.att1, nonnormal = c(\"tlratio\", \"logTL\"), quote = FALSE,\n               noSpaces = TRUE, printToggle = FALSE)\n\nwrite.csv(bb.t1, file = \"data/bb-table1.csv\")\n\nwhich, when dropped into Excel, will look like this:\n\n\n\n\n\nOne thing I would definitely clean up here, in practice, is to change the presentation of the p value for sex from 1 to > 0.99, or just omit it altogether. I’d also drop the computer-ese where possible, add units for the measures, round a lot, identify the outcomes carefully, and use notes to indicate deviations from the main approach.\n\n\n18.12.2 A More Finished Version (after Cleanup in Word)\n\n\n\n\n\n\n\n\n\nBarnett, Peggy A., Simon Roman-Golstein, Fred Ramsey, et al. 1995. “Differential Permeability and Quantitative MR Imaging of a Human Lung Carcinoma Brain Xenograft in the Nude Rat.” American Journal of Pathology 146(2): 436–49. https://www.ncbi.nlm.nih.gov/pmc/articles/PMC1869863/.\n\n\nBerkhemer, Olvert A., Puck S. S. Fransen, Debbie Buemer, et al. 2015. “A Randomized Trial of Intraarterial Treatment for Acute Ischemic Stroke.” New England Journal of Medicine 372: 11–20. http://www.nejm.org/doi/full/10.1056/NEJMoa1411587.\n\n\nRamsey, Fred L., and Daniel W. Schafer. 2002. The Statistical Sleuth: A Course in Methods of Data Analysis. Second Edition. Pacific Grove, CA: Duxbury.\n\n\nRosenbaum, Paul R. 2017. Observation and Experiment: An Introduction to Causal Inference. Cambridge, MA: Harvard University Press.\n\n\nRoy, Denis, Mario Talajic, Stanley Nattel, et al. 2008. “Rhythm Control Versus Rate Control for Atrial Fibrillation and Heart Failure.” New England Journal of Medicine 358: 2667–77. http://www.nejm.org/doi/full/10.1056/NEJMoa0708789.\n\n\nTolaney, Sara M, William T. Barry, T. Dang Chau, et al. 2015. “Adjuvant Paclitaxel and Trastuzumab for Node-Negative, HER2-Positive Breast Cancer.” New England Journal of Medicine 372: 134–41. http://www.nejm.org/doi/full/10.1056/NEJMoa1406281."
  },
  {
    "objectID": "logistic1.html#r-setup-used-here",
    "href": "logistic1.html#r-setup-used-here",
    "title": "19  Logistic Regression: The Foundations",
    "section": "19.1 R Setup Used Here",
    "text": "19.1 R Setup Used Here\n\nknitr::opts_chunk$set(comment = NA)\n\nlibrary(broom)\nlibrary(tidyverse) \n\ntheme_set(theme_bw())"
  },
  {
    "objectID": "logistic1.html#a-first-attempt-a-linear-probability-model",
    "href": "logistic1.html#a-first-attempt-a-linear-probability-model",
    "title": "19  Logistic Regression: The Foundations",
    "section": "19.2 A First Attempt: A Linear Probability Model",
    "text": "19.2 A First Attempt: A Linear Probability Model\nSuppose we want to predict a binary outcome which takes on the value 1 or 0, based on a single quantitative predictor. Let y be a 1/0 outcome, and x be a quantitative predictor in the following simulation.\n\nset.seed(432)\nsim12 <- tibble(x = rnorm(100, 10, 3),\n                err = rnorm(100, 0, 2),\n                y = ifelse(x + err > 10, 1, 0))\n\nsim12 <- select(sim12, x, y)\n\nggplot(sim12, aes(x = x, y = y)) + geom_point()\n\n\n\n\nNow, we want to use our variable x here to predict our variable y (which takes on the values 0 and 1).\nOne approach to doing this would be a linear probability model, as follows:\n\nmod12a <- lm(y ~ x, data = sim12) \n\nsummary(mod12a)\n\n\nCall:\nlm(formula = y ~ x, data = sim12)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.74104 -0.23411 -0.02894  0.23117  0.83153 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) -0.72761    0.12272  -5.929 4.57e-08 ***\nx            0.12620    0.01219  10.349  < 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.3491 on 98 degrees of freedom\nMultiple R-squared:  0.5222,    Adjusted R-squared:  0.5173 \nF-statistic: 107.1 on 1 and 98 DF,  p-value: < 2.2e-16\n\n\nHere’s a picture of this model. What’s wrong here?\n\nggplot(sim12, aes(x = x, y = y)) + \n    geom_point() +\n    geom_smooth(method = \"lm\", formula = y ~ x, se = TRUE) + \n    labs(title = \"Linear Probability Model\")\n\n\n\n\nIf y can only take the values 0 and 1 (or, more precisely, if we’re trying to predict the value \\(\\pi\\) = Pr(y = 1)) then what do we do with the predictions that are outside the range of (0, 1)?"
  },
  {
    "objectID": "logistic1.html#logistic-regression",
    "href": "logistic1.html#logistic-regression",
    "title": "19  Logistic Regression: The Foundations",
    "section": "19.3 Logistic Regression",
    "text": "19.3 Logistic Regression\nLogistic regression is the most common model used when the outcome is binary. Our response variable is assumed to take on two values, zero or one, and we then describe the probability of a “one” response, given a linear function of explanatory predictors. We use logistic regression rather than linear regression for predicting binary outcomes. Linear regression approaches to the problem of predicting probabilities are problematic for several reasons - not least of which being that they predict probabilities greater than one and less than zero. There are several available alternatives, including probit regression and binomial regression, for the problem of predicting a binary outcome.\nLogistic regression is part of a class called generalized linear models which extend the linear regression model in a variety of ways. There are also several extensions to the logistic regression model, including multinomial logistic regression (which is used for nominal categorical outcomes with more than two levels) and ordered logistic regression (used for ordered multi-categorical outcomes.) The methods involved in binary logistic regression may also be extended to the case where the outcomes are proportions based on counts, often through grouped binary responses (the proportion of cells with chromosomal aberrations, or the proportion of subjects who develop a particular condition.)\nAlthough the models are different in some crucial ways, the practical use of logistic regression tracks well with much of what we’ve learned about linear regression."
  },
  {
    "objectID": "logistic1.html#the-logistic-regression-model",
    "href": "logistic1.html#the-logistic-regression-model",
    "title": "19  Logistic Regression: The Foundations",
    "section": "19.4 The Logistic Regression Model",
    "text": "19.4 The Logistic Regression Model\nA generalized linear model (or GLM) is a probability model in which the mean of an outcome is related to predictors through a regression equation. A link function g is used to relate the mean, \\(\\mu\\), to a linear regression of the predictors \\(X_1, X_2, ..., X_k\\).\n\\[\ng(\\mu) = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + ... + \\beta_k X_k\n\\]\nIn the case of a logistic regression model,\n\nthe mean \\(\\mu\\) of our 0/1 outcome is represented by \\(\\pi\\) which describes the probability of a “1” outcome.\nthe linking function we use in logistic regression makes use of the logit function, which is built on the natural logarithm."
  },
  {
    "objectID": "logistic1.html#the-link-function",
    "href": "logistic1.html#the-link-function",
    "title": "19  Logistic Regression: The Foundations",
    "section": "19.5 The Link Function",
    "text": "19.5 The Link Function\nLogistic regression is a non-linear regression approach, since the equation for the mean of the 0/1 Y values conditioned on the values of our predictors \\(X_1, X_2, ..., X_k\\) turns out to be non-linear in the \\(\\beta\\) coefficients. Its nonlinearity, however, is solely found in its link function, hence the term generalized linear model.\nThe particular link function we use in logistic regression is called the logit link.\n\\[\nlogit(\\pi) = log\\left( \\frac{\\pi}{1 - \\pi} \\right) = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + ... + \\beta_k X_k\n\\]\nThe inverse of the logit function is called the logistic function. If logit(\\(\\pi\\)) = \\(\\eta\\), then \\(\\pi = \\frac{exp(\\eta)}{1 + exp(\\eta)}\\)\nThe plot below displays the logistic function \\(y = \\frac{e^x}{1 + e^x}\\)\n\nset.seed(43201)\ntemp <- tibble(\n    x = runif(200, min = -6, max = 6),\n    y = exp(x) / (1 + exp(x)))\n\nggplot(temp, aes(x = x, y = y)) + \n    geom_line(linewidth = 2, col = \"blue\")\n\n\n\n\nAs you can see in the figure above, the logistic function \\(\\frac{e^x}{1 + e^x}\\) takes any value \\(x\\) in the real numbers and returns a value between 0 and 1."
  },
  {
    "objectID": "logistic1.html#the-logit-or-log-odds",
    "href": "logistic1.html#the-logit-or-log-odds",
    "title": "19  Logistic Regression: The Foundations",
    "section": "19.6 The logit or log odds",
    "text": "19.6 The logit or log odds\nWe usually focus on the logit in statistical work, which is the inverse of the logistic function.\n\nIf we have a probability \\(\\pi < 0.5\\), then \\(logit(\\pi) < 0\\).\nIf our probability \\(\\pi > 0.5\\), then \\(logit(\\pi) > 0\\).\nFinally, if \\(\\pi = 0.5\\), then \\(logit(\\pi) = 0\\)."
  },
  {
    "objectID": "logistic1.html#interpreting-the-coefficients-of-a-logistic-regression-model",
    "href": "logistic1.html#interpreting-the-coefficients-of-a-logistic-regression-model",
    "title": "19  Logistic Regression: The Foundations",
    "section": "19.7 Interpreting the Coefficients of a Logistic Regression Model",
    "text": "19.7 Interpreting the Coefficients of a Logistic Regression Model\nThe critical thing to remember in interpreting a logistic regression model is that the logit is the log odds function. Exponentiating the logit yields the odds.\nSo, suppose we have a yes/no outcome variable, where yes = 1, and no = 0, and \\(\\pi\\) = Pr(y = 1). Our model holds that:\n\\[\nlogit(\\pi) = log\\left( \\frac{\\pi}{1 - \\pi} \\right) = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + ... + \\beta_k X_k\n\\]\nThe odds of a yes response (the odds that Y = 1) at the level \\(X_1, X_2, ..., X_k\\) are:\n\\[\nOdds(Y = 1) = exp(\\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + ... + \\beta_k X_k)\n\\]\nThe probability of a yes response (Pr(y = 1), or \\(\\pi\\)) is just\n\\[\n\\pi = Pr(Y = 1) = \\frac{Odds(Y = 1)}{1 + Odds(Y = 1)} = \\frac{exp(\\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + ... + \\beta_k X_k)}{1 + exp(\\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + ... + \\beta_k X_k)}\n\\]"
  },
  {
    "objectID": "logistic1.html#the-logistic-regression-has-non-constant-variance",
    "href": "logistic1.html#the-logistic-regression-has-non-constant-variance",
    "title": "19  Logistic Regression: The Foundations",
    "section": "19.8 The Logistic Regression has non-constant variance",
    "text": "19.8 The Logistic Regression has non-constant variance\nIn ordinary least squares regression, the variance \\(Var(Y | X_1, X_2, ..., X_k) = \\sigma^2\\) is a constant that does not depend on the predictor values. This is not the case in logistic regression. The mean and variance specifications of the logistic regression model are quite different.\n\\[\nlogit(\\pi) = log\\left( \\frac{\\pi}{1 - \\pi} \\right) = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + ... + \\beta_k X_k \\\\\n\\mu[Y | X_1, ..., X_k] = \\pi, \\\\\nVar[Y | X_1, ..., X_k] = \\pi(1 - \\pi)\n\\]\nThe variance is now a function of the mean, and contains no additional parameter for us to estimate."
  },
  {
    "objectID": "logistic1.html#fitting-a-logistic-regression-model-to-our-simulated-data",
    "href": "logistic1.html#fitting-a-logistic-regression-model-to-our-simulated-data",
    "title": "19  Logistic Regression: The Foundations",
    "section": "19.9 Fitting a Logistic Regression Model to our Simulated Data",
    "text": "19.9 Fitting a Logistic Regression Model to our Simulated Data\nRecall the sim12 data we built earlier.\n\nggplot(sim12, aes(x = x, y = y)) + geom_point()\n\n\n\n\nHere is the fitted logistic regression model.\n\nmodel12b <- glm(y ~ x, data = sim12, family = binomial)\n\nmodel12b\n\n\nCall:  glm(formula = y ~ x, family = binomial, data = sim12)\n\nCoefficients:\n(Intercept)            x  \n    -9.1955       0.9566  \n\nDegrees of Freedom: 99 Total (i.e. Null);  98 Residual\nNull Deviance:      138.6 \nResidual Deviance: 70.03    AIC: 74.03\n\n\nThe logistic regression equation is:\n\\[\nlogit(Pr(y = 1)) = log\\left( \\frac{Pr(y = 1)}{1 - Pr(y = 1)} \\right) = -9.1955 + 0.9566 x\n\\]\nWe can exponentiate the results of this model to get to an equation about odds, and eventually, a prediction about probabilities. Suppose, for instance, that we are interested in the prediction when x = 12.\n\\[\nlogit(Pr(y = 1) | X = 12) = log\\left( \\frac{Pr(y = 1)}{1 - Pr(y = 1)} \\right) = -9.1955 + 0.9566 * 12 = 2.2837\n\\]\nAnd we can also get this from the predict function applied to our model, although the predict approach retains a few more decimal places internally:\n\npredict(model12b, newdata = data.frame(x = 12))\n\n       1 \n2.284069 \n\n\n\\[\nOdds(Y = 1 | X = 12) = exp(-9.20 + 0.96 * 12) = exp(2.2837) = 9.812921\n\\]\n\nexp(predict(model12b, newdata = data.frame(x = 12)))\n\n      1 \n9.81654 \n\n\nThe estimated probability of a yes response (Pr(y = 1), or \\(\\pi\\)) if x = 12 is just\n\\[\n\\pi = Pr(Y = 1 | X = 12) = \\frac{Odds(Y = 1 | X = 12)}{1 + Odds(Y = 1 | X = 12)} = \\frac{exp(-9.20 + 0.96 x)}{1 + exp(-9.20 + 0.96 x)} = \\frac{9.812921}{1 + 9.812921} = 0.908\n\\]\nDoes this work out?\n\nexp(predict(model12b, newdata = data.frame(x = 12))) / \n    (1 + exp(predict(model12b, newdata = data.frame(x = 12))))\n\n       1 \n0.907549 \n\n\nwhich is also directly available by running predict with type = \"response\".\n\npredict(model12b, newdata = data.frame(x = 12), type = \"response\")\n\n       1 \n0.907549"
  },
  {
    "objectID": "logistic1.html#plotting-the-logistic-regression-model",
    "href": "logistic1.html#plotting-the-logistic-regression-model",
    "title": "19  Logistic Regression: The Foundations",
    "section": "19.10 Plotting the Logistic Regression Model",
    "text": "19.10 Plotting the Logistic Regression Model\nWe can use the augment function from the broom package to get our fitted probabilities included in the data.\n\nm12b_aug <- augment(model12b, sim12, type.predict = \"response\")\n\nggplot(m12b_aug, aes(x = x, y = y)) +\n    geom_point() +\n    geom_line(aes(x = x, y = .fitted), col = \"blue\") +\n    labs(title = \"Fitted Logistic Regression Model for sim12\")\n\n\n\n\nI’ll add a little jitter on the vertical scale to the points, so we can avoid overlap, and also make the points a little bigger.\n\nggplot(m12b_aug, aes(x = x, y = y)) +\n    geom_jitter(height = 0.05, size = 2, pch = 21, \n                fill = \"cornflowerblue\") +\n    geom_line(aes(x = x, y = .fitted), col = \"blue\") +\n    labs(title = \"Fitted Logistic Regression for sim12\") +\n    theme_bw()\n\n\n\n\nAll right, it’s time to move on to fitting models. We’ll do that next.\n\n\n\n\nFaraway, Julian J. 2006. Extending the Linear Model with r. Boca Raton, FL: CRC Press.\n\n\nHarrell, Frank E. 2001. Regression Modeling Strategies. New York: Springer.\n\n\n———. 2018. Regression Modeling Strategies Course Notes. http://www.fharrell.com/#links.\n\n\nRamsey, Fred L., and Daniel W. Schafer. 2002. The Statistical Sleuth: A Course in Methods of Data Analysis. Second Edition. Pacific Grove, CA: Duxbury.\n\n\nVittinghoff, Eric, David V. Glidden, Stephen C. Shiboski, and Charles E. McCulloch. 2012. Regression Methods in Biostatistics: Linear, Logistic, Survival, and Repeated Measures Models. Second Edition. Springer-Verlag, Inc. http://www.biostat.ucsf.edu/vgsm/."
  },
  {
    "objectID": "logistic2.html#r-setup-used-here",
    "href": "logistic2.html#r-setup-used-here",
    "title": "20  Logistic Regression with glm",
    "section": "20.1 R Setup Used Here",
    "text": "20.1 R Setup Used Here\n\nknitr::opts_chunk$set(comment = NA)\n\nlibrary(caret)\nlibrary(ROCR)\nlibrary(pROC)\nlibrary(broom)\nlibrary(mosaic)\nlibrary(naniar)\nlibrary(tidyverse) \n\ntheme_set(theme_bw())\n\n\n20.1.1 Data Load\n\nresect <- read_csv(\"data/resect.csv\", show_col_types = FALSE)"
  },
  {
    "objectID": "logistic2.html#the-resect-data",
    "href": "logistic2.html#the-resect-data",
    "title": "20  Logistic Regression with glm",
    "section": "20.2 The resect data",
    "text": "20.2 The resect data\nMy source for these data was Riffenburgh (2006). The data describe 134 patients who had undergone resection of the tracheal carina (most often this is done to address tumors in the trachea), and the resect.csv data file contains the following variables:\n\nid = a patient ID #,\nage= the patient’s age at surgery,\nprior = prior tracheal surgery (1 = yes, 0 = no),\nresection = extent of the resection (in cm),\nintubated = whether intubation was required at the end of surgery (1 = yes, 0 = no), and\ndied = the patient’s death status (1 = dead, 0 = alive).\n\n\nmiss_var_summary(resect)\n\n# A tibble: 6 × 3\n  variable  n_miss pct_miss\n  <chr>      <int>    <dbl>\n1 subj_id        0        0\n2 age            0        0\n3 prior          0        0\n4 resection      0        0\n5 intubated      0        0\n6 died           0        0\n\n\n\nresect |> count(died, prior)\n\n# A tibble: 4 × 3\n   died prior     n\n  <dbl> <dbl> <int>\n1     0     0    89\n2     0     1    28\n3     1     0    11\n4     1     1     6\n\n\n\nresect |> inspect()\n\n\nquantitative variables:  \n       name   class min    Q1 median     Q3 max       mean         sd   n\n1   subj_id numeric   1 34.25   67.5 100.75 134 67.5000000 38.8265373 134\n2       age numeric   8 36.00   51.0  61.00  80 47.8432836 15.7775202 134\n3     prior numeric   0  0.00    0.0   0.75   1  0.2537313  0.4367785 134\n4 resection numeric   1  2.00    2.5   4.00   6  2.9634328  1.2402123 134\n5 intubated numeric   0  0.00    0.0   0.00   1  0.1417910  0.3501447 134\n6      died numeric   0  0.00    0.0   0.00   1  0.1268657  0.3340713 134\n  missing\n1       0\n2       0\n3       0\n4       0\n5       0\n6       0\n\n\nWe have no missing data, and 17 of the 134 patients died. Our goal will be to understand the characteristics of the patients, and how they relate to the binary outcome of interest, death."
  },
  {
    "objectID": "logistic2.html#running-a-simple-logistic-regression-model",
    "href": "logistic2.html#running-a-simple-logistic-regression-model",
    "title": "20  Logistic Regression with glm",
    "section": "20.3 Running A Simple Logistic Regression Model",
    "text": "20.3 Running A Simple Logistic Regression Model\nIn the most common scenario, a logistic regression model is used to predict a binary outcome (which can take on the values 0 or 1.) We will eventually fit a logistic regression model in two ways.\n\nThrough the glm function in the base package of R (similar to lm for linear regression)\nThrough the lrm function available in the rms package (similar to ols for linear regression)\n\nWe’ll focus on the glm approach first, and save the lrm ideas for later in this Chapter.\n\n20.3.1 Logistic Regression Can Be Harder than Linear Regression\n\nLogistic regression models are fitted using the method of maximum likelihood in glm, which requires multiple iterations until convergence is reached.\nLogistic regression models are harder to interpret (for most people) than linear regressions.\nLogistic regression models don’t have the same set of assumptions as linear models.\nLogistic regression outcomes (yes/no) carry much less information than quantitative outcomes. As a result, fitting a reasonable logistic regression requires more data than a linear model of similar size.\n\nThe rule I learned in graduate school was that a logistic regression requires 100 observations to fit an intercept plus another 15 observations for each candidate predictor. That’s not terrible, but it’s a very large sample size.\nFrank Harrell recommends that 96 observations + a function of the number of candidate predictors (which depends on the amount of variation in the predictors, but 15 x the number of such predictors isn’t too bad if the signal to noise ratio is pretty good) are required just to get reasonable confidence intervals around your predictions.\n\nIn a twitter note, Frank suggests that 96 + 8 times the number of candidate parameters might be reasonable so long as the smallest cell of interest (combination of an outcome and a split of the covariates) is 96 or more observations.\n\nPeduzzi et al. (1996) suggest that if we let \\(\\pi\\) be the smaller of the proportions of “yes” or “no” cases in the population of interest, and k be the number of inputs under consideration, then \\(N = 10k/\\pi\\) is the minimum number of cases to include, except that if N < 100 by this standard, you should increase it to 100, according to Long (1997).\n\nThat suggests that if you have an outcome that happens 10% of the time, and you are running a model with 3 predictors, then you could get away with \\((10 \\times 3)/(0.10) = 300\\) observations, but if your outcome happened 40% of the time, you could get away with only \\((10 \\times 3)/(0.40) = 75\\) observations, which you’d round up to 100."
  },
  {
    "objectID": "logistic2.html#logistic-regression-using-glm",
    "href": "logistic2.html#logistic-regression-using-glm",
    "title": "20  Logistic Regression with glm",
    "section": "20.4 Logistic Regression using glm",
    "text": "20.4 Logistic Regression using glm\nWe’ll begin by attempting to predict death based on the extent of the resection.\n\nres_modA <- glm(died ~ resection, data=resect, \n               family=\"binomial\"(link=\"logit\"))\n\nres_modA\n\n\nCall:  glm(formula = died ~ resection, family = binomial(link = \"logit\"), \n    data = resect)\n\nCoefficients:\n(Intercept)    resection  \n    -4.4337       0.7417  \n\nDegrees of Freedom: 133 Total (i.e. Null);  132 Residual\nNull Deviance:      101.9 \nResidual Deviance: 89.49    AIC: 93.49\n\n\nNote that the logit link is the default approach with the binomial family, so we could also have used:\n\nres_modA <- glm(died ~ resection, data = resect, \n                family = \"binomial\")\n\nwhich yields the same model.\n\n20.4.1 Interpreting the Coefficients of a Logistic Regression Model\nOur model is:\n\\[\nlogit(died = 1) = log\\left(\\frac{Pr(died = 1)}{1 - Pr(died = 1)}\\right)\n\\]\n\\[\n= \\beta_0 + \\beta_1 x = -4.4337 + 0.7417 \\times resection\n\\]\nThe predicted log odds of death for a subject with a resection of 4 cm is:\n\\[\nlog\\left(\\frac{Pr(died = 1)}{1 - Pr(died = 1)}\\right) = -4.4337 + 0.7417 \\times 4 = -1.467\n\\]\nThe predicted odds of death for a subject with a resection of 4 cm is thus:\n\\[\n\\frac{Pr(died = 1)}{1 - Pr(died = 1)} = e^{-4.4337 + 0.7417 \\times 4} = e^{-1.467} = 0.2306\n\\]\nSince the odds are less than 1, we should find that the probability of death is less than 1/2. With a little algebra, we see that the predicted probability of death for a subject with a resection of 4 cm is:\n\\[\nPr(died = 1) = \\frac{e^{-4.4337 + 0.7417 \\times 4}}{1 + e^{-4.4337 + 0.7417 \\times 4}} = \\frac{e^{-1.467}}{1 + e^{-1.467}} = \\frac{0.2306}{1.2306} = 0.187\n\\]\nIn general, we can frame the model in terms of a statement about probabilities, like this:\n\\[\nPr(died = 1) = \\frac{e^{\\beta_0 + \\beta_1 x}}{1 + {e^{\\beta_0 + \\beta_1 x}}} = \\frac{e^{-4.4337 + 0.7417 \\times resection}}{1 + e^{-4.4337 + 0.7417 \\times resection}}\n\\]\nand so by substituting in values for resection, we can estimate the model’s fitted probabilities of death.\n\n\n20.4.2 Using predict to describe the model’s fits\nTo obtain these fitted odds and probabilities in R, we can use the predict function.\n\nThe default predictions are on the scale of the log odds. These predictions are also available through the type = \"link\" command within the predict function for a generalized linear model like logistic regression.\nHere are the predicted log odds of death for a subject (Sally) with a 4 cm resection and a subject (Harry) who had a 5 cm resection.\n\n\npredict(res_modA, newdata = tibble(resection = c(4,5)))\n\n         1          2 \n-1.4669912 -0.7253027 \n\n\n\nWe can also obtain predictions for each subject on the original response (here, probability) scale, backing out of the logit link.\n\n\npredict(res_modA, newdata = tibble(resection = c(4, 5)), \n        type = \"response\")\n\n        1         2 \n0.1874004 0.3262264 \n\n\nSo the predicted probability of death is 0.187 for Sally, the subject with a 4 cm resection, and 0.326 for Harry, the subject with a 5 cm resection.\n\n\n20.4.3 Odds Ratio interpretation of Coefficients\nOften, we will exponentiate the estimated slope coefficients of a logistic regression model to help us understand the impact of changing a predictor on the odds of our outcome.\n\nexp(coef(res_modA))\n\n(Intercept)   resection \n 0.01186995  2.09947754 \n\n\nTo interpret this finding, suppose we have two subjects, Harry and Sally. Harry had a resection that was 1 cm larger than Sally. This estimated coefficient suggests that the estimated odds for death associated with Harry is 2.099 times larger than the odds for death associated with Sally. In general, the odds ratio comparing two subjects who differ by 1 cm on the resection length is 2.099.\nTo illustrate, again let’s assume that Harry’s resection was 5 cm, and Sally’s was 4 cm. Then we have:\n\\[\nlog\\left(\\frac{Pr(Harry died)}{1 - Pr(Harry died)}\\right) = -4.4337 + 0.7417 \\times 5 = -0.7253\n\\]\n\\[\nlog\\left(\\frac{Pr(Sally died)}{1 - Pr(Sally died)}\\right) = -4.4337 + 0.7417 \\times 4 = -1.4667.\n\\]\nwhich implies that our estimated odds of death for Harry and Sally are:\n\\[\nOdds(Harry died) = \\frac{Pr(Harry died)}{1 - Pr(Harry died)} = e^{-4.4337 + 0.7417 \\times 5} = e^{-0.7253} = 0.4842\n\\]\n\\[\nOdds(Sally died) = \\frac{Pr(Sally died)}{1 - Pr(Sally died)} = e^{-4.4337 + 0.7417 \\times 4} = e^{-1.4667} = 0.2307\n\\]\nand so the odds ratio is:\n\\[\nOR = \\frac{Odds(Harry died)}{Odds(Sally died)} = \\frac{0.4842}{0.2307} = 2.099\n\\]\n\nIf the odds ratio was 1, that would mean that Harry and Sally had the same estimated odds of death, and thus the same estimated probability of death, despite having different sizes of resections.\nSince the odds ratio is greater than 1, it means that Harry has a higher estimated odds of death than Sally, and thus that Harry has a higher estimated probability of death than Sally.\nIf the odds ratio was less than 1, it would mean that Harry had a lower estimated odds of death than Sally, and thus that Harry had a lower estimated probability of death than Sally.\n\nRemember that the odds ratio is a fraction describing two positive numbers (odds can only be non-negative) so that the smallest possible odds ratio is 0.\n\n\n20.4.4 Interpreting the rest of the model output from glm\n\nres_modA\n\n\nCall:  glm(formula = died ~ resection, family = \"binomial\", data = resect)\n\nCoefficients:\n(Intercept)    resection  \n    -4.4337       0.7417  \n\nDegrees of Freedom: 133 Total (i.e. Null);  132 Residual\nNull Deviance:      101.9 \nResidual Deviance: 89.49    AIC: 93.49\n\n\nIn addition to specifying the logistic regression coefficients, we are also presented with information on degrees of freedom, deviance (null and residual) and AIC.\n\nThe degrees of freedom indicate the sample size.\n\nRecall that we had n = 134 subjects in the data. The “Null” model includes only an intercept term (which uses 1 df) and we thus have n - 1 (here 133) degrees of freedom available for estimation.\nIn our res_modA model, a logistic regression is fit including a single slope (resection) and an intercept term. Each uses up one degree of freedom to build an estimate, so we have n - 2 = 134 - 2 = 132 residual df remaining.\n\nThe AIC or Akaike Information Criterion (lower values are better) is also provided. This is helpful if we’re comparing multiple models for the same outcome.\n\n\n\n20.4.5 Deviance and Comparing Our Model to the Null Model\n\nThe deviance (a measure of the model’s lack of fit) is available for both the null model (the model with only an intercept) and for our model (res_modA) predicting our outcome, mortality.\nThe deviance test, though available in R (see below) isn’t really a test of whether the model works well. Instead, it assumes the model is true, and then tests to see if the coefficients are different from zero. So it isn’t of much practical use.\n\nTo compare the deviance statistics, we can subtract the residual deviance from the null deviance to describe the impact of our model on fit.\nNull Deviance - Residual Deviance can be compared to a \\(\\chi^2\\) distribution with Null DF - Residual DF degrees of freedom to obtain a global test of the in-sample predictive power of our model.\nWe can see this comparison more directly by running anova on our model:\n\n\n\nanova(res_modA, test = \"LRT\")\n\nAnalysis of Deviance Table\n\nModel: binomial, link: logit\n\nResponse: died\n\nTerms added sequentially (first to last)\n\n          Df Deviance Resid. Df Resid. Dev  Pr(>Chi)    \nNULL                        133    101.943              \nresection  1    12.45       132     89.493 0.0004179 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nThe test = \"LRT\" section completes a deviance test and provides a p value, which just estimates the probability that a chi-square distribution with a single degree of freedom would exhibit an improvement in deviance as large as 12.45.\nThe p value for the deviance test here is about 0.0004. But, again, this isn’t a test of whether the model is any good - it assumes the model is true, and then tests some consequences.\n\nSpecifically, it tests whether (if the model is TRUE) some of the model’s coefficients are non-zero.\nThat’s not so practically useful, so I discourage you from performing global tests of a logistic regression model with a deviance test.\n\n\n\n20.4.6 Using glance with a logistic regression model\nWe can use the glance function from the broom package to obtain the null and residual deviance and degrees of freedom. Note that the deviance for our model is related to the log likelihood by -2*logLik.\n\nglance(res_modA)\n\n# A tibble: 1 × 8\n  null.deviance df.null logLik   AIC   BIC deviance df.residual  nobs\n          <dbl>   <int>  <dbl> <dbl> <dbl>    <dbl>       <int> <int>\n1          102.     133  -44.7  93.5  99.3     89.5         132   134\n\n\nThe glance result also provides the AIC, and the BIC (Bayes Information Criterion), each of which is helpful in understanding comparisons between multiple models for the same outcome (with smaller values of either criterion indicating better models.) The AIC is based on the deviance, but penalizes you for making the model more complicated. The BIC does the same sort of thing but with a different penalty.\nAgain we see that we have a null deviance of 101.94 on 133 degrees of freedom. Including the resection information in the model decreased the deviance to 89.49 points on 132 degrees of freedom, so that’s a decrease of 12.45 points while using one degree of freedom, which looks like a meaningful reduction in deviance."
  },
  {
    "objectID": "logistic2.html#interpreting-the-model-summary",
    "href": "logistic2.html#interpreting-the-model-summary",
    "title": "20  Logistic Regression with glm",
    "section": "20.5 Interpreting the Model Summary",
    "text": "20.5 Interpreting the Model Summary\nLet’s get a more detailed summary of our res_modA model, including 95% confidence intervals for the coefficients:\n\nsummary(res_modA)\n\n\nCall:\nglm(formula = died ~ resection, family = \"binomial\", data = resect)\n\nDeviance Residuals: \n    Min       1Q   Median       3Q      Max  \n-1.1844  -0.5435  -0.3823  -0.2663   2.4501  \n\nCoefficients:\n            Estimate Std. Error z value Pr(>|z|)    \n(Intercept)  -4.4337     0.8799  -5.039 4.67e-07 ***\nresection     0.7417     0.2230   3.327 0.000879 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 101.943  on 133  degrees of freedom\nResidual deviance:  89.493  on 132  degrees of freedom\nAIC: 93.493\n\nNumber of Fisher Scoring iterations: 5\n\nconfint(res_modA, level = 0.95)\n\nWaiting for profiling to be done...\n\n\n                2.5 %    97.5 %\n(Intercept) -6.344472 -2.855856\nresection    0.322898  1.208311\n\n\nSome elements of this summary are very familiar from our work with linear models.\n\nWe still have a five-number summary of residuals, although these are called deviance residuals.\nWe have a table of coefficients with standard errors, and hypothesis tests, although these are Wald z-tests, rather than the t tests we saw in linear modeling.\nWe have a summary of global fit in the comparison of null deviance and residual deviance, but without a formal p value. And we have the AIC, as discussed above.\nWe also have some new items related to a dispersion parameter and to the number of Fisher Scoring Iterations.\n\nLet’s walk through each of these elements.\n\n20.5.1 Wald Z tests for Coefficients in a Logistic Regression\nThe coefficients output provides the estimated coefficients, and their standard errors, plus a Wald Z statistic, which is just the estimated coefficient divided by its standard error. This is compared to a standard Normal distribution to obtain the two-tailed p values summarized in the Pr(>|z|) column.\n\nThe interesting result is resection, which has a Wald Z = 3.327, yielding a p value of 0.00088.\nThe p value assesses whether the estimated coefficient of resection, 0.7417, is different from 0. If the coefficient (on the logit scale) for resection was truly 0, this would mean that:\n\nthe log odds of death did not change based on the resection size,\nthe odds of death were unchanged based on the resection size (the odds ratio would be 1), and\nthe probability of death was unchanged based on the resection size.\n\n\nIn our case, we have a change in the log odds of died associated with changes in resection, according to this p value. We conclude that resection size is associated with a positive impact on death rates (death rates are generally higher for people with larger resections.)\n\n\n20.5.2 Confidence Intervals for the Coefficients\nAs in linear regression, we can obtain 95% confidence intervals (to get other levels, change the level parameter in confint) for the intercept and slope coefficients.\nHere, we see, for example, that the coefficient of resection has a point estimate of 0.7417, and a confidence interval of (0.3229, 1.208). Since this is on the logit scale, it’s not that interpretable, but we will often exponentiate the model and its confidence interval to obtain a more interpretable result on the odds ratio scale.\n\ntidy(res_modA, exponentiate = TRUE, conf.int = TRUE) |>\n  select(term, estimate, conf.low, conf.high)\n\n# A tibble: 2 × 4\n  term        estimate conf.low conf.high\n  <chr>          <dbl>    <dbl>     <dbl>\n1 (Intercept)   0.0119  0.00176    0.0575\n2 resection     2.10    1.38       3.35  \n\n\nFrom this output, we can estimate the odds ratio for death associated with a 1 cm increase in resection size is 2.099, with a 95% CI of (1.38, 3.35). - If the odds ratio was 1, it would indicate that the odds of death did not change based on the change in resection size. - Here, it’s clear that the estimated odds of death will be larger (odds > 1) for subjects with larger resection sizes. Larger odds of death also indicate larger probabilities of death. This confidence interval indicates that with 95% confidence, we conclude that increases in resection size are associated with increases in the odds of death. - If the odds ratio was less than 1 (remember that it cannot be less than 0) that would mean that subjects with larger resection sizes were associated with smaller estimated odds of death.\n\n\n20.5.3 Deviance Residuals\nIn logistic regression, it’s certainly a good idea to check to see how well the model fits the data. However, there are a few different types of residuals. The residuals presented here by default are called deviance residuals. Other types of residuals are available for generalized linear models, such as Pearson residuals, working residuals, and response residuals. Logistic regression model diagnostics often make use of multiple types of residuals.\nThe deviance residuals for each individual subject sum up to the deviance statistic for the model, and describe the contribution of each point to the model likelihood function.\nThe deviance residual, \\(d_i\\), for the ith observation in a model predicting \\(y_i\\) (a binary variable), with the estimate being \\(\\hat{\\pi}_i\\) is:\n\\[\nd_i = s_i \\sqrt{-2 [y_i log \\hat{\\pi_i} + (1 - y_i) log(1 - \\hat{\\pi_i})]},\n\\]\nwhere \\(s_i\\) is 1 if \\(y_i = 1\\) and \\(s_i = -1\\) if \\(y_i = 0\\).\nAgain, the sum of the deviance residuals is the deviance.\n\n\n20.5.4 Dispersion Parameter\nThe dispersion parameter is taken to be 1 for glm fit using either the binomial or Poisson families. For other sorts of generalized linear models, the dispersion parameter will be of some importance in estimating standard errors sensibly.\n\n\n20.5.5 Fisher Scoring iterations\nThe solution of a logistic regression model involves maximizing a likelihood function. Fisher’s scoring algorithm in our res_modA needed five iterations to perform the logistic regression fit. All that this tells you is that the model converged, and didn’t require a lot of time to do so."
  },
  {
    "objectID": "logistic2.html#plotting-a-simple-logistic-regression-model",
    "href": "logistic2.html#plotting-a-simple-logistic-regression-model",
    "title": "20  Logistic Regression with glm",
    "section": "20.6 Plotting a Simple Logistic Regression Model",
    "text": "20.6 Plotting a Simple Logistic Regression Model\nLet’s plot the logistic regression model res_modA for died using the extent of the resection in terms of probabilities. We can use either of two different approaches:\n\nwe can plot the fitted values from our specific model against the original data, using the augment function from the broom package, or\nwe can create a smooth function called binomial_smooth that plots a simple logistic model in an analogous way to geom_smooth(method = \"lm\") for a simple linear regression.\n\n\n20.6.1 Using augment to capture the fitted probabilities\n\nres_A_aug <- augment(res_modA, resect, \n                     type.predict = \"response\")\nhead(res_A_aug)\n\n# A tibble: 6 × 12\n  subj_id   age prior resection intubated  died .fitted .resid .std.re…¹    .hat\n    <dbl> <dbl> <dbl>     <dbl>     <dbl> <dbl>   <dbl>  <dbl>     <dbl>   <dbl>\n1       1    34     1       2.5         0     0  0.0705 -0.382    -0.384 0.0100 \n2       2    57     0       5           0     0  0.326  -0.889    -0.904 0.0337 \n3       3    60     1       4           1     1  0.187   1.83      1.84  0.0120 \n4       4    62     1       4.2         0     0  0.211  -0.689    -0.693 0.0143 \n5       5    28     0       6           1     1  0.504   1.17      1.22  0.0818 \n6       6    52     0       3           0     0  0.0990 -0.457    -0.459 0.00922\n# … with 2 more variables: .sigma <dbl>, .cooksd <dbl>, and abbreviated\n#   variable name ¹​.std.resid\n\n\nThis approach augments the resect data set with fitted, residual and other summaries of each observation’s impact on the fit, using the “response” type of prediction, which yields the fitted probabilities in the .fitted column.\n\n\n20.6.2 Plotting a Logistic Regression Model’s Fitted Values\n\nggplot(res_A_aug, aes(x = resection, y = died)) +\n    geom_jitter(height = 0.05) +\n    geom_line(aes(x = resection, y = .fitted), \n              col = \"blue\") +\n    labs(title = \"Logistic Regression from Model res_modA\")\n\n\n\n\n\n\n20.6.3 Plotting a Simple Logistic Model using binomial_smooth\n\nbinomial_smooth <- function(...) {\n  geom_smooth(method = \"glm\", formula = y ~ x,\n              method.args = list(family = \"binomial\"), ...)\n}\n\nggplot(resect, aes(x = resection, y = died)) +\n  geom_jitter(height = 0.05) +\n  binomial_smooth() + ## ...smooth(se=FALSE) to leave out interval\n  labs(title = \"Logistic Regression from Model A\") \n\n\n\n\nAs expected, we see an increase in the model probability of death as the extent of the resection grows larger."
  },
  {
    "objectID": "logistic2.html#how-well-does-model-a-classify-subjects",
    "href": "logistic2.html#how-well-does-model-a-classify-subjects",
    "title": "20  Logistic Regression with glm",
    "section": "20.7 How well does Model A classify subjects?",
    "text": "20.7 How well does Model A classify subjects?\nA natural question to ask is how well does our model classify patients in terms of likelihood of death.\nWe could specify a particular rule, for example: if the predicted probability of death is 0.5 or greater, then predict “Died”.\n\nres_A_aug$rule.5 <- ifelse(res_A_aug$.fitted >= 0.5, \n                       \"Predict Died\", \"Predict Alive\")\n\ntable(res_A_aug$rule.5, res_A_aug$died)\n\n               \n                  0   1\n  Predict Alive 114  16\n  Predict Died    3   1\n\n\nAnd perhaps build the linked table of row probabilities which tells us, for example, that 87.69% of the patients predicted by the model to be alive actually did survive.\n\nround(100*prop.table(\n    table(res_A_aug$rule.5, res_A_aug$died), 1), 2)\n\n               \n                    0     1\n  Predict Alive 87.69 12.31\n  Predict Died  75.00 25.00\n\n\nOr the table of column probabilities which tell us, for example, that 97.44% of those who actually survived were predicted by the model to be alive.\n\nround(100*prop.table(\n    table(res_A_aug$rule.5, res_A_aug$died), 2), 2)\n\n               \n                    0     1\n  Predict Alive 97.44 94.12\n  Predict Died   2.56  5.88\n\n\nWe’ll discuss various measures of concordance derived from this sort of classification later."
  },
  {
    "objectID": "logistic2.html#the-confusion-matrix",
    "href": "logistic2.html#the-confusion-matrix",
    "title": "20  Logistic Regression with glm",
    "section": "20.8 The Confusion Matrix",
    "text": "20.8 The Confusion Matrix\nLet’s build this misclassification table in standard epidemiological format.\n\nres_A_aug <- res_A_aug |>\n  mutate(death_predicted = factor(.fitted >= 0.5),\n         death_actual = factor(died == \"1\"),\n         death_predicted = fct_relevel(death_predicted, \"TRUE\"),\n         death_actual = fct_relevel(death_actual, \"TRUE\")) \n\nconfuseA_small <- table(res_A_aug$death_predicted, res_A_aug$death_actual)\n\nconfuseA_small\n\n       \n        TRUE FALSE\n  TRUE     1     3\n  FALSE   16   114\n\n\nIn total, we have 134 observations.\n\n115 correct predictions, or 85.8% accuracy\n17 subjects who died, or 12.6% prevalence of death\n4 subjects who were predicted to die, or 3.0% detection prevalence.\n\nThe sensitivity (also called recall) here is 1 / (1 + 16) = 5.9%.\n\n5.9% of the subjects who actually died were predicted to die by the model.\n\nThe specificity here is 114 / (114 + 3) = 97.4%.\n\n97.4% of the subjects who actually survived were predicted to survive by the model.\n\nThe positive predictive value (PPV: also called precision) is 1 / (1 + 3) = 25%\n\nOur predictions of death were correct 25% of the time.\n\nThe negative predictive value (NPV) is 114 / (114 + 16) = 87.7%\n\nOur predictions of survival were correct 87.7% of the time."
  },
  {
    "objectID": "logistic2.html#using-the-confusionmatrix-tool-from-the-caret-package",
    "href": "logistic2.html#using-the-confusionmatrix-tool-from-the-caret-package",
    "title": "20  Logistic Regression with glm",
    "section": "20.9 Using the confusionMatrix tool from the caret package",
    "text": "20.9 Using the confusionMatrix tool from the caret package\nThis provides a more detailed summary of the classification results from our logistic regression model.\n\nconfusionMatrix(\n    data = factor(res_A_aug$.fitted >= 0.5),\n    reference = factor(res_A_aug$died == 1),\n    positive = \"TRUE\"\n  )\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction FALSE TRUE\n     FALSE   114   16\n     TRUE      3    1\n                                          \n               Accuracy : 0.8582          \n                 95% CI : (0.7875, 0.9124)\n    No Information Rate : 0.8731          \n    P-Value [Acc > NIR] : 0.747802        \n                                          \n                  Kappa : 0.0493          \n                                          \n Mcnemar's Test P-Value : 0.005905        \n                                          \n            Sensitivity : 0.058824        \n            Specificity : 0.974359        \n         Pos Pred Value : 0.250000        \n         Neg Pred Value : 0.876923        \n             Prevalence : 0.126866        \n         Detection Rate : 0.007463        \n   Detection Prevalence : 0.029851        \n      Balanced Accuracy : 0.516591        \n                                          \n       'Positive' Class : TRUE            \n                                          \n\n\n\nThe No Information Rate or NIR is just the percentage of correct predictions we’d get if we just predicted the more common classification (not dead) for every subject.\nKappa is a correlation statistic ranging from -1 to +1. It measures the inter-rater reliability of our predictions and the true classifications, in this context. Complete agreement would be +1, and complete disagreement would be -1."
  },
  {
    "objectID": "logistic2.html#receiver-operating-characteristic-curve-analysis",
    "href": "logistic2.html#receiver-operating-characteristic-curve-analysis",
    "title": "20  Logistic Regression with glm",
    "section": "20.10 Receiver Operating Characteristic Curve Analysis",
    "text": "20.10 Receiver Operating Characteristic Curve Analysis\nOne way to assess the predictive accuracy within the model development sample in a logistic regression is to consider an analyses based on the receiver operating characteristic (ROC) curve. ROC curves are commonly used in assessing diagnoses in medical settings, and in signal detection applications.\nThe accuracy of a “test” can be evaluated by considering two types of errors: false positives and false negatives.\nIn our res_modA model, we use resection size to predict whether the patient died. Suppose we established a value R, so that if the resection size was larger than R cm, we would predict that the patient died, and otherwise we would predict that the patient did not die.\nA good outcome of our model’s “test”, then, would be when the resection size is larger than R for a patient who actually died. Another good outcome would be when the resection size is smaller than R for a patient who survived.\nBut we can make errors, too.\n\nA false positive error in this setting would occur when the resection size is larger than R (so we predict the patient dies) but in fact the patient does not die.\nA false negative error in this case would occur when the resection size is smaller than R (so we predict the patient survives) but in fact the patient dies.\n\nFormally, the true positive fraction (TPF) for a specific resection cutoff \\(R\\), is the probability of a positive test (a prediction that the patient will die) among the people who have the outcome died = 1 (those who actually die).\n\\[\nTPF(R) = Pr(resection > R | subject died)\n\\]\nSimilarly, the false positive fraction (FPF) for a specific cutoff \\(R\\) is the probability of a positive test (prediction that the patient will die) among the people with died = 0 (those who don’t actually die)\n\\[\nFPF(R) = Pr(resection > R | subject did not die)\n\\]\nThe True Positive Rate is referred to as the sensitivity of a diagnostic test, and the True Negative rate (1 - the False Positive rate) is referred to as the specificity of a diagnostic test.\nSince the cutoff \\(R\\) is not fixed in advanced, we can plot the value of TPF (on the y axis) against FPF (on the x axis) for all possible values of \\(R\\), and this is what the ROC curve is. Others refer to the Sensitivity on the Y axis, and 1-Specificity on the X axis, and this is the same idea.\nBefore we get too far into the weeds, let me show you some simple situations so you can understand what you might learn from the ROC curve. The web page http://blog.yhat.com/posts/roc-curves.html provides source materials.\n\n20.10.1 Interpreting the Area under the ROC curve\nThe AUC or Area under the ROC curve is the amount of space underneath the ROC curve. Often referred to as the c statistic, the AUC represents the quality of your TPR and FPR overall in a single number. The C statistic ranges from 0 to 1, with C = 0.5 for a prediction that is no better than random guessing, and C = 1 for a perfect prediction model.\nNext, I’ll build a simulation to demonstrate several possible ROC curves in the sections that follow.\n\nset.seed(432999)\nsim.temp <- tibble(x = rnorm(n = 200), \n                   prob = exp(x)/(1 + exp(x)), \n                   y = as.numeric(1 * runif(200) < prob))\n\nsim.temp <- sim.temp |>\n    mutate(p_guess = 1,\n           p_perfect = y, \n           p_bad = exp(-2*x) / (1 + exp(-2*x)),\n           p_ok = prob + (1-y)*runif(1, 0, 0.05),\n           p_good = prob + y*runif(1, 0, 0.27))\n\n\n20.10.1.1 What if we are guessing?\nIf we’re guessing completely at random, then the model should correctly classify a subject (as died or not died) about 50% of the time, so the TPR and FPR will be equal. This yields a diagonal line in the ROC curve, and an area under the curve (C statistic) of 0.5.\nThere are several ways to do this on the web, but I’ll show this one, which has some bizarre code, but that’s a function of using a package called ROCR to do the work. It comes from this link\n\npred_guess <- prediction(sim.temp$p_guess, sim.temp$y)\nperf_guess <- performance(pred_guess, measure = \"tpr\", x.measure = \"fpr\")\nauc_guess <- performance(pred_guess, measure=\"auc\")\n\nauc_guess <- round(auc_guess@y.values[[1]],3)\nroc_guess <- data.frame(fpr=unlist(perf_guess@x.values),\n                        tpr=unlist(perf_guess@y.values),\n                        model=\"GLM\")\n\nggplot(roc_guess, aes(x=fpr, ymin=0, ymax=tpr)) +\n    geom_ribbon(alpha=0.2, fill = \"blue\") +\n    geom_line(aes(y=tpr), col = \"blue\") +\n    labs(title = paste0(\"Guessing: ROC Curve w/ AUC=\", auc_guess)) +\n    theme_bw()\n\n\n\n\n\n\n20.10.1.2 What if we classify things perfectly?\nIf we’re classifying subjects perfectly, then we have a TPR of 1 and an FPR of 0. That yields an ROC curve that looks like the upper and left edges of a box. If our model correctly classifies a subject (as died or not died) 100% of the time, the area under the curve (c statistic) will be 1.0. We’ll add in the diagonal line here (in a dashed black line) to show how this model compares to random guessing.\n\npred_perf <- prediction(sim.temp$p_perfect, sim.temp$y)\nperf_perf <- performance(pred_perf, measure = \"tpr\", x.measure = \"fpr\")\nauc_perf <- performance(pred_perf, measure=\"auc\")\n\nauc_perf <- round(auc_perf@y.values[[1]],3)\nroc_perf <- data.frame(fpr=unlist(perf_perf@x.values),\n                        tpr=unlist(perf_perf@y.values),\n                        model=\"GLM\")\n\nggplot(roc_perf, aes(x=fpr, ymin=0, ymax=tpr)) +\n    geom_ribbon(alpha=0.2, fill = \"blue\") +\n    geom_line(aes(y=tpr), col = \"blue\") +\n    geom_abline(intercept = 0, slope = 1, lty = \"dashed\") +\n    labs(title = paste0(\"Perfect Prediction: ROC Curve w/ AUC=\", auc_perf)) +\n    theme_bw()\n\n\n\n\n\n\n20.10.1.3 What does “worse than guessing” look like?\nA bad classifier will appear below and to the right of the diagonal line we’d see if we were completely guessing. Such a model will have a c statistic below 0.5, and will be valueless.\n\npred_bad <- prediction(sim.temp$p_bad, sim.temp$y)\nperf_bad <- performance(pred_bad, measure = \"tpr\", x.measure = \"fpr\")\nauc_bad <- performance(pred_bad, measure=\"auc\")\n\nauc_bad <- round(auc_bad@y.values[[1]],3)\nroc_bad <- data.frame(fpr=unlist(perf_bad@x.values),\n                        tpr=unlist(perf_bad@y.values),\n                        model=\"GLM\")\n\nggplot(roc_bad, aes(x=fpr, ymin=0, ymax=tpr)) +\n    geom_ribbon(alpha=0.2, fill = \"blue\") +\n    geom_line(aes(y=tpr), col = \"blue\") +\n    geom_abline(intercept = 0, slope = 1, lty = \"dashed\") +\n    labs(title = paste0(\"A Bad Model: ROC Curve w/ AUC=\", auc_bad)) +\n    theme_bw()\n\n\n\n\n\n\n20.10.1.4 What does “better than guessing” look like?\nAn “OK” classifier will appear above and to the left of the diagonal line we’d see if we were completely guessing. Such a model will have a c statistic above 0.5, and might have some value. The plot below shows a very fairly poor model, but at least it’s better than guessing.\n\npred_ok <- prediction(sim.temp$p_ok, sim.temp$y)\nperf_ok <- performance(pred_ok, measure = \"tpr\", x.measure = \"fpr\")\nauc_ok <- performance(pred_ok, measure=\"auc\")\n\nauc_ok <- round(auc_ok@y.values[[1]],3)\nroc_ok <- data.frame(fpr=unlist(perf_ok@x.values),\n                        tpr=unlist(perf_ok@y.values),\n                        model=\"GLM\")\n\nggplot(roc_ok, aes(x=fpr, ymin=0, ymax=tpr)) +\n    geom_ribbon(alpha=0.2, fill = \"blue\") +\n    geom_line(aes(y=tpr), col = \"blue\") +\n    geom_abline(intercept = 0, slope = 1, lty = \"dashed\") +\n    labs(title = paste0(\"A Mediocre Model: ROC Curve w/ AUC=\", auc_ok)) +\n    theme_bw()\n\n\n\n\nSometimes people grasp for a rough guide as to the accuracy of a model’s predictions based on the area under the ROC curve. A common thought is to assess the C statistic much like you would a class grade.\n\n\n\n\n\n\n\nC statistic\nInterpretation\n\n\n\n\n0.90 to 1.00\nmodel does an excellent job at discriminating “yes” from “no” (A)\n\n\n0.80 to 0.90\nmodel does a good job (B)\n\n\n0.70 to 0.80\nmodel does a fair job (C)\n\n\n0.60 to 0.70\nmodel does a poor job (D)\n\n\n0.50 to 0.60\nmodel fails (F)\n\n\nbelow 0.50\nmodel is worse than random guessing\n\n\n\n\n\n20.10.1.5 What does “pretty good” look like?\nA strong and good classifier will appear above and to the left of the diagonal line we’d see if we were completely guessing, often with a nice curve that is continually increasing and appears to be pulled up towards the top left. Such a model will have a c statistic well above 0.5, but not as large as 1. The plot below shows a stronger model, which appears substantially better than guessing.\n\npred_good <- prediction(sim.temp$p_good, sim.temp$y)\nperf_good <- performance(pred_good, measure = \"tpr\", x.measure = \"fpr\")\nauc_good <- performance(pred_good, measure=\"auc\")\n\nauc_good <- round(auc_good@y.values[[1]],3)\nroc_good <- data.frame(fpr=unlist(perf_good@x.values),\n                        tpr=unlist(perf_good@y.values),\n                        model=\"GLM\")\n\nggplot(roc_good, aes(x=fpr, ymin=0, ymax=tpr)) +\n    geom_ribbon(alpha=0.2, fill = \"blue\") +\n    geom_line(aes(y=tpr), col = \"blue\") +\n    geom_abline(intercept = 0, slope = 1, lty = \"dashed\") +\n    labs(title = paste0(\"A Pretty Good Model: ROC Curve w/ AUC=\", auc_good)) +\n    theme_bw()"
  },
  {
    "objectID": "logistic2.html#the-roc-plot-for-res_moda",
    "href": "logistic2.html#the-roc-plot-for-res_moda",
    "title": "20  Logistic Regression with glm",
    "section": "20.11 The ROC Plot for res_modA",
    "text": "20.11 The ROC Plot for res_modA\nLet me show you the ROC curve for our res_modA model.\n\n## requires ROCR package\nprob <- predict(res_modA, resect, type=\"response\")\npred <- prediction(prob, resect$died)\nperf <- performance(pred, measure = \"tpr\", x.measure = \"fpr\")\nauc <- performance(pred, measure=\"auc\")\n\nauc <- round(auc@y.values[[1]],3)\nroc.data <- data.frame(fpr=unlist(perf@x.values),\n                       tpr=unlist(perf@y.values),\n                       model=\"GLM\")\n\nggplot(roc.data, aes(x=fpr, ymin=0, ymax=tpr)) +\n    geom_ribbon(alpha=0.2, fill = \"blue\") +\n    geom_line(aes(y=tpr), col = \"blue\") +\n    geom_abline(intercept = 0, slope = 1, lty = \"dashed\") +\n    labs(title = paste0(\"ROC Curve w/ AUC=\", auc)) +\n    theme_bw()\n\n\n\n\nBased on the C statistic (AUC = 0.771) this would rank somewhere near the high end of a “fair” predictive model by this standard, not quite to the level of a “good” model.\n\n20.11.1 Another way to plot the ROC Curve\nIf we’ve loaded the pROC package, we can also use the following (admittedly simpler) approach to plot the ROC curve, without ggplot2, and to obtain the C statistic, and a 95% confidence interval around that C statistic.\n\n## requires pROC package\nroc.modA <- \n    roc(resect$died ~ predict(res_modA, type=\"response\"),\n        ci = TRUE)\n\nSetting levels: control = 0, case = 1\n\n\nSetting direction: controls < cases\n\nroc.modA\n\n\nCall:\nroc.formula(formula = resect$died ~ predict(res_modA, type = \"response\"),     ci = TRUE)\n\nData: predict(res_modA, type = \"response\") in 117 controls (resect$died 0) < 17 cases (resect$died 1).\nArea under the curve: 0.7707\n95% CI: 0.67-0.8715 (DeLong)\n\nplot(roc.modA)"
  },
  {
    "objectID": "logistic2.html#assessing-residual-plots-from-model-a",
    "href": "logistic2.html#assessing-residual-plots-from-model-a",
    "title": "20  Logistic Regression with glm",
    "section": "20.12 Assessing Residual Plots from Model A",
    "text": "20.12 Assessing Residual Plots from Model A\n\nResiduals are certainly less informative for logistic regression than they are for linear regression: not only do yes/no outcomes inherently contain less information than continuous ones, but the fact that the adjusted response depends on the fit hampers our ability to use residuals as external checks on the model.\n\n\nThis is mitigated to some extent, however, by the fact that we are also making fewer distributional assumptions in logistic regression, so there is no need to inspect residuals for, say, skewness or heteroskedasticity.\n\n\nPatrick Breheny, University of Kentucky, Slides on GLM Residuals and Diagnostics (no longer online, alas.)\n\nThe usual residual plots are available in R for a logistic regression model, but most of them are irrelevant in the logistic regression setting. The residuals shouldn’t follow a standard Normal distribution, and they will not show constant variance over the range of the predictor variables, so plots looking into those issues aren’t helpful.\nThe only plot from the standard set that we’ll look at in many settings is plot 5, which helps us assess influence (via Cook’s distance contours), and a measure related to leverage (how unusual an observation is in terms of the predictors) and standardized Pearson residuals.\n\nplot(res_modA, which = 5)\n\n\n\n\nIn this case, I don’t see any highly influential points, as no points fall outside of the Cook’s distance (0.5 or 1) contours."
  },
  {
    "objectID": "logistic2.html#model-b-a-kitchen-sink-logistic-regression-model",
    "href": "logistic2.html#model-b-a-kitchen-sink-logistic-regression-model",
    "title": "20  Logistic Regression with glm",
    "section": "20.13 Model B: A “Kitchen Sink” Logistic Regression Model",
    "text": "20.13 Model B: A “Kitchen Sink” Logistic Regression Model\n\nres_modB <- glm(died ~ resection + age + prior + intubated,\n               data = resect, family = binomial)\n\nres_modB\n\n\nCall:  glm(formula = died ~ resection + age + prior + intubated, family = binomial, \n    data = resect)\n\nCoefficients:\n(Intercept)    resection          age        prior    intubated  \n  -5.152886     0.612211     0.001173     0.814691     2.810797  \n\nDegrees of Freedom: 133 Total (i.e. Null);  129 Residual\nNull Deviance:      101.9 \nResidual Deviance: 67.36    AIC: 77.36\n\n\n\n20.13.1 Comparing Model A to Model B\n\nanova(res_modA, res_modB)\n\nAnalysis of Deviance Table\n\nModel 1: died ~ resection\nModel 2: died ~ resection + age + prior + intubated\n  Resid. Df Resid. Dev Df Deviance\n1       132     89.493            \n2       129     67.359  3   22.134\n\n\nThe addition of age, prior and intubated reduces the lack of fit by 22.134 points, at a cost of 3 degrees of freedom.\n\nglance(res_modA)\n\n# A tibble: 1 × 8\n  null.deviance df.null logLik   AIC   BIC deviance df.residual  nobs\n          <dbl>   <int>  <dbl> <dbl> <dbl>    <dbl>       <int> <int>\n1          102.     133  -44.7  93.5  99.3     89.5         132   134\n\nglance(res_modB)\n\n# A tibble: 1 × 8\n  null.deviance df.null logLik   AIC   BIC deviance df.residual  nobs\n          <dbl>   <int>  <dbl> <dbl> <dbl>    <dbl>       <int> <int>\n1          102.     133  -33.7  77.4  91.8     67.4         129   134\n\n\nBy either AIC or BIC, the larger model (res_modB) looks more effective.\n\n\n20.13.2 Interpreting Model B\n\nsummary(res_modB)\n\n\nCall:\nglm(formula = died ~ resection + age + prior + intubated, family = binomial, \n    data = resect)\n\nDeviance Residuals: \n    Min       1Q   Median       3Q      Max  \n-1.7831  -0.3741  -0.2386  -0.2014   2.5228  \n\nCoefficients:\n             Estimate Std. Error z value Pr(>|z|)    \n(Intercept) -5.152886   1.469453  -3.507 0.000454 ***\nresection    0.612211   0.282807   2.165 0.030406 *  \nage          0.001173   0.020646   0.057 0.954700    \nprior        0.814691   0.704785   1.156 0.247705    \nintubated    2.810797   0.658395   4.269 1.96e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 101.943  on 133  degrees of freedom\nResidual deviance:  67.359  on 129  degrees of freedom\nAIC: 77.359\n\nNumber of Fisher Scoring iterations: 6\n\n\nIt appears that the intubated predictor adds some value to the model, by the Wald test.\nLet’s focus on the impact of these variables through odds ratios.\n\ntidy(res_modB, exponentiate = TRUE, conf.int = TRUE) |>\n  select(term, estimate, conf.low, conf.high)\n\n# A tibble: 5 × 4\n  term        estimate conf.low conf.high\n  <chr>          <dbl>    <dbl>     <dbl>\n1 (Intercept)  0.00578 0.000241    0.0837\n2 resection    1.84    1.08        3.35  \n3 age          1.00    0.962       1.04  \n4 prior        2.26    0.549       9.17  \n5 intubated   16.6     4.75       64.6   \n\n\nAt a 5% significance level, we might conclude that:\n\nlarger sized resections are associated with a meaningful rise (est OR: 1.84, 95% CI 1.08, 3.35) in the odds of death, holding all other predictors constant,\nthe need for intubation at the end of surgery is associated with a substantial rise (est OR: 16.6, 95% CI 4.7, 64.7) in the odds of death, holding all other predictors constant, but that\nolder age as well as having a prior tracheal surgery appears to be associated with an increase in death risk, but not with a small p value."
  },
  {
    "objectID": "logistic2.html#plotting-model-b",
    "href": "logistic2.html#plotting-model-b",
    "title": "20  Logistic Regression with glm",
    "section": "20.14 Plotting Model B",
    "text": "20.14 Plotting Model B\nLet’s think about plotting the fitted values from our model, in terms of probabilities.\n\n20.14.1 Using augment to capture the fitted probabilities\n\nres_B_aug <- augment(res_modB, resect, \n                     type.predict = \"response\")\nhead(res_B_aug)\n\n# A tibble: 6 × 12\n  subj_id   age prior resec…¹ intub…²  died .fitted .resid .std.…³   .hat .sigma\n    <dbl> <dbl> <dbl>   <dbl>   <dbl> <dbl>   <dbl>  <dbl>   <dbl>  <dbl>  <dbl>\n1       1    34     1     2.5       0     0  0.0591 -0.349  -0.354 0.0267  0.725\n2       2    57     0     5         0     0  0.117  -0.498  -0.508 0.0380  0.724\n3       3    60     1     4         1     1  0.729   0.794   0.844 0.114   0.722\n4       4    62     1     4.2       0     0  0.155  -0.581  -0.602 0.0704  0.723\n5       5    28     0     6         1     1  0.796   0.675   0.724 0.131   0.723\n6       6    52     0     3         0     0  0.0371 -0.275  -0.277 0.0105  0.725\n# … with 1 more variable: .cooksd <dbl>, and abbreviated variable names\n#   ¹​resection, ²​intubated, ³​.std.resid\n\n\n\n\n20.14.2 Plotting Model B Fits by Observed Mortality\n\nggplot(res_B_aug, aes(x = factor(died), y = .fitted, col = factor(died))) +\n    geom_boxplot() +\n    geom_jitter(width = 0.1) + \n    guides(col = \"none\")\n\n\n\n\nCertainly it appears as though most of our predicted probabilities (of death) for the subjects who actually survived are quite small, but not all of them. We also have at least 6 big “misses” among the 17 subjects who actually died.\n\n\n20.14.3 Confusion Matrix for Model B\n\nconfusionMatrix(\n    data = factor(res_B_aug$.fitted >= 0.5),\n    reference = factor(res_B_aug$died == 1),\n    positive = \"TRUE\"\n  )\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction FALSE TRUE\n     FALSE   113    6\n     TRUE      4   11\n                                         \n               Accuracy : 0.9254         \n                 95% CI : (0.867, 0.9636)\n    No Information Rate : 0.8731         \n    P-Value [Acc > NIR] : 0.03897        \n                                         \n                  Kappa : 0.6453         \n                                         \n Mcnemar's Test P-Value : 0.75183        \n                                         \n            Sensitivity : 0.64706        \n            Specificity : 0.96581        \n         Pos Pred Value : 0.73333        \n         Neg Pred Value : 0.94958        \n             Prevalence : 0.12687        \n         Detection Rate : 0.08209        \n   Detection Prevalence : 0.11194        \n      Balanced Accuracy : 0.80644        \n                                         \n       'Positive' Class : TRUE           \n                                         \n\n\n\n\n20.14.4 The ROC curve for Model B\n\n## requires ROCR package\nprob <- predict(res_modB, resect, type=\"response\")\npred <- prediction(prob, resect$died)\nperf <- performance(pred, measure = \"tpr\", x.measure = \"fpr\")\nauc <- performance(pred, measure=\"auc\")\n\nauc <- round(auc@y.values[[1]],3)\nroc.data <- data.frame(fpr=unlist(perf@x.values),\n                       tpr=unlist(perf@y.values),\n                       model=\"GLM\")\n\nggplot(roc.data, aes(x=fpr, ymin=0, ymax=tpr)) +\n    geom_ribbon(alpha=0.2, fill = \"blue\") +\n    geom_line(aes(y=tpr), col = \"blue\") +\n    geom_abline(intercept = 0, slope = 1, lty = \"dashed\") +\n    labs(title = paste0(\"Model B: ROC Curve w/ AUC=\", auc)) +\n    theme_bw()\n\n\n\n\nThe area under the curve (C-statistic) is 0.86, which certainly looks like a more discriminating fit than model A with resection alone.\n\n\n20.14.5 Residuals, Leverage and Influence\n\nplot(res_modB, which = 5)\n\n\n\n\nAgain, we see no signs of deeply influential points in this model.\nWe’ll continue working with these resect data as we fit logistic regression models with the help of the rms package in our next Chapter.\n\n\n\n\nLong, J. Scott. 1997. Regression Models for Categorical and Limited Dependent Variables. Thousand Oaks, CA: Sage Publications.\n\n\nPeduzzi, Peter, John Concato, Elizabeth Kemper, Theodore R. Holford, and Alvan R. Feinstein. 1996. “A Simulation Study of the Number of Events Per Variable in Logistic Regression Analysis.” Journal of Clinical Epidemiology 49 (12): 1373–79.\n\n\nRiffenburgh, Robert H. 2006. Statistics in Medicine. Second Edition. Burlington, MA: Elsevier Academic Press."
  },
  {
    "objectID": "logistic3.html#r-setup-used-here",
    "href": "logistic3.html#r-setup-used-here",
    "title": "21  Logistic Regression with lrm",
    "section": "21.1 R Setup Used Here",
    "text": "21.1 R Setup Used Here\n\nknitr::opts_chunk$set(comment = NA)\n\nlibrary(ROCR)\nlibrary(pROC)\nlibrary(naniar)\nlibrary(rms)\nlibrary(tidyverse) \n\ntheme_set(theme_bw())\n\n\n21.1.1 Data Load\n\nresect <- read_csv(\"data/resect.csv\", show_col_types = FALSE)"
  },
  {
    "objectID": "logistic3.html#logistic-regression-using-lrm",
    "href": "logistic3.html#logistic-regression-using-lrm",
    "title": "21  Logistic Regression with lrm",
    "section": "21.2 Logistic Regression using lrm",
    "text": "21.2 Logistic Regression using lrm\nTo obtain the Nagelkerke \\(R^2\\) and the C statistic, as well as some other summaries, I’ll now demonstrate the use of lrm from the rms package to fit a logistic regression model.\nWe’ll return to the original model, predicting death using resection size alone.\n\ndd <- datadist(resect)\noptions(datadist=\"dd\")\n\nres_modC <- lrm(died ~ resection, data=resect, x=TRUE, y=TRUE)\nres_modC\n\nLogistic Regression Model\n \n lrm(formula = died ~ resection, data = resect, x = TRUE, y = TRUE)\n \n                       Model Likelihood     Discrimination    Rank Discrim.    \n                             Ratio Test            Indexes          Indexes    \n Obs           134    LR chi2     12.45     R2       0.167    C       0.771    \n  0            117    d.f.            1     R2(1,134)0.082    Dxy     0.541    \n  1             17    Pr(> chi2) 0.0004    R2(1,44.5)0.227    gamma   0.582    \n max |deriv| 2e-06                          Brier    0.103    tau-a   0.121    \n \n           Coef    S.E.   Wald Z Pr(>|Z|)\n Intercept -4.4337 0.8799 -5.04  <0.0001 \n resection  0.7417 0.2230  3.33  0.0009  \n \n\n\nThis output specifies the following:\n\nObs = The number of observations used to fit the model, with 0 = the number of zeros and 1 = the number of ones in our outcome, died. Also specified is the maximum absolute value of the derivative at the point where the maximum likelihood function was estimated. I wouldn’t worry about that practically, as all you will care about is whether the iterative function-fitting process converged, and R will warn you in other ways if it doesn’t.\nA likelihood ratio test (drop in deviance test) subtracting the residual deviance from the null deviance obtain the Likelihood Ratio \\(\\chi^2\\) statistic, subtracting residual df from null df to obtain degrees of freedom, and comparing the resulting test statistic to a \\(\\chi^2\\) distribution with the appropriate degrees of freedom to determine a p value.\nA series of discrimination indexes, including the Nagelkerke \\(R^2\\), symbolized R2, and several others we’ll discuss shortly.\nA series of rank discrimination indexes, including the C statistic (area under the ROC curve) and Somers’ D (Dxy), and several others.\nA table of coefficients, standard errors, Wald Z statistics and p values based on those Wald statistics.\n\nThe C statistic is estimated to be 0.771, with an associated (Nagelkerke) \\(R^2\\) = 0.167, both indicating at best mediocre performance for this model, as it turns out.\n\n21.2.1 Interpreting Nagelkerke \\(R^2\\)\nThere are many ways to calculate \\(R^2\\) for logistic regression.\n\nAt the URL linked here there is a nice summary of the key issue, which is that there are at least three different ways to think about \\(R^2\\) in linear regression that are equivalent in that context, but when you move to a categorical outcome, which interpretation you use leads you down a different path for extension to the new type of outcome. In linear regression…\n\nYou might think of \\(R^2\\) as a measure of the proportion of variability explained.\nYou might think of \\(R^2\\) as measuring the improvement from a null model to a fitted model.\nYou might think of \\(R^2\\) as the square of the correlation coefficient.\n\nPaul Allison, for instance, describes several at this link in a post entitled “What’s the Best R-Squared for Logistic Regression?”\nJonathan Bartlett looks at McFadden’s pseudo \\(R^2\\) in some detail (including some R code) at this link, in a post entitled “R squared in logistic regression”\n\nThe Nagelkerke approach that is presented as R2 in the lrm output is as good as most of the available approaches, and has the positive feature that it does reach 1 if the fitted model shows as much improvement as possible over the null model (which predicts the mean response for all subjects, and has \\(R^2\\) = 0). The greater the improvement, the higher the Nagelkerke \\(R^2\\).\nFor model A, our Nagelkerke \\(R^2\\) = 0.167, which is pretty poor. It doesn’t technically mean that 16.7% of any sort of variation has been explained, though.\n\n\n21.2.2 Interpreting the C statistic and Plotting the ROC Curve\nThe C statistic is a measure of the area under the receiver operating characteristic curve. This link has some nice material that provides some insight into the C statistic and ROC curve.\n\nRecall that C ranges from 0 to 1. 0 = BAD, 1 = GOOD.\n\nvalues of C less than 0.5 indicate that your prediction model is not even as good as simple random guessing of “yes” or “no” for your response.\nC = 0.5 for random guessing\nC = 1 indicates a perfect classification scheme - one that correctly guesses “yes” for all “yes” patients, and for none of the “no” patients.\n\nThe closer C is to 1, the happier we’ll be, most of the time.\n\nOften we’ll call models with 0.5 < C < 0.8 poor or weak in terms of predictive ability by this measure\n0.8 \\(\\leq\\) C < 0.9 are moderately strong in terms of predictive power (indicate good discrimination)\nC \\(\\geq\\) 0.9 usually indicates a very strong model in this regard (indicate excellent discrimination)\n\n\nWe’ve seen the ROC curve for this model before, when we looked at model res_modA fitted using glm in the previous chapter. But, just for completeness, I’ll include it.\nNote. I change the initial predict call from type = \"response\" for a glm fit to type = \"fitted\" in a lrm fit. Otherwise, this is the same approach.\n\n## requires ROCR package\nprob <- predict(res_modC, resect, type=\"fitted\")\npred <- prediction(prob, resect$died)\nperf <- performance(pred, measure = \"tpr\", x.measure = \"fpr\")\nauc <- performance(pred, measure=\"auc\")\n\nauc <- round(auc@y.values[[1]],3)\nroc.data <- data.frame(fpr=unlist(perf@x.values),\n                       tpr=unlist(perf@y.values),\n                       model=\"GLM\")\n\nggplot(roc.data, aes(x=fpr, ymin=0, ymax=tpr)) +\n    geom_ribbon(alpha=0.2, fill = \"blue\") +\n    geom_line(aes(y=tpr), col = \"blue\") +\n    geom_abline(intercept = 0, slope = 1, lty = \"dashed\") +\n    labs(title = paste0(\"Model C: ROC Curve w/ AUC=\", auc)) +\n    theme_bw()\n\n\n\n\n\n\n21.2.3 The C statistic and Somers’ D\n\nThe C statistic is directly related to Somers’ D statistic, abbreviated \\(D_{xy}\\), by the equation C = 0.5 + (D/2).\n\nSomers’ D and the ROC area only measure how well predicted values from the model can rank-order the responses. For example, predicted probabilities of 0.01 and 0.99 for a pair of subjects are no better than probabilities of 0.2 and 0.8 using rank measures, if the first subject had a lower response value than the second.\nThus, the C statistic (or \\(D_{xy}\\)) may not be very sensitive ways to choose between models, even though they provide reasonable summaries of the models individually.\nThis is especially true when the models are strong. The Nagelkerke \\(R^2\\) may be more sensitive.\n\nBut as it turns out, we sometimes have to look at the ROC shapes, as the summary statistic alone isn’t enough.\n\nIn our case, Somers D (Dxy) = .541, so the C statistic is 0.771.\n\n\n21.2.4 Validating the Logistic Regression Model Summary Statistics\nLike other regression-fitting tools in rms, the lrm function has a special validate tool to help perform resampling validation of a model, with or without backwards step-wise variable selection. Here, we’ll validate our model’s summary statistics using 100 bootstrap replications.\n\nset.seed(432001) \nvalidate(res_modC, B = 100)\n\n          index.orig training   test optimism index.corrected   n\nDxy           0.5415   0.5422 0.5415   0.0007          0.5408 100\nR2            0.1666   0.1748 0.1666   0.0083          0.1583 100\nIntercept     0.0000   0.0000 0.1631  -0.1631          0.1631 100\nSlope         1.0000   1.0000 1.0463  -0.0463          1.0463 100\nEmax          0.0000   0.0000 0.0428   0.0428          0.0428 100\nD             0.0854   0.0909 0.0854   0.0055          0.0800 100\nU            -0.0149  -0.0149 0.0017  -0.0167          0.0017 100\nQ             0.1004   0.1058 0.0837   0.0221          0.0783 100\nB             0.1025   0.0986 0.1051  -0.0065          0.1090 100\ng             1.0369   1.0677 1.0369   0.0308          1.0061 100\ngp            0.1101   0.1080 0.1101  -0.0021          0.1122 100\n\n\nRecall that our area under the curve (C statistic) = 0.5 + (Dxy/2), so that we can also use the first row of statistics to validate the C statistic. Accounting for optimism in this manner, our validation-corrected estimates are Dxy = 0.5408, so C = 0.7704, and, from the second row of statistics, we can read off the validated Nagelkerke \\(R^2\\), which is 0.1583.\n\n\n21.2.5 Plotting the Summary of the lrm approach\nThe summary function applied to an lrm fit shows the effect size comparing the 25th percentile to the 75th percentile of resection.\n\nplot(summary(res_modC))\n\n\n\nsummary(res_modC)\n\n             Effects              Response : died \n\n Factor      Low High Diff. Effect S.E.    Lower 0.95 Upper 0.95\n resection   2   4    2     1.4834 0.44591 0.6094      2.3574   \n  Odds Ratio 2   4    2     4.4078      NA 1.8393     10.5630   \n\n\nSo, a move from a resection of 2 cm to a resection of 4 cm is associated with an estimated effect on the log odds of death of 1.48 (with standard error 0.45), or with an estimated effect on the odds ratio for death of 4.41, with 95% CI (1.84, 10.56).\n\n\n21.2.6 Plot In-Sample Predictions for Model C\nHere we plot the effect of resection (and 95% confidence intervals) across the range of observed values of resection on the log odds of death. Note the linear effect of resection size on the log odds scale.\n\nggplot(Predict(res_modC))\n\n\n\n\nBy applying the plogis function within the Predict command, we can plot the effect of resection on the estimated probability of death. Note the non-linear effect on this probability in this logistic regression model.\n\nggplot(Predict(res_modC, fun = plogis)) + \n    labs(y = \"Predicted probability from Model C\",\n         title = \"Model C with the resect data\")\n\n\n\n\nThe Predict function itself provides the raw material being captured in this plot.\n\nhead(Predict(res_modC, fun = plogis))\n\n            resection       yhat       lower      upper .predictor.\nresection.1  1.000000 0.02431476 0.006636502 0.08505223   resection\nresection.2  1.020101 0.02467096 0.006789313 0.08559056   resection\nresection.3  1.040201 0.02503224 0.006945549 0.08613277   resection\nresection.4  1.060302 0.02539867 0.007105283 0.08667889   resection\nresection.5  1.080402 0.02577033 0.007268589 0.08722896   resection\nresection.6  1.100503 0.02614728 0.007435542 0.08778304   resection\n\nResponse variable (y):  \n\nLimits are 0.95 confidence limits\n\n\n\n\n21.2.7 ANOVA from the lrm approach\n\nanova(res_modC)\n\n                Wald Statistics          Response: died \n\n Factor     Chi-Square d.f. P    \n resection  11.07      1    9e-04\n TOTAL      11.07      1    9e-04\n\n\nThe ANOVA approach applied to a lrm fit provides a Wald test for the model as a whole. Here, the use of resection is a significant improvement over a null (intercept-only) model. The p value is 9 x 10-4.\n\n\n21.2.8 Are any points particularly influential?\nI’ll use a cutoff for dfbeta here of 0.3, instead of the default 0.2, because I want to focus on truly influential points. Note that we have to use the data frame version of resect as show.influence isn’t tibble-friendly.\n\ninf.C <- which.influence(res_modC, cutoff=0.3)\ninf.C\n\n$Intercept\n[1]  84 128\n\n$resection\n[1] 84\n\nshow.influence(object = inf.C, dframe = data.frame(resect))\n\n    Count resection\n84      2      *2.0\n128     1       2.5\n\n\nIt appears that observation 84 may have a meaningful effect on both the intercept and the coefficient for resection.\n\n\n21.2.9 A Nomogram for Model C\nWe use the plogis function within a nomogram call to get R to produce fitted probabilities (of our outcome, died) in this case.\n\nplot(nomogram(res_modC, fun=plogis, \n              fun.at=c(0.05, seq(0.1, 0.9, by = 0.1), 0.95), \n              funlabel=\"Pr(died)\"))\n\n\n\n\nSince there’s no non-linearity in the right hand side of our simple logistic regression model, the nomogram is straightforward. We calculate the points based on the resection by traveling up, and then travel down in a straight vertical line from total points through the linear (log odds) predictor straight to a fitted probability. Note that fitted probabilities above 0.5 are not possible within the range of observed resection values in this case."
  },
  {
    "objectID": "logistic3.html#model-d-an-augmented-kitchen-sink-model",
    "href": "logistic3.html#model-d-an-augmented-kitchen-sink-model",
    "title": "21  Logistic Regression with lrm",
    "section": "21.3 Model D: An Augmented Kitchen Sink Model",
    "text": "21.3 Model D: An Augmented Kitchen Sink Model\nCan we predict survival from the patient’s age, whether the patient had prior tracheal surgery or not, the extent of the resection, and whether intubation was required at the end of surgery?\n\n21.3.1 Spearman \\(\\rho^2\\) Plot\nLet’s start by considering the limited use of non-linear terms for predictors that look important in a Spearman \\(\\rho^2\\) plot.\n\nplot(spearman2(died ~ age + prior + resection + intubated, data=resect))\n\n\n\n\nThe most important variable appears to be whether intubation was required, so I’ll include intubated’s interaction with the linear effect of the next most (apparently) important variable, resection, and also a cubic spline for resection, with three knots. Since prior and age look less important, I’ll simply add them as linear terms.\n\n\n21.3.2 Fitting Model D using lrm\nNote the use of %ia% here. This insures that only the linear part of the resection term will be used in the interaction with intubated.\n\ndd <- datadist(resect)\noptions(datadist=\"dd\")\n\nres_modD <- lrm(died ~ age + prior + rcs(resection, 3) +\n                 intubated + intubated %ia% resection, \n               data=resect, x=TRUE, y=TRUE)\n\n\n\n21.3.3 Assessing Model D using lrm’s tools\n\nres_modD\n\nLogistic Regression Model\n \n lrm(formula = died ~ age + prior + rcs(resection, 3) + intubated + \n     intubated %ia% resection, data = resect, x = TRUE, y = TRUE)\n \n                        Model Likelihood     Discrimination    Rank Discrim.    \n                              Ratio Test            Indexes          Indexes    \n Obs           134    LR chi2      38.08     R2       0.464    C       0.880    \n  0            117    d.f.             6     R2(6,134)0.213    Dxy     0.759    \n  1             17    Pr(> chi2) <0.0001    R2(6,44.5)0.513    gamma   0.770    \n max |deriv| 9e-08                           Brier    0.067    tau-a   0.169    \n \n                       Coef     S.E.   Wald Z Pr(>|Z|)\n Intercept             -11.3636 4.9099 -2.31  0.0206  \n age                     0.0000 0.0210  0.00  0.9988  \n prior                   0.6269 0.7367  0.85  0.3947  \n resection               3.3799 1.9700  1.72  0.0862  \n resection'             -4.2104 2.7035 -1.56  0.1194  \n intubated               0.4576 2.7848  0.16  0.8695  \n intubated * resection   0.6188 0.7306  0.85  0.3970  \n \n\n\n\nThe model likelihood ratio test suggests that at least some of these predictors are helpful.\nThe Nagelkerke \\(R^2\\) of 0.46, and the C statistic of 0.88 indicate a meaningful improvement in discrimination over our model with resection alone.\nThe Wald Z tests see some potential need to prune the model, as none of the elements reaches statistical significance without the others. The product term between intubated and resection, in particular, doesn’t appear to have helped much, once we already had the main effects.\n\n\n\n21.3.4 ANOVA and Wald Tests for Model D\n\nanova(res_modD)\n\n                Wald Statistics          Response: died \n\n Factor                                               Chi-Square d.f. P     \n age                                                   0.00      1    0.9988\n prior                                                 0.72      1    0.3947\n resection  (Factor+Higher Order Factors)              4.95      3    0.1753\n  All Interactions                                     0.72      1    0.3970\n  Nonlinear                                            2.43      1    0.1194\n intubated  (Factor+Higher Order Factors)             16.45      2    0.0003\n  All Interactions                                     0.72      1    0.3970\n intubated * resection  (Factor+Higher Order Factors)  0.72      1    0.3970\n TOTAL NONLINEAR + INTERACTION                         2.56      2    0.2783\n TOTAL                                                23.90      6    0.0005\n\n\nNeither the interaction term nor the non-linearity from the cubic spline appears to be statistically significant, based on the Wald tests via ANOVA. However it is clear that intubated has a significant impact as a main effect.\n\n\n21.3.5 Effect Sizes in Model D\n\nplot(summary(res_modD))\n\n\n\nsummary(res_modD)\n\n             Effects              Response : died \n\n Factor      Low High Diff. Effect      S.E.    Lower 0.95 Upper 0.95\n age         36  61   25    -0.00080933 0.52409 -1.02800     1.0264  \n  Odds Ratio 36  61   25     0.99919000      NA  0.35772     2.7910  \n prior        0   1    1     0.62693000 0.73665 -0.81688     2.0707  \n  Odds Ratio  0   1    1     1.87190000      NA  0.44181     7.9307  \n resection    2   4    2     2.42930000 1.43510 -0.38342     5.2419  \n  Odds Ratio  2   4    2    11.35000000      NA  0.68153   189.0400  \n intubated    0   1    1     2.00470000 1.11220 -0.17513     4.1845  \n  Odds Ratio  0   1    1     7.42380000      NA  0.83934    65.6610  \n\nAdjusted to: resection=2.5 intubated=0  \n\n\nThe effect sizes are perhaps best described in terms of odds ratios. The odds ratio for death isn’t significantly different from 1 for any variable, but the impact of resection and intubated, though not strong enough to be significant, look more substantial (if poorly estimated) than the effects of age and prior.\n\n\n21.3.6 Plot In-Sample Predictions for Model D\nHere are plots of the effects across the range of each predictor (holding the others constant) on the log odds scale. Note the non-linear effect of resection implied by the use of a spline there.\n\nggplot(Predict(res_modD))\n\n\n\n\nWe can also capture and plot these results on the probability scale, as follows1.\n\nggplot(Predict(res_modD, fun = plogis))\n\n\n\n\n\n\n21.3.7 Plotting the ROC curve for Model D\nAgain, remember to use type = \"fitted\" with a lrm fit.\n\n## requires ROCR package\nprob <- predict(res_modD, resect, type=\"fitted\")\npred <- prediction(prob, resect$died)\nperf <- performance(pred, measure = \"tpr\", x.measure = \"fpr\")\nauc <- performance(pred, measure=\"auc\")\n\nauc <- round(auc@y.values[[1]],3)\nroc.data <- data.frame(fpr=unlist(perf@x.values),\n                       tpr=unlist(perf@y.values),\n                       model=\"GLM\")\n\nggplot(roc.data, aes(x=fpr, ymin=0, ymax=tpr)) +\n    geom_ribbon(alpha=0.2, fill = \"blue\") +\n    geom_line(aes(y=tpr), col = \"blue\") +\n    geom_abline(intercept = 0, slope = 1, lty = \"dashed\") +\n    labs(title = paste0(\"ROC Curve w/ AUC=\", auc)) +\n    theme_bw()\n\n\n\n\nThe AUC fitted with ROCR (0.883) is slightly different than what lrm has told us (0.880), and this also happens if we use the pROC approach, demonstrated below.\n\n## requires pROC package\nroc.modD <- \n    roc(resect$died ~ predict(res_modD, type=\"fitted\"),\n        ci = TRUE)\n\nSetting levels: control = 0, case = 1\n\n\nSetting direction: controls < cases\n\nroc.modD\n\n\nCall:\nroc.formula(formula = resect$died ~ predict(res_modD, type = \"fitted\"),     ci = TRUE)\n\nData: predict(res_modD, type = \"fitted\") in 117 controls (resect$died 0) < 17 cases (resect$died 1).\nArea under the curve: 0.8826\n95% CI: 0.7952-0.97 (DeLong)\n\nplot(roc.modD)\n\n\n\n\n\n\n21.3.8 Validation of Model D summaries\n\nset.seed(432002)\nvalidate(res_modD, B = 100)\n\n\nDivergence or singularity in 5 samples\n\n\n          index.orig training    test optimism index.corrected  n\nDxy           0.7652   0.8080  0.7352   0.0727          0.6925 95\nR2            0.4643   0.5347  0.4119   0.1228          0.3416 95\nIntercept     0.0000   0.0000 -0.3533   0.3533         -0.3533 95\nSlope         1.0000   1.0000  0.7658   0.2342          0.7658 95\nEmax          0.0000   0.0000  0.1308   0.1308          0.1308 95\nD             0.2767   0.3415  0.2407   0.1008          0.1759 95\nU            -0.0149  -0.0149  0.0883  -0.1032          0.0883 95\nQ             0.2916   0.3564  0.1524   0.2040          0.0876 95\nB             0.0673   0.0640  0.0736  -0.0096          0.0769 95\ng             2.3819   4.0387  2.4635   1.5751          0.8068 95\ngp            0.1720   0.1910  0.1632   0.0278          0.1442 95\n\n\nThe C statistic indicates fairly strong discrimination, at C = 0.88, although after validation, this looks much weaker (based on Dxy = 0.6925, we would have C = 0.5 + 0.6925/2 = 0.85) and the Nagelkerke \\(R^2\\) is also reasonably good, at 0.46, although this, too, is overly optimistic, and we bias-correct through our validation study to 0.34."
  },
  {
    "objectID": "logistic3.html#model-e-fitting-a-reduced-model-in-light-of-model-d",
    "href": "logistic3.html#model-e-fitting-a-reduced-model-in-light-of-model-d",
    "title": "21  Logistic Regression with lrm",
    "section": "21.4 Model E: Fitting a Reduced Model in light of Model D",
    "text": "21.4 Model E: Fitting a Reduced Model in light of Model D\nCan you suggest a reduced model (using a subset of the independent variables in model D) that adequately predicts survival?\nBased on the anova for model D and the Spearman rho-squared plot, it appears that a two-predictor model using intubation and resection may be sufficient. Neither of the other potential predictors shows a statistically detectable effect in its Wald test.\n\nres_modE <- lrm(died ~ intubated + resection, data=resect, \n                x=TRUE, y=TRUE)\nres_modE\n\nLogistic Regression Model\n \n lrm(formula = died ~ intubated + resection, data = resect, x = TRUE, \n     y = TRUE)\n \n                        Model Likelihood     Discrimination    Rank Discrim.    \n                              Ratio Test            Indexes          Indexes    \n Obs           134    LR chi2      33.27     R2       0.413    C       0.867    \n  0            117    d.f.             2     R2(2,134)0.208    Dxy     0.734    \n  1             17    Pr(> chi2) <0.0001    R2(2,44.5)0.505    gamma   0.757    \n max |deriv| 5e-10                           Brier    0.073    tau-a   0.164    \n \n           Coef    S.E.   Wald Z Pr(>|Z|)\n Intercept -4.6370 1.0430 -4.45  <0.0001 \n intubated  2.8640 0.6479  4.42  <0.0001 \n resection  0.5475 0.2689  2.04  0.0418  \n \n\n\nThe model equation is that the log odds of death is -4.637 + 2.864 intubated + 0.548 resection.\nThis implies that:\n\nfor intubated patients, the equation is -1.773 + 0.548 resection, while\nfor non-intubated patients, the equation is -4.637 + 0.548 resection.\n\nWe can use the ilogit function within the faraway package to help plot this.\n\n21.4.1 A Plot comparing the two intubation groups\n\nggplot(resect, aes(x = resection, y = died, \n                   col = factor(intubated))) + \n    scale_color_manual(values = c(\"blue\", \"red\")) +\n    geom_jitter(size = 2, height = 0.1) +\n    geom_line(aes(x = resection, \n                  y = faraway::ilogit(-4.637 + 0.548*resection)),\n              col = \"blue\") +\n    geom_line(aes(x = resection,\n                  y = faraway::ilogit(-1.773 + 0.548*resection)),\n              col = \"red\") +\n    geom_text(x = 4, y = 0.2, label = \"Not Intubated\", \n              col=\"blue\") +\n    geom_text(x = 2.5, y = 0.6, label = \"Intubated Patients\", \n              col=\"red\") +\n    labs(x = \"Extent of Resection (in cm.)\",\n         y = \"Death (1,0) and estimated probability of death\",\n         title = \"resect data, Model E\")\n\n\n\n\nThe effect of intubation appears to be very large, compared to the resection size effect.\n\n\n21.4.2 Nomogram for Model E\nA nomogram of the model would help, too.\n\nplot(nomogram(res_modE, fun=plogis, \n              fun.at=c(0.05, seq(0.1, 0.9, by=0.1), 0.95), \n              funlabel=\"Pr(died)\"))\n\n\n\n\nAgain, we see that the effect of intubation is enormous, compared to the effect of resection. Another way to see this is to plot the effect sizes directly.\n\n\n21.4.3 Effect Sizes from Model E\n\nplot(summary(res_modE))\n\n\n\nsummary(res_modE)\n\n             Effects              Response : died \n\n Factor      Low High Diff. Effect  S.E.    Lower 0.95 Upper 0.95\n intubated   0   1    1      2.8640 0.64790 1.59410     4.1338   \n  Odds Ratio 0   1    1     17.5310      NA 4.92390    62.4160   \n resection   2   4    2      1.0949 0.53783 0.04082     2.1491   \n  Odds Ratio 2   4    2      2.9890      NA 1.04170     8.5769   \n\n\n\n\n21.4.4 Plot In-Sample Predictions for Model E\nHere are plots of the effects across the range of each predictor (holding the other constant) on the log odds scale.\n\nggplot(Predict(res_modE))\n\n\n\n\nWe can also capture and plot these results on the probability scale, as follows.\n\nggplot(Predict(res_modE, fun = plogis))\n\n\n\n\n\n\n21.4.5 ANOVA for Model E\n\nanova(res_modE)\n\n                Wald Statistics          Response: died \n\n Factor     Chi-Square d.f. P     \n intubated  19.54      1    <.0001\n resection   4.14      1    0.0418\n TOTAL      25.47      2    <.0001\n\n\n\n\n21.4.6 Validation of Model E\n\nvalidate(res_modE, method=\"boot\", B=40)\n\n          index.orig training    test optimism index.corrected  n\nDxy           0.7340   0.6896  0.7326  -0.0430          0.7771 40\nR2            0.4128   0.3814  0.4025  -0.0211          0.4339 40\nIntercept     0.0000   0.0000  0.1367  -0.1367          0.1367 40\nSlope         1.0000   1.0000  1.0472  -0.0472          1.0472 40\nEmax          0.0000   0.0000  0.0369   0.0369          0.0369 40\nD             0.2408   0.2183  0.2339  -0.0157          0.2565 40\nU            -0.0149  -0.0149 -0.0001  -0.0148         -0.0001 40\nQ             0.2558   0.2332  0.2340  -0.0009          0.2566 40\nB             0.0727   0.0727  0.0759  -0.0032          0.0759 40\ng             1.3970   1.3391  1.3577  -0.0186          1.4156 40\ngp            0.1597   0.1446  0.1563  -0.0117          0.1714 40\n\n\nOur bootstrap validated assessments of discrimination and goodness of fit look somewhat more reasonable now.\n\n\n21.4.7 Do any points seem particularly influential?\nAs a last step, I’ll look at influence, and residuals, associated with model E.\n\ninf.E <- which.influence(res_modE, cutoff=0.3)\n\ninf.E\n\n$Intercept\n[1] 84 94\n\n$resection\n[1] 84 94\n\nshow.influence(inf.E, dframe = data.frame(resect))\n\n   Count resection\n84     2        *2\n94     2        *6\n\n\n\n\n21.4.8 Fitting Model E using glm to get plots about influence\n\nres_modEglm <- glm(died ~ intubated + resection, \n                  data=resect, family=\"binomial\")\npar(mfrow=c(1,2))\nplot(res_modEglm, which=c(4:5))\n\n\n\n\nUsing this glm residuals approach, we again see that points 84 and 94 have the largest influence on our model E."
  },
  {
    "objectID": "logistic3.html#concordance-comparing-model-c-d-and-es-predictions",
    "href": "logistic3.html#concordance-comparing-model-c-d-and-es-predictions",
    "title": "21  Logistic Regression with lrm",
    "section": "21.5 Concordance: Comparing Model C, D and E’s predictions",
    "text": "21.5 Concordance: Comparing Model C, D and E’s predictions\nTo start, we’ll gather the predictions made by each model (C, D and E) on the probability scale, in one place. Sadly, augment from broom doesn’t work well with lrm fits, so we have to do this on our own.\n\nresect_preds <- resect |>\n    mutate(C = predict(res_modC, type = \"fitted\"),\n           D = predict(res_modD, type = \"fitted\"),\n           E = predict(res_modE, type = \"fitted\"))\n\nhead(resect_preds)\n\n# A tibble: 6 × 9\n  subj_id   age prior resection intubated  died      C      D      E\n    <dbl> <dbl> <dbl>     <dbl>     <dbl> <dbl>  <dbl>  <dbl>  <dbl>\n1       1    34     1       2.5         0     0 0.0705 0.0632 0.0367\n2       2    57     0       5           0     0 0.326  0.0620 0.130 \n3       3    60     1       4           1     1 0.187  0.791  0.603 \n4       4    62     1       4.2         0     0 0.211  0.158  0.0881\n5       5    28     0       6           1     1 0.504  0.711  0.819 \n6       6    52     0       3           0     0 0.0990 0.0737 0.0477\n\n\nAnd now, we’ll use the pivot_longer() function from the tidyr package (part of the tidyverse) to arrange the models and predicted probabilities in a more useful manner for plotting.\n\nres_p <- resect_preds |>\n  pivot_longer(cols = 7:9, names_to = \"model\", values_to = \"prediction\") |>\n  select(subj_id, died, model, prediction)\n\nhead(res_p)\n\n# A tibble: 6 × 4\n  subj_id  died model prediction\n    <dbl> <dbl> <chr>      <dbl>\n1       1     0 C         0.0705\n2       1     0 D         0.0632\n3       1     0 E         0.0367\n4       2     0 C         0.326 \n5       2     0 D         0.0620\n6       2     0 E         0.130 \n\n\nHere’s the resulting plot.\n\nggplot(res_p, aes(x = factor(died), y = prediction, \n                  group = model, col = model)) +\n    geom_jitter(width = 0.25) + \n    geom_hline(yintercept = 0.5) +\n    facet_wrap( ~ model) + \n    guides(color = \"none\") +\n    labs(title = \"Comparing Predictions for our Three Models\",\n         subtitle = \"A graphical view of concordance\",\n         x = \"Actual mortality status (1 = died)\",\n         y = \"Predicted probability of death\")\n\n\n\n\nWe could specify a particular rule, for example: if the predicted probability of death is 0.5 or greater, then predict “Died”.\n\nres_p$rule.5 <- ifelse(res_p$prediction >= 0.5, \n                       \"Predict Died\", \"Predict Alive\")\n\nftable(table(res_p$model, res_p$rule.5, res_p$died))\n\n                   0   1\n                        \nC Predict Alive  114  16\n  Predict Died     3   1\nD Predict Alive  113   7\n  Predict Died     4  10\nE Predict Alive  114   8\n  Predict Died     3   9\n\n\nAnd perhaps build the linked table of row probabilities…\n\nround(100*prop.table(\n    ftable(table(res_p$model, res_p$rule.5, res_p$died))\n    ,1),2)\n\n                     0     1\n                            \nC Predict Alive  87.69 12.31\n  Predict Died   75.00 25.00\nD Predict Alive  94.17  5.83\n  Predict Died   28.57 71.43\nE Predict Alive  93.44  6.56\n  Predict Died   25.00 75.00\n\n\nFor example, in model E, 93.44% of those predicted to be alive actually survived, and 75% of those predicted to die actually died.\n\nModel D does a little better in one direction (94.17% of those predicted by Model D to be alive actually survived) but worse in the other (71.43% of those predicted by Model D to die actually died.)\nModel C does worse than each of the others in both predicting those who survive and those who die.\n\nNote that the approaches discussed here would be useful if we had a new sample to predict on, as well. We could then compare the errors for that new data made by this sort of classification scheme either graphically or in a table."
  },
  {
    "objectID": "logistic3.html#conclusions",
    "href": "logistic3.html#conclusions",
    "title": "21  Logistic Regression with lrm",
    "section": "21.6 Conclusions",
    "text": "21.6 Conclusions\nIt appears that intubated status and, to a lesser degree, the extent of the resection both play a meaningful role in predicting death associated with tracheal carina resection surgery. Patients who are intubated are associated with worse outcomes (greater risk of death) and more extensive resections are also associated with worse outcomes."
  },
  {
    "objectID": "effectsize.html#r-setup-used-here",
    "href": "effectsize.html#r-setup-used-here",
    "title": "22  Estimating and Interpreting Effect Sizes",
    "section": "22.1 R Setup Used Here",
    "text": "22.1 R Setup Used Here\n\nknitr::opts_chunk$set(comment = NA)\n\nlibrary(broom)\nlibrary(knitr)\nlibrary(rms)\nlibrary(tidyverse) \n\ntheme_set(theme_bw())\n\n\n22.1.1 Data Load\nConsider the smalldat.csv data available on our site, which is modeled on the public Framingham data set available from BIOLINCC1. From the BIOLINCC documentation:\n\nThe Framingham Heart Study is a long term prospective study of the etiology of cardiovascular disease among a population of free living subjects in the community of Framingham, Massachusetts. The Framingham Heart Study was a landmark study in epidemiology in that it was the first prospective study of cardiovascular disease and identified the concept of risk factors and their joint effects.\n\n\nsmalldat <- read_csv(\"data/smalldat.csv\", show_col_types = FALSE)"
  },
  {
    "objectID": "effectsize.html#available-variables",
    "href": "effectsize.html#available-variables",
    "title": "22  Estimating and Interpreting Effect Sizes",
    "section": "22.2 Available Variables",
    "text": "22.2 Available Variables\nThe smalldat data contains 150 observations on the following variables2:\n\n\n\nVariable\nDescription\n\n\n\n\nsubject\nSubject identification code\n\n\nsmoker\n1 = current smoker, 0 = not current smoker\n\n\ntotchol\ntotal cholesterol, in mg/dl\n\n\nage\nage in years\n\n\nsex\nsubject’s sex (M or F)\n\n\neduc\nsubject’s educational attainment (4 levels)\n\n\n\n\nggplot(smalldat, aes(x = totchol)) + \n    geom_histogram(bins = 15, col = \"white\", fill = \"dodgerblue\")"
  },
  {
    "objectID": "effectsize.html#effect-interpretation-in-a-linear-regression-model",
    "href": "effectsize.html#effect-interpretation-in-a-linear-regression-model",
    "title": "22  Estimating and Interpreting Effect Sizes",
    "section": "22.3 Effect Interpretation in A Linear Regression Model",
    "text": "22.3 Effect Interpretation in A Linear Regression Model\n\nm1 <- lm(totchol ~ age + sex + factor(educ),\n         data = smalldat)\n\nkable(tidy(m1, conf.int = TRUE), digits = 3)\n\n\n\n\n\n\n\n\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\nconf.low\nconf.high\n\n\n\n\n(Intercept)\n171.197\n20.201\n8.475\n0.000\n131.268\n211.126\n\n\nage\n1.202\n0.367\n3.270\n0.001\n0.475\n1.928\n\n\nsexM\n3.612\n6.441\n0.561\n0.576\n-9.119\n16.343\n\n\nfactor(educ)2_Middle\n11.044\n7.702\n1.434\n0.154\n-4.180\n26.268\n\n\nfactor(educ)3_High\n-2.459\n9.390\n-0.262\n0.794\n-21.019\n16.101\n\n\nfactor(educ)4_VHigh\n10.927\n9.780\n1.117\n0.266\n-8.405\n30.258\n\n\n\n\n\n\nWhat is the effect of age on totchol in Model m1?\n\n\ntemp.a <- tidy(m1, conf.int = TRUE) %>%\n    filter(term == \"age\")\n\nkable(temp.a, digits = 3)\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\nconf.low\nconf.high\n\n\n\n\nage\n1.202\n0.367\n3.27\n0.001\n0.475\n1.928\n\n\n\n\n\nThe coefficient of the age effect on totchol is 1.202. Suppose we have two subjects, Doris and Emily, who are the same sex and have the same level of education, but Doris is one year older than Emily. Our model predicts that Doris’ total cholesterol will be 1.202 mg/dl higher than Emily’s.\nThe 95% confidence interval for this estimated age coefficient is (0.475, 1.928), so holding everything else constant, it seems that older age is associated with higher totchol in this model.\n\nWhat is the effect of sex on totchol in Model m1?\n\n\ntemp.s <- tidy(m1, conf.int = TRUE) %>%\n    filter(term == \"sexM\")\n\nkable(temp.s, digits = 3)\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\nconf.low\nconf.high\n\n\n\n\nsexM\n3.612\n6.441\n0.561\n0.576\n-9.119\n16.343\n\n\n\n\n\nThe model is parametrized to incorporate the sex information with an indicator (and factor) variable called sexM which is interpreted as taking the value 1 when sex = M, and 0 otherwise. The coefficient of the sexM effect on totchol is 3.612. Suppose we have two subjects, David and Emily, who are the same age, have the same level of education, but David is male and Emily is female. Our model predicts that David’s total cholesterol will be 3.612 mg/dl higher than Emily’s.\nThe 95% confidence interval for this estimated sexM coefficient is (-9.119, 16.343), which suggests that the effect of sex on totchol could be quite small, and that the data are consistent with a wide range of estimates for the sexM effect, some of which are negative.\n\nWhat is the effect of educ on totchol in Model m1?\n\nThe educ variable splits the subjects into four categories. In this model the “1_Low” category is used as the baseline, and we have estimates for “2_Middle” (as compared to “1_Low”), for “3_High” (as compared to “1_Low”) and for “4_VHigh” (as compared to “1_Low”.)\n\ntemp.ed <- tidy(m1, conf.int = TRUE) %>% \n          filter(term %in% c(\"factor(educ)2_Middle\", \n                             \"factor(educ)3_High\", \n                             \"factor(educ)4_VHigh\"))\n\nkable(temp.ed, \n      digits = 3)\n\n\n\n\n\n\n\n\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\nconf.low\nconf.high\n\n\n\n\nfactor(educ)2_Middle\n11.044\n7.702\n1.434\n0.154\n-4.180\n26.268\n\n\nfactor(educ)3_High\n-2.459\n9.390\n-0.262\n0.794\n-21.019\n16.101\n\n\nfactor(educ)4_VHigh\n10.927\n9.780\n1.117\n0.266\n-8.405\n30.258\n\n\n\n\n\nThe coefficient of the educ effect comparing the “2_Middle” group to the baseline “1_Low” group on totchol is 11.044.\nNote that none of the educ levels show especially large differences from the baseline group, and each of their 95% confidence intervals contains zero.\n\nSuppose we have two subjects, Lola and Mina, who are the same age, and the same sex, but Lola is in the “1_Low” education group and Mina is in the “2_Middle” education group.\nOur model predicts that Mina’s total cholesterol will be 11.044 mg/dl higher than Lola’s.\n\nThe coefficient of the educ effect comparing the “3_High” group to the baseline “1_Low” group on totchol is -2.459.\n\nSuppose we have two subjects, Lola and Heidi, who are the same age, and the same sex, but Lola is in the “1_Low” education group and Heidi is in the “3_High” education group.\nOur model predicts that Heidi’s total cholesterol will be 2.459 mg/dl lower than Lola’s.\n\nFinally, the coefficient of the educ effect comparing the “4_VHigh” group to the baseline “1_Low” group on totchol is 10.927.\n\nSuppose we have two subjects, Lola and Vera, who are the same age, and the same sex, but Lola is in the “1_Low” education group and Vera is in the “4_VHigh” education group.\nOur model predicts that Vera’s total cholesterol will be 10.927 mg/dl higher than Lola’s."
  },
  {
    "objectID": "effectsize.html#making-a-prediction-and-building-a-prediction-interval-with-an-lm-fit",
    "href": "effectsize.html#making-a-prediction-and-building-a-prediction-interval-with-an-lm-fit",
    "title": "22  Estimating and Interpreting Effect Sizes",
    "section": "22.4 Making a prediction and building a prediction interval with an lm fit",
    "text": "22.4 Making a prediction and building a prediction interval with an lm fit\nSuppose we want to use m1 to make a prediction for Lola and Vera, who we’ll now assume are each Female and 30 years of age, and we want to accompany this with a 90% prediction interval for each subject. Here’s one way to do that.\n\nnew1 <- tibble(\n    name = c(\"Lola\", \"Vera\"),\n    age = c(30, 30),\n    sex = c(\"F\", \"F\"),\n    educ = c(\"1_Low\", \"4_VHigh\")\n)\n\nnew1\n\n# A tibble: 2 × 4\n  name    age sex   educ   \n  <chr> <dbl> <chr> <chr>  \n1 Lola     30 F     1_Low  \n2 Vera     30 F     4_VHigh\n\nres1 <- predict(m1, newdata = new1, \n        interval = \"prediction\", level = 0.9)\n\nres1\n\n       fit     lwr      upr\n1 207.2456 142.322 272.1691\n2 218.1725 152.387 283.9580\n\nnew1_aug <- bind_cols(new1, fit = res1[,\"fit\"],\n                      pi90.lo = res1[,\"lwr\"],\n                      pi90.hi = res1[,\"upr\"])\n\nnew1_aug\n\n# A tibble: 2 × 7\n  name    age sex   educ      fit pi90.lo pi90.hi\n  <chr> <dbl> <chr> <chr>   <dbl>   <dbl>   <dbl>\n1 Lola     30 F     1_Low    207.    142.    272.\n2 Vera     30 F     4_VHigh  218.    152.    284."
  },
  {
    "objectID": "effectsize.html#what-if-we-include-a-spline-or-an-interaction",
    "href": "effectsize.html#what-if-we-include-a-spline-or-an-interaction",
    "title": "22  Estimating and Interpreting Effect Sizes",
    "section": "22.5 What if we include a Spline or an Interaction?",
    "text": "22.5 What if we include a Spline or an Interaction?\nSuppose we fit a new model to predict totchol using a five-knot spline in age and the interaction of sex and educational attainment. How does that change our interpretation of the effect sizes?\nNone of these coefficients show particularly large effects, and zero is contained in each of the 95% confidence intervals provided in the table summarizing model m2.\n\nd <- datadist(smalldat); options(datadist = \"d\")\n\nm2 <- ols(totchol ~ rcs(age, 5) + sex*catg(educ),\n         data = smalldat, x = TRUE, y = TRUE)\n\nkable(summary(m2), digits = 2)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLow\nHigh\nDiff.\nEffect\nS.E.\nLower 0.95\nUpper 0.95\nType\n\n\n\n\nage\n42\n57\n15\n9.99\n9.40\n-8.59\n28.57\n1\n\n\nsex - M:F\n1\n2\nNA\n9.11\n9.87\n-10.41\n28.64\n1\n\n\neduc - 2_Middle:1_Low\n1\n2\nNA\n11.36\n9.81\n-8.04\n30.76\n1\n\n\neduc - 3_High:1_Low\n1\n3\nNA\n7.80\n12.41\n-16.73\n32.34\n1\n\n\neduc - 4_VHigh:1_Low\n1\n4\nNA\n16.17\n14.63\n-12.75\n45.10\n1\n\n\n\n\n\nThe kable approach I used in these notes hides the adjusted values specified at the bottom of the summary table for this ols model, but they are Adjusted to: sex=F educ=1_Low.\nNow, how do we interpret these model m2 results?\n\nplot(summary(m2))\n\n\n\n\n\nThe age interpretation is that if we have two subjects, Al and Bob, who are the same sex and have the same education level, but Al is age 42 and Bob is age 57, then model m2 projects that Bob’s totchol will be 9.993 mg/dl higher than will Al’s.\nBecause of the interaction between sex and educ in our model m2, we must select an educ level in order to cleanly interpret the effect of sex on totchol. The sex - M:F interpretation compares M(ale) to F(emale) sex while requiring3 that educ = 1_Low. The result is that if we have two subjects, Carl and Diane, who are the same age and each is in the low education group, but Carl is Male and Diane is Female, then model m2 predicts that Carl’s totchol will be 9.115 mg/dl higher than will Diane’s.\nBecause of the interaction between sex and educ in our model m2, we must select a sex in order to cleanly interpret the effect of educ on totchol. The educ - 2_Middle:1_Low term, for instance, compares “2_Middle” education to “1_Low” education while requiring that sex is Female4. The result is that if we have two subjects, Lola and Mina, who are the same age and each is Female, but Lola is in the “1_Low” education group and Mina is in the “2_Middle” education group, then model m2 predicts that Mina’s totchol will be 11.363 mg/dl higher than will Lola’s.\n\nHere is a nomogram of model m2.\n\nplot(nomogram(m2))"
  },
  {
    "objectID": "effectsize.html#making-a-prediction-and-building-a-prediction-interval-with-an-ols-fit",
    "href": "effectsize.html#making-a-prediction-and-building-a-prediction-interval-with-an-ols-fit",
    "title": "22  Estimating and Interpreting Effect Sizes",
    "section": "22.6 Making a prediction and building a prediction interval with an ols fit",
    "text": "22.6 Making a prediction and building a prediction interval with an ols fit\nSuppose we want to use m2 to make a prediction for Lola and Vera, who we’ll again assume are each Female and 30 years of age, and we want to accompany this with a 90% prediction interval for each subject. Here’s one way to do that.\n\nres2_lola <- Predict(m2, \n                     age = 30, sex = \"F\", educ = \"1_Low\",\n                     conf.int = 0.90,\n                     conf.type = \"individual\")\nres2_lola\n\n  age sex  educ     yhat    lower    upper\n1  30   F 1_Low 176.7746 96.77241 256.7768\n\nResponse variable (y): totchol \n\nLimits are 0.9 confidence limits\n\nres2_vera <- Predict(m2, \n                     age = 30, sex = \"F\", educ = \"4_VHigh\",\n                     conf.int = 0.90,\n                     conf.type = \"individual\")\n\nres2_vera\n\n  age sex    educ     yhat    lower    upper\n1  30   F 4_VHigh 192.9483 112.3727 273.5238\n\nResponse variable (y): totchol \n\nLimits are 0.9 confidence limits"
  },
  {
    "objectID": "effectsize.html#effect-estimates-in-a-logistic-regression-fit-with-glm",
    "href": "effectsize.html#effect-estimates-in-a-logistic-regression-fit-with-glm",
    "title": "22  Estimating and Interpreting Effect Sizes",
    "section": "22.7 Effect Estimates in A Logistic Regression fit with glm",
    "text": "22.7 Effect Estimates in A Logistic Regression fit with glm\nIn a binary logistic model, where we predict the log odds of smoking (smoker = 1), we will exponentiate so as to interpret the odds ratio estimates associated with each coefficient.\n\nm3 <- glm(smoker ~ age + sex + factor(educ), \n         data = smalldat, family = binomial)\n\nkable(tidy(m3, exponentiate = TRUE, conf.int = TRUE), \n      digits = 3)\n\n\n\n\n\n\n\n\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\nconf.low\nconf.high\n\n\n\n\n(Intercept)\n19.054\n1.152\n2.557\n0.011\n2.082\n195.209\n\n\nage\n0.943\n0.021\n-2.782\n0.005\n0.903\n0.982\n\n\nsexM\n1.795\n0.356\n1.643\n0.100\n0.897\n3.637\n\n\nfactor(educ)2_Middle\n0.690\n0.428\n-0.866\n0.386\n0.295\n1.589\n\n\nfactor(educ)3_High\n0.725\n0.519\n-0.619\n0.536\n0.258\n2.005\n\n\nfactor(educ)4_VHigh\n0.339\n0.571\n-1.895\n0.058\n0.105\n1.008\n\n\n\n\n\n\nWhat is the effect of age on the odds of being a smoker in Model m3?\n\n\ntemp.3a <- tidy(m3, exponentiate = TRUE, conf.int = TRUE) %>%\n    filter(term == \"age\")\n\nkable(temp.3a, digits = 3)\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\nconf.low\nconf.high\n\n\n\n\nage\n0.943\n0.021\n-2.782\n0.005\n0.903\n0.982\n\n\n\n\n\nThe estimated odds ratio for the age effect on smoker is 0.943. Suppose we have two subjects, Doris and Emily, who are the same sex and have the same level of education, but Doris is one year older than Emily. Our model predicts that Doris’ odds of smoking will be 0.943 times as high as Emily’s. Another way to write this would be that Doris’ odds of smoking are estimated to be 94.3% of Emily’s. Yet another way would be to state that Doris’ odds of smoking are estimated to be 5.7% smaller than Emily’s odds.\nThe 95% confidence interval for this estimated odds ratio for the age effect on being a smoker is (0.903, 0.982). This confidence interval for the odds ratio does not include one, and again we see that holding everything else constant, older age is associated with detectably lower odds of being a smoker in this model.\n\nWhat is the effect of sex on the odds of being a smoker in Model m3?\n\n\ntemp.3s <- tidy(m3, exponentiate = TRUE, conf.int = TRUE) %>%\n    filter(term == \"sexM\")\n\nkable(temp.3s, digits = 3)\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\nconf.low\nconf.high\n\n\n\n\nsexM\n1.795\n0.356\n1.643\n0.1\n0.897\n3.637\n\n\n\n\n\nThe model incorporates the sex information with an indicator (and factor) variable called sexM which is interpreted as taking the value 1 when sex = M, and 0 otherwise. The estimated odds ratio describing the sexM effect on being a smoker is 1.795. Suppose we have two subjects, David and Emily, who are the same age, have the same level of education, but David is male and Emily is female. Our model predicts that David’s odds of being a smoker are 1.795 times the odds that Emily is a smoker, or equivalently, that David’s odds are 179.5% of Emily’s odds. Another equivalent statement would be that David’s odds are 79.5% larger than Emily’s odds.\nThe 95% confidence interval for the odds ratio estimate of the effect of sexM on being a smoker is (0.897, 3.637). The effect of sex on the odds of being a smoker appears modest, and 1 is included in the confidence interval.\n\nWhat is the effect of educ on the odds of being a smoker in Model m3?\n\nAgain, the educ variable splits the subjects into four categories. In this model the “1_Low” category is used as the baseline, and we have estimates for “2_Middle” (as compared to “1_Low”), for “3_High” (as compared to “1_Low”) and for “4_VHigh” (as compared to “1_Low”.)\n\ntemp.3ed <- tidy(m3, exponentiate = TRUE, conf.int = TRUE) %>% \n          filter(term %in% c(\"factor(educ)2_Middle\", \n                             \"factor(educ)3_High\", \n                             \"factor(educ)4_VHigh\"))\n\nkable(temp.3ed, \n      digits = 3)\n\n\n\n\n\n\n\n\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\nconf.low\nconf.high\n\n\n\n\nfactor(educ)2_Middle\n0.690\n0.428\n-0.866\n0.386\n0.295\n1.589\n\n\nfactor(educ)3_High\n0.725\n0.519\n-0.619\n0.536\n0.258\n2.005\n\n\nfactor(educ)4_VHigh\n0.339\n0.571\n-1.895\n0.058\n0.105\n1.008\n\n\n\n\n\nThe estimated odds ratio describing the effect of educ being “2_Middle” instead of the baseline “1_Low” on the odds of being a smoker is 0.69, for people of the same age and sex.\nNo educ levels show meaningful differences from the baseline group, and their 95% confidence intervals all include 1, although the comparison of 4_VHigh to 1_Low only barely includes 1.\n\nSuppose we have two subjects, Lola and Mina, who are the same age, and the same sex, but Lola is in the “1_Low” education group and Mina is in the “2_Middle” education group.\nOur model predicts that Mina’s odds of being a smoker will be 0.69 times the odds of Lola being a smoker.\n\nThe estimated odds ratio comparing the educ = “3_High” group to the baseline educ = “1_Low” group on smoker is 0.725.\n\nSuppose we have two subjects, Lola and Heidi, who are the same age, and the same sex, but Lola is in the “1_Low” education group and Heidi is in the “3_High” education group.\nOur model predicts that Heidi’s odds of being a smoker will be 0.725 times the odds of Lola being a smoker.\n\nFinally, The estimated odds ratio comparing the educ = “4_VHigh” group to the baseline educ = “1_Low” group on smoker is 0.339.\n\nSuppose we have two subjects, Lola and Vera, who are the same age, and the same sex, but Lola is in the “1_Low” education group and Vera is in the “4_VHigh” education group.\nOur model predicts that Vera’s odds of being a smoker will be 0.339 times the odds of Lola being a smoker."
  },
  {
    "objectID": "effectsize.html#estimates-in-the-same-logistic-regression-fit-with-lrm",
    "href": "effectsize.html#estimates-in-the-same-logistic-regression-fit-with-lrm",
    "title": "22  Estimating and Interpreting Effect Sizes",
    "section": "22.8 Estimates in The Same Logistic Regression fit with lrm",
    "text": "22.8 Estimates in The Same Logistic Regression fit with lrm\nWhen we fit the same model as m3 using lrm, we get identical results as we get from the glm fit for the categorical predictors, but there’s a change in how the odds ratio for the quantitative predictor (age) is presented.\n\nd <- datadist(smalldat); options(datadist = \"d\")\n\nm3.lrm <- lrm(smoker ~ age + sex + educ, \n         data = smalldat, x = TRUE, y = TRUE)\n\nkable(summary(m3.lrm), digits = 3)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLow\nHigh\nDiff.\nEffect\nS.E.\nLower 0.95\nUpper 0.95\nType\n\n\n\n\nage\n42\n57\n15\n-0.885\n0.318\n-1.508\n-0.261\n1\n\n\nOdds Ratio\n42\n57\n15\n0.413\nNA\n0.221\n0.770\n2\n\n\nsex - M:F\n1\n2\nNA\n0.585\n0.356\n-0.113\n1.283\n1\n\n\nOdds Ratio\n1\n2\nNA\n1.795\nNA\n0.893\n3.607\n2\n\n\neduc - 2_Middle:1_Low\n1\n2\nNA\n-0.370\n0.428\n-1.209\n0.468\n1\n\n\nOdds Ratio\n1\n2\nNA\n0.690\nNA\n0.299\n1.596\n2\n\n\neduc - 3_High:1_Low\n1\n3\nNA\n-0.321\n0.519\n-1.338\n0.696\n1\n\n\nOdds Ratio\n1\n3\nNA\n0.725\nNA\n0.262\n2.005\n2\n\n\neduc - 4_VHigh:1_Low\n1\n4\nNA\n-1.082\n0.571\n-2.201\n0.037\n1\n\n\nOdds Ratio\n1\n4\nNA\n0.339\nNA\n0.111\n1.038\n2\n\n\n\n\n\nNote that the odds ratio effect sizes and confidence intervals are identical to what we saw in the glm fit for the sex and educ variables here, but the age result is presented differently.\n\nThe age interpretation is that if we have two subjects, Al and Bob, who are the same sex and have the same education level, but Al is age 42 and Bob is age 57, then model m3 projects that Bob’s odds of being a smoker will be 0.413 times Al’s odds of being a smoker. Bob’s odds are 41.3% as large as Al’s, equivalently.\nAfter adjustment for sex and educ, increasing age appears to be associated with decreasing odds of smoking. Note, too, that the effect of age on the odds of being a smoker has a confidence interval for the odds ratio entirely below 1.\n\n\nplot(summary(m3.lrm))"
  },
  {
    "objectID": "effectsize.html#estimates-in-a-new-logistic-regression-fit-with-lrm",
    "href": "effectsize.html#estimates-in-a-new-logistic-regression-fit-with-lrm",
    "title": "22  Estimating and Interpreting Effect Sizes",
    "section": "22.9 Estimates in A New Logistic Regression fit with lrm",
    "text": "22.9 Estimates in A New Logistic Regression fit with lrm\nNow, suppose we fit a new model to predict the log odds of being a smoker using a five-knot spline in age and the interaction of sex and educational attainment. How does that change our interpretation of the effect sizes?\n\nd <- datadist(smalldat); options(datadist = \"d\")\n\nm4 <- lrm(smoker ~ rcs(age,5) + sex * catg(educ), \n         data = smalldat, x = TRUE, y = TRUE)\n\nkable(summary(m4), digits = 3)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLow\nHigh\nDiff.\nEffect\nS.E.\nLower 0.95\nUpper 0.95\nType\n\n\n\n\nage\n42\n57\n15\n-0.817\n0.538\n-1.872\n0.237\n1\n\n\nOdds Ratio\n42\n57\n15\n0.442\nNA\n0.154\n1.268\n2\n\n\nsex - M:F\n1\n2\nNA\n1.487\n0.583\n0.345\n2.629\n1\n\n\nOdds Ratio\n1\n2\nNA\n4.422\nNA\n1.412\n13.853\n2\n\n\neduc - 2_Middle:1_Low\n1\n2\nNA\n0.668\n0.546\n-0.402\n1.739\n1\n\n\nOdds Ratio\n1\n2\nNA\n1.951\nNA\n0.669\n5.690\n2\n\n\neduc - 3_High:1_Low\n1\n3\nNA\n0.019\n0.699\n-1.351\n1.389\n1\n\n\nOdds Ratio\n1\n3\nNA\n1.019\nNA\n0.259\n4.011\n2\n\n\neduc - 4_VHigh:1_Low\n1\n4\nNA\n-1.541\n1.159\n-3.813\n0.731\n1\n\n\nOdds Ratio\n1\n4\nNA\n0.214\nNA\n0.022\n2.078\n2\n\n\n\n\n\nAgain, the kable approach I used in these notes hides the adjusted values specified at the bottom of the summary table for this lrm model (model m4), but they are Adjusted to: sex=F educ=1_Low.\nNow, how do we interpret these model m4 results?\n\nplot(summary(m4))\n\n\n\n\n\nThe age interpretation is that if we have two subjects, Al and Bob, who are the same sex and have the same education level, but Al is age 42 and Bob is age 57, then model m4 projects that Bob’s odds of being a smoker will be 0.442 times Al’s odds of being a smoker. Equivalently, Bob’s odds are 44.2% as large as Al’s.\nBecause of the interaction between sex and educ in our model m4, we must select an educ level in order to cleanly interpret the effect of sex on smoker. The sex - M:F interpretation compares M(ale) to F(emale) sex while requiring5 that educ = 1_Low. The result is that if we have two subjects, Carl and Diane, who are the same age and each is in the low education group, but Carl is Male and Diane is Female, then model m4 predicts that Carl’s odds of being a smoker will be 4.422 times the odds for Diane.\nBecause of the interaction between sex and educ in our model m4, we must select a sex in order to cleanly interpret the effect of educ on totchol. The educ - 2_Middle:1_Low term, for instance, compares “2_Middle” education to “1_Low” education while requiring that sex is Female6. The result is that if we have two subjects, Lola and Mina, who are the same age and each is Female, but Lola is in the “1_Low” education group and Mina is in the “2_Middle” education group, then model m4 predicts that Mina’s odds of being a smoker will be 1.951 times Lola’s odds, or, equivalently, that Mina will have 95.1% higher odds than Lola.\n\nIt should be easy to see that one is contained in each of the 95% confidence intervals summarizing model m4 except for the one for the main effect of sex, but we might also consider the impact of the interaction term, as described by the anova result for model m4.\n\nanova(m4)\n\n                Wald Statistics          Response: smoker \n\n Factor                                    Chi-Square d.f. P     \n age                                        8.34       4   0.0799\n  Nonlinear                                 1.78       3   0.6196\n sex  (Factor+Higher Order Factors)        12.64       4   0.0132\n  All Interactions                         10.48       3   0.0149\n educ  (Factor+Higher Order Factors)       12.43       6   0.0531\n  All Interactions                         10.48       3   0.0149\n sex * educ  (Factor+Higher Order Factors) 10.48       3   0.0149\n TOTAL NONLINEAR + INTERACTION             12.60       6   0.0498\n TOTAL                                     21.69      11   0.0269\n\n\nFinally, here is a nomogram of model m4.\n\nplot(nomogram(m4, fun = plogis))"
  },
  {
    "objectID": "binaryregression.html#r-setup-used-here",
    "href": "binaryregression.html#r-setup-used-here",
    "title": "23  Colorectal Cancer Screening and Some Special Cases",
    "section": "23.1 R Setup Used Here",
    "text": "23.1 R Setup Used Here\n\nknitr::opts_chunk$set(comment = NA)\n\nlibrary(janitor)\nlibrary(broom)\nlibrary(rms)\nlibrary(tidyverse) \n\ntheme_set(theme_bw())"
  },
  {
    "objectID": "binaryregression.html#data-load",
    "href": "binaryregression.html#data-load",
    "title": "23  Colorectal Cancer Screening and Some Special Cases",
    "section": "23.2 Data Load",
    "text": "23.2 Data Load\n\ncolscr <- read_csv(\"data/screening.csv\", show_col_types = FALSE) \ncolscr2 <- read_csv(\"data/screening2.csv\", show_col_types = FALSE)"
  },
  {
    "objectID": "binaryregression.html#logistic-regression-for-aggregated-data",
    "href": "binaryregression.html#logistic-regression-for-aggregated-data",
    "title": "23  Colorectal Cancer Screening and Some Special Cases",
    "section": "23.3 Logistic Regression for Aggregated Data",
    "text": "23.3 Logistic Regression for Aggregated Data\n\n23.3.1 Colorectal Cancer Screening Data\nThe screening.csv data (imported into the R tibble colscr are simulated. They mirror a subset of the actual results from the Better Health Partnership’s pilot study of colorectal cancer screening in primary care clinics in Northeast Ohio, but the numbers have been fuzzed slightly, and the clinics have been de-identified and moved around from system to system.\nAvailable to us are the following variables:\n\n\n\n\n\n\n\nVariable\nDescription\n\n\n\n\nlocation\nclinic code\n\n\nsubjects\nnumber of subjects reported by clinic\n\n\nscreen_rate\nproportion of subjects who were screened\n\n\nscreened\nnumber of subjects who were screened\n\n\nnotscreened\nnumber of subjects not screened\n\n\nmeanage\nmean age of clinic’s subjects, years\n\n\nfemale\n% of clinic’s subjects who are female\n\n\npct_lowins\n% of clinic’s subjects who have Medicaid or are uninsured\n\n\nsystem\nsystem code\n\n\n\n\ndescribe(colscr)\n\ncolscr \n\n 9  Variables      26  Observations\n--------------------------------------------------------------------------------\nlocation \n       n  missing distinct \n      26        0       26 \n\nlowest : A B C D E, highest: V W X Y Z\n--------------------------------------------------------------------------------\nsubjects \n       n  missing distinct     Info     Mean      Gmd      .05      .10 \n      26        0       26        1     3247     2134     1249     1475 \n     .25      .50      .75      .90      .95 \n    1915     2766     3608     6523     7068 \n\nlowest :  803 1179 1459 1491 1512, highest: 5451 6061 6985 7095 7677\n--------------------------------------------------------------------------------\nscreen_rate \n       n  missing distinct     Info     Mean      Gmd      .05      .10 \n      26        0       26        1   0.7659  0.08339   0.6682   0.6732 \n     .25      .50      .75      .90      .95 \n  0.7179   0.7579   0.8088   0.8654   0.8899 \n\nlowest : 0.6354901 0.6666667 0.6728914 0.6734521 0.6869919\nhighest: 0.8237129 0.8469110 0.8838745 0.8919112 0.9049108\n--------------------------------------------------------------------------------\nscreened \n       n  missing distinct     Info     Mean      Gmd      .05      .10 \n      26        0       26        1     2584     1895    843.5    993.0 \n     .25      .50      .75      .90      .95 \n  1395.2   2169.5   2716.0   5293.5   6107.2 \n\nlowest :  572  794  992  994 1088, highest: 4818 4848 5739 6230 6947\n--------------------------------------------------------------------------------\nnotscreened \n       n  missing distinct     Info     Mean      Gmd      .05      .10 \n      26        0       26        1    663.2    303.5    336.0    352.5 \n     .25      .50      .75      .90      .95 \n   508.8    611.0    791.0    989.0   1172.5 \n\nlowest :  231  335  339  366  371, highest:  881  927 1051 1213 1356\n--------------------------------------------------------------------------------\nmeanage \n       n  missing distinct     Info     Mean      Gmd      .05      .10 \n      26        0       23    0.999    60.58    2.186    58.23    58.35 \n     .25      .50      .75      .90      .95 \n   58.82    60.50    61.98    62.50    62.90 \n\nlowest : 58.0 58.2 58.3 58.4 58.5, highest: 62.2 62.4 62.6 63.0 65.9\n--------------------------------------------------------------------------------\nfemale \n       n  missing distinct     Info     Mean      Gmd      .05      .10 \n      26        0       23    0.999    58.72    7.118    46.93    48.45 \n     .25      .50      .75      .90      .95 \n   55.42    60.05    62.62    64.90    67.50 \n\nlowest : 46.2 46.6 47.9 49.0 54.3, highest: 63.6 64.1 65.7 68.1 70.3\n--------------------------------------------------------------------------------\npct_lowins \n       n  missing distinct     Info     Mean      Gmd      .05      .10 \n      26        0       24    0.999    24.47    22.12    0.675    1.800 \n     .25      .50      .75      .90      .95 \n   4.800   23.950   44.025   49.500   49.950 \n\nlowest :  0.3  0.4  1.5  2.1  3.0, highest: 45.4 47.1 49.5 50.1 51.3\n--------------------------------------------------------------------------------\nsystem \n       n  missing distinct \n      26        0        4 \n                                  \nValue      Sys_1 Sys_2 Sys_3 Sys_4\nFrequency      7     7     6     6\nProportion 0.269 0.269 0.231 0.231\n--------------------------------------------------------------------------------\n\n\n\n\n23.3.2 Fitting a Logistic Regression Model to Proportion Data\nHere, we have a binary outcome (was the subject screened or not?) but we have aggregated results. We can use the counts of the numbers of subjects at each clinic (in subjects) and the proportion who were screened (in screen_rate) to fit a logistic regression model, as follows:\n\nm_screen1 <-  glm(screen_rate ~ meanage + female + \n                    pct_lowins + system, family = binomial, \n                  weights = subjects, data = colscr)\n\nsummary(m_screen1)\n\n\nCall:\nglm(formula = screen_rate ~ meanage + female + pct_lowins + system, \n    family = binomial, data = colscr, weights = subjects)\n\nDeviance Residuals: \n   Min      1Q  Median      3Q     Max  \n-9.017  -3.705  -2.000   1.380  11.500  \n\nCoefficients:\n              Estimate Std. Error z value Pr(>|z|)    \n(Intercept) -1.3270393  0.5530782  -2.399   0.0164 *  \nmeanage      0.0679866  0.0089754   7.575 3.60e-14 ***\nfemale      -0.0193142  0.0015831 -12.200  < 2e-16 ***\npct_lowins  -0.0134547  0.0008585 -15.672  < 2e-16 ***\nsystemSys_2 -0.1382189  0.0246591  -5.605 2.08e-08 ***\nsystemSys_3 -0.0400170  0.0254505  -1.572   0.1159    \nsystemSys_4  0.0229273  0.0294207   0.779   0.4358    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 2825.28  on 25  degrees of freedom\nResidual deviance:  816.39  on 19  degrees of freedom\nAIC: 1037.9\n\nNumber of Fisher Scoring iterations: 4\n\n\n\n\n23.3.3 Fitting a Logistic Regression Model to Counts of Successes and Failures\n\nm_screen2 <-  glm(cbind(screened, notscreened) ~ \n                    meanage + female + pct_lowins + system, \n           family = binomial, data = colscr)\nsummary(m_screen2)\n\n\nCall:\nglm(formula = cbind(screened, notscreened) ~ meanage + female + \n    pct_lowins + system, family = binomial, data = colscr)\n\nDeviance Residuals: \n   Min      1Q  Median      3Q     Max  \n-9.017  -3.705  -2.000   1.380  11.500  \n\nCoefficients:\n              Estimate Std. Error z value Pr(>|z|)    \n(Intercept) -1.3270392  0.5530782  -2.399   0.0164 *  \nmeanage      0.0679866  0.0089754   7.575 3.60e-14 ***\nfemale      -0.0193142  0.0015831 -12.200  < 2e-16 ***\npct_lowins  -0.0134547  0.0008585 -15.672  < 2e-16 ***\nsystemSys_2 -0.1382189  0.0246591  -5.605 2.08e-08 ***\nsystemSys_3 -0.0400170  0.0254505  -1.572   0.1159    \nsystemSys_4  0.0229273  0.0294207   0.779   0.4358    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 2825.28  on 25  degrees of freedom\nResidual deviance:  816.39  on 19  degrees of freedom\nAIC: 1037.9\n\nNumber of Fisher Scoring iterations: 4\n\n\n\n\n23.3.4 How does one address this problem in rms?\nWe can use Glm. As an example to mirror m_screen1, we have the following…\n\nd <- datadist(colscr)\noptions(datadist = \"d\")\n\nmod_screen_1 <-  Glm(screen_rate ~ meanage + female + \n                         pct_lowins + system, \n                     family = binomial, weights = subjects, \n                     data = colscr, x = T, y = T)\n\nmod_screen_1\n\nGeneral Linear Model\n \n Glm(formula = screen_rate ~ meanage + female + pct_lowins + system, \n     family = binomial, data = colscr, weights = subjects, x = T, \n     y = T)\n \n                      Model Likelihood    \n                            Ratio Test    \n     Obs      26    LR chi2    2008.90    \n Residual d.f.19    d.f.             6    \n     g 0.4614539    Pr(> chi2) <0.0001    \n \n              Coef    S.E.   Wald Z Pr(>|Z|)\n Intercept    -1.3270 0.5531  -2.40 0.0164  \n meanage       0.0680 0.0090   7.57 <0.0001 \n female       -0.0193 0.0016 -12.20 <0.0001 \n pct_lowins   -0.0135 0.0009 -15.67 <0.0001 \n system=Sys_2 -0.1382 0.0247  -5.61 <0.0001 \n system=Sys_3 -0.0400 0.0255  -1.57 0.1159  \n system=Sys_4  0.0229 0.0294   0.78 0.4358"
  },
  {
    "objectID": "binaryregression.html#probit-regression",
    "href": "binaryregression.html#probit-regression",
    "title": "23  Colorectal Cancer Screening and Some Special Cases",
    "section": "23.4 Probit Regression",
    "text": "23.4 Probit Regression\n\n23.4.1 Colorectal Cancer Screening Data on Individuals\nThe data in the colscr2 data frame describe (disguised) data on the status of 172 adults who were eligible for colon cancer screening, with the following information included:\n\n\n\nVariable\nDescription\n\n\n\n\nsubject\nsubject ID code\n\n\nage\nsubject’s age (years)\n\n\nrace\nsubject’s race (White/Black/Other)\n\n\nhispanic\nsubject of Hispanic ethnicity (1 = yes / 0 = no)\n\n\ninsurance\nCommercial, Medicaid, Medicare, Uninsured\n\n\nbmi\nbody mass index at most recent visit\n\n\nsbp\nsystolic blood pressure at most recent visit\n\n\nup_to_date\nmeets colon cancer screening standards\n\n\n\nThe goal is to use the other variables (besides subject ID) to predict whether or not a subject is up to date.\n\ncolscr2 %>% describe()\n\n. \n\n 8  Variables      172  Observations\n--------------------------------------------------------------------------------\nsubject \n       n  missing distinct     Info     Mean      Gmd      .05      .10 \n     172        0      172        1    186.5    57.67    109.6    118.1 \n     .25      .50      .75      .90      .95 \n   143.8    186.5    229.2    254.9    263.4 \n\nlowest : 101 102 103 104 105, highest: 268 269 270 271 272\n--------------------------------------------------------------------------------\nage \n       n  missing distinct     Info     Mean      Gmd      .05      .10 \n     172        0       19    0.995     57.8    5.536    51.00    52.00 \n     .25      .50      .75      .90      .95 \n   54.00    57.00    61.25    65.00    67.00 \n\nlowest : 51 52 53 54 55, highest: 65 66 67 68 69\n                                                                            \nValue         51    52    53    54    55    56    57    58    59    60    61\nFrequency     10    17    14     9    15    13    18    13     4    10     6\nProportion 0.058 0.099 0.081 0.052 0.087 0.076 0.105 0.076 0.023 0.058 0.035\n                                                          \nValue         62    63    64    65    66    67    68    69\nFrequency     11     4     5     7     6     3     4     3\nProportion 0.064 0.023 0.029 0.041 0.035 0.017 0.023 0.017\n--------------------------------------------------------------------------------\nrace \n       n  missing distinct \n     172        0        3 \n                            \nValue      Black Other White\nFrequency    118     9    45\nProportion 0.686 0.052 0.262\n--------------------------------------------------------------------------------\nhispanic \n       n  missing distinct     Info      Sum     Mean      Gmd \n     172        0        2     0.18       11  0.06395   0.1204 \n\n--------------------------------------------------------------------------------\ninsurance \n       n  missing distinct \n     172        0        4 \n                                                      \nValue      Commercial   Medicaid   Medicare  Uninsured\nFrequency          32         81         46         13\nProportion      0.186      0.471      0.267      0.076\n--------------------------------------------------------------------------------\nbmi \n       n  missing distinct     Info     Mean      Gmd      .05      .10 \n     172        0      165        1    31.24    8.982    20.32    21.88 \n     .25      .50      .75      .90      .95 \n   25.48    30.05    36.03    43.06    45.68 \n\nlowest : 17.20 17.59 17.85 18.09 18.44, highest: 49.41 50.83 53.28 54.66 55.41\n--------------------------------------------------------------------------------\nsbp \n       n  missing distinct     Info     Mean      Gmd      .05      .10 \n     172        0       68    0.999    128.9    19.46    101.6    109.1 \n     .25      .50      .75      .90      .95 \n   118.0    127.0    138.0    150.9    162.0 \n\nlowest :  89  90  94  95  96, highest: 166 168 169 170 198\n--------------------------------------------------------------------------------\nup_to_date \n       n  missing distinct     Info      Sum     Mean      Gmd \n     172        0        2    0.717      104   0.6047   0.4809 \n\n--------------------------------------------------------------------------------\n\n\n\n\n23.4.2 A logistic regression model\nHere is a logistic regression model.\n\nm_scr2_logistic <- glm(up_to_date ~ age + race + hispanic + \n                    insurance + bmi + sbp, \n                family = binomial, data = colscr2)\n\nsummary(m_scr2_logistic)\n\n\nCall:\nglm(formula = up_to_date ~ age + race + hispanic + insurance + \n    bmi + sbp, family = binomial, data = colscr2)\n\nDeviance Residuals: \n    Min       1Q   Median       3Q      Max  \n-1.8604  -1.1540   0.6933   0.9564   1.6108  \n\nCoefficients:\n                     Estimate Std. Error z value Pr(>|z|)  \n(Intercept)         2.7040470  2.7418625   0.986   0.3240  \nage                 0.0204901  0.0396920   0.516   0.6057  \nraceOther          -1.9722351  1.0023237  -1.968   0.0491 *\nraceWhite          -0.3210458  0.4001744  -0.802   0.4224  \nhispanic            0.0005855  0.7953482   0.001   0.9994  \ninsuranceMedicaid  -1.0151860  0.4945169  -2.053   0.0401 *\ninsuranceMedicare  -0.5216006  0.5629935  -0.926   0.3542  \ninsuranceUninsured  0.1099966  0.7906196   0.139   0.8893  \nbmi                 0.0155894  0.0213547   0.730   0.4654  \nsbp                -0.0241777  0.0099138  -2.439   0.0147 *\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 230.85  on 171  degrees of freedom\nResidual deviance: 210.55  on 162  degrees of freedom\nAIC: 230.55\n\nNumber of Fisher Scoring iterations: 4\n\nconfint(m_scr2_logistic)\n\nWaiting for profiling to be done...\n\n\n                         2.5 %       97.5 %\n(Intercept)        -2.64274441  8.157738367\nage                -0.05703934  0.099298904\nraceOther          -4.24604744 -0.175953229\nraceWhite          -1.10800168  0.469396096\nhispanic           -1.53691431  1.688738936\ninsuranceMedicaid  -2.03640881 -0.079142467\ninsuranceMedicare  -1.66246932  0.564088260\ninsuranceUninsured -1.40110091  1.759391281\nbmi                -0.02568426  0.058563761\nsbp                -0.04436143 -0.005282686\n\n\nIn this model, there appears to be some link between sbp and screening, as well as, perhaps, some statistically significant differences between some race groups and some insurance groups. We won’t look at this much further for now, though. Instead, we’ll simply describe predictions for two subjects, Harry and Sally.\n\n\n23.4.3 Predicting status for Harry and Sally\n\nHarry is age 65, White, non-Hispanic, with Medicare insurance, a BMI of 28 and SBP of 135.\nSally is age 60, Black, Hispanic, with Medicaid insurance, a BMI of 22 and SBP of 148.\n\n\nnewdat_s2 <- tibble(subject = c(\"Harry\", \"Sally\"),\n                     age = c(65, 60),\n                     race = c(\"White\", \"Black\"),\n                     hispanic = c(0, 1),\n                     insurance = c(\"Medicare\", \"Medicaid\"),\n                     bmi = c(28, 22),\n                     sbp = c(135, 148))\n\npredict(m_scr2_logistic, newdata = newdat_s2, \n        type = \"response\")\n\n        1         2 \n0.5904364 0.4215335 \n\n\nThe prediction for Harry is 0.59, and for Sally, 0.42, by this logistic regression model.\n\n\n23.4.4 A probit regression model\nNow, consider a probit regression, fit by changing the default link for the binomial family as follows:\n\nm_scr2_probit <- glm(up_to_date ~ age + race + hispanic + \n                    insurance + bmi + sbp, \n                family = binomial(link = \"probit\"), \n                data = colscr2)\n\nsummary(m_scr2_probit)\n\n\nCall:\nglm(formula = up_to_date ~ age + race + hispanic + insurance + \n    bmi + sbp, family = binomial(link = \"probit\"), data = colscr2)\n\nDeviance Residuals: \n    Min       1Q   Median       3Q      Max  \n-1.8608  -1.1561   0.6933   0.9607   1.6073  \n\nCoefficients:\n                    Estimate Std. Error z value Pr(>|z|)  \n(Intercept)         1.584604   1.658489   0.955   0.3394  \nage                 0.013461   0.024107   0.558   0.5766  \nraceOther          -1.238445   0.587093  -2.109   0.0349 *\nraceWhite          -0.199260   0.243505  -0.818   0.4132  \nhispanic            0.029483   0.484819   0.061   0.9515  \ninsuranceMedicaid  -0.619277   0.293205  -2.112   0.0347 *\ninsuranceMedicare  -0.322881   0.333549  -0.968   0.3330  \ninsuranceUninsured  0.052776   0.463798   0.114   0.9094  \nbmi                 0.009652   0.012887   0.749   0.4539  \nsbp                -0.014696   0.005944  -2.472   0.0134 *\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 230.85  on 171  degrees of freedom\nResidual deviance: 210.49  on 162  degrees of freedom\nAIC: 230.49\n\nNumber of Fisher Scoring iterations: 4\n\nconfint(m_scr2_probit)\n\nWaiting for profiling to be done...\n\n\n                         2.5 %       97.5 %\n(Intercept)        -1.70739455  4.894713408\nage                -0.03400934  0.061437784\nraceOther          -2.53819772 -0.119812915\nraceWhite          -0.67910311  0.282566349\nhispanic           -0.92476109  1.011089937\ninsuranceMedicaid  -1.21212866 -0.049749873\ninsuranceMedicare  -0.99315267  0.329851215\ninsuranceUninsured -0.83335121  0.966504724\nbmi                -0.01577917  0.035632196\nsbp                -0.02659318 -0.003148488\n\n\n\n\n23.4.5 Interpreting the Probit Model’s Coefficients\nIt is possible to use any number of link functions to ensure that predicted values in a generalized linear model fall between 0 and 1. The probit regression model, for instance, uses the inverse of the cumulative distribution function of the Normal model as its link function. Let’s look more closely at the coefficients of the probit model we just fit.\n\nm_scr2_probit$coef\n\n       (Intercept)                age          raceOther          raceWhite \n       1.584603569        0.013461338       -1.238445198       -0.199260184 \n          hispanic  insuranceMedicaid  insuranceMedicare insuranceUninsured \n       0.029483051       -0.619276718       -0.322880519        0.052775722 \n               bmi                sbp \n       0.009652339       -0.014695526 \n\n\nThe probit regression coefficients give the change in the z-score of the outcome of interest (here, up_to_date) for a one-unit change in the target predictor, holding all other predictors constant.\n\nSo, for a one-year increase in age, holding all other predictors constant, the z-score for up_to_date increases by 0.013\nAnd for a Medicaid subject as compared to a Commercial subject of the same age, race, ethnicity, bmi and sbp, the z-score for the Medicaid subject is predicted to be -0.619 lower, according to this model.\n\n\n\n23.4.6 What about Harry and Sally?\nDo the predictions for Harry and Sally change much with this probit model, as compared to the logistic regression?\n\npredict(m_scr2_probit, newdata = newdat_s2, type = \"response\")\n\n        1         2 \n0.5885511 0.4364027"
  },
  {
    "objectID": "count1.html#r-setup-used-here",
    "href": "count1.html#r-setup-used-here",
    "title": "24  Modeling a Count Outcome",
    "section": "24.1 R Setup Used Here",
    "text": "24.1 R Setup Used Here\n\nknitr::opts_chunk$set(comment = NA)\n\nlibrary(broom)\nlibrary(boot)\nlibrary(countreg)\nlibrary(GGally)\nlibrary(lmtest)\nlibrary(rms)\nlibrary(sandwich)\nlibrary(tidyverse) \n\ntheme_set(theme_bw())"
  },
  {
    "objectID": "count1.html#data-load",
    "href": "count1.html#data-load",
    "title": "24  Modeling a Count Outcome",
    "section": "24.2 Data Load",
    "text": "24.2 Data Load\n\nsmart_oh <- readRDS(\"data/smart_ohio.Rds\")"
  },
  {
    "objectID": "count1.html#creating-a-useful-analytic-subset-ohioa",
    "href": "count1.html#creating-a-useful-analytic-subset-ohioa",
    "title": "24  Modeling a Count Outcome",
    "section": "24.3 Creating A Useful Analytic Subset, ohioA",
    "text": "24.3 Creating A Useful Analytic Subset, ohioA\nFor this work, I’ll include the subset of all observations in smart_oh with complete data on these 14 variables.\n\n\n\n\n\n\n\nVariable\nDescription\n\n\n\n\nSEQNO\nSubject identification code\n\n\nmmsa_name\nName of statistical area\n\n\ngenhealth\nFive categories (E, VG, G, F, P) on general health\n\n\nphyshealth\nNow thinking about your physical health, which includes physical illness and injury, for how many days during the past 30 days was your physical health not good?\n\n\nmenthlth\nNow thinking about your mental health, which includes stress, depression, and problems with emotions, for how many days during the past 30 days was your mental health not good?\n\n\nhealthplan\n1 if the subject has any kind of health care coverage, 0 otherwise\n\n\ncostprob\n1 indicates Yes to “Was there a time in the past 12 months when you needed to see a doctor but could not because of cost?”\n\n\nagegroup\n13 age groups from 18 through 80+\n\n\nfemale\n1 if subject is female\n\n\nincomegroup\n8 income groups from < 10,000 to 75,000 or more\n\n\nbmi\nbody-mass index\n\n\nsmoke100\n1 if Yes to “Have you smoked at least 100 cigarettes in your entire life?”\n\n\nalcdays\n# of days out of the past 30 on which the subject had at least one alcoholic drink\n\n\n\n\nohioA <- smart_oh |>\n    select(SEQNO, mmsa_name, genhealth, physhealth, \n           menthealth, healthplan, costprob, \n           agegroup, female, incomegroup, bmi, smoke100, \n           alcdays) |>\n    drop_na()\n\n\n24.3.1 Is age group associated with physhealth?\n\nggplot(ohioA, aes(x = agegroup, y = physhealth)) +\n    geom_violin(col = \"blue\")\n\n\n\n\nIt’s hard to see much of anything here. The main conclusion seems to be that 0 is by far the most common response.\nHere’s a table by age group of:\n\nthe number of respondents in that age group,\nthe group’s mean physhealth response (remember that these are the number of poor physical health days in the last 30),\ntheir median physhealth response (which turns out to be 0 in each group), and\nthe percentage of group members who responded 0.\n\n\nohioA |> group_by(agegroup) |>\n    summarize(n = n(), mean = round(mean(physhealth),2), \n              median = median(physhealth),\n              percent_0s = round(100*sum(physhealth == 0)/n,1))\n\n# A tibble: 13 × 5\n   agegroup     n  mean median percent_0s\n   <fct>    <int> <dbl>  <dbl>      <dbl>\n 1 18-24      297  2.31      0       59.6\n 2 25-29      259  2.53      0       66  \n 3 30-34      296  2.14      0       69.3\n 4 35-39      366  3.67      0       63.9\n 5 40-44      347  4.18      0       62.8\n 6 45-49      409  4.46      0       63.3\n 7 50-54      472  4.76      0       60.4\n 8 55-59      608  6.71      0       57.1\n 9 60-64      648  5.9       0       57.6\n10 65-69      604  6.09      0       56  \n11 70-74      490  4.89      0       61.6\n12 75-79      338  6.38      0       54.1\n13 80-96      374  6.1       0       56.4\n\n\nWe can see a real change between the 45-49 age group and the 50-54 age group. The mean difference is clear from the table above, and the plot below (of the percentage with a zero response) in each age group identifies the same story.\n\nohioA |> group_by(agegroup) |>\n    summarize(n = n(), \n              percent_0s = round(100*sum(physhealth == 0)/n,1)) |>\n    ggplot(aes(y = agegroup, x = percent_0s)) +\n    geom_label(aes(label = percent_0s)) +\n    labs(x = \"% with no Bad Physical Health Days in last 30\",\n         y = \"Age Group\")\n\n\n\n\nIt looks like we have a fairly consistent result in the younger age range (18-49) or the older range (50+). On the theory that most of the people reading this document are in that younger range, we’ll focus on those respondents in what follows."
  },
  {
    "objectID": "count1.html#exploratory-data-analysis-in-the-18-49-group",
    "href": "count1.html#exploratory-data-analysis-in-the-18-49-group",
    "title": "24  Modeling a Count Outcome",
    "section": "24.4 Exploratory Data Analysis (in the 18-49 group)",
    "text": "24.4 Exploratory Data Analysis (in the 18-49 group)\nWe want to predict the 0-30 physhealth count variable for the 18-49 year old respondents.\nTo start, we’ll use two predictors:\n\nthe respondent’s body mass index, and\nwhether the respondent has smoked 100 cigarettes in their lifetime.\n\nWe anticipate that each of these variables will have positive associations with the physhealth score. That is, heavier people, and those who have used tobacco will be less healthy, and thus have higher numbers of poor physical health days.\n\n24.4.1 Build a subset of those ages 18-49\nFirst, we’ll identify the subset of respondents who are between 18 and 49 years of age.\n\nohioA_young.raw <- ohioA |>\n    filter(agegroup %in% c(\"18-24\", \"25-29\", \"30-34\", \n                           \"35-39\", \"40-44\", \"45-49\")) |>\n    droplevels()\n\nohioA_young.raw |> \n    select(physhealth, bmi, smoke100, agegroup) |>\n    summary()\n\n   physhealth          bmi           smoke100       agegroup  \n Min.   : 0.000   Min.   :14.00   Min.   :0.0000   18-24:297  \n 1st Qu.: 0.000   1st Qu.:23.74   1st Qu.:0.0000   25-29:259  \n Median : 0.000   Median :27.32   Median :0.0000   30-34:296  \n Mean   : 3.337   Mean   :28.79   Mean   :0.4189   35-39:366  \n 3rd Qu.: 2.000   3rd Qu.:32.43   3rd Qu.:1.0000   40-44:347  \n Max.   :30.000   Max.   :75.52   Max.   :1.0000   45-49:409  \n\n\n\n\n24.4.2 Centering bmi\nI’m going to center the bmi variable to help me interpret the final models later.\n\nohioA_young <- ohioA_young.raw |>\n    mutate(bmi_c = bmi - mean(bmi)) \n\nNow, let’s look more closely at the distribution of these variables, starting with our outcome.\n\n\n24.4.3 Distribution of the Outcome\nWhat’s the distribution of physhealth?\n\nggplot(ohioA_young.raw, aes(x = physhealth)) +\n    geom_histogram(binwidth = 1, fill = \"red\", col = \"white\")\n\n\n\n\n\nohioA_young.raw |>\n    count(physhealth == 0, physhealth == 30)\n\n# A tibble: 3 × 3\n  `physhealth == 0` `physhealth == 30`     n\n  <lgl>             <lgl>              <int>\n1 FALSE             FALSE                612\n2 FALSE             TRUE                  98\n3 TRUE              FALSE               1264\n\n\nMost of our respondents said zero, the minimum allowable value, although there is also a much smaller bump at 30, the maximum value we will allow.\nDealing with this distribution is going to be a bit of a challenge. We will develop a series of potential modeling approaches for this sort of data, but before we do that, let’s look at the distribution of our other two variables, and the pairwise associations, in a scatterplot matrix.\n\n\n24.4.4 Scatterplot Matrix\nNow, here’s the scatterplot matrix for those 1974 subjects, using the centered bmi data captured in the bmi_c variable.\n\ntemp <- ohioA_young |> select(bmi_c, smoke100, physhealth)\n\nggpairs(temp)\n\n\n\n\nSo bmi_c and smoke100 each have modest positive correlations with physhealth and only a very small correlation with each other. Here are some summary statistics for this final data.\n\n\n24.4.5 Summary of the final subset of data\nRemember that since the mean of bmi is 28.8, the bmi_c values are just bmi - 28.8 for each subject.\n\nohioA_young |> \n    select(bmi, bmi_c, smoke100, physhealth) |>\n    summary()\n\n      bmi            bmi_c            smoke100        physhealth    \n Min.   :14.00   Min.   :-14.791   Min.   :0.0000   Min.   : 0.000  \n 1st Qu.:23.74   1st Qu.: -5.051   1st Qu.:0.0000   1st Qu.: 0.000  \n Median :27.32   Median : -1.471   Median :0.0000   Median : 0.000  \n Mean   :28.79   Mean   :  0.000   Mean   :0.4189   Mean   : 3.337  \n 3rd Qu.:32.43   3rd Qu.:  3.636   3rd Qu.:1.0000   3rd Qu.: 2.000  \n Max.   :75.52   Max.   : 46.729   Max.   :1.0000   Max.   :30.000"
  },
  {
    "objectID": "count1.html#modeling-strategies-explored-here",
    "href": "count1.html#modeling-strategies-explored-here",
    "title": "24  Modeling a Count Outcome",
    "section": "24.5 Modeling Strategies Explored Here",
    "text": "24.5 Modeling Strategies Explored Here\nWe are going to predict physhealth using bmi_c and smoke100.\n\nRemember that physhealth is a count of the number of poor physical health days in the past 30.\nAs a result, physhealth is restricted to taking values between 0 and 30.\n\nWe will demonstrate the use of each of the following regression models, some of which are better choices than others.\n\nOrdinary Least Squares (OLS) predicting physhealth\nOLS predicting the logarithm of (physhealth + 1)\nPoisson regression, which is appropriate for predicting counts\nPoisson regression, adjusted to account for overdispersion\n\nand, in Chapter 25:\n\nNegative binomial regression, also used for counts and which adjusts for overdispersion\n\nand, in Chapter 26:\n\nZero-inflated models, in both the Poisson and Negative Binomial varieties, which allow us to fit counts that have lots of zero values\nA “hurdle” model, which allows us to separately fit a model to predict the incidence of “0” and then a separate model to predict the value of physhealth when we know it is not zero\nTobit regression, where a lower (and upper) bound may be set, but the underlying model describes a latent variable which can extend beyond these boundaries\n\n\n24.5.1 What Will We Demonstrate?\nWith each approach, we will fit the model and specify procedures for doing so in R. Then we will:\n\nSpecify the fitted model equation\nInterpret the model’s coefficient estimates and 95% confidence intervals around those estimates.\nPerform a test of whether each variable adds value to the model, when the other one is already included.\nStore the fitted values and appropriate residuals for each model.\nSummarize the model’s apparent \\(R^2\\) value, the proportion of variation explained, and the model log likelihood.\nPerform checks of model assumptions as appropriate.\nDescribe how predictions would be made for two new subjects.\n\nHarry has a BMI that is 10 kg/m2 higher than the average across all respondents and has smoked more than 100 cigarettes in his life.\nSally has a BMI that is 5 kg/m2 less than the average across all respondents and has not smoked more than 100 cigarettes in her life.\n\n\nIn addition, for some of the new models, we provide a little of the mathematical background, and point to other resources you can use to learn more about the model.\n\n\n24.5.2 Extra Data File for Harry and Sally\nTo make our lives a little easier, I’ll create a little tibble containing the necessary data for Harry and Sally.\n\nhs_data <- tibble(subj = c(\"Harry\", \"Sally\"), \n                  bmi_c = c(10, -5), smoke100 = c(1, 0))\nhs_data\n\n# A tibble: 2 × 3\n  subj  bmi_c smoke100\n  <chr> <dbl>    <dbl>\n1 Harry    10        1\n2 Sally    -5        0"
  },
  {
    "objectID": "count1.html#the-ols-approach",
    "href": "count1.html#the-ols-approach",
    "title": "24  Modeling a Count Outcome",
    "section": "24.6 The OLS Approach",
    "text": "24.6 The OLS Approach\n\nmod_ols1 <- lm(physhealth ~ bmi_c + smoke100, \n               data = ohioA_young)\n\nsummary(mod_ols1)\n\n\nCall:\nlm(formula = physhealth ~ bmi_c + smoke100, data = ohioA_young)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-11.1472  -3.6639  -2.2426  -0.7807  28.8777 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  2.57046    0.21648  11.874  < 2e-16 ***\nbmi_c        0.14437    0.02305   6.263 4.61e-10 ***\nsmoke100     1.83061    0.33469   5.470 5.09e-08 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 7.328 on 1971 degrees of freedom\nMultiple R-squared:  0.0356,    Adjusted R-squared:  0.03462 \nF-statistic: 36.37 on 2 and 1971 DF,  p-value: 3.072e-16\n\nconfint(mod_ols1)\n\n                2.5 %   97.5 %\n(Intercept) 2.1459143 2.995005\nbmi_c       0.0991636 0.189573\nsmoke100    1.1742213 2.486995\n\n\n\n24.6.1 Interpreting the Coefficients\n\nThe intercept, 2.57, is the predicted physhealth (in days) for a subject with average BMI who has not smoked 100 cigarettes or more.\nThe bmi_c coefficient, 0.144, indicates that for each additional kg/m2 of BMI, while holding smoke100 constant, the predicted physhealth value increases by 0.144 day.\nThe smoke100 coefficient, 1.83, indicates that a subject who has smoked 100 cigarettes or more has a predicted physhealth value 1.83 days larger than another subject with the same bmi but who has not smoked 100 cigarettes.\n\n\n\n24.6.2 Store fitted values and residuals\nWe can use broom to do this. Here, for instance, is a table of the first six predictions and residuals.\n\nsm_ols_1 <- augment(mod_ols1, ohioA_young)\n\nsm_ols_1 |> select(physhealth, .fitted, .resid) |> head()\n\n# A tibble: 6 × 3\n  physhealth .fitted .resid\n       <dbl>   <dbl>  <dbl>\n1          0    2.13  -2.13\n2          0    2.25  -2.25\n3          0    3.14  -3.14\n4         30    5.72  24.3 \n5          0    3.20  -3.20\n6          0    3.83  -3.83\n\n\nIt turns out that 0 of the 1974 predictions that we make are below 0, and the largest prediction made by this model is 11.15 days.\n\n\n24.6.3 Specify the \\(R^2\\) and log(likelihood) values\nThe glance function in the broom package gives us the raw and adjusted \\(R^2\\) values, and the model log(likelihood), among other summaries.\n\nglance(mod_ols1) |> round(3)\n\n# A tibble: 1 × 12\n  r.squared adj.r.squ…¹ sigma stati…² p.value    df logLik    AIC    BIC devia…³\n      <dbl>       <dbl> <dbl>   <dbl>   <dbl> <dbl>  <dbl>  <dbl>  <dbl>   <dbl>\n1     0.036       0.035  7.33    36.4       0     2 -6731. 13470. 13492. 105831.\n# … with 2 more variables: df.residual <dbl>, nobs <dbl>, and abbreviated\n#   variable names ¹​adj.r.squared, ²​statistic, ³​deviance\n\n\nHere, we have\n\n\n\nModel\n\\(R^2\\)\nlog(likelihood)\n\n\n\n\nOLS\n0.036\n-6730.98\n\n\n\n\n\n24.6.4 Check model assumptions\nHere is a plot of the residuals vs. the fitted values for this OLS model.\n\nggplot(sm_ols_1, aes(x = .fitted, y = .resid)) +\n    geom_point() +\n    labs(title = \"Residuals vs. Fitted Values for OLS model\")\n\n\n\n\nAs usual, we can check OLS assumptions (linearity, homoscedasticity and normality) with R’s complete set of residual plots.\n\npar(mfrow = c(2,2))\nplot(mod_ols1)\n\n\n\npar(mfrow = c(1,1))\n\nWe see the problem with our residuals. They don’t follow a Normal distribution.\n\n\n24.6.5 Predictions for Harry and Sally\n\npredict(mod_ols1, newdata = hs_data,\n        interval = \"prediction\")\n\n       fit        lwr      upr\n1 5.844750  -8.541164 20.23067\n2 1.848618 -12.529923 16.22716\n\n\nThe prediction for Harry is 5.8 days, and for Sally is 1.8 days. The prediction intervals for each include some values below 0, even though 0 is the smallest possible value.\n\n\n24.6.6 Notes\n\nThis model could have been estimated using the ols function in the rms package, as well.\n\n\ndd <- datadist(ohioA_young)\noptions(datadist = \"dd\")\n\n(mod_ols1a <- ols(physhealth ~ bmi_c + smoke100,\n                 data = ohioA_young, x = TRUE, y = TRUE))\n\nLinear Regression Model\n \n ols(formula = physhealth ~ bmi_c + smoke100, data = ohioA_young, \n     x = TRUE, y = TRUE)\n \n                 Model Likelihood    Discrimination    \n                       Ratio Test           Indexes    \n Obs    1974    LR chi2     71.55    R2       0.036    \n sigma7.3276    d.f.            2    R2 adj   0.035    \n d.f.   1971    Pr(> chi2) 0.0000    g        1.570    \n \n Residuals\n \n      Min       1Q   Median       3Q      Max \n -11.1472  -3.6639  -2.2426  -0.7807  28.8777 \n \n \n           Coef   S.E.   t     Pr(>|t|)\n Intercept 2.5705 0.2165 11.87 <0.0001 \n bmi_c     0.1444 0.0230  6.26 <0.0001 \n smoke100  1.8306 0.3347  5.47 <0.0001"
  },
  {
    "objectID": "count1.html#ols-model-on-logphyshealth-1-days",
    "href": "count1.html#ols-model-on-logphyshealth-1-days",
    "title": "24  Modeling a Count Outcome",
    "section": "24.7 OLS model on log(physhealth + 1) days",
    "text": "24.7 OLS model on log(physhealth + 1) days\nWe could try to solve the problem of fitting some predictions below 0 by log-transforming the data, so as to force values to be at least 0. Since we have undefined values when we take the log of 0, we’ll add one to each of the physhealth values before taking logs, and then transform back when we want to make predictions.\n\nmod_ols_log1 <- lm(log(physhealth + 1) ~ bmi_c + smoke100,\n                   data = ohioA_young)\n\nsummary(mod_ols_log1)\n\n\nCall:\nlm(formula = log(physhealth + 1) ~ bmi_c + smoke100, data = ohioA_young)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-1.7079 -0.7058 -0.5051  0.5053  3.0484 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  0.57746    0.03099  18.634  < 2e-16 ***\nbmi_c        0.01912    0.00330   5.796 7.91e-09 ***\nsmoke100     0.23679    0.04791   4.942 8.38e-07 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.049 on 1971 degrees of freedom\nMultiple R-squared:  0.03003,   Adjusted R-squared:  0.02905 \nF-statistic: 30.51 on 2 and 1971 DF,  p-value: 8.897e-14\n\nconfint(mod_ols_log1)\n\n                 2.5 %     97.5 %\n(Intercept) 0.51668682 0.63823590\nbmi_c       0.01265192 0.02559421\nsmoke100    0.14282518 0.33075147\n\n\n\n24.7.1 Interpreting the Coefficients\n\nThe intercept, 0.58, is the predicted logarithm of (physhealth + 1) (in days) for a subject with average BMI who has not smoked 100 cigarettes or more.\n\nWe can exponentiate to see that the prediction for (physhealth + 1) here is exp(0.58) = 1.79 so the predicted physhealth for a subject with average BMI who has not smoked 100 cigarettes is 0.79 days.\n\nThe bmi_c coefficient, 0.019, indicates that for each additional kg/m2 of BMI, while holding smoke100 constant, the predicted logarithm of (physhealth + 1) increases by 0.019\nThe smoke100 coefficient, 0.24, indicates that a subject who has smoked 100 cigarettes or more has a predicted log of (physhealth + 1) value that is 0.24 larger than another subject with the same bmi but who has not smoked 100 cigarettes.\n\n\n\n24.7.2 Store fitted values and residuals\nWe can use broom to help us with this. Here, for instance, is a table of the first six predictions and residuals, on the scale of our transformed response, log(physhealth + 1).\n\nsm_ols_log1 <- augment(mod_ols_log1, ohioA_young)\n\nsm_ols_log1 <- sm_ols_log1 |> \n    mutate(outcome = log(physhealth + 1))\n\nsm_ols_log1 |> \n    select(physhealth, outcome, .fitted, .resid) |>\n    head()\n\n# A tibble: 6 × 4\n  physhealth outcome .fitted .resid\n       <dbl>   <dbl>   <dbl>  <dbl>\n1          0    0      0.520 -0.520\n2          0    0      0.535 -0.535\n3          0    0      0.647 -0.647\n4         30    3.43   0.995  2.44 \n5          0    0      0.656 -0.656\n6          0    0      0.739 -0.739\n\n\nNote that the outcome used in this model is log(physhealth + 1), so the .fitted and .resid values react to that outcome, and not to our original physhealth.\nAnother option would be to calculate the model-predicted physhealth, which I’ll call ph for a moment, with the formula:\n\\[\nph = e^{.fitted} - 1\n\\]\n\nsm_ols_log1 <- sm_ols_log1 |> \n    mutate(pred.physhealth = exp(.fitted) - 1,\n           res.physhealth = physhealth - pred.physhealth)\n\nsm_ols_log1 |>\n    select(physhealth, pred.physhealth, res.physhealth) |> \n    head()\n\n# A tibble: 6 × 3\n  physhealth pred.physhealth res.physhealth\n       <dbl>           <dbl>          <dbl>\n1          0           0.681         -0.681\n2          0           0.708         -0.708\n3          0           0.910         -0.910\n4         30           1.70          28.3  \n5          0           0.926         -0.926\n6          0           1.09          -1.09 \n\n\nIt turns out that 0 of the 1974 predictions that we make are below 0, and the largest prediction made by this model is 4.52 days.\n\n\n24.7.3 Specify the \\(R^2\\) and log(likelihood) values\nThe glance function in the broom package gives us the raw and adjusted \\(R^2\\) values, and the model log(likelihood), among other summaries.\n\nglance(mod_ols_log1) |> round(3)\n\n# A tibble: 1 × 12\n  r.squ…¹ adj.r…² sigma stati…³ p.value    df logLik   AIC   BIC devia…⁴ df.re…⁵\n    <dbl>   <dbl> <dbl>   <dbl>   <dbl> <dbl>  <dbl> <dbl> <dbl>   <dbl>   <dbl>\n1    0.03   0.029  1.05    30.5       0     2 -2894. 5796. 5818.   2169.    1971\n# … with 1 more variable: nobs <dbl>, and abbreviated variable names\n#   ¹​r.squared, ²​adj.r.squared, ³​statistic, ⁴​deviance, ⁵​df.residual\n\n\nHere, we have\n\n\n\nModel\nScale\n\\(R^2\\)\nlog(likelihood)\n\n\n\n\nOLS on log\nlog(physhealth + 1)\n0.03\n-2893.83\n\n\n\n\n\n24.7.4 Getting \\(R^2\\) on the scale of physhealth\nWe could find the correlation of our model-predicted physhealth values, after back-transformation, and our observed physhealth values, if we wanted to, and then square that to get a sort of \\(R^2\\) value. But this model is not linear in physhealth, of course, so it’s not completely comparable to our prior OLS model.\n\n\n24.7.5 Check model assumptions\nAs usual, we can check OLS assumptions (linearity, homoscedasticity and normality) with R’s complete set of residual plots. Of course, these residuals and fitted values are now on the log(physhealth + 1) scale.\n\npar(mfrow = c(2,2))\nplot(mod_ols_log1)\n\n\n\npar(mfrow = c(1,1))\n\n\n\n24.7.6 Predictions for Harry and Sally\n\npredict(mod_ols_log1, newdata = hs_data, \n        interval = \"prediction\", type = \"response\")\n\n       fit       lwr      upr\n1 1.005480 -1.053893 3.064854\n2 0.481846 -1.576472 2.540164\n\n\nAgain, these predictions are on the log(physhealth + 1) scale, and so we have to exponentiate them, and then subtract 1, to see them on the original physhealth scale.\n\nexp(predict(mod_ols_log1, newdata = hs_data, \n            interval = \"prediction\", type = \"response\")) - 1\n\n        fit        lwr      upr\n1 1.7332198 -0.6514221 20.43133\n2 0.6190605 -0.7932970 11.68175\n\n\nThe prediction for Harry is now 1.73 days, and for Sally is 0.62 days. The prediction intervals for each again include some values below 0, which is the smallest possible value."
  },
  {
    "objectID": "count1.html#a-poisson-regression-model",
    "href": "count1.html#a-poisson-regression-model",
    "title": "24  Modeling a Count Outcome",
    "section": "24.8 A Poisson Regression Model",
    "text": "24.8 A Poisson Regression Model\nThe physhealth data describe a count. Specifically a count of the number of days where the subject felt poorly in the last 30. Why wouldn’t we model this count with linear regression?\n\nA count can only be positive. Linear regression would estimate some subjects as having negative counts.\nA count is unlikely to follow a Normal distribution. In fact, it’s far more likely that the log of the counts will follow a Poisson distribution.\n\nSo, we’ll try that. The Poisson distribution is used to model a count outcome - that is, an outcome with possible values (0, 1, 2, …). The model takes a somewhat familiar form to the models we’ve used for linear and logistic regression1. If our outcome is y and our linear predictors X, then the model is:\n\\[\ny_i \\sim \\mbox{Poisson}(\\lambda_i)\n\\]\nThe parameter \\(\\lambda\\) must be positive, so it makes sense to fit a linear regression on the logarithm of this…\n\\[\n\\lambda_i = exp(\\beta_0 + \\beta_1 X_1 + ... \\beta_k X_k)\n\\]\nThe coefficients \\(\\beta\\) can be exponentiated and treated as multiplicative effects.\nWe’ll run a generalized linear model with a log link function, ensuring that all of the predicted values will be positive, and using a Poisson error distribution. This is called Poisson regression.\nPoisson regression may be appropriate when the dependent variable is a count of events. The events must be independent - the occurrence of one event must not make any other more or less likely. That’s hard to justify in our case, but we can take a look.\n\nmod_poiss1 <- glm(physhealth ~ bmi_c + smoke100, \n                  family = poisson(),\n                  data = ohioA_young)\n\nsummary(mod_poiss1)\n\n\nCall:\nglm(formula = physhealth ~ bmi_c + smoke100, family = poisson(), \n    data = ohioA_young)\n\nDeviance Residuals: \n   Min      1Q  Median      3Q     Max  \n-6.588  -2.604  -2.087  -0.533  10.688  \n\nCoefficients:\n            Estimate Std. Error z value Pr(>|z|)    \n(Intercept) 0.906991   0.018699   48.51   <2e-16 ***\nbmi_c       0.035051   0.001421   24.66   <2e-16 ***\nsmoke100    0.532505   0.024903   21.38   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for poisson family taken to be 1)\n\n    Null deviance: 20222  on 1973  degrees of freedom\nResidual deviance: 19151  on 1971  degrees of freedom\nAIC: 21645\n\nNumber of Fisher Scoring iterations: 6\n\nconfint(mod_poiss1)\n\nWaiting for profiling to be done...\n\n\n                 2.5 %     97.5 %\n(Intercept) 0.87011622 0.94342323\nbmi_c       0.03225125 0.03782255\nsmoke100    0.48373882 0.58136497\n\n\n\n24.8.1 The Fitted Equation\nThe model equation is\nlog(physhealth) = 0.91 + 0.035 bmi_c + 0.53 smoke100\nIt looks like both bmi and smoke_100 have confidence intervals excluding 0.\n\n\n24.8.2 Interpreting the Coefficients\nOur new model for \\(y_i\\) = counts of poor physhealth days in the last 30, follows the regression equation:\n\\[\ny_i \\sim \\mbox{Poisson}(exp(0.91 + 0.035 bmi_c + 0.53 smoke100))\n\\]\nwhere smoke100 is 1 if the subject has smoked 100 cigarettes (lifetime) and 0 otherwise, and bmi_c is just the centered body-mass index value in kg/m2. We interpret the coefficients as follows:\n\nThe constant term, 0.91, gives us the intercept of the regression - the prediction if smoke100 = 0 and bmi_c = 0. In this case, because we’ve centered BMI, it implies that exp(0.91) = 2.48 is the predicted days of poor physhealth for a non-smoker with average BMI.\nThe coefficient of bmi_c, 0.035, is the expected difference in count of poor physhealth days (on the log scale) for each additional kg/m2 of body mass index. The expected multiplicative increase is \\(e^{0.035}\\) = 1.036, corresponding to a 3.6% difference in the count.\nThe coefficient of smoke100, 0.53, tells us that the predictive difference between those who have and who have not smoked 100 cigarettes can be found by multiplying the physhealth count by exp(0.53) = 1.7, yielding a 70% increase of the physhealth count.\n\nAs with linear or logistic regression, each coefficient is interpreted as a comparison where one predictor changes by one unit, while the others remain constant.\n\n\n24.8.3 Testing the Predictors\nWe can use the Wald tests (z tests) provided with the Poisson regression output, or we can fit the model and then run an ANOVA to obtain a test based on the deviance (a simple transformation of the log likelihood ratio.)\n\nBy the Wald tests shown above, each predictor clearly adds significant predictive value to the model given the other predictor, and we note that the p values are as small as R will support.\nThe ANOVA approach for this model lets us check the impact of adding smoke100 to a model already containing bmi_c.\n\n\nanova(mod_poiss1, test = \"Chisq\")\n\nAnalysis of Deviance Table\n\nModel: poisson, link: log\n\nResponse: physhealth\n\nTerms added sequentially (first to last)\n\n         Df Deviance Resid. Df Resid. Dev  Pr(>Chi)    \nNULL                      1973      20222              \nbmi_c     1   609.46      1972      19612 < 2.2e-16 ***\nsmoke100  1   461.46      1971      19151 < 2.2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nTo obtain a p value for smoke100’s impact after bmi_c is accounted for, we compare the difference in deviance to a chi-square distribution with 1 degree of freedom. To check the effect of bmi_c, we could refit the model with bmi_c entering last, and again run an ANOVA.\nWe could also run a likelihood-ratio test for each predictor, by fitting the model with and without that predictor.\n\nmod_poiss1_without_bmi <- glm(physhealth ~ smoke100,\n                              family = poisson(),\n                              data = ohioA_young)\n\nanova(mod_poiss1, mod_poiss1_without_bmi, test = \"Chisq\")\n\nAnalysis of Deviance Table\n\nModel 1: physhealth ~ bmi_c + smoke100\nModel 2: physhealth ~ smoke100\n  Resid. Df Resid. Dev Df Deviance  Pr(>Chi)    \n1      1971      19151                          \n2      1972      19692 -1  -540.98 < 2.2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\n\n24.8.4 Correcting for Overdispersion with coeftest/coefci\nThe main assumption we’ll think about in a Poisson model is about overdispersion. We might deal with the overdispersion we see in this model by changing the nature of the tests we run within this model, using the coeftest or coefci approaches from the lmtest package, as I’ll demonstrate next, or we might refit the model using a quasi-likelihood approach, as I’ll show in the material to come.\nHere, we’ll use the coeftest and coefci approach from lmtest combined with robust sandwich estimation (via the sandwich package) to re-compute the Wald tests.\n\ncoeftest(mod_poiss1, vcov. = sandwich)\n\n\nz test of coefficients:\n\n             Estimate Std. Error z value  Pr(>|z|)    \n(Intercept) 0.9069908  0.0717221 12.6459 < 2.2e-16 ***\nbmi_c       0.0350508  0.0061178  5.7293 1.008e-08 ***\nsmoke100    0.5325053  0.1004300  5.3023 1.144e-07 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\ncoefci(mod_poiss1, vcov. = sandwich)\n\n                 2.5 %     97.5 %\n(Intercept) 0.76641801 1.04756366\nbmi_c       0.02306016 0.04704145\nsmoke100    0.33566606 0.72934460\n\n\nBoth predictors are still significant, but the standard errors are more appropriate. Later, we’ll fit this approach by changing the estimation method to a quasi-likelihood approach.\n\n\n24.8.5 Store fitted values and residuals\nWhat happens if we try using the broom package in this case? We can, if we like, get our residuals and predicted values right on the scale of our physhealth response.\n\nsm_poiss1 <- augment(mod_poiss1, ohioA_young,\n                     type.predict = \"response\")\n\nsm_poiss1 |> \n    select(physhealth, .fitted) |>\n    head()\n\n# A tibble: 6 × 2\n  physhealth .fitted\n       <dbl>   <dbl>\n1          0    2.23\n2          0    2.29\n3          0    3.10\n4         30    5.32\n5          0    3.15\n6          0    3.68\n\n\n\n\n24.8.6 Rootogram: see the fit of a count regression model\nA rootogram is a very useful way to visualize the fit of a count regression model2. The rootogram function in the countreg package makes this pretty straightforward. By default, this fits a hanging rootogram on the square root of the frequencies.\n\nrootogram(mod_poiss1, max = 30)\n\n\n\n\nThe red curved line is the theoretical Poisson fit. “Hanging” from each point on the red line is a bar, the height of which represents the difference between expected and observed counts. A bar hanging below 0 indicates underfitting. A bar hanging above 0 indicates overfitting. The counts have been transformed with a square root transformation to prevent smaller counts from getting obscured and overwhelmed by larger counts. We see a great deal of underfitting for counts of 0, and overfitting for most other counts, especially 1-6, with some underfitting again by physhealth above 14 days.\n\n\n24.8.7 Specify the \\(R^2\\) and log(likelihood) values\nWe can calculate the \\(R^2\\) as the squared correlation of the fitted values and the observed values.\n\n# The correlation of observed and fitted values\n(poiss_r <- with(sm_poiss1, cor(physhealth, .fitted)))\n\n[1] 0.1846814\n\n# R-square\npoiss_r^2\n\n[1] 0.03410723\n\n\nThe glance function in the broom package gives us model log(likelihood), among other summaries.\n\nglance(mod_poiss1) |> round(3)\n\n# A tibble: 1 × 8\n  null.deviance df.null  logLik    AIC    BIC deviance df.residual  nobs\n          <dbl>   <dbl>   <dbl>  <dbl>  <dbl>    <dbl>       <dbl> <dbl>\n1        20222.    1973 -10820. 21645. 21662.   19151.        1971  1974\n\n\nHere, we have\n\n\n\nModel\nScale\n\\(R^2\\)\nlog(likelihood)\n\n\n\n\nPoisson\nlog(physhealth)\n0.185\n-10189.33\n\n\n\n\n\n24.8.8 Check model assumptions\nThe Poisson model is a classical generalized linear model, estimated using the method of maximum likelihood. While the default plot option for a glm still shows the plots we would use to assess the assumptions of an OLS model, we don’t actually get much from that, since our Poisson model has different assumptions. It can be useful to look at a plot of residuals vs. fitted values on the original physhealth scale.\n\nggplot(sm_poiss1, aes(x = .fitted, y = .resid)) +\n    geom_point() +\n    labs(title = \"Residuals vs. Fitted `physhealth`\",\n         subtitle = \"Original Poisson Regression model\")\n\n\n\n\n\n\n24.8.9 Using glm.diag.plots from the boot package\nThe glm.diag.plots function from the boot package makes a series of diagnostic plots for generalized linear models.\n\n(Top, Left) Jackknife deviance residuals against fitted values. This is essentially identical to what you obtain with plot(mod_poiss1, which = 1). A jackknife deviance residual is also called a likelihood residual. It is the change in deviance when this observation is omitted from the data.\n(Top, Right) Normal Q-Q plot of standardized deviance residuals. (Dotted line shows expectation if those standardized residuals followed a Normal distribution, and these residuals generally should.) The result is similar to what you obtain with plot(mod_poiss1, which = 2).\n(Bottom, Left) Cook statistic vs. standardized leverage\n\nn = # of observations, p = # of parameters estimated\nHorizontal dotted line is at \\(\\frac{8}{n - 2p}\\). Points above the line have high influence on the model.\nVertical line is at \\(\\frac{2p}{n - 2p}\\). Points to the right of the line have high leverage.\n\n(Bottom, Right) Index plot of Cook’s statistic to help identify the observations with high influence. This is essentially the same plot as plot(mod_poiss1, which = 4)\n\n\nglm.diag.plots(mod_poiss1)\n\n\n\n\nWhen working with these plots, it is possible to use the iden command to perform some interactive identification of points in your R terminal. But that doesn’t play out effectively in an HTML summary document like this, so we’ll leave that out.\n\n\n24.8.10 Predictions for Harry and Sally\nThe predictions from a glm fit like this don’t include prediction intervals. But we can get predictions on the scale of our original response variable, physhealth, like this.\n\npredict(mod_poiss1, newdata = hs_data, se.fit = TRUE,\n        type = \"response\")\n\n$fit\n       1        2 \n5.989478 2.078688 \n\n$se.fit\n         1          2 \n0.11544326 0.04314273 \n\n$residual.scale\n[1] 1\n\n\nBy using response as the type, these predictions fall on the original physhealth scale. The prediction for Harry is now 5.99 days, and for Sally is 2.08 days."
  },
  {
    "objectID": "count1.html#overdispersion-in-a-poisson-model",
    "href": "count1.html#overdispersion-in-a-poisson-model",
    "title": "24  Modeling a Count Outcome",
    "section": "24.9 Overdispersion in a Poisson Model",
    "text": "24.9 Overdispersion in a Poisson Model\nPoisson regressions do not supply an independent variance parameter \\(\\sigma\\), and as a result can be overdispersed, and usually are. Under the Poisson distribution, the variance equals the mean - so the standard deviation equals the square root of the mean. The notion of overdispersion arises here. When fitting generalized linear models with Poisson error distributions, the residual deviance and its degrees of freedom should be approximately equal if the model fits well.\nIf the residual deviance is far greater than the degrees of freedom, then overdispersion may well be a problem. In this case, the residual deviance is about 8.5 times the size of the residual degrees of freedom, so that’s a clear indication of overdispersion. We saw earlier that the Poisson regression model requires that the outcome (here the physhealth counts) be independent. A possible reason for the overdispersion we see here is that physhealth on different days likely do not occur independently of one another but likely “cluster” together.\n\n24.9.1 Testing for Overdispersion?\nGelman and Hill provide an overdispersion test in R for a Poisson model as follows…\n\nyhat <- predict(mod_poiss1, type = \"response\")\nn <- arm::display(mod_poiss1)$n\n\nglm(formula = physhealth ~ bmi_c + smoke100, family = poisson(), \n    data = ohioA_young)\n            coef.est coef.se\n(Intercept) 0.91     0.02   \nbmi_c       0.04     0.00   \nsmoke100    0.53     0.02   \n---\n  n = 1974, k = 3\n  residual deviance = 19150.9, null deviance = 20221.8 (difference = 1070.9)\n\nk <- arm::display(mod_poiss1)$k\n\nglm(formula = physhealth ~ bmi_c + smoke100, family = poisson(), \n    data = ohioA_young)\n            coef.est coef.se\n(Intercept) 0.91     0.02   \nbmi_c       0.04     0.00   \nsmoke100    0.53     0.02   \n---\n  n = 1974, k = 3\n  residual deviance = 19150.9, null deviance = 20221.8 (difference = 1070.9)\n\nz <- (ohioA_young$physhealth - yhat) / sqrt(yhat)\ncat(\"overdispersion ratio is \", sum(z^2)/ (n - k), \"\\n\")\n\noverdispersion ratio is  15.58261 \n\ncat(\"p value of overdispersion test is \", \n    pchisq(sum(z^2), df = n-k, lower.tail = FALSE), \"\\n\")\n\np value of overdispersion test is  0 \n\n\nThe p value here is 0, indicating that the probability is essentially zero that a random variable from a \\(\\chi^2\\) distribution with (n - k) = 1971 degrees of freedom would be as large as what we observed in this case. So there is significant overdispersion.\nIn summary, the physhealth counts are overdispersed by a factor of 15.581, which is enormous (even a factor of 2 would be considered large) and also highly statistically significant. The basic correction for overdisperson is to multiply all regression standard errors by \\(\\sqrt{15.581}\\) = 3.95.\nThe quasipoisson model and the negative binomial model that we’ll fit below are very similar. We write the overdispersed “quasiPoisson” model as:\n\\[\ny_i \\sim \\mbox{overdispersed Poisson} (\\mu_i exp(X_i \\beta), \\omega)\n\\]\nwhere \\(\\omega\\) is the overdispersion parameter, 15.581, in our case. The Poisson model we saw previously is then just the overdispersed Poisson model with \\(\\omega = 1\\)."
  },
  {
    "objectID": "count1.html#fitting-the-quasi-poisson-model",
    "href": "count1.html#fitting-the-quasi-poisson-model",
    "title": "24  Modeling a Count Outcome",
    "section": "24.10 Fitting the Quasi-Poisson Model",
    "text": "24.10 Fitting the Quasi-Poisson Model\nTo deal with overdispersion, one useful approach is to apply a quasi-likelihood estimation procedure, as follows:\n\nmod_poiss_od1 <- glm(physhealth ~ bmi_c + smoke100, \n                  family = quasipoisson(),\n                  data = ohioA_young)\n\nsummary(mod_poiss_od1)\n\n\nCall:\nglm(formula = physhealth ~ bmi_c + smoke100, family = quasipoisson(), \n    data = ohioA_young)\n\nDeviance Residuals: \n   Min      1Q  Median      3Q     Max  \n-6.588  -2.604  -2.087  -0.533  10.688  \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 0.906991   0.073818  12.287  < 2e-16 ***\nbmi_c       0.035051   0.005611   6.247 5.11e-10 ***\nsmoke100    0.532505   0.098308   5.417 6.81e-08 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for quasipoisson family taken to be 15.58436)\n\n    Null deviance: 20222  on 1973  degrees of freedom\nResidual deviance: 19151  on 1971  degrees of freedom\nAIC: NA\n\nNumber of Fisher Scoring iterations: 6\n\nconfint(mod_poiss_od1)\n\nWaiting for profiling to be done...\n\n\n                 2.5 %     97.5 %\n(Intercept) 0.75877174 1.04831740\nbmi_c       0.02383574 0.04583252\nsmoke100    0.34039368 0.72606948\n\n\nThis “quasi-Poisson regression” model uses the same mean function as Poisson regression, but now estimated by quasi-maximum likelihood estimation or, equivalently, through the method of generalized estimating equations, where the inference is adjusted by an estimated dispersion parameter. Sometimes, though I won’t demonstrate this here, people fit an “adjusted” Poisson regression model, where this estimation by quasi-ML is augmented to adjust the inference via sandwich estimates of the covariances3.\n\n24.10.1 The Fitted Equation\nThe model equation is still log(physhealth) = 0.91 + 0.035 bmi_c + 0.53 smoke100. The estimated coefficients are still statistically significant, but the standard errors for each coefficient are considerably larger when we account for overdispersion.\nThe dispersion parameter for the quasi-Poisson family is now taken to be a bit less than the square root of the ratio of the residual deviance and its degrees of freedom. This is a much more believable model, as a result.\n\n\n24.10.2 Interpreting the Coefficients\nNo meaningful change from the Poisson model we saw previously.\n\n\n24.10.3 Testing the Predictors\nAgain, we can use the Wald tests (z tests) provided with the Poisson regression output, or we can fit the model and then run an ANOVA to obtain a test based on the deviance (a simple transformation of the log likelihood ratio.)\n\nBy the Wald tests shown above, each predictor clearly adds significant predictive value to the model given the other predictor, and we note that the p values are as small as R will support.\nThe ANOVA approach for this model lets us check the impact of adding smoke100 to a model already containing bmi_c.\n\n\nanova(mod_poiss_od1, test = \"Chisq\")\n\nAnalysis of Deviance Table\n\nModel: quasipoisson, link: log\n\nResponse: physhealth\n\nTerms added sequentially (first to last)\n\n         Df Deviance Resid. Df Resid. Dev  Pr(>Chi)    \nNULL                      1973      20222              \nbmi_c     1   609.46      1972      19612 4.011e-10 ***\nsmoke100  1   461.46      1971      19151 5.282e-08 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nThe result is unchanged. To obtain a p value for smoke100’s impact after bmi_c is accounted for, we compare the difference in deviance to a chi-square distribution with 1 degree of freedom. The result is incredibly statistically significant.\nTo check the effect of bmi_c, we could refit the model with and without bmi_c, and again run an ANOVA. I’ll skip that here.\n\n\n24.10.4 Store fitted values and residuals\nWhat happens if we try using the broom package in this case? We can, if we like, get our predicted values right on the scale of our physhealth response.\n\nsm_poiss_od1 <- augment(mod_poiss_od1, ohioA_young,\n                     type.predict = \"response\")\n\nsm_poiss_od1 |> \n    select(physhealth, .fitted) |>\n    head()\n\n# A tibble: 6 × 2\n  physhealth .fitted\n       <dbl>   <dbl>\n1          0    2.23\n2          0    2.29\n3          0    3.10\n4         30    5.32\n5          0    3.15\n6          0    3.68\n\n\nIt turns out that 0 of the 1974 predictions that we make are below 0, and the largest prediction made by this model is 21.7 days.\nThe rootogram function we’ve shown doesn’t support overdispersed Poisson models at the moment.\n\n\n24.10.5 Specify the \\(R^2\\) and log(likelihood) values\nWe can calculate the \\(R^2\\) as the squared correlation of the fitted values and the observed values.\n\n# The correlation of observed and fitted values\n(poiss_od_r <- with(sm_poiss_od1, cor(physhealth, .fitted)))\n\n[1] 0.1846814\n\n# R-square\npoiss_od_r^2\n\n[1] 0.03410723\n\n\nThe glance function in the broom package gives us model log(likelihood), among other summaries.\n\nglance(mod_poiss_od1) |> round(3)\n\n# A tibble: 1 × 8\n  null.deviance df.null logLik   AIC   BIC deviance df.residual  nobs\n          <dbl>   <dbl>  <dbl> <dbl> <dbl>    <dbl>       <dbl> <dbl>\n1        20222.    1973     NA    NA    NA   19151.        1971  1974\n\n\nHere, we have\n\n\n\nModel\nScale\n\\(R^2\\)\nlog(likelihood)\n\n\n\n\nPoisson\nlog(physhealth)\n0.034\nNA\n\n\n\n\n\n24.10.6 Check model assumptions\nHaving dealt with the overdispersion, this should be a cleaner model in some ways, but the diagnostics (other than the dispersion) will be the same. Here is a plot of residuals vs. fitted values on the original physhealth scale.\n\nggplot(sm_poiss_od1, aes(x = .fitted, y = .resid)) +\n    geom_point() +\n    labs(title = \"Residuals vs. Fitted `physhealth`\",\n         subtitle = \"Overdispersed Poisson Regression model\")\n\n\n\n\nI’ll skip the glm.diag.plots results, since you’ve already seen them.\n\n\n24.10.7 Predictions for Harry and Sally\nThe predictions from this overdispersed Poisson regression will match those in the original Poisson regression, but the standard error will be larger.\n\npredict(mod_poiss_od1, newdata = hs_data, se.fit = TRUE,\n        type = \"response\")\n\n$fit\n       1        2 \n5.989478 2.078688 \n\n$se.fit\n        1         2 \n0.4557357 0.1703147 \n\n$residual.scale\n[1] 3.947703\n\n\nBy using response as the type, these predictions fall on the original physhealth scale. Again, the prediction for Harry is 5.99 days, and for Sally is 2.08 days."
  },
  {
    "objectID": "count1.html#poisson-and-quasi-poisson-models-using-glm-from-the-rms-package",
    "href": "count1.html#poisson-and-quasi-poisson-models-using-glm-from-the-rms-package",
    "title": "24  Modeling a Count Outcome",
    "section": "24.11 Poisson and Quasi-Poisson models using Glm from the rms package",
    "text": "24.11 Poisson and Quasi-Poisson models using Glm from the rms package\nThe Glm function in the rms package can be used to fit both the original Poisson regression and the quasi-Poisson model accounting for overdispersion.\n\n24.11.1 Refitting the original Poisson regression with Glm\n\nd <- datadist(ohioA_young)\noptions(datadist = \"d\")\n\nmod_poi_Glm_1 <- Glm(physhealth ~ bmi_c + smoke100,\n                     family = poisson(), \n                     data = ohioA_young, \n                     x = T, y = T)\n\nmod_poi_Glm_1\n\nGeneral Linear Model\n \n Glm(formula = physhealth ~ bmi_c + smoke100, family = poisson(), \n     data = ohioA_young, x = T, y = T)\n \n                        Model Likelihood    \n                              Ratio Test    \n       Obs    1974    LR chi2    1070.93    \n Residual d.f.1971    d.f.             2    \n       g 0.4182128    Pr(> chi2) <0.0001    \n \n           Coef   S.E.   Wald Z Pr(>|Z|)\n Intercept 0.9070 0.0187 48.50  <0.0001 \n bmi_c     0.0351 0.0014 24.66  <0.0001 \n smoke100  0.5325 0.0249 21.38  <0.0001 \n \n\n\n\n\n24.11.2 Refitting the overdispersed Poisson regression with Glm\n\nd <- datadist(ohioA_young)\noptions(datadist = \"d\")\n\nmod_poi_od_Glm_1 <- Glm(physhealth ~ bmi_c + smoke100,\n                     family = quasipoisson(), \n                     data = ohioA_young, \n                     x = T, y = T)\n\nmod_poi_od_Glm_1\n\nGeneral Linear Model\n \n Glm(formula = physhealth ~ bmi_c + smoke100, family = quasipoisson(), \n     data = ohioA_young, x = T, y = T)\n \n                        Model Likelihood    \n                              Ratio Test    \n       Obs    1974    LR chi2    1070.93    \n Residual d.f.1971    d.f.             2    \n       g 0.4182128    Pr(> chi2) <0.0001    \n \n           Coef   S.E.   Wald Z Pr(>|Z|)\n Intercept 0.9070 0.0738 12.29  <0.0001 \n bmi_c     0.0351 0.0056  6.25  <0.0001 \n smoke100  0.5325 0.0983  5.42  <0.0001 \n \n\n\nThe big advantage here is that we have access to the usual ANOVA, summary, and nomogram features that rms brings to fitting models.\n\n\n24.11.3 ANOVA on a Glm fit\n\nanova(mod_poi_od_Glm_1)\n\n                Wald Statistics          Response: physhealth \n\n Factor     Chi-Square d.f. P     \n bmi_c      39.03      1    <.0001\n smoke100   29.34      1    <.0001\n TOTAL      74.39      2    <.0001\n\n\nThis shows the individual Wald \\(\\chi^2\\) tests without having to refit the model.\n\n\n24.11.4 ggplots from Glm fit\n\nggplot(Predict(mod_poi_od_Glm_1, fun = exp))\n\n\n\n\n\n\n24.11.5 Summary of a Glm fit\n\nsummary(mod_poi_od_Glm_1)\n\n             Effects              Response : physhealth \n\n Factor   Low     High   Diff.  Effect  S.E.     Lower 0.95 Upper 0.95\n bmi_c    -5.0513 3.6362 8.6875 0.30450 0.048742 0.20891    0.4001    \n smoke100  0.0000 1.0000 1.0000 0.53251 0.098308 0.33971    0.7253    \n\n\n\n\n24.11.6 Plot of the Summary\n\nplot(summary(mod_poi_od_Glm_1))\n\n\n\n\n\n\n24.11.7 Nomogram of a Glm fit\n\nplot(nomogram(mod_poi_od_Glm_1, fun = exp, \n              funlabel = \"physhealth days\"))\n\n\n\n\nNote the use of fun=exp in both the ggplot of Predict and the nomogram. What’s that doing?\nIn the next chapter, we’ll expand beyond Poisson regression to consider a Negative Binomial model."
  },
  {
    "objectID": "count2.html#r-setup-used-here",
    "href": "count2.html#r-setup-used-here",
    "title": "25  Negative Binomial Models for Count Data",
    "section": "25.1 R Setup Used Here",
    "text": "25.1 R Setup Used Here\n\nknitr::opts_chunk$set(comment = NA)\n\nlibrary(broom)\nlibrary(boot)\nlibrary(countreg)\nlibrary(MASS)\nlibrary(tidyverse) \n\ntheme_set(theme_bw())"
  },
  {
    "objectID": "count2.html#data-load-and-subset-creation",
    "href": "count2.html#data-load-and-subset-creation",
    "title": "25  Negative Binomial Models for Count Data",
    "section": "25.2 Data Load and Subset Creation",
    "text": "25.2 Data Load and Subset Creation\n\nsmart_oh <- readRDS(\"data/smart_ohio.Rds\")\n\nAs in Chapter 24, we’ll create a subset of these data for analysis.\n\nohioA <- smart_oh |>\n    select(SEQNO, mmsa_name, genhealth, physhealth, \n           menthealth, healthplan, costprob, \n           agegroup, female, incomegroup, bmi, smoke100, \n           alcdays) |>\n    drop_na()\n\nohioA_young <- ohioA |>\n    filter(agegroup %in% c(\"18-24\", \"25-29\", \"30-34\", \n                           \"35-39\", \"40-44\", \"45-49\")) |>\n    droplevels() |>\n  mutate(bmi_c = bmi - mean(bmi))"
  },
  {
    "objectID": "count2.html#setup-for-this-chapter",
    "href": "count2.html#setup-for-this-chapter",
    "title": "25  Negative Binomial Models for Count Data",
    "section": "25.3 Setup for this Chapter",
    "text": "25.3 Setup for this Chapter\nAgain, we’re going to predict physhealth using bmi_c and smoke100.\n\nRemember that physhealth is a count of the number of poor physical health days in the past 30.\nAs a result, physhealth is restricted to taking values between 0 and 30.\n\nIn this chapter, we demonstrate Negative binomial regression, which (like Poisson regression discussed in Chapter 24) is also used for counts and adjusts for overdispersion.\n\n25.3.1 What Will We Demonstrate?\nWith this new approach, we will again fit the model and specify procedures for doing so in R. Then we will:\n\nSpecify the fitted model equation\nInterpret the model’s coefficient estimates and 95% confidence intervals around those estimates.\nPerform a test of whether each variable adds value to the model, when the other one is already included.\nStore the fitted values and appropriate residuals for each model.\nSummarize the model’s apparent \\(R^2\\) value, the proportion of variation explained, and the model log likelihood.\nPerform checks of model assumptions as appropriate.\nDescribe how predictions would be made for two new subjects.\n\nHarry has a BMI that is 10 kg/m2 higher than the average across all respondents and has smoked more than 100 cigarettes in his life.\nSally has a BMI that is 5 kg/m2 less than the average across all respondents and has not smoked more than 100 cigarettes in her life.\n\n\nIn addition, for some of the new models, we provide a little of the mathematical background, and point to other resources you can use to learn more about the model.\n\n\n25.3.2 Extra Data File for Harry and Sally\nTo make our lives a little easier, I’ll create a little tibble containing the necessary data for Harry and Sally.\n\nhs_data <- tibble(subj = c(\"Harry\", \"Sally\"),\n                  bmi_c = c(10, -5),\n                  smoke100 = c(1, 0))\nhs_data\n\n# A tibble: 2 × 3\n  subj  bmi_c smoke100\n  <chr> <dbl>    <dbl>\n1 Harry    10        1\n2 Sally    -5        0\n\n\n\n\n25.3.3 Our Poisson Model (for comparison)\n\nmod_poiss1 <- glm(physhealth ~ bmi_c + smoke100, \n                  family = poisson(),\n                  data = ohioA_young)"
  },
  {
    "objectID": "count2.html#negative-binomial-model",
    "href": "count2.html#negative-binomial-model",
    "title": "25  Negative Binomial Models for Count Data",
    "section": "25.4 Negative Binomial Model",
    "text": "25.4 Negative Binomial Model\nAnother approach to dealing with overdispersion is to fit a negative binomial model1 to predict the log(physhealth) counts. This involves the fitting of an additional parameter, \\(\\theta\\). That’s our dispersion parameter2\nSometimes, people will fit a model where \\(\\theta\\) is known, for instance a geometric model (where \\(\\theta\\) = 1), and then this can be directly plugged into a glm() fit, but the more common scenario is that we are going to iteratively estimate the \\(\\beta\\) coefficients and \\(\\theta\\). To do this, I’ll use the glm.nb function from the MASS package.\n\nmod_nb1 <- glm.nb(physhealth ~ bmi_c + smoke100, link = log,\n                  data = ohioA_young)\n\nsummary(mod_nb1)\n\n\nCall:\nglm.nb(formula = physhealth ~ bmi_c + smoke100, data = ohioA_young, \n    link = log, init.theta = 0.1487673114)\n\nDeviance Residuals: \n    Min       1Q   Median       3Q      Max  \n-1.2260  -0.9712  -0.8985  -0.1128   1.9929  \n\nCoefficients:\n            Estimate Std. Error z value Pr(>|z|)    \n(Intercept) 0.874530   0.078994  11.071  < 2e-16 ***\nbmi_c       0.035712   0.008317   4.294 1.76e-05 ***\nsmoke100    0.596396   0.121166   4.922 8.56e-07 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for Negative Binomial(0.1488) family taken to be 1)\n\n    Null deviance: 1468.1  on 1973  degrees of freedom\nResidual deviance: 1422.6  on 1971  degrees of freedom\nAIC: 6976.6\n\nNumber of Fisher Scoring iterations: 1\n\n              Theta:  0.14877 \n          Std. Err.:  0.00705 \n\n 2 x log-likelihood:  -6968.55300 \n\nconfint(mod_nb1)\n\nWaiting for profiling to be done...\n\n\n                 2.5 %     97.5 %\n(Intercept) 0.72304817 1.03304660\nbmi_c       0.02072601 0.05124925\nsmoke100    0.35977590 0.83584673\n\n\n\n25.4.1 The Fitted Equation\nThe form of the model equation for a negative binomial regression is the same as that for Poisson regression.\nlog(physhealth) = 0.87 + 0.036 bmi_c + 0.60 smoke100\n\n\n25.4.2 Comparison with the (raw) Poisson model\nTo compare the negative binomial model to the Poisson model (without the overdispersion) we can use the logLik function to make a comparison. Note that the Poisson model is a subset of the negative binomial.\n\nlogLik(mod_nb1)\n\n'log Lik.' -3484.277 (df=4)\n\nlogLik(mod_poiss1)\n\n'log Lik.' -10819.6 (df=3)\n\n2 * (logLik(mod_nb1) - logLik(mod_poiss1))\n\n'log Lik.' 14670.65 (df=4)\n\npchisq(2 * (logLik(mod_nb1) - logLik(mod_poiss1)), df = 1, lower.tail = FALSE)\n\n'log Lik.' 0 (df=4)\n\n\nHere, the difference in the log likelihoods is large enough that the resulting p value is very small. This strongly suggests that the negative binomial model, which adds the dispersion parameter, is more appropriate than the raw Poisson model.\nHowever, both the regression coefficients and the standard errors are rather similar to the quasi-Poisson and the sandwich-adjusted Poisson results above. Thus, in terms of predicted means, all three models give very similar results; the associated Wald tests also lead to the same conclusions.\n\n\n25.4.3 Interpreting the Coefficients\nThere’s only a small change here from the Poisson models we saw previously.\n\nThe constant term, 0.87, gives us the intercept of the regression - the prediction if smoke100 = 0 and bmi_c = 0. In this case, because we’ve centered BMI, it implies that exp(0.87) = 2.39 is the predicted days of poor physhealth for a non-smoker with average BMI.\nThe coefficient of bmi_c, 0.036, is the expected difference in count of poor physhealth days (on the log scale) for each additional kg/m2 of body mass index. The expected multiplicative increase is \\(e^{0.036}\\) = 1.037, corresponding to a 3.7% difference in the count.\nThe coefficient of smoke100, 0.60, tells us that the predictive difference between those who have and who have not smoked 100 cigarettes can be found by multiplying the physhealth count by exp(0.6) = 1.82, yielding essentially an 82% increase of the physhealth count.\n\n\n\n25.4.4 Interpretation of Coefficients in terms of IRRs\nWe might be interested in looking at incident rate ratios rather than coefficients. The coefficients have an additive effect in the log(y) scale, and the IRR have a multiplicative effect in the y scale. To do this, we can exponentiate our model coefficients. This also applies to the confidence intervals.\n\nexp(coef(mod_nb1))\n\n(Intercept)       bmi_c    smoke100 \n   2.397748    1.036357    1.815563 \n\nexp(confint(mod_nb1))\n\nWaiting for profiling to be done...\n\n\n               2.5 %   97.5 %\n(Intercept) 2.060705 2.809613\nbmi_c       1.020942 1.052585\nsmoke100    1.433008 2.306766\n\n\nAs an example, then, the incident rate for smoke100 = 1 is 1.82 times the incident rate of physhealth days for the reference group (smoke100 = 0). The percent change in the incident rate of physhealth is a 3.6% increase for every kg/m2 increase in centered bmi.\n\n\n25.4.5 Testing the Predictors\nAgain, we can use the Wald tests (z tests) provided with the negative binomial regression output.\nAs an alternative, we probably should not use the standard anova process, because the models there don’t re-estimate \\(\\theta\\) for each new model, as the warning message below indicates.\n\nanova(mod_nb1)\n\nWarning in anova.negbin(mod_nb1): tests made without re-estimating 'theta'\n\n\nAnalysis of Deviance Table\n\nModel: Negative Binomial(0.1488), link: log\n\nResponse: physhealth\n\nTerms added sequentially (first to last)\n\n         Df Deviance Resid. Df Resid. Dev  Pr(>Chi)    \nNULL                      1973     1468.0              \nbmi_c     1   20.837      1972     1447.2 5.001e-06 ***\nsmoke100  1   24.584      1971     1422.6 7.115e-07 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nSo, instead, if we want, for instance to assess the significance of bmi_c, after smoke100 is already included in the model, we fit both models (with and without bmi_c) and then compare those models with a likelihood ratio test.\n\nmod_nb1_without_bmi <- glm.nb(physhealth ~ smoke100, \n                              link = log,\n                              data = ohioA_young)\n\nanova(mod_nb1, mod_nb1_without_bmi)\n\nLikelihood ratio tests of Negative Binomial Models\n\nResponse: physhealth\n             Model     theta Resid. df    2 x log-lik.   Test    df LR stat.\n1         smoke100 0.1452219      1972       -6991.195                      \n2 bmi_c + smoke100 0.1487673      1971       -6968.553 1 vs 2     1  22.6421\n       Pr(Chi)\n1             \n2 1.951615e-06\n\n\nAnd we could compare the negative binomial models with and without smoke100 in a similar way.\n\nmod_nb1_without_smoke <- glm.nb(physhealth ~ bmi_c, \n                                link = log,\n                                data = ohioA_young)\n\nanova(mod_nb1, mod_nb1_without_smoke)\n\nLikelihood ratio tests of Negative Binomial Models\n\nResponse: physhealth\n             Model     theta Resid. df    2 x log-lik.   Test    df LR stat.\n1            bmi_c 0.1449966      1972       -6992.839                      \n2 bmi_c + smoke100 0.1487673      1971       -6968.553 1 vs 2     1 24.28569\n       Pr(Chi)\n1             \n2 8.305388e-07\n\n\n\n\n25.4.6 Store fitted values and residuals\nThe broom package works in this case, too. We’ll look here at predicted (fitted) values on the scale of our physhealth response.\n\nsm_nb1 <- augment(mod_nb1, ohioA_young,\n                     type.predict = \"response\")\n\nWarning: The `augment()` method for objects of class `negbin` is not maintained by the broom team, and is only supported through the `glm` tidier method. Please be cautious in interpreting and reporting broom output.\n\nThis warning is displayed once per session.\n\nsm_nb1 |> \n    select(physhealth, .fitted) |>\n    head()\n\n# A tibble: 6 × 2\n  physhealth .fitted\n       <dbl>   <dbl>\n1          0    2.15\n2          0    2.22\n3          0    3.18\n4         30    5.23\n5          0    3.24\n6          0    3.78\n\n\n\n\n25.4.7 Rootogram for Negative Binomial model\nHere’s the rootogram for the negative binomial model.\n\nrootogram(mod_nb1, max = 30)\n\n\n\n\nAgain, the red curved line is the theoretical (negative binomial) fit. “Hanging” from each point on the red line is a bar, the height of which represents the difference between expected and observed counts. A bar hanging below 0 indicates underfitting. A bar hanging above 0 indicates overfitting. The counts have been transformed with a square root transformation to prevent smaller counts from getting obscured and overwhelmed by larger counts.\nThe match looks much better than the Poisson model, which is a sign that accounting for overdispersion is very important. Even this model badly underfits the number of 30 values, however.\n\n\n25.4.8 Simulating what the Negative Binomial model predicts\nWe can use the parameters of the negative binomial model to simulate data3 and compare the simulated results to our observed physhealth data.\n\npar(mfrow=c(1,2))\nohioA_young$physhealth |> \n    table() |> barplot(main = \"Observed physhealth\")\nset.seed(432122)\nrnbinom(n = nrow(ohioA_young), \n        size = mod_nb1$theta,\n        mu = exp(coef(mod_nb1)[1])) |>\n    table() |> barplot(main = \"Simulated physhealth\")\n\n\n\n\nAgain we see that the simulated data badly underfits the 30 values, and includes some predictions larger than 30.\n\n\n25.4.9 Specify the \\(R^2\\) and log(likelihood) values\nWe can calculate the \\(R^2\\) as the squared correlation of the fitted values and the observed values.\n\n# The correlation of observed and fitted values\n(nb_r <- with(sm_nb1, cor(physhealth, .fitted)))\n\n[1] 0.183675\n\n# R-square\nnb_r^2\n\n[1] 0.03373649\n\n\nThe glance function in the broom package gives us model log(likelihood), among other summaries.\n\nglance(mod_nb1) |> round(3)\n\n# A tibble: 1 × 8\n  null.deviance df.null logLik      AIC   BIC deviance df.residual  nobs\n          <dbl>   <dbl> <logLik>  <dbl> <dbl>    <dbl>       <dbl> <dbl>\n1         1468.    1973 -3484.277 6977. 6999.    1423.        1971  1974\n\n\nHere, we have\n\n\n\nModel\nScale\n\\(R^2\\)\nlog(likelihood)\n\n\n\n\nNegative Binomial\nlog(physhealth)\n.034\n-3484.27\n\n\n\n\n\n25.4.10 Check model assumptions\nHere is a plot of residuals vs. fitted values on the original physhealth scale.\n\nggplot(sm_nb1, aes(x = .fitted, y = .resid)) +\n    geom_point() +\n    labs(title = \"Residuals vs. Fitted `physhealth`\",\n         subtitle = \"Negative Binomial Regression model\")\n\n\n\n\nHere are the glm diagnostic plots from the boot package.\n\nglm.diag.plots(mod_nb1)\n\n\n\n\nFrom the lower left plot, we see fewer points with large values of both Cook’s distance and leverage, so that’s a step in the right direction. The upper right plot still has some issues, but we’re closer to a desirable result there, too.\n\n\n25.4.11 Predictions for Harry and Sally\nThe predictions from this negative binomial regression model will be only a little different than those from the Poisson models.\n\npredict(mod_nb1, newdata = hs_data, se.fit = TRUE,\n        type = \"response\")\n\n$fit\n       1        2 \n6.221696 2.005657 \n\n$se.fit\n        1         2 \n0.7537535 0.1773595 \n\n$residual.scale\n[1] 1\n\n\nAs we’ve seen in the past, when we use response as the type, the predictions fall on the original physhealth scale. The prediction for Harry is 6.2 days, and for Sally is 2.0 days."
  },
  {
    "objectID": "count2.html#the-problem-too-few-zeros",
    "href": "count2.html#the-problem-too-few-zeros",
    "title": "25  Negative Binomial Models for Count Data",
    "section": "25.5 The Problem: Too Few Zeros",
    "text": "25.5 The Problem: Too Few Zeros\nRemember that we observe more than 1000 zeros in our physhealth data.\n\nohioA_young |> count(physhealth == 0)\n\n# A tibble: 2 × 2\n  `physhealth == 0`     n\n  <lgl>             <int>\n1 FALSE               710\n2 TRUE               1264\n\n\nLet’s go back to our Poisson model (without overdispersion) for a moment, and concentrate on the zero values.\n\n# predict expected mean physhealth for each subject\nmu <- predict(mod_poiss1, type = \"response\")\n\n# sum the probabilities of a zero count for each mean\nexp <- sum(dpois(x = 0, lambda = mu))\n\n# predicted number of zeros from Poisson model\nround(exp)\n\n[1] 124\n\n\nAs we’ve seen previously, we’re severely underfitting zero counts. We can compare the observed number of zero physhealth results to the expected number of zero values from the likelihood-based models.\n\nround(c(\"Obs\" = sum(ohioA_young$physhealth == 0),\n  \"Poisson\" = sum(dpois(0, fitted(mod_poiss1))),\n  \"NB\" = sum(dnbinom(0, mu = fitted(mod_nb1), size = mod_nb1$theta))),0)\n\n    Obs Poisson      NB \n   1264     124    1250 \n\n\nThere are at least two ways to tackle this problem.\n\nFitting a model which deliberately inflates the number of zeros that are fitted\nFitting a hurdle model\n\nWe’ll look at those options, next."
  },
  {
    "objectID": "count3.html#r-setup-used-here",
    "href": "count3.html#r-setup-used-here",
    "title": "26  Zero-Inflated Models for Count Data",
    "section": "26.1 R Setup Used Here",
    "text": "26.1 R Setup Used Here\n\nknitr::opts_chunk$set(comment = NA)\n\nlibrary(broom)\nlibrary(lmtest)\nlibrary(MASS)\nlibrary(pscl)\nlibrary(VGAM)\nlibrary(tidyverse) \n\ntheme_set(theme_bw())"
  },
  {
    "objectID": "count3.html#data-load-and-subset-creation",
    "href": "count3.html#data-load-and-subset-creation",
    "title": "26  Zero-Inflated Models for Count Data",
    "section": "26.2 Data Load and Subset Creation",
    "text": "26.2 Data Load and Subset Creation\n\nsmart_oh <- readRDS(\"data/smart_ohio.Rds\")\n\nAs in Chapter 24, we’ll create a subset of these data for analysis.\n\nohioA <- smart_oh |>\n    select(SEQNO, mmsa_name, genhealth, physhealth, \n           menthealth, healthplan, costprob, \n           agegroup, female, incomegroup, bmi, smoke100, \n           alcdays) |>\n    drop_na()\n\nohioA_young <- ohioA |>\n    filter(agegroup %in% c(\"18-24\", \"25-29\", \"30-34\", \n                           \"35-39\", \"40-44\", \"45-49\")) |>\n    droplevels() |>\n  mutate(bmi_c = bmi - mean(bmi))"
  },
  {
    "objectID": "count3.html#setup-for-this-chapter",
    "href": "count3.html#setup-for-this-chapter",
    "title": "26  Zero-Inflated Models for Count Data",
    "section": "26.3 Setup for this Chapter",
    "text": "26.3 Setup for this Chapter\nAgain, we’re going to predict physhealth using bmi_c and smoke100.\n\nRemember that physhealth is a count of the number of poor physical health days in the past 30.\nAs a result, physhealth is restricted to taking values between 0 and 30.\n\nIn this chapter, we demonstrate:\n\nZero-inflated Poisson models\nZero-inflated Negative Binomial models\nHurdle models\nTobit models\n\n\n26.3.1 What Will We Demonstrate?\nWith each new approach, we again will fit the model and specify procedures for doing so in R. Then we will:\n\nSpecify the fitted model equation\nInterpret the model’s coefficient estimates and 95% confidence intervals around those estimates.\nPerform a test of whether each variable adds value to the model, when the other one is already included.\nStore the fitted values and appropriate residuals for each model.\nSummarize the model’s apparent \\(R^2\\) value, the proportion of variation explained, and the model log likelihood.\nPerform checks of model assumptions as appropriate.\nDescribe how predictions would be made for two new subjects.\n\nHarry has a BMI that is 10 kg/m2 higher than the average across all respondents and has smoked more than 100 cigarettes in his life.\nSally has a BMI that is 5 kg/m2 less than the average across all respondents and has not smoked more than 100 cigarettes in her life.\n\n\nIn addition, for some of the new models, we provide a little of the mathematical background, and point to other resources you can use to learn more about the model.\n\n\n26.3.2 Extra Data File for Harry and Sally\nTo make our lives a little easier, I’ll create a little tibble containing the necessary data for Harry and Sally.\n\nhs_data <- tibble(subj = c(\"Harry\", \"Sally\"),\n                  bmi_c = c(10, -5),\n                  smoke100 = c(1, 0))\nhs_data\n\n# A tibble: 2 × 3\n  subj  bmi_c smoke100\n  <chr> <dbl>    <dbl>\n1 Harry    10        1\n2 Sally    -5        0\n\n\n\n\n26.3.3 Previous Models (for comparison)\n\nmod_poiss1 <- glm(physhealth ~ bmi_c + smoke100, \n                  family = poisson(),\n                  data = ohioA_young)\n\nmod_nb1 <- glm.nb(physhealth ~ bmi_c + smoke100, \n                  link = log,\n                  data = ohioA_young)"
  },
  {
    "objectID": "count3.html#the-zero-inflated-poisson-regression-model",
    "href": "count3.html#the-zero-inflated-poisson-regression-model",
    "title": "26  Zero-Inflated Models for Count Data",
    "section": "26.4 The Zero-Inflated Poisson Regression Model",
    "text": "26.4 The Zero-Inflated Poisson Regression Model\nThere are at least two ways to tackle the problem of not predicting enough 0 values.\n\nFitting a model which deliberately inflates the number of zeros that are fitted\nFitting a hurdle model\n\nThe zero-inflated Poisson or (ZIP) model is used to describe count data with an excess of zero counts1. The model posits that there are two processes involved:\n\na logit model is used to predict excess zeros\nwhile a Poisson model is used to predict the counts, generally\n\nThe pscl package is used here, which can conflict with the countreg package we used to fit rootograms.\nTo run the zero-inflated Poisson model, we use the following:\n\nmod_zip1 <- zeroinfl(physhealth ~ bmi_c + smoke100, \n                     data = ohioA_young)\n\nsummary(mod_zip1)\n\n\nCall:\nzeroinfl(formula = physhealth ~ bmi_c + smoke100, data = ohioA_young)\n\nPearson residuals:\n    Min      1Q  Median      3Q     Max \n-1.4389 -0.6987 -0.6138 -0.1947  9.3036 \n\nCount model coefficients (poisson with log link):\n            Estimate Std. Error z value Pr(>|z|)    \n(Intercept) 1.997594   0.018797   106.3   <2e-16 ***\nbmi_c       0.018176   0.001398    13.0   <2e-16 ***\nsmoke100    0.393383   0.024889    15.8   <2e-16 ***\n\nZero-inflation model coefficients (binomial with logit link):\n             Estimate Std. Error z value Pr(>|z|)    \n(Intercept)  0.682427   0.062853  10.857  < 2e-16 ***\nbmi_c       -0.027731   0.006509  -4.260 2.04e-05 ***\nsmoke100    -0.237224   0.095318  -2.489   0.0128 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 \n\nNumber of iterations in BFGS optimization: 8 \nLog-likelihood: -5680 on 6 Df\n\nconfint(mod_zip1)\n\n                        2.5 %      97.5 %\ncount_(Intercept)  1.96075280  2.03443546\ncount_bmi_c        0.01543576  0.02091718\ncount_smoke100     0.34460096  0.44216461\nzero_(Intercept)   0.55923725  0.80561744\nzero_bmi_c        -0.04048828 -0.01497359\nzero_smoke100     -0.42404310 -0.05040515\n\n\nThe output describes two separate regression models. Below the model call, we see information on a Poisson regression model. Then we see another block describing the inflation model.\nEach predictor (bmi_c and smoke100) appears to be statistically significant in each part of the model.\n\n26.4.1 Comparison to a null model\nTo show that this model fits better than the null model (the model with intercept only), we can compare them directly with a chi-squared test. Since we have two predictors in the full model, the degrees of freedom for this test is 2.\n\nmod_zipnull <- pscl::zeroinfl(physhealth ~ 1, \n                     data = ohioA_young)\n\nsummary(mod_zipnull)\n\n\nCall:\npscl::zeroinfl(formula = physhealth ~ 1, data = ohioA_young)\n\nPearson residuals:\n    Min      1Q  Median      3Q     Max \n-0.6934 -0.6934 -0.6934 -0.2779  5.5400 \n\nCount model coefficients (poisson with log link):\n            Estimate Std. Error z value Pr(>|z|)    \n(Intercept)  2.22764    0.01233   180.7   <2e-16 ***\n\nZero-inflation model coefficients (binomial with logit link):\n            Estimate Std. Error z value Pr(>|z|)    \n(Intercept)   0.5767     0.0469   12.29   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 \n\nNumber of iterations in BFGS optimization: 2 \nLog-likelihood: -5908 on 2 Df\n\npchisq(2 * (logLik(mod_zip1) - logLik(mod_zipnull)), df = 2, lower.tail = FALSE)\n\n'log Lik.' 8.59676e-100 (df=6)\n\n\n\n\n26.4.2 Comparison to a Poisson Model with the Vuong test\n\nvuong(mod_zip1, mod_poiss1)\n\nVuong Non-Nested Hypothesis Test-Statistic: \n(test-statistic is asymptotically distributed N(0,1) under the\n null that the models are indistinguishible)\n-------------------------------------------------------------\n              Vuong z-statistic             H_A    p-value\nRaw                    19.90843 model1 > model2 < 2.22e-16\nAIC-corrected          19.89681 model1 > model2 < 2.22e-16\nBIC-corrected          19.86434 model1 > model2 < 2.22e-16\n\n\nCertainly, the ZIP model is a significant improvement over the standard Poisson model, as the Vuong test reveals.\n\n\n26.4.3 The Fitted Equation\nThe form of the model equation for a zero-inflated Poisson regression requires us to take two separate models into account. First we have a logistic regression model to predict the log odds of zero physhealth days. That takes care of the extra zeros. Then, to predict the number of physhealth days, we have a Poisson model, which may produce some additional zero count estimates.\n\n\n26.4.4 Interpreting the Coefficients\nWe can exponentiate the logistic regression coefficients to obtain results in terms of odds ratios for that model, and that can be of some help in understanding the process behind excess zeros.\nAlso, exponentiating the coefficients of the count model help us describe those counts on the original scale of physhealth.\n\nexp(coef(mod_zip1))\n\ncount_(Intercept)       count_bmi_c    count_smoke100  zero_(Intercept) \n        7.3713003         1.0183427         1.4819856         1.9786748 \n       zero_bmi_c     zero_smoke100 \n        0.9726500         0.7888145 \n\n\nFor example,\n\nin the model for physhealth = 0, the odds of physhealth = 0 are 79% as high for subjects with smoke100 = 1 as for non-smokers with the same BMI.\nin the Poisson model for physhealth, the physhealth count is estimated to increase by 1.48 for smokers as compared to non-smokers with the same BMI.\n\n\n\n26.4.5 Testing the Predictors\nWe can test the model with and without bmi_c, for example, by fitting the model both ways, and comparing the results with either a Wald or Likelihood Ratio test, each of which is available in the lmtest package.\n\nmod_zip1_nobmi <- zeroinfl(physhealth ~ smoke100, \n                     data = ohioA_young)\n\nlmtest::waldtest(mod_zip1, mod_zip1_nobmi)\n\nWald test\n\nModel 1: physhealth ~ bmi_c + smoke100\nModel 2: physhealth ~ smoke100\n  Res.Df Df Chisq Pr(>Chisq)    \n1   1968                        \n2   1970 -2 187.2  < 2.2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\nlmtest::lrtest(mod_zip1, mod_zip1_nobmi)\n\nLikelihood ratio test\n\nModel 1: physhealth ~ bmi_c + smoke100\nModel 2: physhealth ~ smoke100\n  #Df  LogLik Df  Chisq Pr(>Chisq)    \n1   6 -5679.8                         \n2   4 -5769.5 -2 179.35  < 2.2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\n\n26.4.6 Store fitted values and residuals\nThe broom package does not work with the zeroinfl tool. So we need to build up the fitted values and residuals ourselves.\n\nsm_zip1 <- ohioA_young |>\n    mutate(fitted = fitted(mod_zip1, type = \"response\"),\n           resid = resid(mod_zip1, type = \"response\"))\n\nsm_zip1 |> \n    dplyr::select(physhealth, fitted, resid) |>\n    head()\n\n# A tibble: 6 × 3\n  physhealth fitted resid\n       <dbl>  <dbl> <dbl>\n1          0   2.21 -2.21\n2          0   2.28 -2.28\n3          0   3.12 -3.12\n4         30   5.27 24.7 \n5          0   3.17 -3.17\n6          0   3.71 -3.71\n\n\n\n\n26.4.7 Modeled Number of Zero Counts\nThe zero-inflated model is designed to perfectly match the number of observed zeros. We can compare the observed number of zero physhealth results to the expected number of zero values from the likelihood-based models.\n\nround(c(\"Obs\" = sum(ohioA_young$physhealth == 0),\n  \"Poisson\" = sum(dpois(0, fitted(mod_poiss1))),\n  \"NB\" = sum(dnbinom(0, mu = fitted(mod_nb1), size = mod_nb1$theta)),\n  \"ZIP\" = sum(predict(mod_zip1, type = \"prob\")[,1])), 0)\n\n    Obs Poisson      NB     ZIP \n   1264     124    1250    1264 \n\n\n\n\n26.4.8 Rootogram for ZIP model\nHere’s the rootogram for the zero-inflated Poisson model.\n\ncountreg::rootogram(mod_zip1, max = 30)\n\nRegistered S3 methods overwritten by 'countreg':\n  method                 from\n  print.zeroinfl         pscl\n  print.summary.zeroinfl pscl\n  summary.zeroinfl       pscl\n  coef.zeroinfl          pscl\n  vcov.zeroinfl          pscl\n  logLik.zeroinfl        pscl\n  predict.zeroinfl       pscl\n  residuals.zeroinfl     pscl\n  fitted.zeroinfl        pscl\n  terms.zeroinfl         pscl\n  model.matrix.zeroinfl  pscl\n  extractAIC.zeroinfl    pscl\n  print.hurdle           pscl\n  print.summary.hurdle   pscl\n  summary.hurdle         pscl\n  coef.hurdle            pscl\n  vcov.hurdle            pscl\n  logLik.hurdle          pscl\n  predict.hurdle         pscl\n  residuals.hurdle       pscl\n  fitted.hurdle          pscl\n  terms.hurdle           pscl\n  model.matrix.hurdle    pscl\n  extractAIC.hurdle      pscl\n\n\n\n\n\nThe zero frequencies are perfectly matched here, but we can see that counts of 1 and 2 are now substantially underfit, and values between 6 and 13 are overfit.\n\n\n26.4.9 Specify the \\(R^2\\) and log (likelihood) values\nWe can calculate a proxy for \\(R^2\\) as the squared correlation of the fitted values and the observed values.\n\n# The correlation of observed and fitted values\n(zip_r <- with(sm_zip1, cor(physhealth, fitted)))\n\n[1] 0.187328\n\n# R-square\nzip_r^2\n\n[1] 0.03509178\n\n\n\nlogLik(mod_zip1)\n\n'log Lik.' -5679.794 (df=6)\n\n\nHere, we have\n\n\n\nModel\nScale\n\\(R^2\\)\nlog(likelihood)\n\n\n\n\nZero-Inflated Poisson\nComplex: log(physhealth)\n.035\n-5679.83\n\n\n\n\n\n26.4.10 Check model assumptions\nHere is a plot of residuals vs. fitted values on the original physhealth scale.\n\nggplot(sm_zip1, aes(x = fitted, y = resid)) +\n    geom_point() +\n    labs(title = \"Residuals vs. Fitted `physhealth`\",\n         subtitle = \"Zero-Inflated Poisson Regression model\")\n\n\n\n\n\n\n26.4.11 Predictions for Harry and Sally\nThe predictions from this ZIP regression model are obtained as follows…\n\npredict(mod_zip1, newdata = hs_data, type = \"response\")\n\n       1        2 \n6.002212 2.056525 \n\n\nAs we’ve seen in the past, when we use response as the type, the predictions fall on the original physhealth scale. The prediction for Harry is 6.0 days, and for Sally is 2.1 days."
  },
  {
    "objectID": "count3.html#the-zero-inflated-negative-binomial-regression-model",
    "href": "count3.html#the-zero-inflated-negative-binomial-regression-model",
    "title": "26  Zero-Inflated Models for Count Data",
    "section": "26.5 The Zero-Inflated Negative Binomial Regression Model",
    "text": "26.5 The Zero-Inflated Negative Binomial Regression Model\nAs an alternative to the ZIP model, we might consider a zero-inflated negative binomial regression2. This will involve a logistic regression to predict the probability of a 0, and then a negative binomial model to describe the counts of physhealth.\nTo run the zero-inflated negative binomial model, we use the following code:\n\nmod_zinb1 <- pscl::zeroinfl(physhealth ~ bmi_c + smoke100, \n                      dist = \"negbin\", data = ohioA_young)\n\nsummary(mod_zinb1)\n\n\nCall:\npscl::zeroinfl(formula = physhealth ~ bmi_c + smoke100, data = ohioA_young, \n    dist = \"negbin\")\n\nPearson residuals:\n    Min      1Q  Median      3Q     Max \n-0.5579 -0.4192 -0.3957 -0.1166  6.4358 \n\nCount model coefficients (negbin with log link):\n             Estimate Std. Error z value Pr(>|z|)    \n(Intercept)  1.545599   0.099781  15.490  < 2e-16 ***\nbmi_c        0.024614   0.006641   3.707  0.00021 ***\nsmoke100     0.517559   0.110772   4.672 2.98e-06 ***\nLog(theta)  -0.874034   0.143876  -6.075 1.24e-09 ***\n\nZero-inflation model coefficients (binomial with logit link):\n             Estimate Std. Error z value Pr(>|z|)   \n(Intercept) -0.071629   0.164039  -0.437  0.66236   \nbmi_c       -0.027607   0.009313  -2.964  0.00303 **\nsmoke100    -0.127004   0.137171  -0.926  0.35451   \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 \n\nTheta = 0.4173 \nNumber of iterations in BFGS optimization: 14 \nLog-likelihood: -3469 on 7 Df\n\nconfint(mod_zinb1)\n\n                        2.5 %       97.5 %\ncount_(Intercept)  1.35003234  1.741165036\ncount_bmi_c        0.01159892  0.037629890\ncount_smoke100     0.30045008  0.734668671\nzero_(Intercept)  -0.39313901  0.249880357\nzero_bmi_c        -0.04585974 -0.009354261\nzero_smoke100     -0.39585468  0.141845790\n\n\n\n26.5.1 Comparison to a null model\nTo show that this model fits better than the null model (the model with intercept only), we can compare them directly with a chi-squared test. Since we have two predictors in the full model, the degrees of freedom for this test is 2.\n\nmod_zinbnull <- pscl::zeroinfl(physhealth ~ 1, dist = \"negbin\",\n                     data = ohioA_young)\n\nsummary(mod_zinbnull)\n\n\nCall:\npscl::zeroinfl(formula = physhealth ~ 1, data = ohioA_young, dist = \"negbin\")\n\nPearson residuals:\n    Min      1Q  Median      3Q     Max \n-0.4048 -0.4048 -0.4048 -0.1622  3.2340 \n\nCount model coefficients (negbin with log link):\n            Estimate Std. Error z value Pr(>|z|)    \n(Intercept)   1.7766     0.0964  18.429  < 2e-16 ***\nLog(theta)   -1.0445     0.1612  -6.479 9.25e-11 ***\n\nZero-inflation model coefficients (binomial with logit link):\n            Estimate Std. Error z value Pr(>|z|)\n(Intercept)  -0.2605     0.1920  -1.357    0.175\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 \n\nTheta = 0.3519 \nNumber of iterations in BFGS optimization: 12 \nLog-likelihood: -3498 on 3 Df\n\npchisq(2 * (logLik(mod_nb1) - logLik(mod_zinbnull)), df = 2, lower.tail = FALSE)\n\n'log Lik.' 8.538917e-07 (df=4)\n\n\n\n\n26.5.2 Comparison to a Negative Binomial Model: Vuong test\n\nvuong(mod_zinb1, mod_nb1)\n\nVuong Non-Nested Hypothesis Test-Statistic: \n(test-statistic is asymptotically distributed N(0,1) under the\n null that the models are indistinguishible)\n-------------------------------------------------------------\n              Vuong z-statistic             H_A   p-value\nRaw                   3.0753499 model1 > model2 0.0010513\nAIC-corrected         2.4597116 model1 > model2 0.0069524\nBIC-corrected         0.7396743 model1 > model2 0.2297488\n\n\nThe zero-inflated negative binomial model is a significant improvement over the standard negative binomial model according to the the raw or AIC-corrected Vuong tests, but not according to the BIC-corrected test.\n\n\n26.5.3 The Fitted Equation\nLike the ZIP, the zero-inflated negative binomial regression also requires us to take two separate models into account. First we have a logistic regression model to predict the log odds of zero physhealth days. That takes care of the extra zeros. Then, to predict the number of physhealth days, we have a negative binomial regression, with a \\(\\theta\\) term, and this negative binomial regression model may also produce some additional zero count estimates.\n\n\n26.5.4 Interpreting the Coefficients\nAs with the zip, we can exponentiate the logistic regression coefficients to obtain results in terms of odds ratios for that model, and that can be of some help in understanding the process behind excess zeros.\n\nexp(coef(mod_zinb1))\n\ncount_(Intercept)       count_bmi_c    count_smoke100  zero_(Intercept) \n        4.6907791         1.0249198         1.6779275         0.9308759 \n       zero_bmi_c     zero_smoke100 \n        0.9727706         0.8807298 \n\n\nFor example,\n\nin the model for physhealth = 0, the odds of physhealth = 0 are 88.1% as high for subjects with smoke100 = 1 as for non-smokers with the same BMI.\n\nInterpreting the negative binomial piece works the same way as it did in the negative binomial regression.\n\n\n26.5.5 Testing the Predictors\nWe can test the model with and without bmi_c, for example, by fitting the model both ways, and comparing the results with either a Wald or Likelihood Ratio test, each of which is available in the lmtest package.\n\nmod_zinb1_nobmi <- pscl::zeroinfl(physhealth ~ smoke100, \n                            dist = \"negbin\",\n                            data = ohioA_young)\n\nlmtest::waldtest(mod_zinb1, mod_zinb1_nobmi)\n\nWald test\n\nModel 1: physhealth ~ bmi_c + smoke100\nModel 2: physhealth ~ smoke100\n  Res.Df Df  Chisq Pr(>Chisq)    \n1   1967                         \n2   1969 -2 29.589  3.757e-07 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\nlmtest::lrtest(mod_zinb1, mod_zinb1_nobmi)\n\nLikelihood ratio test\n\nModel 1: physhealth ~ bmi_c + smoke100\nModel 2: physhealth ~ smoke100\n  #Df  LogLik Df  Chisq Pr(>Chisq)    \n1   7 -3469.3                         \n2   5 -3485.0 -2 31.418  1.506e-07 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\n\n26.5.6 Store fitted values and residuals\nAgain, we need to build up the fitted values and residuals without the broom package.\n\nsm_zinb1 <- ohioA_young |>\n    mutate(fitted = fitted(mod_zinb1, type = \"response\"),\n           resid = resid(mod_zinb1, type = \"response\"))\n\nsm_zip1 |> \n    dplyr::select(physhealth, fitted, resid) |>\n    head()\n\n# A tibble: 6 × 3\n  physhealth fitted resid\n       <dbl>  <dbl> <dbl>\n1          0   2.21 -2.21\n2          0   2.28 -2.28\n3          0   3.12 -3.12\n4         30   5.27 24.7 \n5          0   3.17 -3.17\n6          0   3.71 -3.71\n\n\n\n\n26.5.7 Modeled Number of Zero Counts\nOnce again, we can compare the observed number of zero physhealth results to the expected number of zero values from the likelihood-based models.\n\nround(c(\"Obs\" = sum(ohioA_young$physhealth == 0),\n  \"Poisson\" = sum(dpois(0, fitted(mod_poiss1))),\n  \"NB\" = sum(dnbinom(0, mu = fitted(mod_nb1), size = mod_nb1$theta)),\n  \"ZIP\" = sum(predict(mod_zip1, type = \"prob\")[,1]),\n  \"ZINB\" = sum(predict(mod_zinb1, type = \"prob\")[,1])),0)\n\n    Obs Poisson      NB     ZIP    ZINB \n   1264     124    1250    1264    1264 \n\n\nSo, the Poisson model is clearly inappropriate, but the zero-inflated (Poisson and NB) and the negative binomial model all give reasonable fits in this regard.\n\n\n26.5.8 Rootogram for Zero-Inflated Negative Binomial model\nHere’s the rootogram for the zero-inflated negative binomial model.\n\ncountreg::rootogram(mod_zinb1, max = 30)\n\n\n\n\nAs in the ZIP model, the zero frequencies are perfectly matched here, but we can see that counts of 1 and 2 are now closer to the data we observe than in the ZIP model. We are still substantially underfitting values of 30.\n\n\n26.5.9 Specify the \\(R^2\\) and log (likelihood) values\nWe can calculate a proxy for \\(R^2\\) as the squared correlation of the fitted values and the observed values.\n\n# The correlation of observed and fitted values\n(zinb_r <- with(sm_zinb1, cor(physhealth, fitted)))\n\n[1] 0.1858969\n\n# R-square\nzinb_r^2\n\n[1] 0.03455767\n\n\n\nlogLik(mod_zinb1)\n\n'log Lik.' -3469.29 (df=7)\n\n\nHere, we have\n\n\n\n\n\n\n\n\n\nModel\nScale\n\\(R^2\\)\nlog(likelihood)\n\n\n\n\nZero-Inflated Negative Binomial\nComplex: log(physhealth)\n.035\n-3469.27\n\n\n\n\n\n26.5.10 Check model assumptions\nHere is a plot of residuals vs. fitted values on the original physhealth scale.\n\nggplot(sm_zinb1, aes(x = fitted, y = resid)) +\n    geom_point() +\n    labs(title = \"Residuals vs. Fitted `physhealth`\",\n         subtitle = \"Zero-Inflated Negative Binomial Regression model\")\n\n\n\n\n\n\n26.5.11 Predictions for Harry and Sally\nThe predictions from this zero-inflated negative binomial regression model are obtained as follows…\n\npredict(mod_zinb1, newdata = hs_data, type = \"response\")\n\n       1        2 \n6.206514 2.004963 \n\n\nAs we’ve seen in the past, when we use response as the type, the predictions fall on the original physhealth scale. The prediction for Harry is 6.2 days, and for Sally is 2.0 days."
  },
  {
    "objectID": "count3.html#a-hurdle-model-with-poisson",
    "href": "count3.html#a-hurdle-model-with-poisson",
    "title": "26  Zero-Inflated Models for Count Data",
    "section": "26.6 A “hurdle” model (with Poisson)",
    "text": "26.6 A “hurdle” model (with Poisson)\nMuch of the discussion here of hurdle models comes from Clay Ford at the University of Virginia3. Ford describes a hurdle model as follows:\n\nThe hurdle model is a two-part model that specifies one process for zero counts and another process for positive counts. The idea is that positive counts occur once a threshold is crossed, or put another way, a hurdle is cleared. If the hurdle is not cleared, then we have a count of 0.\n\n\nThe first part of the model is typically a binary logit model. This models whether an observation takes a positive count or not. The second part of the model is usually a truncated Poisson or Negative Binomial model. Truncated means we’re only fitting positive counts. If we were to fit a hurdle model to our [medicare] data, the interpretation would be that one process governs whether a patient visits a doctor or not, and another process governs how many visits are made.\n\nTo fit a hurdle model, we’ll use the hurdle function in the pscl package.\n\nmod_hur1 <- pscl::hurdle(physhealth ~ bmi_c + smoke100,\n                   dist = \"poisson\", zero.dist = \"binomial\",\n                   data = ohioA_young)\n\nsummary(mod_hur1)\n\n\nCall:\npscl::hurdle(formula = physhealth ~ bmi_c + smoke100, data = ohioA_young, \n    dist = \"poisson\", zero.dist = \"binomial\")\n\nPearson residuals:\n    Min      1Q  Median      3Q     Max \n-1.4403 -0.6987 -0.6139 -0.1946  9.2997 \n\nCount model coefficients (truncated poisson with log link):\n            Estimate Std. Error z value Pr(>|z|)    \n(Intercept) 1.997612   0.018797   106.3   <2e-16 ***\nbmi_c       0.018178   0.001398    13.0   <2e-16 ***\nsmoke100    0.393348   0.024889    15.8   <2e-16 ***\nZero hurdle model coefficients (binomial with logit link):\n             Estimate Std. Error z value Pr(>|z|)    \n(Intercept) -0.683509   0.062827 -10.879  < 2e-16 ***\nbmi_c        0.027777   0.006508   4.268 1.97e-05 ***\nsmoke100     0.238300   0.095301   2.500   0.0124 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 \n\nNumber of iterations in BFGS optimization: 14 \nLog-likelihood: -5680 on 6 Df\n\nconfint(mod_hur1)\n\n                        2.5 %      97.5 %\ncount_(Intercept)  1.96077105  2.03445261\ncount_bmi_c        0.01543722  0.02091901\ncount_smoke100     0.34456599  0.44212932\nzero_(Intercept)  -0.80664805 -0.56037031\nzero_bmi_c         0.01502124  0.04053230\nzero_smoke100      0.05151314  0.42508607\n\n\nWe are using the default settings here, using the same predictors for both models:\n\na Binomial model to predict the probability of physhealth = 0 given our predictors, as specified by the zero.dist argument in the hurdle function, and\na (truncated) Poisson model to predict the positive-count of physhealth given those same predictors, as specified by the dist argument in the hurdle function.\n\n\n26.6.1 Comparison to a null model\nTo show that this model fits better than the null model (the model with intercept only), we can compare them directly with a chi-squared test. Since we have two predictors in the full model, the degrees of freedom for this test is 2.\n\nmod_hurnull <- pscl::hurdle(physhealth ~ 1, dist = \"poisson\", \n                      zero.dist = \"binomial\", \n                      data = ohioA_young)\n\nsummary(mod_hurnull)\n\n\nCall:\npscl::hurdle(formula = physhealth ~ 1, data = ohioA_young, dist = \"poisson\", \n    zero.dist = \"binomial\")\n\nPearson residuals:\n    Min      1Q  Median      3Q     Max \n-0.6934 -0.6934 -0.6934 -0.2779  5.5399 \n\nCount model coefficients (truncated poisson with log link):\n            Estimate Std. Error z value Pr(>|z|)    \n(Intercept)  2.22765    0.01233   180.7   <2e-16 ***\nZero hurdle model coefficients (binomial with logit link):\n            Estimate Std. Error z value Pr(>|z|)    \n(Intercept)  -0.5768     0.0469   -12.3   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 \n\nNumber of iterations in BFGS optimization: 8 \nLog-likelihood: -5908 on 2 Df\n\npchisq(2 * (logLik(mod_hur1) - logLik(mod_hurnull)), df = 2, lower.tail = FALSE)\n\n'log Lik.' 8.577393e-100 (df=6)\n\n\n\n\n26.6.2 Comparison to a Poisson Model: Vuong test\n\nvuong(mod_hur1, mod_poiss1)\n\nVuong Non-Nested Hypothesis Test-Statistic: \n(test-statistic is asymptotically distributed N(0,1) under the\n null that the models are indistinguishible)\n-------------------------------------------------------------\n              Vuong z-statistic             H_A    p-value\nRaw                    19.90847 model1 > model2 < 2.22e-16\nAIC-corrected          19.89685 model1 > model2 < 2.22e-16\nBIC-corrected          19.86438 model1 > model2 < 2.22e-16\n\n\nThe hurdle model shows a detectable improvement over the standard Poisson model according to this test.\n\n\n26.6.3 Comparison to a Zero-Inflated Poisson Model: Vuong test\nIs the hurdle model comparable to the zero-inflated Poisson?\n\nvuong(mod_hur1, mod_zip1)\n\nVuong Non-Nested Hypothesis Test-Statistic: \n(test-statistic is asymptotically distributed N(0,1) under the\n null that the models are indistinguishible)\n-------------------------------------------------------------\n              Vuong z-statistic             H_A p-value\nRaw                   0.2046181 model1 > model2 0.41894\nAIC-corrected         0.2046181 model1 > model2 0.41894\nBIC-corrected         0.2046181 model1 > model2 0.41894\n\n\nThe hurdle model doesn’t show a substantial improvement over the zero-inflated Poisson model according to this test.\n\n\n26.6.4 The Fitted Equation\nThe form of the model equation for this hurdle also requires us to take two separate models into account. First we have a logistic regression model to predict the log odds of zero physhealth days. That takes care of the zeros. Then, to predict the number of physhealth days, we use a truncated Poisson model, which is truncated to produce only estimates greater than zero.\n\n\n26.6.5 Interpreting the Coefficients\nWe can exponentiate the logistic regression coefficients to obtain results in terms of odds ratios for that model, and that can be of some help in understanding the process behind excess zeros.\nAlso, exponentiating the coefficients of the count model help us describe those counts on the original scale of physhealth.\n\nexp(coef(mod_hur1))\n\ncount_(Intercept)       count_bmi_c    count_smoke100  zero_(Intercept) \n        7.3714308         1.0183443         1.4819335         0.5048423 \n       zero_bmi_c     zero_smoke100 \n        1.0281661         1.2690894 \n\n\nFor example,\n\nin the model for physhealth = 0, the odds of physhealth = 0 are 127% as high for subjects with smoke100 = 1 as for non-smokers with the same BMI.\nin the Poisson model for physhealth, the physhealth count is estimated to increase by 1.48 for smokers as compared to non-smokers with the same BMI.\n\n\n\n26.6.6 Testing the Predictors\nWe can test the model with and without bmi_c, for example, by fitting the model both ways, and comparing the results with either a Wald or Likelihood Ratio test, each of which is available in the lmtest package.\n\nmod_hur1_nobmi <- pscl::hurdle(physhealth ~ smoke100,\n                         dist = \"poisson\", \n                         zero.dist = \"binomial\",\n                         data = ohioA_young)\n\nlmtest::waldtest(mod_hur1, mod_hur1_nobmi)\n\nWald test\n\nModel 1: physhealth ~ bmi_c + smoke100\nModel 2: physhealth ~ smoke100\n  Res.Df Df  Chisq Pr(>Chisq)    \n1   1968                         \n2   1970 -2 187.19  < 2.2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\nlmtest::lrtest(mod_hur1, mod_hur1_nobmi)\n\nLikelihood ratio test\n\nModel 1: physhealth ~ bmi_c + smoke100\nModel 2: physhealth ~ smoke100\n  #Df  LogLik Df  Chisq Pr(>Chisq)    \n1   6 -5679.8                         \n2   4 -5769.5 -2 179.35  < 2.2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\n\n26.6.7 Store fitted values and residuals\nThe broom package does not work with the hurdle class of models. Again we need to build up the fitted values and residuals ourselves.\n\nsm_hur1 <- ohioA_young |>\n    mutate(fitted = fitted(mod_hur1, type = \"response\"),\n           resid = resid(mod_hur1, type = \"response\"))\n\nsm_hur1 |> \n    dplyr::select(physhealth, fitted, resid) |>\n    head()\n\n# A tibble: 6 × 3\n  physhealth fitted resid\n       <dbl>  <dbl> <dbl>\n1          0   2.21 -2.21\n2          0   2.28 -2.28\n3          0   3.12 -3.12\n4         30   5.27 24.7 \n5          0   3.17 -3.17\n6          0   3.71 -3.71\n\n\n\n\n26.6.8 Modeled Number of Zero Counts\nOnce again, we can compare the observed number of zero physhealth results to the expected number of zero values from the likelihood-based models.\n\nround(c(\"Obs\" = sum(ohioA_young$physhealth == 0),\n  \"Poisson\" = sum(dpois(0, fitted(mod_poiss1))),\n  \"NB\" = sum(dnbinom(0, mu = fitted(mod_nb1), size = mod_nb1$theta)),\n  \"ZIP\" = sum(predict(mod_zip1, type = \"prob\")[,1]),\n  \"ZINB\" = sum(predict(mod_zinb1, type = \"prob\")[,1]),\n  \"Hurdle\" = sum(predict(mod_hur1, type = \"prob\")[,1])),0)\n\n    Obs Poisson      NB     ZIP    ZINB  Hurdle \n   1264     124    1250    1264    1264    1264 \n\n\nThe hurdle model does about as well as the negative binomial and zero-inflated models. All but the Poisson give reasonable fits in this regard.\n\n\n26.6.9 Rootogram for Hurdle Model\n\ncountreg::rootogram(mod_hur1, max = 30)\n\n\n\n\nThe results are still not perfect, of course. In fitting the zeros exactly, we’re underfitting counts of 1, 2, and 30, and overfitting many of the counts between 6 and 20. We still have a problem here with overdispersion. That’s why we’ll consider a hurdle model with a negative binomial regression for the counts in a moment.\n\n\n26.6.10 Understanding the Modeled Counts in Detail\nThe expected mean count uses both parts of the hurdle model. Mathematically, we want…\n\\[\nE[y | {\\bf x}] = \\frac{1 - f_1(0 | {\\bf x})}{1 - f_2(0 | {\\bf x})} \\mu_2({\\bf x})\n\\]\nwhere\n\nour count of physhealth is \\(y\\)\nour predictors are represented by x\nand the expected count is the product of a ratio and a mean.\n\n\nThe ratio is the probability of a non-zero in the first process divided the probability of a non-zero in the second untruncated process. The f symbols represent distributions. Recall these are logistic and Poisson, respectively, by default but can be others. The mean is for the untruncated version of the positive-count process.\n\nIf we want to see the expected hurdle counts, we can get them using some clever applications of the predict function.\nThe first six expected mean counts (\\(E[y | {\\bf x}]\\) from the equation above) are:\n\nhead(predict(mod_hur1, type = \"response\"))\n\n       1        2        3        4        5        6 \n2.214179 2.281482 3.116124 5.267881 3.167890 3.712120 \n\n\nThe ratio of non-zero probabilities, \\(\\frac{1 - f_1(0 | {\\bf x})}{1 - f_2(0 | {\\bf x})}\\), from the mathematical expression above can be extracted by:\n\nhead(predict(mod_hur1, type = \"zero\"))\n\n        1         2         3         4         5         6 \n0.3173312 0.3221978 0.3344433 0.4806542 0.3372292 0.3649208 \n\n\nThe mean for the untruncated process, \\(\\mu_2({\\bf x})\\), can also be obtained by:\n\nhead(predict(mod_hur1, type = \"count\"))\n\n        1         2         3         4         5         6 \n 6.977501  7.081000  9.317347 10.959814  9.393877 10.172401 \n\n\nand we can multiply these last two pieces together to verify that they match our expected hurdle counts.\n\nhead(predict(mod_hur1, type = \"zero\") * predict(mod_hur1, type = \"count\"),5)\n\n       1        2        3        4        5 \n2.214179 2.281482 3.116124 5.267881 3.167890 \n\n\n\n\n26.6.11 Specify the \\(R^2\\) and log (likelihood) values\nWe can calculate a proxy for \\(R^2\\) as the squared correlation of the fitted values and the observed values.\n\n# The correlation of observed and fitted values\n(hur1_r <- with(sm_hur1, cor(physhealth, fitted)))\n\n[1] 0.1873104\n\n# R-square\nhur1_r^2\n\n[1] 0.03508517\n\n\n\nlogLik(mod_hur1)\n\n'log Lik.' -5679.792 (df=6)\n\n\nHere, we have\n\n\n\nModel\nScale\n\\(R^2\\)\nlog(likelihood)\n\n\n\n\nHurdle Model (Poisson)\nComplex: log(physhealth)\n.035\n-5679.83\n\n\n\n\n\n26.6.12 Check model assumptions\nHere is a plot of residuals vs. fitted values on the original physhealth scale.\n\nggplot(sm_hur1, aes(x = fitted, y = resid)) +\n    geom_point() +\n    labs(title = \"Residuals vs. Fitted `physhealth`\",\n         subtitle = \"Hurdle model with Poisson counts\")\n\n\n\n\n\n\n26.6.13 Predictions for Harry and Sally\nThe predictions from this zero-inflated negative binomial regression model are obtained as follows…\n\npredict(mod_hur1, newdata = hs_data, type = \"response\")\n\n       1        2 \n6.003689 2.057127 \n\n\nAs we’ve seen in the past, when we use response as the type, the predictions fall on the original physhealth scale. The prediction for Harry is 6.0 days, and for Sally is 2.1 days."
  },
  {
    "objectID": "count3.html#a-hurdle-model-with-negative-binomial-for-overdispersion",
    "href": "count3.html#a-hurdle-model-with-negative-binomial-for-overdispersion",
    "title": "26  Zero-Inflated Models for Count Data",
    "section": "26.7 A “hurdle” model (with negative binomial for overdispersion)",
    "text": "26.7 A “hurdle” model (with negative binomial for overdispersion)\nLet’s account for overdispersion better with a negative binomial model for the counts in our hurdle model. We specify that the positive-count process be fit with this NB model using dist = negbin.\n\nmod_hur_nb1 <- pscl::hurdle(physhealth ~ bmi_c + smoke100,\n                   dist = \"negbin\", zero.dist = \"binomial\",\n                   data = ohioA_young)\n\nsummary(mod_hur_nb1)\n\n\nCall:\npscl::hurdle(formula = physhealth ~ bmi_c + smoke100, data = ohioA_young, \n    dist = \"negbin\", zero.dist = \"binomial\")\n\nPearson residuals:\n    Min      1Q  Median      3Q     Max \n-0.5749 -0.4178 -0.3948 -0.1165  6.4023 \n\nCount model coefficients (truncated negbin with log link):\n            Estimate Std. Error z value Pr(>|z|)    \n(Intercept)  1.54170    0.10101  15.263  < 2e-16 ***\nbmi_c        0.02434    0.00677   3.595 0.000324 ***\nsmoke100     0.51792    0.11101   4.666 3.08e-06 ***\nLog(theta)  -0.88245    0.14653  -6.023 1.72e-09 ***\nZero hurdle model coefficients (binomial with logit link):\n             Estimate Std. Error z value Pr(>|z|)    \n(Intercept) -0.683509   0.062827 -10.879  < 2e-16 ***\nbmi_c        0.027777   0.006508   4.268 1.97e-05 ***\nsmoke100     0.238300   0.095301   2.500   0.0124 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 \n\nTheta: count = 0.4138\nNumber of iterations in BFGS optimization: 17 \nLog-likelihood: -3469 on 7 Df\n\nconfint(mod_hur_nb1)\n\n                        2.5 %      97.5 %\ncount_(Intercept)  1.34373051  1.73966938\ncount_bmi_c        0.01107002  0.03760871\ncount_smoke100     0.30035006  0.73549531\nzero_(Intercept)  -0.80664805 -0.56037031\nzero_bmi_c         0.01502124  0.04053230\nzero_smoke100      0.05151314  0.42508607\n\n\n\n26.7.1 Comparison to a null model\nTo show that this model fits better than the null model (the model with intercept only), we can compare them directly with a chi-squared test. Since we have two predictors in the full model, the degrees of freedom for this test is 2.\n\nmod_hur_nb_null <- pscl::hurdle(physhealth ~ 1, dist = \"negbin\", \n                      zero.dist = \"binomial\", \n                      data = ohioA_young)\n\nsummary(mod_hur_nb_null)\n\n\nCall:\npscl::hurdle(formula = physhealth ~ 1, data = ohioA_young, dist = \"negbin\", \n    zero.dist = \"binomial\")\n\nPearson residuals:\n    Min      1Q  Median      3Q     Max \n-0.4048 -0.4048 -0.4048 -0.1622  3.2340 \n\nCount model coefficients (truncated negbin with log link):\n            Estimate Std. Error z value Pr(>|z|)    \n(Intercept)  1.77653    0.09641  18.427  < 2e-16 ***\nLog(theta)  -1.04455    0.16123  -6.479 9.25e-11 ***\nZero hurdle model coefficients (binomial with logit link):\n            Estimate Std. Error z value Pr(>|z|)    \n(Intercept)  -0.5768     0.0469   -12.3   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 \n\nTheta: count = 0.3518\nNumber of iterations in BFGS optimization: 12 \nLog-likelihood: -3498 on 3 Df\n\npchisq(2 * (logLik(mod_hur_nb1) - logLik(mod_hur_nb_null)), df = 2, lower.tail = FALSE)\n\n'log Lik.' 2.17419e-13 (df=7)\n\n\n\n\n26.7.2 Comparison to a Negative Binomial Model: Vuong test\n\nvuong(mod_hur_nb1, mod_nb1)\n\nVuong Non-Nested Hypothesis Test-Statistic: \n(test-statistic is asymptotically distributed N(0,1) under the\n null that the models are indistinguishible)\n-------------------------------------------------------------\n              Vuong z-statistic             H_A    p-value\nRaw                   3.0953118 model1 > model2 0.00098303\nAIC-corrected         2.4837306 model1 > model2 0.00650071\nBIC-corrected         0.7750288 model1 > model2 0.21916133\n\n\nThe hurdle model is a significant improvement over the standard negative binomial model according to the raw and AIC-corrected versions of this test, but not the BIC-corrected version.\n\n\n26.7.3 Comparison to a Zero-Inflated NB Model: Vuong test\nIs the hurdle model comparable to the zero-inflated Poisson?\n\nvuong(mod_hur_nb1, mod_zinb1)\n\nVuong Non-Nested Hypothesis Test-Statistic: \n(test-statistic is asymptotically distributed N(0,1) under the\n null that the models are indistinguishible)\n-------------------------------------------------------------\n              Vuong z-statistic             H_A p-value\nRaw                     0.91178 model1 > model2 0.18094\nAIC-corrected           0.91178 model1 > model2 0.18094\nBIC-corrected           0.91178 model1 > model2 0.18094\n\n\nThe hurdle model is just barely a significant improvement over the zero-inflated Negative Binomial model.\n\n\n26.7.4 Comparing the Hurdle Models with AIC and BIC\n\nAIC(mod_hur1); BIC(mod_hur1)\n\n[1] 11371.58\n\n\n[1] 11405.11\n\n\n\nAIC(mod_hur_nb1); BIC(mod_hur_nb1)\n\n[1] 6952.186\n\n\n[1] 6991.301\n\n\nThe negative binomial approach certainly looks better than the Poisson here.\n\n\n26.7.5 The Fitted Equation\nThe form of the model equation for this hurdle also requires us to take two separate models into account. First we have a logistic regression model to predict the log odds of zero physhealth days. That takes care of the zeros. Then, to predict the number of physhealth days, we use a truncated negative binomial model, which is truncated to produce only estimates greater than zero, with \\(\\theta\\) estimated as exp(-1.123) or 0.325.\n\n\n26.7.6 Interpreting the Coefficients\nWe can exponentiate the logistic regression coefficients to obtain results in terms of odds ratios for that model, and that can be of some help in understanding the process behind excess zeros.\n\nexp(coef(mod_hur_nb1))\n\ncount_(Intercept)       count_bmi_c    count_smoke100  zero_(Intercept) \n        4.6725266         1.0246380         1.6785372         0.5048423 \n       zero_bmi_c     zero_smoke100 \n        1.0281661         1.2690894 \n\n\nFor example,\n\nin the model for physhealth = 0, the odds of physhealth = 0 are 127% as high for subjects with smoke100 = 1 as for non-smokers with the same BMI.\n\n\n\n26.7.7 Testing the Predictors\nWe can test the model with and without bmi_c, for example, by fitting the model both ways, and comparing the results with either a Wald or Likelihood Ratio test, each of which is available in the lmtest package.\n\nmod_hurnb1_nobmi <- pscl::hurdle(physhealth ~ smoke100,\n                         dist = \"negbin\", \n                         zero.dist = \"binomial\",\n                         data = ohioA_young)\n\nlmtest::waldtest(mod_hur_nb1, mod_hurnb1_nobmi)\n\nWald test\n\nModel 1: physhealth ~ bmi_c + smoke100\nModel 2: physhealth ~ smoke100\n  Res.Df Df  Chisq Pr(>Chisq)    \n1   1967                         \n2   1969 -2 31.141  1.729e-07 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\nlmtest::lrtest(mod_hur_nb1, mod_hurnb1_nobmi)\n\nLikelihood ratio test\n\nModel 1: physhealth ~ bmi_c + smoke100\nModel 2: physhealth ~ smoke100\n  #Df  LogLik Df  Chisq Pr(>Chisq)    \n1   7 -3469.1                         \n2   5 -3485.0 -2 31.812  1.236e-07 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\n\n26.7.8 Store fitted values and residuals\nAgain we need to build up the fitted values and residuals, without broom to help.\n\nsm_hur_nb1 <- ohioA_young |>\n    mutate(fitted = fitted(mod_hur_nb1, type = \"response\"),\n           resid = resid(mod_hur_nb1, type = \"response\"))\n\nsm_hur_nb1 |> \n    dplyr::select(physhealth, fitted, resid) |>\n    head()\n\n# A tibble: 6 × 3\n  physhealth fitted resid\n       <dbl>  <dbl> <dbl>\n1          0   2.16 -2.16\n2          0   2.23 -2.23\n3          0   3.09 -3.09\n4         30   5.37 24.6 \n5          0   3.15 -3.15\n6          0   3.72 -3.72\n\n\n\n\n26.7.9 Rootogram for NB Hurdle Model\n\ncountreg::rootogram(mod_hur_nb1, max = 30)\n\n\n\n\nThis improves the situation, but we’re still underfitting the 30s.\n\n\n26.7.10 Specify the \\(R^2\\) and log (likelihood) values\nWe can calculate a proxy for \\(R^2\\) as the squared correlation of the fitted values and the observed values.\n\n# The correlation of observed and fitted values\n(hurnb1_r <- with(sm_hur_nb1, cor(physhealth, fitted)))\n\n[1] 0.1856393\n\n# R-square\nhurnb1_r^2\n\n[1] 0.03446194\n\n\n\nlogLik(mod_hur_nb1)\n\n'log Lik.' -3469.093 (df=7)\n\n\nHere, we have\n\n\n\n\n\n\n\n\n\nModel\nScale\n\\(R^2\\)\nlog(likelihood)\n\n\n\n\nHurdle Model (Neg. Bin.)\nComplex: log(physhealth)\n.035\n-3469.07\n\n\n\n\n\n26.7.11 Check model assumptions\nHere is a plot of residuals vs. fitted values on the original physhealth scale.\n\nggplot(sm_hur_nb1, aes(x = fitted, y = resid)) +\n    geom_point() +\n    labs(title = \"Residuals vs. Fitted `physhealth`\",\n         subtitle = \"Hurdle model with Negative Binomial counts\")\n\n\n\n\n\n\n26.7.12 Predictions for Harry and Sally\nThe predictions from this zero-inflated negative binomial regression model are obtained as follows…\n\npredict(mod_hur_nb1, newdata = hs_data, type = \"response\")\n\n       1        2 \n6.222041 2.007095 \n\n\nThe prediction for Harry is 6.22 days, and for Sally is 2.01 days.\n\n\n26.7.13 Note: Fitting a Different Hurdle Model for Counts and Pr(zero)\nSuppose we wanted to use only bmi_c to predict the probability of a zero count, but use both predictors in the model for the positive counts. We use the | command.\n\nmod_hur_new1 <- \n    pscl::hurdle(physhealth ~ bmi_c + smoke100 | bmi_c,\n           dist = \"negbin\", zero.dist = \"binomial\",\n           data = ohioA_young)\n\nsummary(mod_hur_new1)\n\n\nCall:\npscl::hurdle(formula = physhealth ~ bmi_c + smoke100 | bmi_c, data = ohioA_young, \n    dist = \"negbin\", zero.dist = \"binomial\")\n\nPearson residuals:\n    Min      1Q  Median      3Q     Max \n-0.5630 -0.4179 -0.3971 -0.1150  6.1958 \n\nCount model coefficients (truncated negbin with log link):\n            Estimate Std. Error z value Pr(>|z|)    \n(Intercept)  1.54170    0.10101  15.263  < 2e-16 ***\nbmi_c        0.02434    0.00677   3.595 0.000324 ***\nsmoke100     0.51792    0.11101   4.666 3.08e-06 ***\nLog(theta)  -0.88245    0.14653  -6.023 1.72e-09 ***\nZero hurdle model coefficients (binomial with logit link):\n            Estimate Std. Error z value Pr(>|z|)    \n(Intercept) -0.58186    0.04718 -12.332  < 2e-16 ***\nbmi_c        0.02853    0.00649   4.396  1.1e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 \n\nTheta: count = 0.4138\nNumber of iterations in BFGS optimization: 17 \nLog-likelihood: -3472 on 6 Df\n\n\n\n\n26.7.14 Hanging Rootogram for this new Hurdle Model\n\ncountreg::rootogram(mod_hur_new1, max = 30)\n\n\n\n\nNot a meaningful improvement, certainly."
  },
  {
    "objectID": "count3.html#a-tobit-censored-regression-model",
    "href": "count3.html#a-tobit-censored-regression-model",
    "title": "26  Zero-Inflated Models for Count Data",
    "section": "26.8 A Tobit (Censored) Regression Model",
    "text": "26.8 A Tobit (Censored) Regression Model\nThe idea of the tobit model (sometimes called a censored regression model) is to estimate associations for outcomes where we can see either left-censoring (censoring from below) or right-censoring (censoring from above.)\nConsider the variable physhealth, which is restricted to fall between 0 and 30 by the way the measure was constructed. But supposed we think about a broader and latent (unobserved) variable describing physical health. Among the people with physhealth = 0, some would be incredible athletes and others would be in much poorer physical health but still healthy enough to truthfully answer 0. On the other end, some of the people responding 30 are in substantially worse physical health than others with that same response.\n\nCensoring from below takes place when values at or below a threshold (in this case 0) take that value.\nCensoring from above takes place when values at or above a threshold (here, 30) take that value.\n\nSeveral examples of tobit analysis are available at https://stats.idre.ucla.edu/r/dae/tobit-models/, which is my primary source for the material here on those models.\nThe tobit model postulates that the value 0 in our model is just the lower limit of the underlying measure of poor physical health that we would actually observe in the population if we had a stronger measure. Similarly, we’ll postulate that 30 is just the upper limit of “poor health” that we can see. The approach I’ll take to run the tobit model comes from the vglm function in the VGAM package.\nHere’s the model, and its summary. Note that the default Lower value for a tobit model is 0, so we didn’t technically have to list that here.\n\nmod_tob1 <- vglm(physhealth ~ bmi_c + smoke100, \n                 tobit(Lower = 0, Upper = 30),\n                 type.fitted = \"censored\",\n                 data = ohioA_young)\n\nsummary(mod_tob1)\n\n\nCall:\nvglm(formula = physhealth ~ bmi_c + smoke100, family = tobit(Lower = 0, \n    Upper = 30), data = ohioA_young, type.fitted = \"censored\")\n\nCoefficients: \n              Estimate Std. Error z value Pr(>|z|)    \n(Intercept):1 -9.29104    0.75509 -12.305  < 2e-16 ***\n(Intercept):2  2.87081    0.03028  94.818  < 2e-16 ***\nbmi_c          0.37434    0.06537   5.727 1.02e-08 ***\nsmoke100       4.35390    0.97223   4.478 7.52e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nNames of linear predictors: mu, loglink(sd)\n\nLog-likelihood: -3420.605 on 3944 degrees of freedom\n\nNumber of Fisher scoring iterations: 6 \n\nNo Hauck-Donner effect found in any of the estimates\n\nconfint(mod_tob1)\n\n                    2.5 %     97.5 %\n(Intercept):1 -10.7709932 -7.8110877\n(Intercept):2   2.8114721  2.9301560\nbmi_c           0.2462244  0.5024551\nsmoke100        2.4483714  6.2594277\n\n\n\n26.8.1 The Fitted Equation\nBecause we’ve used the censoring approach, our model will limit its predictions to the range of [0, 30], where any predictions outside that range are censored. Values below 0 are fitted as 0, and values above 30 are fitted as 30.\nThe model equation is\nphyshealth = -9.29 + 0.37 bmi_c + 4.35 smoke100\n\n\n26.8.2 Interpreting the Coefficients\nTobit model regression coefficients are interpreted as we would a set of OLS coefficients, except that the linear effect is on the uncensored latent variable, rather than on the observed outcome.\nIn our case,\n\na one-unit increase in bmi_c is associated with a 0.37 day increase in the predicted value of physhealth, holding smoke100 constant\na move from smoke100 = 0 to 1 is associated with a 4.35 day increase in the predicted value of physhealth, holding bmi_c constant\nthe coefficient labeled (Intercept):1 is the intercept for the model and is the predicted value of physhealth when smoke100 = 0 and bmi_c = 0. Note that this value is -9.29, which is outside the range of physhealth values we observed.\nthe coefficient labeled (Intercept):2 is a statistic we can use after we exponentiate it, as follows:\n\nhere (Intercept):2 = 2.87, and exp(2.87) = 17.6370182, which is analogous to the square root of the residual variance in OLS regression, which is summarized for our OLS model as Residual standard error: 17.64.\n\n\n\n\n26.8.3 Testing the Predictors\nWe can test the model with and without bmi_c, for example, by fitting the model both ways, and comparing the results with either a Wald or Likelihood Ratio test, each of which is available in the lmtest package.\n\nmod_tob_nobmi <- vglm(physhealth ~ smoke100, \n                      tobit(Lower = 0, Upper = 30),\n                      type.fitted = \"censored\",\n                      data = ohioA_young)\n\nlmtest::waldtest(mod_tob1, mod_tob_nobmi)\n\nWald test\n\nModel 1: physhealth ~ bmi_c + smoke100\nModel 2: physhealth ~ smoke100\n  Res.Df Df  Chisq Pr(>Chisq)    \n1   3944                         \n2   3945 -1 32.796  1.023e-08 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nThe likelihood ratio test we have used in some other settings isn’t available here.\n\n\n26.8.4 Store fitted values and residuals\nThe residuals and fitted values from the tobit model can be stored and then summarized in several ways:\n\nsm_tob1 <- ohioA_young |>\n    mutate(fitted = fitted(mod_tob1,\n                           type.fitted = \"censored\"),\n           resid = physhealth - fitted)\n\nsm_tob1 |> \n    dplyr::select(physhealth, fitted, resid) |>\n    head()\n\n# A tibble: 6 × 3\n  physhealth fitted[,1] resid[,1]\n       <dbl>      <dbl>     <dbl>\n1          0          0         0\n2          0          0         0\n3          0          0         0\n4         30          0        30\n5          0          0         0\n6          0          0         0\n\n\n\n\n26.8.5 Building Something Like a Rootogram\nBuilding a rootogram is tricky for a tobit model, to say the least, but we can approximate a piece of the issue by plotting the rounded fitted values against the observed physhealth data.\n\nggplot(sm_tob1, aes(x = physhealth, y = round(fitted))) +\n    geom_jitter(width = 0.2) + \n    geom_abline(intercept = 0, slope = 1, col = \"red\")\n\n\n\n\nNote that the model never predicts a subject to have an underlying physhealth worse than about 13 (remember that larger numbers indicate worse health here.)\n\n\n26.8.6 Tables of the Observed and Fitted physhealth from Tobit\n\naddmargins(table(round(sm_tob1$physhealth)))\n\n\n   0    1    2    3    4    5    6    7    8   10   12   13   14   15   16   17 \n1264  112  139   78   29   50    5   40    9   41    4    1   24   38    2    1 \n  18   20   21   25   26   28   29   30  Sum \n   1   23    3    8    2    1    1   98 1974 \n\n\n\naddmargins(table(round(sm_tob1$fitted)))\n\n\n   0    1    2    3    4    5    6    8   13  Sum \n1924   20    7   10    4    6    1    1    1 1974 \n\n\n\n\n26.8.7 Specify the \\(R^2\\) and log (likelihood) values\nWe can calculate a proxy for \\(R^2\\) as the squared correlation of the fitted values and the observed values.\n\n# The correlation of observed and fitted values\n(tob1_r <- with(sm_tob1, cor(physhealth, fitted)))\n\n           [,1]\n[1,] 0.09329619\n\n# R-square\ntob1_r^2\n\n            [,1]\n[1,] 0.008704178\n\n\n\nlogLik(mod_tob1)\n\n[1] -3420.605\n\n\nHere, we have\n\n\n\nModel\nScale\n\\(R^2\\)\nlog(likelihood)\n\n\n\n\nTobit\nphyshealth\n.008\n-3420.58\n\n\n\n\n\n26.8.8 Check model assumptions\nHere is a plot of residuals vs. fitted values.\n\nggplot(sm_tob1, aes(x = fitted, y = resid)) +\n    geom_point() +\n    labs(title = \"Residuals vs. Fitted Values for Tobit 1\")\n\n\n\n\nHere is a normal Q-Q plot of the Tobit Model 1 residuals.\n\nqqnorm(sm_tob1$resid)\n\n\n\n\n\n\n26.8.9 Predictions for Harry and Sally\nThe predictions from this tobit model are obtained as follows…\n\npredict(mod_tob1, newdata = hs_data, type = \"response\")\n\n        [,1]\n1  -1.193743\n2 -11.162739\n\n\nThe prediction for both Harry and Sally under the tobit model would be truncated to 0 days."
  },
  {
    "objectID": "ordinaloutcome.html#r-setup-used-here",
    "href": "ordinaloutcome.html#r-setup-used-here",
    "title": "27  Modeling an Ordinal Categorical Outcome",
    "section": "27.1 R Setup Used Here",
    "text": "27.1 R Setup Used Here\n\nknitr::opts_chunk$set(comment = NA)\n\nlibrary(broom)\nlibrary(gmodels)\nlibrary(MASS)\nlibrary(nnet)\nlibrary(rms)\nlibrary(tidyverse) \n\ntheme_set(theme_bw())\n\n\n27.1.1 Data Load\n\nsmart_oh <- readRDS(\"data/smart_ohio.Rds\")"
  },
  {
    "objectID": "ordinaloutcome.html#a-subset-of-the-ohio-smart-data",
    "href": "ordinaloutcome.html#a-subset-of-the-ohio-smart-data",
    "title": "27  Modeling an Ordinal Categorical Outcome",
    "section": "27.2 A subset of the Ohio SMART data",
    "text": "27.2 A subset of the Ohio SMART data\nLet’s consider the following data, which uses part of the smart_oh data we built in Chapter 6. The outcome we’ll study now is genhealth, which has five ordered categories. I’ll include the subset of all observations in smart_oh with complete data on these 7 variables.\n\n\n\n\n\n\n\nVariable\nDescription\n\n\n\n\nSEQNO\nSubject identification code\n\n\ngenhealth\nFive categories (1 = Excellent, 2 = Very Good, 3 = Good, 4 = Fair, 5 = Poor) on general health\n\n\nphyshealth\nNow thinking about your physical health, which includes physical illness and injury, for how many days during the past 30 days was your physical health not good?\n\n\nveg_day\nmean number of vegetable servings consumed per day\n\n\ncostprob\n1 indicates Yes to “Was there a time in the past 12 months when you needed to see a doctor but could not because of cost?”, and 0 otherwise.\n\n\nincomegroup\n8 income groups from < 10,000 to 75,000 or more\n\n\nbmi\nbody-mass index\n\n\n\nTo make my life easier later, I’m going to drop any subjects with missing data on these variables. I’m also going to drop the subjects who have no missing data, but have a listed bmi above 60.\n\nsm1 <- smart_oh |>\n    select(SEQNO, genhealth, physhealth, costprob, veg_day,\n           incomegroup, bmi) |>\n    filter(bmi <= 60) |>\n    drop_na()\n\nIn total, we have 5394 subjects in the sm1 sample.\n\n27.2.1 Several Ways of Storing Multi-Categorical data\nWe will store the information in our outcome, genhealth in both a numeric form (gen_n) and an ordered factor (gen_h) with some abbreviated labels) because we’ll have some use for each approach in this material.\n\nsm1 <- sm1 |>\n    mutate(genh = fct_recode(genhealth,\n                             \"1-E\" = \"1_Excellent\",\n                             \"2_VG\" = \"2_VeryGood\",\n                             \"3_G\" = \"3_Good\",\n                             \"4_F\" = \"4_Fair\",\n                             \"5_P\" = \"5_Poor\"),\n           genh = factor(genh, ordered = TRUE),\n           gen_n = as.numeric(genhealth))\n\nsm1 |> count(genh, gen_n, genhealth)\n\n# A tibble: 5 × 4\n  genh  gen_n genhealth       n\n  <ord> <dbl> <fct>       <int>\n1 1-E       1 1_Excellent   822\n2 2_VG      2 2_VeryGood   1805\n3 3_G       3 3_Good       1667\n4 4_F       4 4_Fair        801\n5 5_P       5 5_Poor        299"
  },
  {
    "objectID": "ordinaloutcome.html#building-cross-tabulations",
    "href": "ordinaloutcome.html#building-cross-tabulations",
    "title": "27  Modeling an Ordinal Categorical Outcome",
    "section": "27.3 Building Cross-Tabulations",
    "text": "27.3 Building Cross-Tabulations\nIs income group associated with general health?\n\n27.3.1 Using base table functions\n\naddmargins(table(sm1$incomegroup, sm1$genh))\n\n        \n          1-E 2_VG  3_G  4_F  5_P  Sum\n  0-9K     14   45   60   74   40  233\n  10-14K   12   41   66   93   46  258\n  15-19K   40   76  119   96   61  392\n  20-24K   51  129  175  100   50  505\n  25-34K   51  172  215  123   36  597\n  35-49K   97  270  303  118   24  812\n  50-74K  128  337  265   94   16  840\n  75K+    429  735  464  103   26 1757\n  Sum     822 1805 1667  801  299 5394\n\n\nMore people answer Very Good and Good than choose the other categories. It might be easier to look at percentages here.\n\n27.3.1.1 Adding percentages within each row\nHere are the percentages giving each genhealth response within each income group.\n\naddmargins(\n    round(100*prop.table(\n        table(sm1$incomegroup, sm1$genh)\n        ,1)\n        ,1)\n    )\n\n        \n           1-E  2_VG   3_G   4_F   5_P   Sum\n  0-9K     6.0  19.3  25.8  31.8  17.2 100.1\n  10-14K   4.7  15.9  25.6  36.0  17.8 100.0\n  15-19K  10.2  19.4  30.4  24.5  15.6 100.1\n  20-24K  10.1  25.5  34.7  19.8   9.9 100.0\n  25-34K   8.5  28.8  36.0  20.6   6.0  99.9\n  35-49K  11.9  33.3  37.3  14.5   3.0 100.0\n  50-74K  15.2  40.1  31.5  11.2   1.9  99.9\n  75K+    24.4  41.8  26.4   5.9   1.5 100.0\n  Sum     91.0 224.1 247.7 164.3  72.9 800.0\n\n\nSo, for example, 11.3% of the genhealth responses in subjects with incomes between 25 and 34 thousand dollars were Excellent.\n\n\n27.3.1.2 Adding percentages within each column\nHere are the percentages in each incomegroup within each genhealth response.\n\naddmargins(\n    round(100*prop.table(\n        table(sm1$incomegroup, sm1$genh)\n        ,2)\n        ,1)\n    )\n\n        \n           1-E  2_VG   3_G   4_F   5_P   Sum\n  0-9K     1.7   2.5   3.6   9.2  13.4  30.4\n  10-14K   1.5   2.3   4.0  11.6  15.4  34.8\n  15-19K   4.9   4.2   7.1  12.0  20.4  48.6\n  20-24K   6.2   7.1  10.5  12.5  16.7  53.0\n  25-34K   6.2   9.5  12.9  15.4  12.0  56.0\n  35-49K  11.8  15.0  18.2  14.7   8.0  67.7\n  50-74K  15.6  18.7  15.9  11.7   5.4  67.3\n  75K+    52.2  40.7  27.8  12.9   8.7 142.3\n  Sum    100.1 100.0 100.0 100.0 100.0 500.1\n\n\nFrom this table, we see that 7.4% of the Excellent genhealth responses were given by people with incomes between 25 and 34 thousand dollars.\n\n\n\n27.3.2 Using xtabs\nThe xtabs function provides a formula method for obtaining cross-tabulations.\n\nxtabs(~ incomegroup + genh, data = sm1)\n\n           genh\nincomegroup 1-E 2_VG 3_G 4_F 5_P\n     0-9K    14   45  60  74  40\n     10-14K  12   41  66  93  46\n     15-19K  40   76 119  96  61\n     20-24K  51  129 175 100  50\n     25-34K  51  172 215 123  36\n     35-49K  97  270 303 118  24\n     50-74K 128  337 265  94  16\n     75K+   429  735 464 103  26\n\n\n\n\n27.3.3 Storing a table in a tibble\nWe can store the elements of a cross-tabulation in a tibble, like this:\n\n(sm1.tableA <- sm1 |> count(incomegroup, genh))\n\n# A tibble: 40 × 3\n   incomegroup genh      n\n   <fct>       <ord> <int>\n 1 0-9K        1-E      14\n 2 0-9K        2_VG     45\n 3 0-9K        3_G      60\n 4 0-9K        4_F      74\n 5 0-9K        5_P      40\n 6 10-14K      1-E      12\n 7 10-14K      2_VG     41\n 8 10-14K      3_G      66\n 9 10-14K      4_F      93\n10 10-14K      5_P      46\n# … with 30 more rows\n\n\nFrom such a tibble, we can visualize the data in many ways, but we can also return to xtabs and include the frequencies (n) in that setup.\n\nxtabs(n ~ incomegroup + genh, data = sm1.tableA)\n\n           genh\nincomegroup 1-E 2_VG 3_G 4_F 5_P\n     0-9K    14   45  60  74  40\n     10-14K  12   41  66  93  46\n     15-19K  40   76 119  96  61\n     20-24K  51  129 175 100  50\n     25-34K  51  172 215 123  36\n     35-49K  97  270 303 118  24\n     50-74K 128  337 265  94  16\n     75K+   429  735 464 103  26\n\n\nAnd, we can get the \\(\\chi^2\\) test of independence, with:\n\nsummary(xtabs(n ~ incomegroup + genh, data = sm1.tableA))\n\nCall: xtabs(formula = n ~ incomegroup + genh, data = sm1.tableA)\nNumber of cases in table: 5394 \nNumber of factors: 2 \nTest for independence of all factors:\n    Chisq = 894.2, df = 28, p-value = 3.216e-170\n\n\n\n\n27.3.4 Using CrossTable from the gmodels package\nThe CrossTable function from the gmodels package produces a cross-tabulation with various counts and proportions like people often generate with SPSS and SAS.\n\nCrossTable(sm1$incomegroup, sm1$genh, chisq = T)\n\n\n \n   Cell Contents\n|-------------------------|\n|                       N |\n| Chi-square contribution |\n|           N / Row Total |\n|           N / Col Total |\n|         N / Table Total |\n|-------------------------|\n\n \nTotal Observations in Table:  5394 \n\n \n                | sm1$genh \nsm1$incomegroup |       1-E |      2_VG |       3_G |       4_F |       5_P | Row Total | \n----------------|-----------|-----------|-----------|-----------|-----------|-----------|\n           0-9K |        14 |        45 |        60 |        74 |        40 |       233 | \n                |    13.027 |    13.941 |     2.002 |    44.865 |    56.796 |           | \n                |     0.060 |     0.193 |     0.258 |     0.318 |     0.172 |     0.043 | \n                |     0.017 |     0.025 |     0.036 |     0.092 |     0.134 |           | \n                |     0.003 |     0.008 |     0.011 |     0.014 |     0.007 |           | \n----------------|-----------|-----------|-----------|-----------|-----------|-----------|\n         10-14K |        12 |        41 |        66 |        93 |        46 |       258 | \n                |    18.980 |    23.806 |     2.366 |    78.061 |    70.259 |           | \n                |     0.047 |     0.159 |     0.256 |     0.360 |     0.178 |     0.048 | \n                |     0.015 |     0.023 |     0.040 |     0.116 |     0.154 |           | \n                |     0.002 |     0.008 |     0.012 |     0.017 |     0.009 |           | \n----------------|-----------|-----------|-----------|-----------|-----------|-----------|\n         15-19K |        40 |        76 |       119 |        96 |        61 |       392 | \n                |     6.521 |    23.208 |     0.038 |    24.531 |    70.973 |           | \n                |     0.102 |     0.194 |     0.304 |     0.245 |     0.156 |     0.073 | \n                |     0.049 |     0.042 |     0.071 |     0.120 |     0.204 |           | \n                |     0.007 |     0.014 |     0.022 |     0.018 |     0.011 |           | \n----------------|-----------|-----------|-----------|-----------|-----------|-----------|\n         20-24K |        51 |       129 |       175 |       100 |        50 |       505 | \n                |     8.756 |     9.463 |     2.296 |     8.340 |    17.301 |           | \n                |     0.101 |     0.255 |     0.347 |     0.198 |     0.099 |     0.094 | \n                |     0.062 |     0.071 |     0.105 |     0.125 |     0.167 |           | \n                |     0.009 |     0.024 |     0.032 |     0.019 |     0.009 |           | \n----------------|-----------|-----------|-----------|-----------|-----------|-----------|\n         25-34K |        51 |       172 |       215 |       123 |        36 |       597 | \n                |    17.567 |     3.862 |     5.042 |    13.307 |     0.255 |           | \n                |     0.085 |     0.288 |     0.360 |     0.206 |     0.060 |     0.111 | \n                |     0.062 |     0.095 |     0.129 |     0.154 |     0.120 |           | \n                |     0.009 |     0.032 |     0.040 |     0.023 |     0.007 |           | \n----------------|-----------|-----------|-----------|-----------|-----------|-----------|\n         35-49K |        97 |       270 |       303 |       118 |        24 |       812 | \n                |     5.779 |     0.011 |    10.798 |     0.055 |     9.808 |           | \n                |     0.119 |     0.333 |     0.373 |     0.145 |     0.030 |     0.151 | \n                |     0.118 |     0.150 |     0.182 |     0.147 |     0.080 |           | \n                |     0.018 |     0.050 |     0.056 |     0.022 |     0.004 |           | \n----------------|-----------|-----------|-----------|-----------|-----------|-----------|\n         50-74K |       128 |       337 |       265 |        94 |        16 |       840 | \n                |     0.000 |    11.121 |     0.112 |     7.575 |    20.061 |           | \n                |     0.152 |     0.401 |     0.315 |     0.112 |     0.019 |     0.156 | \n                |     0.156 |     0.187 |     0.159 |     0.117 |     0.054 |           | \n                |     0.024 |     0.062 |     0.049 |     0.017 |     0.003 |           | \n----------------|-----------|-----------|-----------|-----------|-----------|-----------|\n           75K+ |       429 |       735 |       464 |       103 |        26 |      1757 | \n                |    97.108 |    36.780 |    11.492 |    95.573 |    52.335 |           | \n                |     0.244 |     0.418 |     0.264 |     0.059 |     0.015 |     0.326 | \n                |     0.522 |     0.407 |     0.278 |     0.129 |     0.087 |           | \n                |     0.080 |     0.136 |     0.086 |     0.019 |     0.005 |           | \n----------------|-----------|-----------|-----------|-----------|-----------|-----------|\n   Column Total |       822 |      1805 |      1667 |       801 |       299 |      5394 | \n                |     0.152 |     0.335 |     0.309 |     0.148 |     0.055 |           | \n----------------|-----------|-----------|-----------|-----------|-----------|-----------|\n\n \nStatistics for All Table Factors\n\n\nPearson's Chi-squared test \n------------------------------------------------------------\nChi^2 =  894.1685     d.f. =  28     p =  3.216132e-170"
  },
  {
    "objectID": "ordinaloutcome.html#graphing-categorical-data",
    "href": "ordinaloutcome.html#graphing-categorical-data",
    "title": "27  Modeling an Ordinal Categorical Outcome",
    "section": "27.4 Graphing Categorical Data",
    "text": "27.4 Graphing Categorical Data\n\n27.4.1 A Bar Chart for a Single Variable\n\nggplot(sm1, aes(x = genhealth, fill = genhealth)) + \n    geom_bar() +\n    scale_fill_brewer(palette = \"Set1\") +\n    guides(fill = \"none\")\n\n\n\n\nor, you might prefer to plot percentages, perhaps like this:\n\nggplot(sm1, aes(x = genhealth, fill = genhealth)) + \n    geom_bar(aes(y = (..count..)/sum(..count..))) +\n    geom_text(aes(y = (..count..)/sum(..count..), \n                  label = scales::percent((..count..) / \n                                        sum(..count..))),\n              stat = \"count\", vjust = 1, \n              color = \"white\", size = 5) +\n    scale_y_continuous(labels = scales::percent) +\n    scale_fill_brewer(palette = \"Dark2\") +\n    guides(fill = \"none\") + \n    labs(y = \"Percentage\")\n\nWarning: The dot-dot notation (`..count..`) was deprecated in ggplot2 3.4.0.\nℹ Please use `after_stat(count)` instead.\n\n\n\n\n\nUse bar charts, rather than pie charts.\n\n\n27.4.2 A Counts Chart for a 2-Way Cross-Tabulation\n\nggplot(sm1, aes(x = genhealth, y = incomegroup)) + \n    geom_count()"
  },
  {
    "objectID": "ordinaloutcome.html#building-a-model-for-genh-using-veg_day",
    "href": "ordinaloutcome.html#building-a-model-for-genh-using-veg_day",
    "title": "27  Modeling an Ordinal Categorical Outcome",
    "section": "27.5 Building a Model for genh using veg_day",
    "text": "27.5 Building a Model for genh using veg_day\nTo begin, we’ll predict each subject’s genh response using just one predictor, veg_day.\n\n27.5.1 A little EDA\nLet’s start with a quick table of summary statistics.\n\nsm1 |> group_by(genh) |>\n    summarize(n(), mean(veg_day), sd(veg_day), median(veg_day))\n\n# A tibble: 5 × 5\n  genh  `n()` `mean(veg_day)` `sd(veg_day)` `median(veg_day)`\n  <ord> <int>           <dbl>         <dbl>             <dbl>\n1 1-E     822            2.16          1.46              1.87\n2 2_VG   1805            1.99          1.13              1.78\n3 3_G    1667            1.86          1.11              1.71\n4 4_F     801            1.74          1.18              1.57\n5 5_P     299            1.71          1.06              1.57\n\n\nTo actually see what’s going on, we might build a comparison boxplot, or violin plot. The plot below shows both, together, with the violin plot helping to indicate the skewed nature of the veg_day data and the boxplot indicating quartiles and outlying values within each genhealth category.\n\nggplot(sm1, aes(x = genhealth, y = veg_day)) +\n    geom_violin(aes(fill = genhealth), trim = TRUE) +\n    geom_boxplot(width = 0.2) +\n    guides(fill = \"none\", color = \"none\") +\n    theme_bw()\n\n\n\n\n\n\n27.5.2 Describing the Proportional-Odds Cumulative Logit Model\nTo fit the ordinal logistic regression model (specifically, a proportional-odds cumulative-logit model) in this situation, we’ll use the polr function in the MASS package.\n\nOur outcome is genh, which has five ordered levels, with 1-E best and 5-P worst.\nOur model will include one quantitative predictor, veg_day.\n\nThe model will have four logit equations:\n\none estimating the log odds that genh will be less than or equal to 1 (i.e. genhealth = 1_Excellent,)\none estimating the log odds that genh \\(\\leq\\) 2 (i.e. genhealth = 1_Excellent or 2_VeryGood,)\nanother estimating the log odds that genh \\(\\leq\\) 3 (i.e. genhealth = 1_Excellent, 2_VeryGood or 3_Good,) and, finally,\none estimating the log odds that genh \\(\\leq\\) 4 (i.e. genhealth = 1_Excellent, 2_VeryGood, 3_Good or 4_Fair)\n\nThat’s all we need to estimate the five categories, since Pr(genh \\(\\leq\\) 5) = 1, because (5_Poor) is the maximum category for genhealth.\nWe’ll have a total of five free parameters when we add in the slope for veg_day, and I’ll label these parameters as \\(\\zeta_1, \\zeta_2, \\zeta_3, \\zeta_4\\) and \\(\\beta_1\\). The \\(\\zeta\\)s are read as “zeta” values, and the people who built the polr function use that term.\nThe four logistic equations that will be fit differ only by their intercepts. They are:\n\\[\nlogit[Pr(genh \\leq 1)] = log \\frac{Pr(genh \\leq 1}{Pr(genh > 1)} = \\zeta_1 - \\beta_1 veg_day\n\\]\nwhich describes the log odds of a genh value of 1 (Excellent) as compared to a genh value greater than 1 (which includes Very Good, Good, Fair and Poor).\nThe second logit model is:\n\\[\nlogit[Pr(genh \\leq 2)] = log \\frac{Pr(genh \\leq 2}{Pr(genh > 2)} = \\zeta_2 - \\beta_1 veg_day\n\\]\nwhich describes the log odds of a genh value of 1 (Excellent) or 2 (Very Good) as compared to a genh value greater than 2 (which includes Good, Fair and Poor).\nNext we have:\n\\[\nlogit[Pr(genh \\leq 3)] = log \\frac{Pr(genh \\leq 3}{Pr(genh > 3)} = \\zeta_3 - \\beta_1 veg_day\n\\]\nwhich describes the log odds of a genh value of 1 (Excellent) or 2 (Very Good) or 3 (Good) as compared to a genh value greater than 3 (which includes Fair and Poor).\nFinally, we have\n\\[\nlogit[Pr(genh \\leq 4)] = log \\frac{Pr(genh \\leq 4}{Pr(genh > 4)} = \\zeta_4 - \\beta_1 veg_day\n\\]\nwhich describes the log odds of a genh value of 4 or less, which includes Excellent, Very Good, Good and Fair as compared to a genh value greater than 4 (which is Poor).\nAgain, the intercept term is the only piece that varies across the four equations.\nIn this case, a positive coefficient \\(\\beta_1\\) for veg_day means that increasing the value of veg_day would increase the genh category (describing a worse level of general health, since higher values of genh are associated with worse health.)\n\n\n27.5.3 Fitting a Proportional Odds Logistic Regression with polr\nOur model m1 will use proportional odds logistic regression (sometimes called an ordered logit model) to predict genh on the basis of veg_day. The polr function from the MASS package will be our main tool. Note that we include Hess = TRUE to retain what is called the Hessian matrix, which lets R calculate standard errors more effectively in summary and other follow-up descriptions of the model.\n\nm1 <- polr(genh ~ veg_day, \n            data = sm1, Hess = TRUE)\n\nsummary(m1)\n\nCall:\npolr(formula = genh ~ veg_day, data = sm1, Hess = TRUE)\n\nCoefficients:\n          Value Std. Error t value\nveg_day -0.1847    0.02178   -8.48\n\nIntercepts:\n         Value    Std. Error t value \n1-E|2_VG  -2.0866   0.0584   -35.7590\n2_VG|3_G  -0.4065   0.0498    -8.1621\n3_G|4_F    1.0202   0.0521    19.5771\n4_F|5_P    2.5002   0.0710    35.2163\n\nResidual Deviance: 15669.85 \nAIC: 15679.85 \n\nconfint(m1)\n\nWaiting for profiling to be done...\n\n\n     2.5 %     97.5 % \n-0.2277073 -0.1423088"
  },
  {
    "objectID": "ordinaloutcome.html#interpreting-model-m1",
    "href": "ordinaloutcome.html#interpreting-model-m1",
    "title": "27  Modeling an Ordinal Categorical Outcome",
    "section": "27.6 Interpreting Model m1",
    "text": "27.6 Interpreting Model m1\n\n27.6.1 Looking at Predictions\nConsider two individuals:\n\nHarry, who eats an average of 2.0 servings of vegetables per day, so Harry’s veg_day = 2, and\nSally, who eats an average of 1.0 serving of vegetables per day, so Sally’s veg_day = 1.\n\nWe’re going to start by using our model m1 to predict the genh for Harry and Sally, so we can see the effect (on the predicted genh probabilities) of a change of one unit in veg_day.\nFor example, what are the log odds that Harry, with veg_day = 2, will describe his genh as Excellent (genh \\(\\leq\\) 1)?\n\\[\nlogit[Pr(genh \\leq 1)] = \\zeta_1 - \\beta_1 veg\\_day\n\\]\n\\[\nlogit[Pr(genh \\leq 1)] = -2.0866 - (-0.1847) veg\\_day\n\\]\n\\[\nlogit[Pr(genh \\leq 1)] = -2.0866 - (-0.1847) (2) = -1.7172\n\\]\nThat’s not much help. So we’ll convert it to a probability by taking the inverse logit. The formula is\n\\[\nPr(genh \\leq 1) = \\frac{exp(\\zeta_1 + \\beta_1 veg_day)}{1 + exp(\\zeta_1 + \\beta_1 veg_day)} =\n\\frac{exp(-1.7172)}{1 + exp(-1.7172)} = \\frac{0.180}{1.180} = 0.15\n\\]\nSo the model estimates a 15% probability that Harry will describe his genh as Excellent.\nOK. Now, what are the log odds that Harry, who eats 2 servings per day, will describe his genh as either Excellent or Very Good (genh \\(\\leq\\) 2)?\n\\[\nlogit[Pr(genh \\leq 2)] = \\zeta_2 - \\beta_1 veg\\_day\n\\]\n\\[\nlogit[Pr(genh \\leq 2)] = -0.4065 - (-0.1847) veg\\_day \\\\\n\\]\n\\[\nlogit[Pr(genh \\leq 2)] = -0.4065 - (-0.1847) (2) = -0.0371\n\\]\nAgain, we’ll convert this to a probability by taking the inverse logit.\n\\[\nPr(genh \\leq 2) = \\frac{exp(\\zeta_2 + \\beta_1 veg_day)}{1 + exp(\\zeta_2 + \\beta_1 veg_day)} =\n\\frac{exp(-0.0371)}{1 + exp(-0.0371)} = \\frac{0.964}{1.964} = 0.49\n\\]\nSo, the model estimates a probability of .49 that Harry will describe his genh as either Excellent or Very Good, so by subtraction, that’s a probability of .34 that Harry describes his genh as Very Good.\nHappily, that’s the last time we’ll calculate this by hand.\n\n\n27.6.2 Making Predictions for Harry (and Sally) with predict\nSuppose Harry eats 2 servings of vegetables per day on average, and Sally eats 1.\n\ntemp.dat <- data.frame(name = c(\"Harry\", \"Sally\"), \n                       veg_day = c(2,1))\n\npredict(m1, temp.dat, type = \"p\")\n\n        1-E      2_VG       3_G       4_F        5_P\n1 0.1522351 0.3385119 0.3097906 0.1457864 0.05367596\n2 0.1298931 0.3148971 0.3246105 0.1667285 0.06387071\n\n\nThe predicted probabilities of falling into each category of genh are:\n\n\n\nSubject\nveg_day\nPr(1_E)\nPr(2_VG)\nPr(3_G)\nPr(4_F)\nPr(5_P)\n\n\n\n\nHarry\n2\n15.2\n33.9\n31.0\n14.6\n5.4\n\n\nSally\n1\n13.0\n31.4\n32.5\n16.7\n6.4\n\n\n\n\nHarry has a higher predicted probability of lower (healthier) values of genh. Specifically, Harry has a higher predicted probability than Sally of falling into the Excellent and Very Good categories, and a lower probability than Sally of falling into the Good, Fair and Poor categories.\nThis means that Harry, with a higher veg_day is predicted to have, on average, a lower (that is to say, healthier) value of genh.\nAs we’ll see, this association will be indicated by a negative coefficient of veg_day in the proportional odds logistic regression model.\n\n\n\n27.6.3 Predicting the actual classification of genh\nThe default prediction approach actually returns the predicted genh classification for Harry and Sally, which is just the classification with the largest predicted probability. Here, for Harry that is Very Good, and for Sally, that’s Good.\n\npredict(m1, temp.dat)\n\n[1] 2_VG 3_G \nLevels: 1-E 2_VG 3_G 4_F 5_P\n\n\n\n\n27.6.4 A Cross-Tabulation of Predictions?\n\naddmargins(table(predict(m1), sm1$genh))\n\n      \n        1-E 2_VG  3_G  4_F  5_P  Sum\n  1-E     6    3    3    3    0   15\n  2_VG  647 1398 1198  525  192 3960\n  3_G   169  404  466  273  107 1419\n  4_F     0    0    0    0    0    0\n  5_P     0    0    0    0    0    0\n  Sum   822 1805 1667  801  299 5394\n\n\nThe m1 model classifies all subjects in the sm1 sample as either Excellent, Very Good or Good, and most subjects as Very Good or Good.\n\n\n27.6.5 The Fitted Model Equations\n\nsummary(m1)\n\nCall:\npolr(formula = genh ~ veg_day, data = sm1, Hess = TRUE)\n\nCoefficients:\n          Value Std. Error t value\nveg_day -0.1847    0.02178   -8.48\n\nIntercepts:\n         Value    Std. Error t value \n1-E|2_VG  -2.0866   0.0584   -35.7590\n2_VG|3_G  -0.4065   0.0498    -8.1621\n3_G|4_F    1.0202   0.0521    19.5771\n4_F|5_P    2.5002   0.0710    35.2163\n\nResidual Deviance: 15669.85 \nAIC: 15679.85 \n\n\nThe first part of the output provides coefficient estimates for the veg_day predictor, and these are followed by the estimates for the various model intercepts. Plugging in the estimates, we have:\n\\[\nlogit[Pr(genh \\leq 1)] = -2.0866 - (-0.1847) veg_day\n\\]\n\\[\nlogit[Pr(genh \\leq 2)]  = -0.4065 - (-0.1847) veg_day\n\\]\n\\[\nlogit[Pr(genh \\leq 3)]  =  1.0202 - (-0.1847) veg_day\n\\]\n\\[\nlogit[Pr(genh \\leq 4)] =  2.5002 - (-0.1847) veg_day\n\\]\nNote that we can obtain these pieces separately as follows:\n\nm1$zeta\n\n  1-E|2_VG   2_VG|3_G    3_G|4_F    4_F|5_P \n-2.0866313 -0.4064704  1.0202035  2.5001655 \n\n\nshows the boundary intercepts, and\n\nm1$coefficients\n\n   veg_day \n-0.1847272 \n\n\nshows the regression coefficient for veg_day.\n\n\n27.6.6 Interpreting the veg_day coefficient\nThe first part of the output provides coefficient estimates for the veg_day predictor.\n\nThe estimated slope for veg_day is -0.1847\n\nRemember Harry and Sally, who have the same values of bmi and costprob, but Harry eats one more serving than Sally does. We noted that Harry is predicted by the model to have a smaller (i.e. healthier) genh response than Sally.\nSo a negative coefficient here means that higher values of veg_day are associated with more of the probability distribution falling in lower values of genh.\nWe usually don’t interpret this slope (on the log odds scale) directly, but rather exponentiate it.\n\n\n\n\n27.6.7 Exponentiating the Slope Coefficient to facilitate Interpretation\nWe can compute the odds ratio associated with veg_day and its confidence interval as follows…\n\nexp(coef(m1))\n\n  veg_day \n0.8313311 \n\nexp(confint(m1))\n\nWaiting for profiling to be done...\n\n\n    2.5 %    97.5 % \n0.7963573 0.8673534 \n\n\n\nSo, if Harry eats one more serving of vegetables than Sally, our model predicts that Harry will have 83.1% of the odds of Sally of having a larger genh score. That means that Harry is likelier to have a smaller genh score.\n\nSince genh gets larger as a person’s general health gets worse (moves from Excellent towards Poor), this means that since Harry is predicted to have smaller odds of a larger genh score, he is also predicted to have smaller odds of worse general health.\nOur 95% confidence interval around that estimated odds ratio of 0.831 is (0.796, 0.867). Since that interval is entirely below 1, the odds of having the larger (worse) genh for Harry are detectably lower than the odds for Sally.\nSo, an increase in veg_day is associated with smaller (better) genh scores.\n\n\n\n\n27.6.8 Comparison to a Null Model\nWe can fit a model with intercepts only to test the significance of veg_day in our model m1, using the anova function.\n\nm0 <- polr(genh ~ 1, data = sm1)\n\nanova(m1, m0)\n\nLikelihood ratio tests of ordinal regression models\n\nResponse: genh\n    Model Resid. df Resid. Dev   Test    Df LR stat. Pr(Chi)\n1       1      5390   15744.89                              \n2 veg_day      5389   15669.85 1 vs 2     1 75.04297       0\n\n\nWe could also compare model m1 to the null model m0 with AIC or BIC.\n\nAIC(m1, m0)\n\n   df      AIC\nm1  5 15679.85\nm0  4 15752.89\n\n\n\nBIC(m1,m0)\n\n   df      BIC\nm1  5 15712.81\nm0  4 15779.26"
  },
  {
    "objectID": "ordinaloutcome.html#the-assumption-of-proportional-odds",
    "href": "ordinaloutcome.html#the-assumption-of-proportional-odds",
    "title": "27  Modeling an Ordinal Categorical Outcome",
    "section": "27.7 The Assumption of Proportional Odds",
    "text": "27.7 The Assumption of Proportional Odds\nLet us calculate the odds for all levels of genh if a person eats two servings of vegetables. First, we’ll get the probabilities, in another way, to demonstrate how to do so…\n\n(prob.2 <- exp(m1$zeta - 2*m1$coefficients)/(1 + exp(m1$zeta - 2*m1$coefficients)))\n\n 1-E|2_VG  2_VG|3_G   3_G|4_F   4_F|5_P \n0.1522351 0.4907471 0.8005376 0.9463240 \n\n(prob.1 <- exp(m1$zeta - 1*m1$coefficients)/(1 + exp(m1$zeta - 1*m1$coefficients)))\n\n 1-E|2_VG  2_VG|3_G   3_G|4_F   4_F|5_P \n0.1298931 0.4447902 0.7694008 0.9361293 \n\n\nNow, we’ll calculate the odds, first for a subject eating two servings:\n\n(odds.2 = prob.2/(1-prob.2))\n\n  1-E|2_VG   2_VG|3_G    3_G|4_F    4_F|5_P \n 0.1795724  0.9636607  4.0134766 17.6303153 \n\n\nAnd here are the odds, for a subject eating one serving per day:\n\n(odds.1 = prob.1/(1-prob.1))\n\n  1-E|2_VG   2_VG|3_G    3_G|4_F    4_F|5_P \n 0.1492841  0.8011211  3.3365277 14.6566285 \n\n\nNow, let’s take the ratio of the odds for someone who eats two servings over the odds for someone who eats one.\n\nodds.2/odds.1\n\n1-E|2_VG 2_VG|3_G  3_G|4_F  4_F|5_P \n 1.20289  1.20289  1.20289  1.20289 \n\n\nThey are all the same. The odds ratios are equal, which means they are proportional. For any level of genh, the estimated odds that a person who eats 2 servings has better (lower) genh is about 1.2 times the odds for someone who eats one serving. Those who eat more vegetables have higher odds of better (lower) genh. Less than 1 means lower odds, and more than 1 means greater odds.\nNow, let’s take the log of the odds ratios:\n\nlog(odds.2/odds.1)\n\n 1-E|2_VG  2_VG|3_G   3_G|4_F   4_F|5_P \n0.1847272 0.1847272 0.1847272 0.1847272 \n\n\nThat should be familiar. It is the slope coefficient in the model summary, without the minus sign. R tacks on a minus sign so that higher levels of predictors correspond to the ordinal outcome falling in the higher end of its scale.\nIf we exponentiate the slope estimated by R (-0.1847), we get 0.83. If we have two people, and A eats one more serving of vegetables on average than B, then the estimated odds of A having a higher ‘genh’ (i.e. worse general health) are 83% as high as B’s.\n\n27.7.1 Testing the Proportional Odds Assumption\nOne way to test the proportional odds assumption is to compare the fit of the proportional odds logistic regression to a model that does not make that assumption. A natural candidate is a multinomial logit model, which is typically used to model unordered multi-categorical outcomes, and fits a slope to each level of the genh outcome in this case, as opposed to the proportional odds logit, which fits only one slope across all levels.\nSince the proportional odds logistic regression model is nested in the multinomial logit, we can perform a likelihood ratio test. To do this, we first fit the multinomial logit model, with the multinom function from the nnet package.\n\n(m1_multi <- multinom(genh ~ veg_day, data = sm1))\n\n# weights:  15 (8 variable)\ninitial  value 8681.308100 \niter  10 value 7890.985276\nfinal  value 7835.248471 \nconverged\n\n\nCall:\nmultinom(formula = genh ~ veg_day, data = sm1)\n\nCoefficients:\n     (Intercept)     veg_day\n2_VG   0.9791063 -0.09296694\n3_G    1.0911990 -0.19260067\n4_F    0.5708594 -0.31080687\n5_P   -0.3583310 -0.34340619\n\nResidual Deviance: 15670.5 \nAIC: 15686.5 \n\n\nThe multinomial logit fits four intercepts and four slopes, for a total of 8 estimated parameters. The proportional odds logit, as we’ve seen, fits four intercepts and one slope, for a total of 5. The difference is 3, and we use that number in the sequence below to build our test of the proportional odds assumption.\n\nLL_1 <- logLik(m1)\nLL_1m <- logLik(m1_multi)\n(G <- -2 * (LL_1[1] - LL_1m[1]))\n\n[1] -0.6488392\n\npchisq(G, 3, lower.tail = FALSE)\n\n[1] 1\n\n\nThe p value is very large, so it indicates that the proportional odds model fits about as well as the more complex multinomial logit. A non-significant p value here isn’t always the best way to assess the proportional odds assumption, but it does provide some evidence of model adequacy."
  },
  {
    "objectID": "ordinaloutcome.html#can-model-m1-be-fit-using-rms-tools",
    "href": "ordinaloutcome.html#can-model-m1-be-fit-using-rms-tools",
    "title": "27  Modeling an Ordinal Categorical Outcome",
    "section": "27.8 Can model m1 be fit using rms tools?",
    "text": "27.8 Can model m1 be fit using rms tools?\nYes.\n\nd <- datadist(sm1)\noptions(datadist = \"d\")\nm1_lrm <- lrm(genh ~ veg_day, data = sm1, x = T, y = T)\n\nm1_lrm\n\nLogistic Regression Model\n \n lrm(formula = genh ~ veg_day, data = sm1, x = T, y = T)\n \n \n Frequencies of Responses\n \n  1-E 2_VG  3_G  4_F  5_P \n  822 1805 1667  801  299 \n \n                        Model Likelihood     Discrimination    Rank Discrim.    \n                              Ratio Test            Indexes          Indexes    \n Obs          5394    LR chi2      75.04     R2       0.015    C       0.555    \n max |deriv| 3e-13    d.f.             1    R2(1,5394)0.014    Dxy     0.111    \n                      Pr(> chi2) <0.0001    R2(1,4995)0.015    gamma   0.111    \n                                             Brier    0.247    tau-a   0.082    \n \n         Coef    S.E.   Wald Z Pr(>|Z|)\n y>=2_VG  2.0866 0.0584  35.76 <0.0001 \n y>=3_G   0.4065 0.0498   8.16 <0.0001 \n y>=4_F  -1.0202 0.0521 -19.58 <0.0001 \n y>=5_P  -2.5002 0.0710 -35.22 <0.0001 \n veg_day -0.1847 0.0218  -8.48 <0.0001 \n \n\n\nThe model is highly significant (remember the large sample size) but very weak, with a Nagelkerke \\(R^2\\) of 0.015, and a C statistic of 0.555.\n\nsummary(m1_lrm)\n\n             Effects              Response : genh \n\n Factor      Low  High Diff. Effect   S.E.     Lower 0.95 Upper 0.95\n veg_day     1.21 2.36 1.15  -0.21243 0.025051 -0.26153   -0.16334  \n  Odds Ratio 1.21 2.36 1.15   0.80861       NA  0.76987    0.84931  \n\n\nA change from 1.21 to 2.36 servings in veg_day is associated with an odds ratio of 0.81, with 95% confidence interval (0.77, 0.85). Since these values are all below 1, we have a clear indication of a statistically detectable effect of veg_day with higher veg_day associated with lower genh, which means, in this case, better health.\nThere is also a tool in rms called orm which may be used to fit a wide array of ordinal regression models. I suggest you read Frank Harrell’s book on Regression Modeling Strategies if you want to learn more."
  },
  {
    "objectID": "ordinaloutcome.html#building-a-three-predictor-model",
    "href": "ordinaloutcome.html#building-a-three-predictor-model",
    "title": "27  Modeling an Ordinal Categorical Outcome",
    "section": "27.9 Building a Three-Predictor Model",
    "text": "27.9 Building a Three-Predictor Model\nNow, we’ll model genh using veg_day, bmi and costprob.\n\n27.9.1 Scatterplot Matrix\n\nGGally::ggpairs(sm1 |> \n                    select(bmi, veg_day, costprob, genh))\n\n\n\n\nWe might choose to plot the costprob data as a binary factor, rather than the raw 0-1 numbers included above, but not at this time.\n\n\n27.9.2 Our Three-Predictor Model, m2\n\nm2 <- polr(genh ~ veg_day + bmi + costprob, data = sm1)\n\nsummary(m2)\n\n\nRe-fitting to get Hessian\n\n\nCall:\npolr(formula = genh ~ veg_day + bmi + costprob, data = sm1)\n\nCoefficients:\n            Value Std. Error t value\nveg_day  -0.17130   0.021783  -7.864\nbmi       0.06673   0.003855  17.311\ncostprob  0.96825   0.084871  11.409\n\nIntercepts:\n         Value   Std. Error t value\n1-E|2_VG -0.1252  0.1229    -1.0183\n2_VG|3_G  1.6358  0.1234    13.2572\n3_G|4_F   3.1534  0.1294    24.3755\n4_F|5_P   4.6881  0.1412    33.1928\n\nResidual Deviance: 15229.24 \nAIC: 15243.24 \n\n\nThis model contains four intercepts (to cover the five genh categories) and three slopes (one each for veg_day, bmi and costprob.)\n\n\n27.9.3 Does the three-predictor model outperform m1?\n\nanova(m1, m2)\n\nLikelihood ratio tests of ordinal regression models\n\nResponse: genh\n                     Model Resid. df Resid. Dev   Test    Df LR stat. Pr(Chi)\n1                  veg_day      5389   15669.85                              \n2 veg_day + bmi + costprob      5387   15229.24 1 vs 2     2 440.6041       0\n\n\nThere is a statistically significant improvement in fit from model 1 to model 2. The AIC and BIC are also better for the three-predictor model than they were for the model with veg_day alone.\n\nAIC(m1, m2)\n\n   df      AIC\nm1  5 15679.85\nm2  7 15243.24\n\nBIC(m1, m2)\n\n   df      BIC\nm1  5 15712.81\nm2  7 15289.40\n\n\n\n\n27.9.4 Wald tests for individual predictors\nTo obtain the appropriate Wald tests, we can use lrm to fit the model instead.\n\nd <- datadist(sm1)\noptions(datadist = \"d\")\nm2_lrm <- lrm(genh ~ veg_day + bmi + costprob, \n              data = sm1, x = T, y = T)\nm2_lrm\n\nLogistic Regression Model\n \n lrm(formula = genh ~ veg_day + bmi + costprob, data = sm1, x = T, \n     y = T)\n \n \n Frequencies of Responses\n \n  1-E 2_VG  3_G  4_F  5_P \n  822 1805 1667  801  299 \n \n                        Model Likelihood     Discrimination    Rank Discrim.    \n                              Ratio Test            Indexes          Indexes    \n Obs          5394    LR chi2     515.65     R2       0.096    C       0.629    \n max |deriv| 4e-09    d.f.             3    R2(3,5394)0.091    Dxy     0.258    \n                      Pr(> chi2) <0.0001    R2(3,4995)0.098    gamma   0.258    \n                                             Brier    0.231    tau-a   0.192    \n \n          Coef    S.E.   Wald Z Pr(>|Z|)\n y>=2_VG   0.1252 0.1229   1.02 0.3085  \n y>=3_G   -1.6358 0.1234 -13.26 <0.0001 \n y>=4_F   -3.1534 0.1294 -24.38 <0.0001 \n y>=5_P   -4.6881 0.1412 -33.19 <0.0001 \n veg_day  -0.1713 0.0218  -7.86 <0.0001 \n bmi       0.0667 0.0039  17.31 <0.0001 \n costprob  0.9683 0.0849  11.41 <0.0001 \n \n\n\nIt appears that each of the added predictors (bmi and costprob) adds statistically detectable value to the model.\n\n\n27.9.5 A Cross-Tabulation of Predictions?\n\naddmargins(table(predict(m2), sm1$genh))\n\n      \n        1-E 2_VG  3_G  4_F  5_P  Sum\n  1-E     6    5    4    1    0   16\n  2_VG  686 1295  950  388  141 3460\n  3_G   128  495  672  374  135 1804\n  4_F     1    9   38   36   20  104\n  5_P     1    1    3    2    3   10\n  Sum   822 1805 1667  801  299 5394\n\n\nAt least the m2 model predicted that a few of the cases will fall in the Fair and Poor categories, but still, this isn’t impressive.\n\n\n27.9.6 Interpreting the Effect Sizes\nWe can do this in two ways:\n\nBy exponentiating the polr output, which shows the effect of increasing each predictor by a single unit\n\nIncreasing veg_day by 1 serving while holding the other predictors constant is associated with reducing the odds (by a factor of 0.84 with 95% CI 0.81, 0.88)) of higher values of genh: hence increasing veg_day is associated with increasing the odds of a response indicating better health.\nIncreasing bmi by 1 kg/m2 while holding the other predictors constant is associated with increasing the odds (by a factor of 1.07 with 95% CI 1.06, 1.08)) of higher values of genh: hence increasing bmi is associated with reducing the odds of a response indicating better health.\nIncreasing costprob from 0 to 1 while holding the other predictors constant is associated with an increase (by a factor of 2.63 with 95% CI 2.23, 3.11)) of a higher genh value. Since higher genh values indicate worse health, those with costprob = 1 are modeled to have generally worse health.\n\n\n\nexp(coef(m2))\n\n  veg_day       bmi  costprob \n0.8425722 1.0690045 2.6333356 \n\nexp(confint(m2))\n\nWaiting for profiling to be done...\n\n\n\nRe-fitting to get Hessian\n\n\n             2.5 %   97.5 %\nveg_day  0.8071346 0.879096\nbmi      1.0609722 1.077126\ncostprob 2.2301783 3.110633\n\n\n\nOr by looking at the summary provided by lrm, which like all such summaries produced by rms shows the impact of moving from the 25th to the 75th percentile on all continuous predictors.\n\n\nsummary(m2_lrm)\n\n             Effects              Response : genh \n\n Factor      Low   High   Diff.  Effect   S.E.     Lower 0.95 Upper 0.95\n veg_day      1.21  2.360 1.1500 -0.19699 0.025051 -0.24609   -0.14789  \n  Odds Ratio  1.21  2.360 1.1500  0.82120       NA  0.78185    0.86252  \n bmi         24.33 31.988 7.6575  0.51097 0.029515  0.45312    0.56882  \n  Odds Ratio 24.33 31.988 7.6575  1.66690       NA  1.57320    1.76620  \n costprob     0.00  1.000 1.0000  0.96825 0.084871  0.80191    1.13460  \n  Odds Ratio  0.00  1.000 1.0000  2.63330       NA  2.22980    3.10990  \n\nplot(summary(m2_lrm))\n\n\n\n\n\n\n27.9.7 Quality of the Model Fit\nModel m2, as we can see from the m2_lrm output, is still weak, with a Nagelkerke \\(R^2\\) of 0.10, and a C statistic of 0.63.\n\n\n27.9.8 Validating the Summary Statistics in m2_lrm\n\nset.seed(43203); validate(m2_lrm)\n\n          index.orig training    test optimism index.corrected  n\nDxy           0.2583   0.2625  0.2579   0.0046          0.2537 40\nR2            0.0964   0.0989  0.0960   0.0029          0.0935 40\nIntercept     0.0000   0.0000 -0.0023   0.0023         -0.0023 40\nSlope         1.0000   1.0000  0.9876   0.0124          0.9876 40\nEmax          0.0000   0.0000  0.0031   0.0031          0.0031 40\nD             0.0954   0.0980  0.0950   0.0030          0.0924 40\nU            -0.0004  -0.0004 -1.5149   1.5146         -1.5149 40\nQ             0.0958   0.0984  1.6099  -1.5115          1.6073 40\nB             0.2314   0.2308  0.2315  -0.0007          0.2321 40\ng             0.6199   0.6278  0.6182   0.0096          0.6103 40\ngp            0.1434   0.1448  0.1430   0.0018          0.1417 40\n\n\nAs in our work with binary logistic regression, we can convert the index-corrected Dxy to an index-corrected C with C = 0.5 + (Dxy/2). Both the \\(R^2\\) and C statistics are pretty consistent with what we saw above.\n\n\n27.9.9 Testing the Proportional Odds Assumption\nAgain, we’ll fit the analogous multinomial logit model, with the multinom function from the nnet package.\n\n(m2_multi <- multinom(genh ~ veg_day + bmi + costprob, \n                      data = sm1))\n\n# weights:  25 (16 variable)\ninitial  value 8681.308100 \niter  10 value 8025.745934\niter  20 value 7605.878993\nfinal  value 7595.767250 \nconverged\n\n\nCall:\nmultinom(formula = genh ~ veg_day + bmi + costprob, data = sm1)\n\nCoefficients:\n     (Intercept)    veg_day        bmi  costprob\n2_VG  -0.9126285 -0.0905958 0.06947231 0.3258568\n3_G   -2.1886806 -0.1893454 0.11552563 1.0488262\n4_F   -3.4095145 -0.3056028 0.13679908 1.4422074\n5_P   -4.2629564 -0.3384199 0.13178846 1.8612088\n\nResidual Deviance: 15191.53 \nAIC: 15223.53 \n\n\nThe multinomial logit fits four intercepts and 12 slopes, for a total of 16 estimated parameters. The proportional odds logit in model m2, as we’ve seen, fits four intercepts and three slopes, for a total of 7. The difference is 9, and we use that number in the sequence below to build our test of the proportional odds assumption.\n\nLL_2 <- logLik(m2)\nLL_2m <- logLik(m2_multi)\n(G <- -2 * (LL_2[1] - LL_2m[1]))\n\n[1] 37.70952\n\npchisq(G, 9, lower.tail = FALSE)\n\n[1] 1.965186e-05\n\n\nThe result is highly significant, suggesting that we have a problem somewhere with the proportional odds assumption. When this happens, I suggest you build the following plot of score residuals:\n\npar(mfrow = c(2,2))\nresid(m2_lrm, 'score.binary', pl=TRUE)\npar(mfrow= c(1,1))\n\n\n\n\nFrom this plot, bmi (especially) and costprob vary as we move from the Very Good toward the Poor cutpoints, relative to veg_day, which is more stable.\n\n\n27.9.10 Plotting the Fitted Model\n\n27.9.10.1 Nomogram\n\nfun.ge3 <- function(x) plogis(x - m2_lrm$coef[1] + m2_lrm$coef[2])\nfun.ge4 <- function(x) plogis(x - m2_lrm$coef[1] + m2_lrm$coef[3])\nfun.ge5 <- function(x) plogis(x - m2_lrm$coef[1] + m2_lrm$coef[4])\n\nplot(nomogram(m2_lrm, fun=list('Prob Y >= 2 (VG or worse)' = plogis, \n                               'Prob Y >= 3 (Good or worse)' = fun.ge3,\n                               'Prob Y >= 4 (Fair or Poor)' = fun.ge4,\n                               'Prob Y = 5 (Poor)' = fun.ge5)))\n\n\n\n\n\n\n27.9.10.2 Using Predict and showing mean prediction on 1-5 scale\n\nggplot(Predict(m2_lrm, fun = Mean(m2_lrm, code = TRUE)))\n\n\n\n\nThe nomogram and Predict results would be more interesting, of course, if we included a spline or interaction term. Let’s do that in model m3_lrm, and also add the incomegroup information."
  },
  {
    "objectID": "ordinaloutcome.html#a-larger-model-including-income-group",
    "href": "ordinaloutcome.html#a-larger-model-including-income-group",
    "title": "27  Modeling an Ordinal Categorical Outcome",
    "section": "27.10 A Larger Model, including income group",
    "text": "27.10 A Larger Model, including income group\n\nm3_lrm <- lrm(gen_n ~ rcs(veg_day,3) + rcs(bmi, 4) + \n                  incomegroup + catg(costprob) + \n                  bmi %ia% costprob, \n              data = sm1, x = T, y = T)\n\nm3_lrm\n\nLogistic Regression Model\n \n lrm(formula = gen_n ~ rcs(veg_day, 3) + rcs(bmi, 4) + incomegroup + \n     catg(costprob) + bmi %ia% costprob, data = sm1, x = T, y = T)\n \n \n Frequencies of Responses\n \n    1    2    3    4    5 \n  822 1805 1667  801  299 \n \n                        Model Likelihood      Discrimination    Rank Discrim.    \n                              Ratio Test             Indexes          Indexes    \n Obs          5394    LR chi2    1190.35      R2       0.209    C       0.696    \n max |deriv| 7e-12    d.f.            14    R2(14,5394)0.196    Dxy     0.391    \n                      Pr(> chi2) <0.0001    R2(14,4995)0.210    gamma   0.392    \n                                              Brier    0.214    tau-a   0.291    \n \n                    Coef    S.E.   Wald Z Pr(>|Z|)\n y>=2                3.7535 0.4852   7.74 <0.0001 \n y>=3                1.8717 0.4838   3.87 0.0001  \n y>=4                0.2035 0.4831   0.42 0.6737  \n y>=5               -1.4386 0.4846  -2.97 0.0030  \n veg_day            -0.2602 0.0633  -4.11 <0.0001 \n veg_day'            0.1756 0.0693   2.53 0.0113  \n bmi                -0.0325 0.0203  -1.60 0.1086  \n bmi'                0.5422 0.0989   5.48 <0.0001 \n bmi''              -1.4579 0.2663  -5.47 <0.0001 \n incomegroup=10-14K  0.2445 0.1705   1.43 0.1516  \n incomegroup=15-19K -0.2626 0.1582  -1.66 0.0969  \n incomegroup=20-24K -0.6434 0.1501  -4.29 <0.0001 \n incomegroup=25-34K -0.7427 0.1459  -5.09 <0.0001 \n incomegroup=35-49K -1.1621 0.1415  -8.21 <0.0001 \n incomegroup=50-74K -1.4579 0.1418 -10.28 <0.0001 \n incomegroup=75K+   -1.8592 0.1361 -13.66 <0.0001 \n costprob=1          1.4576 0.3528   4.13 <0.0001 \n bmi * costprob     -0.0259 0.0116  -2.24 0.0250  \n \n\n\nAnother option here would have been to consider building incomegroup as a scored variable, with an order on its own, but I won’t force that here. Here’s the polr version…\n\nm3 <- polr(genh ~ rcs(veg_day,3) + rcs(bmi, 4) + \n               incomegroup + costprob + \n               bmi %ia% costprob, data = sm1)\n\n\n27.10.1 Cross-Tabulation of Predicted/Observed Classifications\n\naddmargins(table(predict(m3), sm1$genh))\n\n      \n        1-E 2_VG  3_G  4_F  5_P  Sum\n  1-E     3    2    0    0    0    5\n  2_VG  642 1200  815  221   49 2927\n  3_G   170  565  754  468  182 2139\n  4_F     7   37   96  108   65  313\n  5_P     0    1    2    4    3   10\n  Sum   822 1805 1667  801  299 5394\n\n\nThis model predicts more Fair results, but still far too many Very Good with no Excellent at all.\n\n\n27.10.2 Nomogram\n\nfun.ge3 <- function(x) plogis(x - m3_lrm$coef[1] + m3_lrm$coef[2])\nfun.ge4 <- function(x) plogis(x - m3_lrm$coef[1] + m3_lrm$coef[3])\nfun.ge5 <- function(x) plogis(x - m3_lrm$coef[1] + m3_lrm$coef[4])\n\nplot(nomogram(m3_lrm, fun=list('Prob Y >= 2 (VG or worse)' = plogis, \n                               'Prob Y >= 3 (Good or worse)' = fun.ge3,\n                               'Prob Y >= 4 (Fair or Poor)' = fun.ge4,\n                               'Prob Y = 5 (Poor)' = fun.ge5)))\n\n\n\n\n\n\n27.10.3 Using Predict and showing mean prediction on 1-5 scale\n\nggplot(Predict(m3_lrm, fun = Mean(m3_lrm, code = TRUE)))\n\n\n\n\nHere, we’re plotting the mean score on the 1-5 gen_n scale.\n\n\n27.10.4 Validating the Summary Statistics in m3_lrm\n\nset.seed(43221); validate(m3_lrm)\n\n          index.orig training    test optimism index.corrected  n\nDxy           0.3915   0.3928  0.3899   0.0029          0.3886 40\nR2            0.2093   0.2112  0.2071   0.0041          0.2053 40\nIntercept     0.0000   0.0000  0.0019  -0.0019          0.0019 40\nSlope         1.0000   1.0000  0.9871   0.0129          0.9871 40\nEmax          0.0000   0.0000  0.0032   0.0032          0.0032 40\nD             0.2205   0.2227  0.2179   0.0049          0.2156 40\nU            -0.0004  -0.0004 -1.4667   1.4663         -1.4667 40\nQ             0.2209   0.2231  1.6845  -1.4614          1.6823 40\nB             0.2137   0.2134  0.2142  -0.0007          0.2145 40\ng             1.0363   1.0410  1.0276   0.0133          1.0229 40\ngp            0.2262   0.2265  0.2244   0.0022          0.2240 40\n\n\nStill not very impressive, but much better than where we started. It’s not crazy to suggest that in new data, we might expect a Nagelkerke \\(R^2\\) of 0.205 and a C statistic of 0.5 + (0.3886/2) = 0.6943."
  },
  {
    "objectID": "ordinaloutcome.html#references-for-this-chapter",
    "href": "ordinaloutcome.html#references-for-this-chapter",
    "title": "27  Modeling an Ordinal Categorical Outcome",
    "section": "27.11 References for this Chapter",
    "text": "27.11 References for this Chapter\n\nSome of the material here is adapted from http://stats.idre.ucla.edu/r/dae/ordinal-logistic-regression/.\nI also found great guidance at http://data.library.virginia.edu/fitting-and-interpreting-a-proportional-odds-model/\nOther parts are based on the work of Jeffrey S. Simonoff (2003) Analyzing Categorical Data in Chapter 10. Related data and R code are available at http://people.stern.nyu.edu/jsimonof/AnalCatData/Splus/.\nAnother good source for a simple example is https://onlinecourses.science.psu.edu/stat504/node/177.\nAlso helpful is https://onlinecourses.science.psu.edu/stat504/node/178 which shows a more complex example nicely."
  },
  {
    "objectID": "multinomial.html#r-setup-used-here",
    "href": "multinomial.html#r-setup-used-here",
    "title": "28  Multinomial Logistic Regression",
    "section": "28.1 R Setup Used Here",
    "text": "28.1 R Setup Used Here\n\nknitr::opts_chunk$set(comment = NA)\n\nlibrary(broom)\nlibrary(knitr)\nlibrary(MASS)\nlibrary(nnet)\nlibrary(tidyverse) \n\ntheme_set(theme_bw())\n\n\n28.1.1 Data Load\n\nauthorship <- read_csv(\"data/authorship.csv\", show_col_types = FALSE)"
  },
  {
    "objectID": "multinomial.html#the-authorship-example",
    "href": "multinomial.html#the-authorship-example",
    "title": "28  Multinomial Logistic Regression",
    "section": "28.2 The Authorship Example",
    "text": "28.2 The Authorship Example\nThis example is based on the work of Jeffrey S. Simonoff (2003) Analyzing Categorical Data in Chapter 10. Related data and R code are available at this link. Meanwhile, the data set and analysis are based on the work of Peng RD and Hengartner NW (2002) Quantitative analysis of literary styles, The American Statistician, 56, 175-185.\nThe authorship.csv data file contains 841 rows. Each row describes a block of text that contains 1700 total words from one of several books by four authors: Jane Austen (samples from 7 books), Jack London (6 books), John Milton (2 books), or William Shakespeare (12 books). The data include counts within the blocks of text of 69 function words, such as “a”, “by”, “no”, “that” and “with”. The goal of our analysis, mirroring that of Simonoff, will be to use the incidence of these function words to build a model that distinguishes the authors.\n\nauthorship$Author <- factor(authorship$Author, \n    levels = c(\"Shakespeare\", \"Austen\", \"London\", \"Milton\"))\n\nauthorship\n\n# A tibble: 841 × 71\n   BookID Author     a   all  also    an   and   any   are    as    at    be\n    <dbl> <fct>  <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl>\n 1      1 Austen    46    12     0     3    66     9     4    16    13    13\n 2      1 Austen    35    10     0     7    44     4     3    18    16     9\n 3      1 Austen    46     2     0     3    40     1    13    11     9    23\n 4      1 Austen    40     7     0     4    64     3     3    20    13    20\n 5      1 Austen    29     5     0     6    52     5    14    17     6    16\n 6      1 Austen    27     8     0     3    42     2    15    11    14    12\n 7      1 Austen    34     8     0    15    44     2     6    16    14    11\n 8      1 Austen    38     6     1     2    67     3     6    17     4    21\n 9      1 Austen    34    12     0     5    50     2     8     7    13     7\n10      1 Austen    54     8     0     6    44     4     8    13    17    14\n# … with 831 more rows, and 59 more variables: been <dbl>, but <dbl>, by <dbl>,\n#   can <dbl>, do <dbl>, down <dbl>, even <dbl>, every <dbl>, `for` <dbl>,\n#   from <dbl>, had <dbl>, has <dbl>, have <dbl>, her <dbl>, his <dbl>,\n#   `if` <dbl>, `in` <dbl>, into <dbl>, is <dbl>, it <dbl>, its <dbl>,\n#   may <dbl>, more <dbl>, must <dbl>, my <dbl>, no <dbl>, not <dbl>,\n#   now <dbl>, of <dbl>, on <dbl>, one <dbl>, only <dbl>, or <dbl>, our <dbl>,\n#   should <dbl>, so <dbl>, some <dbl>, such <dbl>, than <dbl>, that <dbl>, …\n\n\n\nTo-morrow, and to-morrow, and to-morrow, Creeps in this petty pace from day to day, To the last syllable of recorded time; And all our yesterdays have lighted fools The way to dusty death. Out, out, brief candle! Life’s but a walking shadow, a poor player, That struts and frets his hour upon the stage, And then is heard no more. It is a tale Told by an idiot, full of sound and fury, Signifying nothing."
  },
  {
    "objectID": "multinomial.html#focus-on-11-key-words",
    "href": "multinomial.html#focus-on-11-key-words",
    "title": "28  Multinomial Logistic Regression",
    "section": "28.3 Focus on 11 key words",
    "text": "28.3 Focus on 11 key words\nAgain, following Simonoff, we will focus on 11 words from the set of 69 potential predictors in the data, specifically…\n\n“be”, “been”, “had”, “it”, “may”, “not”, “on”, “the”, “upon”, “was” and “which”\n\n\nauth2 <- authorship |>\n    select(Author, BookID, be, been, had, it, may, not, \n           on, the, upon, was, which)\n\nauth2.long <- auth2 |>\n    gather(\"word\", \"n\", 3:13)\n\nauth2.long\n\n# A tibble: 9,251 × 4\n   Author BookID word      n\n   <fct>   <dbl> <chr> <dbl>\n 1 Austen      1 be       13\n 2 Austen      1 be        9\n 3 Austen      1 be       23\n 4 Austen      1 be       20\n 5 Austen      1 be       16\n 6 Austen      1 be       12\n 7 Austen      1 be       11\n 8 Austen      1 be       21\n 9 Austen      1 be        7\n10 Austen      1 be       14\n# … with 9,241 more rows\n\n\n\n28.3.1 Side by Side Boxplots\n\nggplot(auth2.long, aes(x = Author, y = n)) +\n    geom_boxplot() +\n    facet_wrap(~ word, ncol = 3, scales = \"free_y\") + \n    labs(x = \"\", y = \"\")\n\n\n\n\n\nOh! do not attack me with your watch. A watch is always too fast or too slow. I cannot be dictated to by a watch."
  },
  {
    "objectID": "multinomial.html#a-multinomial-logistic-regression-model",
    "href": "multinomial.html#a-multinomial-logistic-regression-model",
    "title": "28  Multinomial Logistic Regression",
    "section": "28.4 A Multinomial Logistic Regression Model",
    "text": "28.4 A Multinomial Logistic Regression Model\nLet’s start with a multinomial model to predict Author on the basis of these 11 key predictors, using the multinom function from the nnet package.\n\nauthnom1 <- multinom(Author ~ be + been + had + it + may + not + on + \n                         the + upon + was + which, data=authorship, \n                     maxit=200)\n\n# weights:  52 (36 variable)\ninitial  value 1165.873558 \niter  10 value 293.806160\niter  20 value 273.554538\niter  30 value 192.309644\niter  40 value 71.091334\niter  50 value 48.419335\niter  60 value 46.808141\niter  70 value 46.184752\niter  80 value 46.026162\niter  90 value 45.932823\niter 100 value 45.897793\niter 110 value 45.868017\niter 120 value 45.863256\nfinal  value 45.863228 \nconverged\n\nsummary(authnom1)\n\nCall:\nmultinom(formula = Author ~ be + been + had + it + may + not + \n    on + the + upon + was + which, data = authorship, maxit = 200)\n\nCoefficients:\n       (Intercept)          be       been       had          it         may\nAusten  -15.504834  0.48974946  0.5380318 0.4620513  0.00388835 -0.15025084\nLondon  -14.671720 -0.07497073  0.1733116 0.4842272  0.08674782 -0.01590702\nMilton   -1.776866 -0.10891178 -0.9127155 0.5319573 -0.82046587 -0.06760436\n               not        on         the      upon       was       which\nAusten -0.08861462 0.5967404 -0.02361614 -2.119001 0.7021371  0.10370827\nLondon -0.32567063 0.5749969  0.12039782 -1.914428 0.6767581 -0.59121054\nMilton  0.05575887 0.5198173  0.08739368 -2.042475 0.3048202 -0.05939104\n\nStd. Errors:\n       (Intercept)        be      been       had         it       may       not\nAusten    4.892258 0.1643694 0.3117357 0.2695081 0.09554376 0.3008164 0.1078329\nLondon    5.372898 0.1916618 0.3308759 0.2812555 0.11355697 0.4946804 0.1440760\nMilton    5.417300 0.1613282 0.5910561 0.3187304 0.23015421 0.3500753 0.1309306\n              on        the      upon       was     which\nAusten 0.2213827 0.03457288 0.6426484 0.1808681 0.1646472\nLondon 0.2251642 0.03088881 0.7072129 0.1768901 0.2542886\nMilton 0.2223575 0.04783646 0.6436399 0.1820885 0.2111105\n\nResidual Deviance: 91.72646 \nAIC: 163.7265 \n\n\n\n28.4.1 Testing Model 1\n\nz1 <- summary(authnom1)$coefficients/summary(authnom1)$standard.errors\nround(z1,2)\n\n       (Intercept)    be  been  had    it   may   not   on   the  upon  was\nAusten       -3.17  2.98  1.73 1.71  0.04 -0.50 -0.82 2.70 -0.68 -3.30 3.88\nLondon       -2.73 -0.39  0.52 1.72  0.76 -0.03 -2.26 2.55  3.90 -2.71 3.83\nMilton       -0.33 -0.68 -1.54 1.67 -3.56 -0.19  0.43 2.34  1.83 -3.17 1.67\n       which\nAusten  0.63\nLondon -2.32\nMilton -0.28\n\np1 <- (1 - pnorm(abs(z1), 0, 1)) * 2\nkable(round(p1,3))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n(Intercept)\nbe\nbeen\nhad\nit\nmay\nnot\non\nthe\nupon\nwas\nwhich\n\n\n\n\nAusten\n0.002\n0.003\n0.084\n0.086\n0.968\n0.617\n0.411\n0.007\n0.495\n0.001\n0.000\n0.529\n\n\nLondon\n0.006\n0.696\n0.600\n0.085\n0.445\n0.974\n0.024\n0.011\n0.000\n0.007\n0.000\n0.020\n\n\nMilton\n0.743\n0.500\n0.123\n0.095\n0.000\n0.847\n0.670\n0.019\n0.068\n0.002\n0.094\n0.778\n\n\n\n\n\nSimonoff suggests that “been” and “may” can be dropped. What do we think?\n\nThe proper function of man is to live, not to exist. I shall not waste my days in trying to prolong them. I shall use my time."
  },
  {
    "objectID": "multinomial.html#model-2",
    "href": "multinomial.html#model-2",
    "title": "28  Multinomial Logistic Regression",
    "section": "28.5 Model 2",
    "text": "28.5 Model 2\n\nauthnom2 <- multinom(Author ~ be + had + it + not + on + \n                         the + upon + was + which, data=authorship, \n                     maxit=200)\n\n# weights:  44 (30 variable)\ninitial  value 1165.873558 \niter  10 value 304.985478\niter  20 value 285.428679\niter  30 value 143.301103\niter  40 value 54.589791\niter  50 value 52.140470\niter  60 value 51.421454\niter  70 value 51.012790\niter  80 value 50.888718\niter  90 value 50.834262\niter 100 value 50.743136\nfinal  value 50.743111 \nconverged\n\nsummary(authnom2)\n\nCall:\nmultinom(formula = Author ~ be + had + it + not + on + the + \n    upon + was + which, data = authorship, maxit = 200)\n\nCoefficients:\n       (Intercept)          be       had          it         not        on\nAusten   -16.55647  0.45995950 0.6698612  0.02621612 -0.03684654 0.4676716\nLondon   -16.06419 -0.13378141 0.6052164  0.10517792 -0.27934022 0.4958923\nMilton    -2.22344 -0.07031256 0.1737526 -0.81984885  0.05444678 0.5363108\n                the      upon       was       which\nAusten -0.001852454 -1.950761 0.6543956  0.06363998\nLondon  0.128565811 -1.643829 0.6418607 -0.54690144\nMilton  0.074236636 -1.762533 0.2932065 -0.08748272\n\nStd. Errors:\n       (Intercept)        be       had         it        not        on\nAusten    4.723001 0.1293729 0.2201823 0.08657746 0.08771157 0.1949021\nLondon    5.202732 0.1587639 0.2306803 0.10117217 0.11608348 0.2072383\nMilton    4.593806 0.1499103 0.2057258 0.21551377 0.12103678 0.1895226\n              the      upon       was     which\nAusten 0.02945139 0.5620273 0.1524982 0.1466250\nLondon 0.02739965 0.6219927 0.1512911 0.2087120\nMilton 0.04463721 0.6246766 0.1601393 0.1928361\n\nResidual Deviance: 101.4862 \nAIC: 161.4862 \n\n\n\n28.5.1 Comparing Model 2 to Model 1\n\nanova(authnom1, authnom2)\n\nLikelihood ratio tests of Multinomial Models\n\nResponse: Author\n                                                             Model Resid. df\n1              be + had + it + not + on + the + upon + was + which      2493\n2 be + been + had + it + may + not + on + the + upon + was + which      2487\n  Resid. Dev   Test    Df LR stat.   Pr(Chi)\n1  101.48622                                \n2   91.72646 1 vs 2     6 9.759767 0.1351402\n\n\n\n\n28.5.2 Testing Model 2\n\nz2 <- summary(authnom2)$coefficients/summary(authnom2)$standard.errors\nround(z2,2)\n\n       (Intercept)    be  had    it   not   on   the  upon  was which\nAusten       -3.51  3.56 3.04  0.30 -0.42 2.40 -0.06 -3.47 4.29  0.43\nLondon       -3.09 -0.84 2.62  1.04 -2.41 2.39  4.69 -2.64 4.24 -2.62\nMilton       -0.48 -0.47 0.84 -3.80  0.45 2.83  1.66 -2.82 1.83 -0.45\n\np2 <- (1 - pnorm(abs(z2), 0, 1)) * 2\nround(p2,3)\n\n       (Intercept)    be   had    it   not    on   the  upon   was which\nAusten       0.000 0.000 0.002 0.762 0.674 0.016 0.950 0.001 0.000 0.664\nLondon       0.002 0.399 0.009 0.299 0.016 0.017 0.000 0.008 0.000 0.009\nMilton       0.628 0.639 0.398 0.000 0.653 0.005 0.096 0.005 0.067 0.650\n\n\n\n\n28.5.3 A little history\nSimonoff has an interesting note: Consider the lifetimes of these four authors:\n\nWilliam Shakespeare was born in 1564 and died in 1616\nJohn Milton was born in 1608 (44 years after Shakespeare) and died in 1674\nJane Austen was born in 1775 (211 years after Shakespeare) and died in 1817\nJack London was born in 1876 (312 years after Shakespeare) and died in 1916\n\nHow many significant coefficients does each author display relative to Shakespeare?"
  },
  {
    "objectID": "multinomial.html#classification-table",
    "href": "multinomial.html#classification-table",
    "title": "28  Multinomial Logistic Regression",
    "section": "28.6 Classification Table",
    "text": "28.6 Classification Table\nHow well does this model (model 2) distinguish these authors based on blocks of 1700 words of text?\n\ntable(authorship$Author, predict(authnom2))\n\n             \n              Shakespeare Austen London Milton\n  Shakespeare         168      3      1      1\n  Austen                4    308      5      0\n  London                0      1    294      1\n  Milton                2      0      1     52\n\n\nBased on this classification table, I’d say it does a nice job. Almost 98% of the blocks of text are correctly classified.\n\nFly, envious Time, till thou run out thy race; Call on the lazy leaden-stepping hours, Whose speed is but the heavy plummet’s pace; And glut thyself with what thy womb devours, Which is no more then what is false and vain, And merely mortal dross; So little is our loss, So little is thy gain. For when, as each thing bad thou hast entomb’d And last of all thy greedy self consumed, Then long Eternity shall greet our bliss, With an individual kiss; And Joy shall overtake us, as a flood, When every thing that is sincerely good, And perfectly divine, With truth, and peace, and love, shall ever shine, About the supreme throne Of Him, to whose happy-making sight, alone, When once our heavenly-guided soul shall climb, Then all this earthly grossness quit, Attired with stars, we shall for ever sit, Triumphing over Death, and Chance, and thee, O Time!"
  },
  {
    "objectID": "multinomial.html#probability-curves-based-on-a-single-predictor",
    "href": "multinomial.html#probability-curves-based-on-a-single-predictor",
    "title": "28  Multinomial Logistic Regression",
    "section": "28.7 Probability Curves based on a Single Predictor",
    "text": "28.7 Probability Curves based on a Single Predictor\nIn situations where only one predictor is used, we can develop nice plots of estimated probabilities for each group as a function of the predictor. Suppose we look at the single word “been” (note that this was left out of Model 2.)\nNote that the possible values for counts of “been” in the data range from 0 to 27…\n\nsummary(authorship$been)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  0.000   2.000   4.000   4.614   7.000  27.000 \n\n\nNow, we’ll build a model to predict the author based solely on the counts of the word “been”.\n\nauthnom3 <- multinom(Author ~ been, \n                     data=authorship, maxit=200)\n\n# weights:  12 (6 variable)\ninitial  value 1165.873558 \niter  10 value 757.915093\niter  20 value 755.454631\nfinal  value 755.454551 \nconverged\n\n\nNext, we’ll build a grid of the predicted log odds for each author (as compared to Shakespeare) using the fitted coefficients. The grid will cover every possible value from 0 to 27, increasing by 0.1, using the following trick in R.\n\nbeengrid <- cbind(1,c(0:270)/10)\naustenlogit <- beengrid %*% coef(authnom3)[1,]\nlondonlogit <- beengrid %*% coef(authnom3)[2,]\nmiltonlogit <- beengrid %*% coef(authnom3)[3,]\n\nNext, we’ll use that grid of logit values to estimate the fitted probabilities for each value of “been” between 0 and 27.\n\naustenprob <- exp(austenlogit)/ \n    (exp(austenlogit) + exp(londonlogit) + \n         exp(miltonlogit) + 1)\nlondonprob <- exp(londonlogit)/ \n    (exp(austenlogit) + exp(londonlogit) + \n         exp(miltonlogit) + 1)\nmiltonprob <- exp(miltonlogit)/ \n    (exp(austenlogit) + exp(londonlogit) + \n         exp(miltonlogit) + 1)\nshakesprob <- 1 - austenprob - londonprob - miltonprob\n\nbeen_dat <- data_frame(been_count = beengrid[,2], \n                       austen = austenprob[,1], \n                       london = londonprob[,1], \n                       milton = miltonprob[,1], \n                       shakespeare = shakesprob[,1])\n\nWarning: `data_frame()` was deprecated in tibble 1.1.0.\nℹ Please use `tibble()` instead.\n\nbeen_dat\n\n# A tibble: 271 × 5\n   been_count austen london milton shakespeare\n        <dbl>  <dbl>  <dbl>  <dbl>       <dbl>\n 1        0   0.0258  0.136  0.285       0.553\n 2        0.1 0.0288  0.147  0.272       0.553\n 3        0.2 0.0321  0.158  0.258       0.551\n 4        0.3 0.0357  0.171  0.245       0.548\n 5        0.4 0.0396  0.184  0.232       0.545\n 6        0.5 0.0438  0.197  0.219       0.540\n 7        0.6 0.0484  0.211  0.207       0.534\n 8        0.7 0.0534  0.225  0.195       0.527\n 9        0.8 0.0587  0.240  0.183       0.518\n10        0.9 0.0644  0.256  0.171       0.509\n# … with 261 more rows\n\n\nNow, we gather the data by author name and probability\n\nbeen_dat_long <- been_dat |>\n    gather(\"name\", \"prob\", 2:5)\nbeen_dat_long\n\n# A tibble: 1,084 × 3\n   been_count name     prob\n        <dbl> <chr>   <dbl>\n 1        0   austen 0.0258\n 2        0.1 austen 0.0288\n 3        0.2 austen 0.0321\n 4        0.3 austen 0.0357\n 5        0.4 austen 0.0396\n 6        0.5 austen 0.0438\n 7        0.6 austen 0.0484\n 8        0.7 austen 0.0534\n 9        0.8 austen 0.0587\n10        0.9 austen 0.0644\n# … with 1,074 more rows\n\n\n\n28.7.1 Produce the Plot of Estimated Probabilities based on “been” counts\n\nggplot(been_dat_long, aes(x = been_count, y = prob, \n                          col = name)) +\n    geom_line(linewidth = 1.5) +\n    labs(x = \"Count of the word `been`\", \n         y = \"Model probability\")\n\n\n\n\n\n\n28.7.2 Boxplot of “been” counts\nCompare this to what we see in the raw counts of the word “been”.\n\nbeen.long <- filter(auth2.long, word == \"been\")\nbeen.long$Auth <- fct_relevel(been.long$Author, \n            \"Austen\", \"London\", \"Milton\", \"Shakespeare\")\n# releveling to make the colors match the model plot\n\nggplot(been.long, aes(x = Auth, y = n, fill = Auth)) +\n    geom_boxplot() +\n    guides(fill = \"none\") +\n    labs(x = \"\", y = \"Count of the word `been`\")\n\n\n\n\n\n\n28.7.3 Quote Sources\n\nTo-morrow, and to-morrow, and to-morrow … Shakespeare Macbeth Act 5.\nOh! do not attack me with your watch. … Jane Austen Mansfield Park\nThe proper function of man is to live, not to exist. … Jack London The Bulletin San Francisco 1916-12-02.\nFly, envious Time, till thou run out thy race … John Milton On Time"
  },
  {
    "objectID": "survival_data.html#r-setup-used-here",
    "href": "survival_data.html#r-setup-used-here",
    "title": "29  Time To Event / Survival Data",
    "section": "29.1 R Setup Used Here",
    "text": "29.1 R Setup Used Here\n\nknitr::opts_chunk$set(comment = NA)\n\nlibrary(mosaic)\nlibrary(survival) \nlibrary(survminer)\nlibrary(tidyverse) \n\ntheme_set(theme_bw())\n\n\n29.1.1 Data Load\n\nhem <- read_csv(\"data/hem.csv\", show_col_types = FALSE)"
  },
  {
    "objectID": "survival_data.html#an-outline-of-key-topics-discussed-in-these-notes",
    "href": "survival_data.html#an-outline-of-key-topics-discussed-in-these-notes",
    "title": "29  Time To Event / Survival Data",
    "section": "29.2 An Outline of Key Topics Discussed in these Notes",
    "text": "29.2 An Outline of Key Topics Discussed in these Notes\nIn this chapter, we tackle the building blocks of survival analysis, and use R to work with survival objects.\n\nThe Survival Function, \\(S(t)\\)\n\nThe Kaplan-Meier Estimate/Plot\nComparing Survival Functions with log rank test\n\nThe Hazard Function, \\(H(t) = -log(S(t))\\)\nUsing survival and related packages in R\n\nIn the next chapter, we introduce the the Cox Proportional Hazards Regression Model, one of several available models for fitting regressions to time-to-event (survival) outcomes."
  },
  {
    "objectID": "survival_data.html#foundations-of-survival-analysis",
    "href": "survival_data.html#foundations-of-survival-analysis",
    "title": "29  Time To Event / Survival Data",
    "section": "29.3 Foundations of Survival Analysis",
    "text": "29.3 Foundations of Survival Analysis\nSurvival analysis is concerned with prospective studies, where we start with a cohort of subjects and follow them forwards in time to determine some clinical outcome. Follow-up continues until either some event of interest occurs, the study ends, or further observation becomes impossible.\nThe outcomes in a survival analysis consist of the subject’s fate and length of follow-up at the end of the study.\n\nFor some patients, the outcome of interest may not occur during follow-up.\nFor such patients, whose follow-up time is censored, we know only that this event did not occur while the patient was being followed. We do not know whether or not it will occur at some later time.\n\nThe primary problems with survival data are non-normality and censoring…\n\nSurvival data are quantitative, but not symmetrically distributed. They will often appear positively skewed, with a few people surviving a very long time compared with the majority; so assuming a normal distribution will not be reasonable.\nAt the completion of the study, some patients may not have reached the endpoint of interest (death, relapse, etc.). Consequently, the exact survival times are not known.\n\nAll that is known is that the survival times are greater than the amount of time the individual has been in the study.\nThe survival times of these individuals are said to be censored (precisely, they are right-censored).\n\n\n\n29.3.1 The Survival Function, \\(S(t)\\)\nThe survival function, \\(S(t)\\) (sometimes called the survivor function) is the probability that the survival time, \\(T\\), is greater than or equal to a particular time, \\(t\\).\n\n\\(S(t)\\) = proportion of people surviving to time \\(t\\) or beyond\n\nIf there’s no censoring, the survival function is easy to estimate.\n\\[\n\\hat{S}(t) = \\frac{\\# \\mbox{ of subjects with survival times } \\geq t}{n}\n\\]\nbut this won’t work if there is censoring.\n\n\n29.3.2 Kaplan-Meier Estimator of the Survival Function\nThe survival function \\(S(t)\\) is the probability of surviving until at least time \\(t\\). It is essentially estimated by the number of patients alive at time \\(t\\) divided by the total number of study subjects remaining at that time.\nThe Kaplan-Meier estimator first orders the (unique) survival times from smallest to largest, then estimates the survival function at each unique survival time.\n\nThe survival function at the second death time, \\(t_{(2)}\\) is equal to the estimated probability of not dying at time \\(t_{(2)}\\) conditional on the individual being still at risk at time \\(t_{(2)}\\).\n\nIn the presence of censoring, the survival function is estimated as follows.\n\nOrder the survival times from smallest to largest, where t_{(j)} is the \\(j\\)th largest unique survival time, so we have…\n\n\\[\nt_{(1)} \\leq t_{(2)} \\leq t_{(3)} \\leq ... t_{(n)}\n\\]\n\nThe Kaplan-Meier estimate of the survival function is\n\n\\[\n\\hat{S}(t) = \\prod_{j: t_{(j)} \\leq t} (1 - \\frac{d_j}{r_j})\n\\]\nwhere \\(r_j\\) is the number of people at risk just before \\(t_{(j)}\\), including those censored at time \\(t_{(j)}\\), and \\(d_j\\) is the number of people who experience the event at time \\(t_{(j)}\\).\nWhen we want to compare survival functions (or their Kaplan-Meier estimates, at least) we’ll use a log rank test or one of several extensions of that test.\n\n\n29.3.3 Creating a Survival Object in R\nTo do survival analysis in R, we’re going to start with three main functions, all in the survival package:\n\nSurv creates a survival object\nsurvfit builds a Kaplan-Meier test, and the results may be plotted, as we’ve seen.\nsurvdiff builds a log rank test, that will let us compare two survival functions, as well as running several alternatives.\n\nPlus, we’ll build out some estimates of the hazard function.\nThe Surv function, part of the survival package in R, will create a survival object from two arguments:\n\ntime = follow-up time\nevent = a status indicator, where\n\nevent = 1 or TRUE means the event was observed (for instance, the patient died)\nevent = 0 or FALSE means the follow-up time was censored"
  },
  {
    "objectID": "survival_data.html#a-first-example-recurrent-lobar-intracerebral-hemorrhage",
    "href": "survival_data.html#a-first-example-recurrent-lobar-intracerebral-hemorrhage",
    "title": "29  Time To Event / Survival Data",
    "section": "29.4 A First Example: Recurrent Lobar Intracerebral Hemorrhage",
    "text": "29.4 A First Example: Recurrent Lobar Intracerebral Hemorrhage\nO’Donnell et al. (2000) studied the effect of the apolipoprotein E gene on the risk of recurrent lobar intracerebral hemorrhage in 70 patients who survived such a hemorrhage1. Patients in the study are classified by:\n\ntime = follow-up time, in months\nrecur = indicator of whether or not they had a recurrent event (1 = subject had a recurrence, 0 subject did not have a recurrence), and\ngenotype = the subject’s apolipoprotein E genotype (0 = Homozygous \\(\\epsilon3/\\epsilon3\\) and 1 = At least one \\(\\epsilon2\\) or \\(\\epsilon4\\) allele)\n\n\nhem |> head(4)\n\n# A tibble: 4 × 4\n     id genotype  time recur\n  <dbl>    <dbl> <dbl> <dbl>\n1     1        0 0.230     1\n2     2        0 1.05      0\n3     3        1 1.38      0\n4     4        1 1.41      1\n\n\n\ntable(hem$recur)\n\n\n 0  1 \n52 18 \n\n\nWe have 70 patients at the start, and observe 18 events (rest are censored.)\n\nfavstats(time ~ recur, data = hem)\n\n  recur       min        Q1   median       Q3      max     mean       sd  n\n1     0 1.0513350 14.414783 23.01437 38.64477 53.88091 25.51129 14.68242 52\n2     1 0.2299795  3.367556 10.92403 24.84600 42.87474 14.98517 13.88893 18\n  missing\n1       0\n2       0\n\n\nThe median survival time looks like 23 weeks in the patients who do not exhibit a recurrence, but only 11 weeks in those who do."
  },
  {
    "objectID": "survival_data.html#building-a-survival-object",
    "href": "survival_data.html#building-a-survival-object",
    "title": "29  Time To Event / Survival Data",
    "section": "29.5 Building a Survival Object",
    "text": "29.5 Building a Survival Object\n\nhemsurv <- Surv(time = hem$time, event = hem$recur)\n\nhead(hemsurv, 4)\n\n[1] 0.2299795  1.0513350+ 1.3798770+ 1.4127311 \n\n\nThis object both displays the survival time for each subject, and indicates whether or not the subject’s follow-up was censored before a recurrent event occurred. Survival times with a + sign indicate censoring.\n\nSubject 1 lived for 0.23 months hemorrhage-free and then had a recurrence.\nSubject 2 lived for 1.05 months hemorrhage-free, at which point they were censored (perhaps because the study ended, or perhaps because the subject was no longer available for follow-up)\n\nRemember that 18 of the subjects experienced a recurrent hemorrhage, and the other 52 are therefore censored."
  },
  {
    "objectID": "survival_data.html#kaplan-meier-estimate-of-the-survival-function",
    "href": "survival_data.html#kaplan-meier-estimate-of-the-survival-function",
    "title": "29  Time To Event / Survival Data",
    "section": "29.6 Kaplan-Meier Estimate of the Survival Function",
    "text": "29.6 Kaplan-Meier Estimate of the Survival Function\nTo build a Kaplan-Meier estimate of the survival function (to account properly for censoring), we take the survival object we have created, and use the survfit function from the survival package.\n\nhemfit1 <- survfit(hemsurv ~ 1)\n\nWe can look at the hemfit1 object directly, although the K-M estimate is usually plotted.\n\nprint(hemfit1, print.rmean=TRUE)\n\nCall: survfit(formula = hemsurv ~ 1)\n\n      n events rmean* se(rmean) median 0.95LCL 0.95UCL\n[1,] 70     18   40.4      2.63     NA    42.9      NA\n    * restricted mean with upper limit =  53.9 \n\n\nWe see that 18 events occurred out of a total of 70 subjects. The median survival time is listed as NA (missing) which implies it cannot be estimated by this simple model.\n\nThis is because only 18 of our 70 subjects have a known recurrence-free survival time (the rest are censored), so we don’t actually know what the median survival time will be across our 70 subjects. Apparently, R can produce a lower bound on a 95% confidence interval for the median survival time, but not the actual point estimate.\n\nWe also observe a restricted mean survival time estimate. The restricted mean uses as its upper limit the largest observed or censored survival time, which here is a censored value: 53.9 months. So it is the mean survival time, assuming all censored subjects lived hemorrhage-free for 53.9 months.\n\nsummary(hemfit1)\n\nCall: survfit(formula = hemsurv ~ 1)\n\n  time n.risk n.event survival std.err lower 95% CI upper 95% CI\n  0.23     70       1    0.986  0.0142        0.958        1.000\n  1.41     67       1    0.971  0.0202        0.932        1.000\n  1.58     65       1    0.956  0.0248        0.909        1.000\n  3.06     63       1    0.941  0.0287        0.886        0.999\n  3.32     62       1    0.926  0.0320        0.865        0.991\n  3.52     61       1    0.911  0.0349        0.845        0.982\n  3.55     60       1    0.895  0.0375        0.825        0.972\n  4.76     57       1    0.880  0.0400        0.805        0.962\n  9.53     54       1    0.863  0.0424        0.784        0.951\n 12.32     50       1    0.846  0.0449        0.762        0.939\n 15.57     46       1    0.828  0.0476        0.740        0.926\n 19.15     38       1    0.806  0.0511        0.712        0.912\n 24.77     32       1    0.781  0.0553        0.679        0.897\n 24.87     31       1    0.756  0.0590        0.648        0.881\n 28.09     26       1    0.726  0.0635        0.612        0.862\n 33.61     22       1    0.693  0.0687        0.571        0.842\n 37.52     17       1    0.653  0.0758        0.520        0.819\n 42.87      8       1    0.571  0.1011        0.404        0.808\n\n\nThis written summary provides us with lots of detail on the Kaplan-Meier estimate. In particular, the first two lines of this summary can be read to indicate the following.\n\nUp to time 0.23 months, no patients had a recurrence. Then, an event occurred, and the estimated survival (i.e. non-recurrence) probability is reduced from 1 to 0.986.\nBy time 1.41 months, when the next event occurred, only 67 patients remained at risk. This is because one of them had a recurrent hemorrhage already (at 0.23 months) and two others had been right-censored. The estimated hemorrhage-free survival probability estimate starting at time 1.41 months is now 0.971.\n\nA Kaplan-Meier plot graphically represents this summary.\n\n29.6.1 The Kaplan-Meier Plot, using Base R\nNow, let’s plot the Kaplan-Meier estimate, so we can see what is going on.\n\nplot(hemfit1, ylab=\"Pr(Hemorrhage-Free Survival)\",\n     xlab=\"Months of Follow-Up\",\n     main=\"Kaplan-Meier Plot for hem Data\")\n\n\n\n\nThe solid line indicates estimated hemorrhage-free survival probability. The dotted lines identify pointwise confidence intervals (default 95%).\n\nFor example, we see that the estimated probability of hemorrhage-free survival to 20 months is estimated to be about 0.8\nThe estimated probability of hemorrhage-free survival to 50 months is estimated to be about 0.6\n\nThe steps down indicate events (recurrences.) The estimated probability of survival to 0 months starts at 1, and drops down at each time point where an event (or more than one event) is observed.\n\n\n29.6.2 Using survminer to draw survival curves\nAnother approach to plotting the Kaplan-Meier estimate comes from ggsurvplot, from the survminer package.\n\nggsurvplot(hemfit1, data = hem)\n\n\n\n\nAgain, the solid line indicates estimated hemorrhage-free survival probability. The crosses indicate censoring. The steps down indicate events (recurrences,) and the shading indicates (default 95%) pointwise confidence intervals. By pointwise confidence intervals, I mean that these bounds apply only to individual points in the time scale.\nFor more on an alternative approach, using simultaneous confidence bands, visit the OpenIntro Statistics Survival Analysis in R materials, written by David Diez, which are also posted on our web site.\n\n\n29.6.3 A “Fancy” K-M Plot with a number at risk table\nWe can do a lot more with these plots. Following the suggestions at https://github.com/kassambara/survminer/ we can create the following…\n\nggsurvplot(hemfit1, data = hem,\n   conf.int = TRUE, # Add confidence interval\n   risk.table = TRUE, # Add risk table\n   xlab = \"Time in months\", # adjust X axis label\n   break.time.by = 12 # add tick every 12 months\n   )\n\n\n\n\nThis sort of plot is really designed to work best when we compare multiple groups in terms of their survival. So let’s do that."
  },
  {
    "objectID": "survival_data.html#comparing-survival-across-the-two-genotypes",
    "href": "survival_data.html#comparing-survival-across-the-two-genotypes",
    "title": "29  Time To Event / Survival Data",
    "section": "29.7 Comparing Survival Across the Two Genotypes",
    "text": "29.7 Comparing Survival Across the Two Genotypes\nNow, suppose we want to compare the hemorrhage-free survival functions for subjects classified by their apoliprotein E genotype. Working with the same survival object hemsurv we now run the survfit function to compare across the two genotype groups.\n\nhemfit2 <- survfit(hemsurv ~ hem$genotype)\nprint(hemfit2, print.rmean=TRUE)\n\nCall: survfit(formula = hemsurv ~ hem$genotype)\n\n                n events rmean* se(rmean) median 0.95LCL 0.95UCL\nhem$genotype=0 32      4   47.8      2.87     NA      NA      NA\nhem$genotype=1 38     14   33.9      3.77   37.5    24.9      NA\n    * restricted mean with upper limit =  53.9 \n\n\n\nIn genotype = 0 (the subjects who are Homozygous \\(\\epsilon3/\\epsilon3\\),) we had 32 subjects, and observed 4 recurrent hemorrhages. Our estimated restricted mean survival time in those subjects is 44.8 months and we cannot estimate a median survival time because only a small fraction of our subjects were not censored.\nIn genotype = 1 (subjects who have at least one \\(\\epsilon2\\) or \\(\\epsilon4\\) allele,) we had 38 subjects and observed 14 recurrences. The estimated restricted mean survival time is 32.7 months in these subjects, and we can (it seems) estimate a median survival time in this group of 37.5 months. Note that we don’t actually need to observe the event in half of the subjects to estimate a median survival time.\n\n\n29.7.1 Kaplan-Meier Survival Function Estimates, by Genotype\nI find I have to crank the figure height in Quarto up to at least 6 to get the risk table to show up nicely in this setting.\n\nggsurvplot(hemfit2, data = hem,\n           conf.int = TRUE,\n           xlab = \"Time in months\",\n           break.time.by = 12,\n           legend.labs = c(\"Homozygous\", \"Heterozygous\"),\n           risk.table = TRUE,\n           risk.table.height = 0.25\n           )\n\n\n\n\nIt appears that patients who were homozygous for the \\(\\epsilon3\\) allele of this gene (i.e. genotype = 0 in the hemorrhage data) had a much better prognosis than others (genotype = 1.)"
  },
  {
    "objectID": "survival_data.html#testing-the-difference-between-two-survival-curves",
    "href": "survival_data.html#testing-the-difference-between-two-survival-curves",
    "title": "29  Time To Event / Survival Data",
    "section": "29.8 Testing the difference between two survival curves",
    "text": "29.8 Testing the difference between two survival curves\nTo obtain a significance test comparing these two survival curves, we turn to a log rank test, which tests the null hypothesis \\(H_0: S_1(t) = S_2(t)\\) for all \\(t\\) where the two exposures have survival functions \\(S_1(t)\\) and \\(S_2(t)\\). We use the survdiff function to explore this test, which uses a \\(\\chi^2\\) statistic to do the testing.\n\nsurvdiff(hemsurv ~ hem$genotype)\n\nCall:\nsurvdiff(formula = hemsurv ~ hem$genotype)\n\n                N Observed Expected (O-E)^2/E (O-E)^2/V\nhem$genotype=0 32        4     9.28      3.00      6.28\nhem$genotype=1 38       14     8.72      3.19      6.28\n\n Chisq= 6.3  on 1 degrees of freedom, p= 0.01 \n\n\nBased on the log rank test, we conclude that there is a statistically significant difference (\\(p\\) = .0122) between the hemorrhage-free survival curves for the two genotypes, as shown in the Kaplan-Meier plot.\n\nThe log rank test generalizes to permit survival comparisons across more than two groups, with the test statistic having an asymptotic chi-squared distribution with one degree of freedom less than the number of patient groups being compared.\n\n\n29.8.1 Alternative log rank tests\nAn alternative approach to testing is the Peto and Peto modification of the Gehan-Wilcoxon test, which results from adding rho=1 to the survdiff function (rho=0, the default, yields the log rank test.)\n\nsurvdiff(hemsurv ~ hem$genotype, rho=1)\n\nCall:\nsurvdiff(formula = hemsurv ~ hem$genotype, rho = 1)\n\n                N Observed Expected (O-E)^2/E (O-E)^2/V\nhem$genotype=0 32     3.63     7.87      2.29      5.46\nhem$genotype=1 38    11.79     7.54      2.39      5.46\n\n Chisq= 5.5  on 1 degrees of freedom, p= 0.02 \n\n\nAs compared to the log rank test, this Peto-Peto modification (and others using rho > 0) give greater weight to the left hand (earlier) side of the survival curves.\n\nTo obtain chi-square tests that give greater weight to the right hand (later) side of the survival curves than the log rank test, use a rho value which is less than 0."
  },
  {
    "objectID": "survival_data.html#a-fancy-k-m-plot-with-a-number-at-risk-table-1",
    "href": "survival_data.html#a-fancy-k-m-plot-with-a-number-at-risk-table-1",
    "title": "29  Time To Event / Survival Data",
    "section": "29.9 A “Fancy” K-M Plot with a number at risk table",
    "text": "29.9 A “Fancy” K-M Plot with a number at risk table\nWe can add the log rank test result to our “fancy” K-M plot. Visit https://github.com/kassambara/survminer/ for more options.\n\nggsurvplot(hemfit2, data = hem, size = 1,\n   palette = c(\"purple\", \"darkgoldenrod\"), # custom colors\n   conf.int = TRUE, # Add confidence interval\n   pval = TRUE, # Add p-value\n   risk.table = TRUE, # Add risk table\n   risk.table.height = 0.25, # change if you have >2 groups\n   risk.table.y.text.col = T, # show colors in table listing\n   xlab = \"Time in months\", # adjust X axis label\n   break.time.by = 12, # break X axis in time intervals\n   legend.labs = c(\"Homozygous\", \"Heterozygous\"), # labels\n   ggtheme = theme_bw() # Change ggplot2 theme\n   )\n\n\n\n\n\n29.9.1 Customizing the Kaplan-Meier Plot Presentation Further\nWe can even add a plot of the number of censored subjects at each time point, as well as a median survival pointer (which, of course, we’ve seen that we can’t estimate in one of the groups), and customize the style of the confidence intervals. Again, see https://github.com/kassambara/survminer/ for even more customized results.\n\nggsurvplot(hemfit2,              \n           data = hem, \n           palette = c(\"#E7B800\", \"#2E9FDF\"),\n           risk.table = TRUE,      \n           pval = TRUE,           \n           conf.int = TRUE,       \n           xlab = \"Time in months\", \n           break.time.by = 12,     \n           ggtheme = theme_light(),\n           risk.table.y.text.col = T,\n           risk.table.height = 0.25, \n           risk.table.y.text = FALSE,\n           ncensor.plot = TRUE,\n           ncensor.plot.height = 0.25,\n           conf.int.style = \"step\",\n           surv.median.line = \"hv\",\n           legend.labs = c(\"Homozygous\", \"Heterozygous\")\n        )"
  },
  {
    "objectID": "survival_data.html#the-hazard-function",
    "href": "survival_data.html#the-hazard-function",
    "title": "29  Time To Event / Survival Data",
    "section": "29.10 The Hazard Function",
    "text": "29.10 The Hazard Function\nTo build regression models for time-to-event data, we will need to introduce the hazard function. Consider a subject in the hemorrhage study who has a hemorrhage-free survival time of 9 months.\n\nFor this subject to have had a recurrent hemorrhage at 9 months, they had to be hemorrhage-free for the first 8 months.\nThe subject’s hazard at 9 months is the failure rate “per month” conditional on the subject being hemorrhage-free for the first 8 months.\n\nIf \\(S(t)\\) is the survival function, and time \\(t\\) is taken to be continuous, then \\(S(t) = e^{H(t)}\\) defines the hazard function \\(H(t)\\).\n\nNote that \\(H(t)\\) = \\(-ln(S(t))\\).\nThe function \\(H(t)\\) is an important analytic tool.\n\nIt’s used to describe the concept of the risk of “failure” in an interval after time \\(t\\), conditioned on the subject having survived to time \\(t\\).\nIt’s often called the cumulative hazard function, to emphasize the fact that its value is the “sum” of the hazard up to time \\(t\\).\n\n\nThere are several different methods to estimate \\(H(t)\\), but we’ll focus on two…\n\nThe inverse Kaplan-Meier estimator\nThe Nelson-Aalen estimator\n\n\n29.10.1 The Inverse Kaplan-Meier Estimator of \\(H(t)\\)\nOur first estimator of the hazard function, \\(H(t)\\) will be the inverse Kaplan-Meier estimate, which I’ll place in an R object called H_est1.\n\nTo start, we will take the negative of the log of the Kaplan-Meier survival estimate. That takes care of the first t-1 levels of the eventual estimate.\nTo complete the process, we will repeat the final one of those time-specific estimates at the end.\n\n\nH_est1 <- -log(hemfit1$surv)\nH_est1 <- c(H_est1, tail(H_est1, 1))\n\nHere are the first five, and last five values of the hazard function estimate.\n\nhead(H_est1,5) # first 5 values\n\n[1] 0.01438874 0.01438874 0.01438874 0.02942661 0.02942661\n\ntail(H_est1, 5) # last 5\n\n[1] 0.5602049 0.5602049 0.5602049 0.5602049 0.5602049\n\n\nWe can create a little tibble containing the times and hazard estimates, like this:\n\nhaz_hem <- tibble(\n    time = c(hemfit1$time, tail(hemfit1$time, 1)),\n    inverse_KM = H_est1\n)\n\n\n\n29.10.2 Cumulative Hazard Function from Inverse K-M\nSince we’ve built the data set of times and hazard values, we can use the geom_step function in ggplot2.\n\nggplot(haz_hem, aes(x = time, y = inverse_KM)) + \n    geom_step() + \n    scale_x_continuous(breaks = c(0, 12, 24, 36, 48)) +\n    labs(x = \"Months of Follow-Up\", \n         y = \"Cumulative Hazard\",\n         title = \"Cumulative Hazard Function via Inverse K-M\")\n\n\n\n\n\n\n29.10.3 The Nelson-Aalen Estimator of \\(H(t)\\)\nAn alternative estimate of the cumulative hazard is called the Nelson-Aalen estimate, captured here in H_est2.\n\nh_st <- hemfit1$n.event / hemfit1$n.risk\nH_est2 <- cumsum(h_st)\nH_est2 <- c(H_est2, tail(H_est2, 1))\nhaz_hem$Nelson_Aalen <- H_est2\n\nhead(haz_hem)\n\n# A tibble: 6 × 3\n   time inverse_KM Nelson_Aalen\n  <dbl>      <dbl>        <dbl>\n1 0.230     0.0144       0.0143\n2 1.05      0.0144       0.0143\n3 1.38      0.0144       0.0143\n4 1.41      0.0294       0.0292\n5 1.51      0.0294       0.0292\n6 1.58      0.0449       0.0446\n\n\n\n\n29.10.4 Convert Wide Data to Long\nIn order to easily plot the two hazard function estimates in the same graph, we’ll want to convert these data from wide format to long format, with the pivot_longer function.\n\nhaz_hem_comp <- pivot_longer(data = haz_hem, cols = 2:3,\n                             names_to = \"method\", values_to = \"hazardest\")\n\nhead(haz_hem_comp)\n\n# A tibble: 6 × 3\n   time method       hazardest\n  <dbl> <chr>            <dbl>\n1 0.230 inverse_KM      0.0144\n2 0.230 Nelson_Aalen    0.0143\n3 1.05  inverse_KM      0.0144\n4 1.05  Nelson_Aalen    0.0143\n5 1.38  inverse_KM      0.0144\n6 1.38  Nelson_Aalen    0.0143\n\n\n\n\n29.10.5 Plot Comparison of Hazard Estimates\n\nggplot(haz_hem_comp, aes(x = time, y = hazardest, \n                    col = method)) + \n    geom_step(linewidth = 2) + \n    scale_x_continuous(breaks = c(0, 12, 24, 36, 48)) +\n    labs(x = \"Months of Follow-Up\", \n         y = \"Cumulative Hazard\",\n         title = \"Cumulative Hazard Function\") + \n    theme_bw()\n\n\n\n\nWe can see that the two cumulative hazard function estimates are nearly identical in this case. We could instead compare the two functions in faceted plots, if that would be helpful.\n\nggplot(haz_hem_comp, aes(x = time, y = hazardest)) + \n    geom_step() + \n    scale_x_continuous(breaks = c(0, 12, 24, 36, 48)) +\n    labs(x = \"Months of Follow-Up\", \n         y = \"Cumulative Hazard\",\n         title = \"Cumulative Hazard Function\") +\n    facet_grid(method ~ .) + theme_bw()\n\n\n\n\nNext, we will consider the issue of modeling a survival outcome using Cox proportional hazards regression."
  },
  {
    "objectID": "cox1.html#r-setup-used-here",
    "href": "cox1.html#r-setup-used-here",
    "title": "30  Cox Regression Models, Part 1",
    "section": "30.1 R Setup Used Here",
    "text": "30.1 R Setup Used Here\n\nknitr::opts_chunk$set(comment = NA)\n\nlibrary(broom)\nlibrary(survival) \nlibrary(rms)\nlibrary(tidyverse) \n\ntheme_set(theme_bw())\n\n\n30.1.1 Data Load\n\nhem <- read_csv(\"data/hem.csv\", show_col_types = FALSE) \n\nThe Cox proportional hazards (Cox regression) model fits survival data with a constant (i.e. not varying over time) covariate \\(x\\) to a hazard function of the form:\n\\[\nh(t | x) = h_0(t) exp[\\beta_1 x]\n\\]\nwhere we will estimate the unknown value of \\(\\beta_1\\) and where \\(h_0(t)\\) is the baseline hazard, which is a non-parametric and unspecified value which depends on \\(t\\) but not on \\(x\\).\n\nFor particular \\(x\\) values, we will be able to estimate the survival function if we have an estimate of the baseline survival function, \\(\\hat{S_0}(t)\\).\n\nThe estimated survival function for an individual with covariate value \\(x_k\\) turns out to be\n\\[\n\\hat{S}(t | x_k) = [\\hat{S_0}(t)]^{exp(\\beta_1 x_k)}\n\\]\nFrom Wikipedia (yes, really) …\n\nSurvival models can be viewed as consisting of two parts: the underlying hazard function, describing how the risk of event per time unit changes over time at baseline levels of covariates; and the effect parameters, describing how the hazard varies in response to explanatory covariates.\n\nThe key assumption in a Cox model is that the hazards are proportional - other types of survival models need not have this restriction. Quoting the always reliable (well, it’s better than you think) Wikipedia …\n\nIn a proportional hazards model, the unique effect of a unit increase in a covariate is multiplicative with respect to the hazard rate. For example, taking a drug may halve one’s hazard rate for a stroke occurring, or, changing the material from which a manufactured component is constructed may double its hazard rate for failure.\n\nThere are two main approaches to fitting Cox models in R.\n\nthe coxph function in the survival package, and\nthe cph function in the rms package."
  },
  {
    "objectID": "cox1.html#sources-used-in-building-this-material",
    "href": "cox1.html#sources-used-in-building-this-material",
    "title": "30  Cox Regression Models, Part 1",
    "section": "30.2 Sources used in building this material",
    "text": "30.2 Sources used in building this material\n\nDavid Diez’s excellent supplement for the OpenIntro Statistics project, on Survival Analysis in R. I’ve posted it on our web site, as well.\nSome tools in R to do some fancier work can be viewed at https://cran.r-project.org/web/views/Survival.html\nYou might also look at these two blog posts, originally from the Just Another Data blog.\n\nhttps://www.r-bloggers.com/survival-analysis-1/\n\nhttps://www.r-bloggers.com/survival-analysis-2/\n\nhttps://rpubs.com/daspringate/survival has some great slides, and I’ve stolen from them quite a bit here."
  },
  {
    "objectID": "cox1.html#fitting-a-cox-model-in-r-with-coxph",
    "href": "cox1.html#fitting-a-cox-model-in-r-with-coxph",
    "title": "30  Cox Regression Models, Part 1",
    "section": "30.3 Fitting a Cox Model in R with coxph",
    "text": "30.3 Fitting a Cox Model in R with coxph\nAs a first example, I’ll fit a model to predict time to recurrence in the hem data, on the basis of a single predictor: genotype.\n\ncfit <- with(hem, coxph(Surv(time, recur) ~ genotype))\ncfit\n\nCall:\ncoxph(formula = Surv(time, recur) ~ genotype)\n\n           coef exp(coef) se(coef)     z      p\ngenotype 1.3317    3.7874   0.5699 2.337 0.0195\n\nLikelihood ratio test=6.61  on 1 df, p=0.01015\nn= 70, number of events= 18 \n\n\nThis summary provides an overall comparison of the two genotypes, using a proportional hazards model.\n\nThe default approach in R is to use the “efron” method of breaking ties: other options include “breslow” and “exact”.\n\n\n30.3.1 Summarizing the Fit\n\nsummary(cfit)\n\nCall:\ncoxph(formula = Surv(time, recur) ~ genotype)\n\n  n= 70, number of events= 18 \n\n           coef exp(coef) se(coef)     z Pr(>|z|)  \ngenotype 1.3317    3.7874   0.5699 2.337   0.0195 *\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n         exp(coef) exp(-coef) lower .95 upper .95\ngenotype     3.787      0.264     1.239     11.57\n\nConcordance= 0.622  (se = 0.061 )\nLikelihood ratio test= 6.61  on 1 df,   p=0.01\nWald test            = 5.46  on 1 df,   p=0.02\nScore (logrank) test = 6.28  on 1 df,   p=0.01\n\n\nThis provides estimates of the \\(\\beta\\) value for genotype, including standard errors and \\(p\\) values for a Wald test. Also included is an estimate of the hazard ratio and its confidence interval.\n\nHere we have a hazard ratio estimate of exp(coef) = 3.787, with 95% CI (1.24, 11.57).\nThe hazard ratio is the multiplicative effect of the covariate (here, having at least one of the \\(\\epsilon 2\\) or \\(\\epsilon 4\\) allele) on the hazard function for recurrent hemorrhage\n\nA hazard ratio of 1 indicates no effect\nA hazard ratio > 1 indicates an increase in the hazard as the covariate rises\nA hazard ratio < 1 indicates a decrease in the hazard as the covariate rises\n\n\nWe can also tidy the hazard ratio estimate with the broom package.\n\ntidy(cfit, exponentiate = TRUE)\n\n# A tibble: 1 × 5\n  term     estimate std.error statistic p.value\n  <chr>       <dbl>     <dbl>     <dbl>   <dbl>\n1 genotype     3.79     0.570      2.34  0.0195\n\n\nIn addition, we have several other summaries:\n\nThe concordance measure is only appropriate when we have at least one continuous predictor in our Cox model.\nThe Cox & Snell pseudo-\\(R^2\\) reflects the improvement of the model we’ve fit over the model with an intercept alone, but isn’t a proportion of anything (hence the listing of the maximum possible value).\nThe Likelihood ratio, Wald and Score (logrank) tests provide insight into the overall significance of the model.\n\nWe can obtain a more detailed description of the likelihood-ratio test of the model with anova.\n\nanova(cfit)\n\nAnalysis of Deviance Table\n Cox model: response is Surv(time, recur)\nTerms added sequentially (first to last)\n\n          loglik  Chisq Df Pr(>|Chi|)  \nNULL     -66.675                       \ngenotype -63.371 6.6078  1    0.01015 *\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\n\n30.3.2 Glancing at the model?\n\nglance(cfit)\n\n# A tibble: 1 × 18\n      n nevent statist…¹ p.val…² stati…³ p.val…⁴ stati…⁵ p.val…⁶ stati…⁷ p.val…⁸\n  <int>  <dbl>     <dbl>   <dbl>   <dbl>   <dbl>   <dbl>   <dbl>   <dbl>   <dbl>\n1    70     18      6.61  0.0102    6.28  0.0122    5.46  0.0195      NA      NA\n# … with 8 more variables: r.squared <dbl>, r.squared.max <dbl>,\n#   concordance <dbl>, std.error.concordance <dbl>, logLik <dbl>, AIC <dbl>,\n#   BIC <dbl>, nobs <int>, and abbreviated variable names ¹​statistic.log,\n#   ²​p.value.log, ³​statistic.sc, ⁴​p.value.sc, ⁵​statistic.wald, ⁶​p.value.wald,\n#   ⁷​statistic.robust, ⁸​p.value.robust\n\n\nHere, we obtain several additional summaries of the model, including most of the important information from a summary of cfit.\n\n\n30.3.3 Plot the baseline survival function\nHere, we’ll plot the time in terms of months, but scaled to 12 month (one year) groups.\n\nplot(survfit(cfit), xscale = 12,\n     xlab = \"Years after initial hemorrhage\",\n     ylab = \"Proportion without recurrent hemorrhage\",\n     main = \"Baseline Survival Function\")\n\n\n\n\n\n\n30.3.4 Plot the genotype effect\nThere are several ways to build these plots. One approach follows. Another uses a cph fit and the survplot function from the rms package.\n\nnewdat <- with(hem, \n               data.frame(\n                 genotype = c(1, 0)\n               )\n)\n\nnewdat\n\n  genotype\n1        1\n2        0\n\n\n\nplot(survfit(cfit, newdata = newdat), xscale = 12,\n     conf.int = TRUE,\n     col = c(\"red\", \"blue\"),\n     xlab = \"Years after initial hemorrhage\",\n     ylab = \"Proportion without recurrent hemorrhage\",\n     main = \"Plotting the genotype effects from the cfit Model\")\nlegend(0.5, 0.2, \n       legend=c(\n         expression(paste(\"Homozygous \", epsilon, \"3 / \", \n                   epsilon, \"3\")), \n         expression(paste(\"At least one \", epsilon,\"2 or \",\n                   epsilon,\"4 Allele\"))\n       ),\n       lty = 1, \n       col = c(\"red\", \"blue\"),\n       text.col = c(\"red\", \"blue\"))\n\n\n\n\n\n\n30.3.5 Testing the Key Assumption: Proportional Hazards\nThe cox.zph function in the survival package will test the proportionality of all of the predictors included in your model by creating interactions with time.\n\nA small \\(p\\) value would indicate a violation of the proportionality assumption.\n\n\ncox.zph(cfit, transform=\"km\", global=TRUE)\n\n         chisq df    p\ngenotype  2.09  1 0.15\nGLOBAL    2.09  1 0.15\n\n\nSince the p value here is not significant, we may be all right. But it’s sensible to focus further on plots derived from the model fit, rather than relying solely on this test.\n\n\n30.3.6 Plotting the cox.zph results for the cfit model\ncox.zph function can be used to generate a plot for each of the individual predictors in the model. Of course, in this case, we have just one predictor: genotype. If the proportional hazards assumption is appropriate, then we should see a slope of essentially zero in each such plot. A slope that is seriously different from zero suggests a violation of the proportional hazards assumption.\n\nplot(cox.zph(cfit, transform=\"km\", global=TRUE))\n\n\n\n\nThe plot suggests only a slight rise in the plotted values over time, suggesting no serious problem with the proportional hazards assumption. This combined testing and plotting approach is a reasonable starting place for assessing the proportional hazards assumption, but it’s likely insufficient for good practical work.\nShould the proportional hazards assumption fit less well, we have two main options: (1) fit a non-linear term in the covariate in question, and (2) fit a different type of regression model that doesn’t require the proportional hazards assumption."
  },
  {
    "objectID": "cox1.html#fitting-a-cox-model-using-cph-from-the-rms-package",
    "href": "cox1.html#fitting-a-cox-model-using-cph-from-the-rms-package",
    "title": "30  Cox Regression Models, Part 1",
    "section": "30.4 Fitting a Cox Model using cph from the rms package",
    "text": "30.4 Fitting a Cox Model using cph from the rms package\nTo set up a cph fit for our comparison of genotypes in the hem data, we’ll follow these steps.\n\nunits(hem$time) <- \"month\"\nd <- datadist(hem)\noptions(datadist = \"d\")\n\nhemsurv <- Surv(time = hem$time, event = hem$recur)\n\nmodel_hem <- cph(hemsurv ~ genotype, data = hem, \n                 x = TRUE, y = TRUE, surv = TRUE)\n\nNote that the surv = TRUE piece is required to get some of the follow-up analyses to work smoothly.\n\n30.4.1 The Main cph results\n\nmodel_hem\n\nCox Proportional Hazards Model\n \n cph(formula = hemsurv ~ genotype, data = hem, x = TRUE, y = TRUE, \n     surv = TRUE)\n \n                        Model Tests    Discrimination    \n                                              Indexes    \n Obs        70    LR chi2      6.61    R2       0.106    \n Events     18    d.f.            1    R2(1,70) 0.077    \n Center 0.7229    Pr(> chi2) 0.0102    R2(1,18) 0.268    \n                  Score chi2   6.28    Dxy      0.244    \n                  Pr(> chi2) 0.0122                      \n \n          Coef   S.E.   Wald Z Pr(>|Z|)\n genotype 1.3317 0.5699 2.34   0.0195  \n \n\n\nIncluded here are likelihood ratio and score tests for the model as a whole (as compared to the intercept-only model), as well as the usual discrimination indexes.\n\nThese include both an \\(R^2\\) analog due to Nagelkerke (which can go all the way up to 1), and\nSomers’ \\(Dxy\\), which can also produce an estimate of the C statistic (area under the curve) via the formula C = 0.5 + Dxy / 2, so here C = 0.5 + (.244/2) = 0.622\nFor lots more on survival analysis C statistics, look at the survAUC package in R.\n\nThese results are followed by a table of Wald tests for each of the coefficients in the model.\n\n\n30.4.2 Using anova with cph\nAs in other rms fits, we can use anova to obtain more detailed (in terms of combining nonlinear terms and, if available, interactions) tests.\n\nanova(model_hem)\n\n                Wald Statistics          Response: hemsurv \n\n Factor     Chi-Square d.f. P     \n genotype   5.46       1    0.0195\n TOTAL      5.46       1    0.0195\n\n\n\n\n30.4.3 Effect Sizes after cph fit\nWe can use summary on a cph object to get and plot effect size estimates (here, these are hazard ratios.)\n\nsummary(model_hem)\n\n             Effects              Response : hemsurv \n\n Factor        Low High Diff. Effect S.E.   Lower 0.95 Upper 0.95\n genotype      0   1    1     1.3317 0.5699 0.21468     2.4486   \n  Hazard Ratio 0   1    1     3.7873     NA 1.23950    11.5730   \n\nplot(summary(model_hem))\n\n\n\n\n\n\n30.4.4 Validating cph summaries\nFor details on these last few indices (D, U, Q, etc.), visit ?validate.cph in R.\n\nset.seed(43201); validate(model_hem)\n\n      index.orig training    test optimism index.corrected  n\nDxy       0.2441   0.2508  0.2197   0.0311          0.2130 40\nR2        0.1058   0.1203  0.1058   0.0144          0.0914 40\nSlope     1.0000   1.0000 -6.4747   7.4747         -6.4747 40\nD         0.0421   0.0532  0.0421   0.0111          0.0310 40\nU        -0.0150  -0.0168  0.0198  -0.0366          0.0216 40\nQ         0.0571   0.0699  0.0222   0.0477          0.0094 40\ng         0.6705   0.7908  0.6705   0.1203          0.5502 40\n\n\n\n\n30.4.5 Plotting Survival Functions for each Genotype\nHere is the survplot approach I mentioned earlier.\n\nsurvplot(model_hem, genotype, \n         lty = c(1,2), n.risk=TRUE, time.inc=12,\n         col=c(\"magenta\", \"dodgerblue\"),\n         xlab=\"Hemorrhage-Free Survival in Months\")\n\n\n\n\nWe can add, for instance, confidence interval bars, with:\n\nsurvplot(model_hem, genotype, \n         lwd=3, lty = c(1,2), conf.int=.95,\n         n.risk=TRUE, time.inc = 12, conf='bars',\n         col=c(\"magenta\", \"dodgerblue\"),\n         xlab=\"Hemorrhage-Free Survival Time in Months\")\n\n\n\n\nFor more details, check out R’s help file on survplot.\n\n\n30.4.6 Genotype’s effect on log relative hazard\n\nggplot(Predict(model_hem, genotype))\n\n\n\n\n\n\n30.4.7 Nomogram of our simple hem model\nWe can estimate 1-year and 3-year hemorrhage-free survival probabilities, for example, with this model, and incorporate these results into our nomogram.\n\nsurvx <- Survival(model_hem)\nplot(nomogram(model_hem, fun=list(function(x) survx(12, x),\n                            function(x) survx(36, x)),\n            funlabel=c(\"12-month Pr(Survival)\", \n                       \"36-month Pr(Survival)\")))\n\n\n\n\nAgain, this is just a very simple model, with one binary predictor.\n\n\n30.4.8 Assessing the Proportional Hazards Assumption\n\ncox.zph(model_hem, transform=\"km\")\n\n         chisq df    p\ngenotype  2.09  1 0.15\nGLOBAL    2.09  1 0.15\n\n\n\nConsider using transform=\"rank\" to transform the survival times by their ranks prior to performing the test.\nOr use transform=\"identity\" as we’ll do in the plot below.\n\n\n\n30.4.9 Plot to Check PH Assumption\n\nplot(cox.zph(model_hem, \"identity\"))"
  },
  {
    "objectID": "cox2.html#r-setup-used-here",
    "href": "cox2.html#r-setup-used-here",
    "title": "31  Cox Regression Models, Part 2",
    "section": "31.1 R Setup Used Here",
    "text": "31.1 R Setup Used Here\n\nknitr::opts_chunk$set(comment = NA)\n\nlibrary(broom)\nlibrary(survival) \nlibrary(survminer)\nlibrary(rms)\nlibrary(tidyverse) \n\ntheme_set(theme_bw())\n\n\n31.1.1 Data Load\n\nleukem <- read_csv(\"data/leukem.csv\", show_col_types = FALSE)"
  },
  {
    "objectID": "cox2.html#a-second-example-the-leukem-data",
    "href": "cox2.html#a-second-example-the-leukem-data",
    "title": "31  Cox Regression Models, Part 2",
    "section": "31.2 A Second Example: The leukem data",
    "text": "31.2 A Second Example: The leukem data\n\nleukem\n\n# A tibble: 51 × 8\n      id   age pblasts  pinf  plab maxtemp months alive\n   <dbl> <dbl>   <dbl> <dbl> <dbl>   <dbl>  <dbl> <dbl>\n 1     1    20      78    39     7    99       18     0\n 2     2    25      64    61    16   103       31     1\n 3     3    26      61    55    12    98.2     31     0\n 4     4    26      64    64    16   100       31     0\n 5     5    27      95    95     6    98       36     0\n 6     6    27      80    64     8   101        1     0\n 7     7    28      88    88    20    98.6      9     0\n 8     8    28      70    70    14   101       39     1\n 9     9    31      72    72     5    98.8     20     1\n10    10    33      58    58     7    98.6      4     0\n# … with 41 more rows\n\n\nThe data describe 51 leukemia patients. The variables are:\n\nid, a patient identification code\nage, age at diagnosis\npblasts, the Smear differential percentage of blasts\npinf, the Percentage of absolute marrow leukemia infiltrate\nplab, the Percentage labeling index of the bone marrow leukemia cells\nmaxtemp, Highest temperature prior to treatment (in \\(^\\circ F\\))\nmonths, which is Survival time from diagnosis (in months)\nalive, which indicates Status as of the end of the study (1 = alive and thus censored, 0 = dead)\n\n\nglimpse(leukem)\n\nRows: 51\nColumns: 8\n$ id      <dbl> 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18,…\n$ age     <dbl> 20, 25, 26, 26, 27, 27, 28, 28, 31, 33, 33, 33, 34, 36, 37, 40…\n$ pblasts <dbl> 78, 64, 61, 64, 95, 80, 88, 70, 72, 58, 92, 42, 26, 55, 71, 91…\n$ pinf    <dbl> 39, 61, 55, 64, 95, 64, 88, 70, 72, 58, 92, 38, 26, 55, 71, 91…\n$ plab    <dbl> 7, 16, 12, 16, 6, 8, 20, 14, 5, 7, 5, 12, 7, 14, 15, 9, 12, 4,…\n$ maxtemp <dbl> 99.0, 103.0, 98.2, 100.0, 98.0, 101.0, 98.6, 101.0, 98.8, 98.6…\n$ months  <dbl> 18, 31, 31, 31, 36, 1, 9, 39, 20, 4, 45, 36, 12, 8, 1, 15, 24,…\n$ alive   <dbl> 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,…\n\n\n\n31.2.1 Creating our response: A survival time object\nRegardless of how we’re going to fit a survival model, we start by creating a survival time object that combines the information in months (the survival times, possibly censored) and alive (the censoring indicator) into a single variable we’ll call stime in this example.\nThe function below correctly registers the survival time, and censors subjects who are alive at the end of the study (we need to indicate those whose times are known, and they are identified by alive == 0). All other subjects are alive for at least as long as we observe them, but their exact survival times are right-censored.\n\nstime <- Surv(leukem$months, leukem$alive == 0)\nstime\n\n [1] 18  31+ 31  31  36   1   9  39+ 20+  4  45+ 36  12   8   1  15  24   2  33 \n[20] 29+  7   0   1   2  12   9   1   1   9   5  27+  1  13   1   5   1   3   4 \n[39]  1  18   1   2   1   8   3   4  14   3  13  13   1 \n\n\n\n\n31.2.2 Models We’ll Fit\nWe’ll fit several models here, including:\n\nModel A: A model for survival time using age at diagnosis alone.\nModel B: A model for survival time using the main effects of 5 predictors, specifically, age, pblasts, pinf, plab, and maxtemp.\nModel B2: The model we get after applying stepwise variable selection to Model B, which will include age, pinf and plab.\nModel C: A model using age (with a restricted cubic spline), plab and maxtemp"
  },
  {
    "objectID": "cox2.html#model-a-coxph-model-for-survival-time-using-age-at-diagnosis",
    "href": "cox2.html#model-a-coxph-model-for-survival-time-using-age-at-diagnosis",
    "title": "31  Cox Regression Models, Part 2",
    "section": "31.3 Model A: coxph Model for Survival Time using age at diagnosis",
    "text": "31.3 Model A: coxph Model for Survival Time using age at diagnosis\nWe’ll start by using age at diagnosis to predict our survival object (survival time, accounting for censoring).\n\nmodA <- coxph(Surv(months, alive==0) ~ age, \n              data=leukem, model=TRUE)\n\nsummary(modA)\n\nCall:\ncoxph(formula = Surv(months, alive == 0) ~ age, data = leukem, \n    model = TRUE)\n\n  n= 51, number of events= 45 \n\n        coef exp(coef) se(coef)     z Pr(>|z|)    \nage 0.032397  1.032927 0.009521 3.403 0.000667 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n    exp(coef) exp(-coef) lower .95 upper .95\nage     1.033     0.9681     1.014     1.052\n\nConcordance= 0.65  (se = 0.047 )\nLikelihood ratio test= 11.85  on 1 df,   p=6e-04\nWald test            = 11.58  on 1 df,   p=7e-04\nScore (logrank) test = 12.29  on 1 df,   p=5e-04\n\nglance(modA) %>%\n    select(r.squared, r.squared.max, \n           concordance, std.error.concordance)\n\n# A tibble: 1 × 4\n  r.squared r.squared.max concordance std.error.concordance\n      <dbl>         <dbl>       <dbl>                 <dbl>\n1     0.207         0.996       0.650                0.0465\n\n\nAcross these 51 subjects, we observe 45 events (deaths) and 6 subjects are censored. The hazard ratio (shown under exp(coef)) is 1.0329272, and this means each additional year of age at diagnosis is associated with a 1.03-fold increase in the hazard of death.\nFor this simple Cox regression model, we will focus on interpreting\n\nthe hazard ratio (specified by the exp(coef) result and associated confidence interval) as a measure of effect size,\n\nHere, the hazard ratio associated with a 1-year increase in age is 1.033, and its 95% confidence interval is: (1.014, 1.052).\nSince this confidence interval ratio does not include 1 (barely), we can conclude that there is a significant association between age and stime at the 5% significance level.\n\nthe concordance and Rsquare as measures of fit quality, and\n\nconcordance is only appropriate when we have at least one continuous predictor in our Cox model, in which case it assesses the probability of agreement between the survival time and the risk score generated by the predictor (or set of predictors.) A value of 1 indicates perfect agreement, but values of 0.6 to 0.7 are more common in survival data. 0.5 is an agreement that is no better than chance. Here, our concordance is 0.65, which is a fairly typical value.\nRsquare in this setting is Cox and Snell’s pseudo-\\(R^2\\), which reflects the improvement of the model we have fit over the model with the intercept alone - a comparison that is tested by the likelihood ratio test. The maximum value of this statistic is often less than one, in which case R will tell you that. Here, our observed pseudo-\\(R^2\\) is 0.207 and that is out of a possible maximum of 0.996.\n\nthe significance tests, particularly the Wald test (shown next to the coefficient estimates in the position of a t test in linear regression), and the Likelihood ratio test at the bottom of the output, which compares this model to a null model which predicts the mean survival time for all subjects.\n\nThe Wald test for an individual predictor compares the coefficient to its standard error, just like a t test in linear regression.\nThe likelihood ratio test compares the entire model to the null model (intercept-only). Again, run an ANOVA (technically an analysis of deviance) to get more details on the likelihood-ratio test.\n\n\n\nanova(modA)\n\nAnalysis of Deviance Table\n Cox model: response is Surv(months, alive == 0)\nTerms added sequentially (first to last)\n\n      loglik  Chisq Df Pr(>|Chi|)    \nNULL -142.94                         \nage  -137.02 11.849  1   0.000577 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\n31.3.1 Plotting the Survival Curve implied by Model A\n\nplot(survfit(modA), ylab=\"Probability of Survival\",\n     xlab=\"Months in Study\", col=c(\"red\", \"black\", \"black\"))\n\n\n\n\n\n\n31.3.2 Testing the Proportional Hazards Assumption\nAs we’ve noted, the key assumption in a Cox model is that the hazards are proportional.\n\ncox.zph(modA)\n\n       chisq df    p\nage     1.05  1 0.31\nGLOBAL  1.05  1 0.31\n\n\nA significant result here would indicate a problem with the proportional hazards assumption - again, not the case here. We can also plot the results:\n\nplot(cox.zph(modA))\n\n\n\n\nWe’re looking for the smooth curve to be fairly level across the time horizon here, as opposed to substantially increasing or decreasing in level as time passes."
  },
  {
    "objectID": "cox2.html#building-model-a-with-cph-for-the-leukem-data",
    "href": "cox2.html#building-model-a-with-cph-for-the-leukem-data",
    "title": "31  Cox Regression Models, Part 2",
    "section": "31.4 Building Model A with cph for the leukem data",
    "text": "31.4 Building Model A with cph for the leukem data\n\nunits(leukem$months) <- \"month\"\nd <- datadist(leukem)\noptions(datadist=\"d\")\nmodA_cph <- cph(Surv(months, alive==0) ~ age, data=leukem, \n              x=TRUE, y=TRUE, surv=TRUE, time.inc=12)\n\n\nmodA_cph\n\nCox Proportional Hazards Model\n \n cph(formula = Surv(months, alive == 0) ~ age, data = leukem, \n     x = TRUE, y = TRUE, surv = TRUE, time.inc = 12)\n \n                        Model Tests    Discrimination    \n                                              Indexes    \n Obs        51    LR chi2     11.85    R2       0.208    \n Events     45    d.f.            1    R2(1,51) 0.192    \n Center 1.6152    Pr(> chi2) 0.0006    R2(1,45) 0.214    \n                  Score chi2  12.29    Dxy      0.301    \n                  Pr(> chi2) 0.0005                      \n \n     Coef   S.E.   Wald Z Pr(>|Z|)\n age 0.0324 0.0095 3.40   0.0007  \n \n\nexp(coef(modA_cph)) # hazard ratio estimate\n\n     age \n1.032923 \n\nexp(confint(modA_cph)) # hazard ratio 95% CI\n\n       2.5 %   97.5 %\nage 1.013826 1.052379\n\n\n\n31.4.1 Plotting the age effect implied by our model.\nWe can plot the age effect implied by the model, using ggplot2, as follows…\n\nggplot(Predict(modA_cph, age))\n\n\n\n\n\n\n31.4.2 Survival Plots (Kaplan-Meier) of the age effect\nThe first survival plot I’ll show displays 95% confidence intervals for the probability of survival at the median age at diagnosis in the sample, which turns out to be 50 years, with numbers of patients still at risk indicated every 12 months of time in the study. We can substitute in conf = bars to get a different flavor for this sort of plot.\n\nsurvplot(modA_cph, age=median(leukem$age), conf.int=.95, \n         col='blue', time.inc=12, n.risk=TRUE,\n         conf='bands', type=\"kaplan-meier\", \n         xlab=\"Study Survival Time in Months\")\n\n\n\n\nOr we can generate a survival plot that shows survival probabilities over time across a range of values for age at diagnosis, as follows…\n\nsurvplot(modA_cph, levels.only=TRUE, time.inc=12, \n         type=\"kaplan-meier\", \n         xlab=\"Study Survival Time in Months\")\n\nWarning in regularize.values(x, y, ties, missing(ties), na.rm = na.rm):\ncollapsing to unique 'x' values\n\nWarning in regularize.values(x, y, ties, missing(ties), na.rm = na.rm):\ncollapsing to unique 'x' values\n\nWarning in regularize.values(x, y, ties, missing(ties), na.rm = na.rm):\ncollapsing to unique 'x' values\n\nWarning in regularize.values(x, y, ties, missing(ties), na.rm = na.rm):\ncollapsing to unique 'x' values\n\nWarning in regularize.values(x, y, ties, missing(ties), na.rm = na.rm):\ncollapsing to unique 'x' values\n\n\n\n\n\nThis plot shows a series of modeled survival probabilities, for five different diagnosis age levels, as identified by the labels. Generally we see that the younger the subject is at diagnosis, the longer their survival time in the study.\n\n\n31.4.3 ANOVA test for the cph-built model for leukem\nWe can run a likelihood-ratio (drop in deviance) test of the significance of the age effect…\n\nanova(modA_cph)\n\n                Wald Statistics          Response: Surv(months, alive == 0) \n\n Factor     Chi-Square d.f. P    \n age        11.57      1    7e-04\n TOTAL      11.57      1    7e-04\n\n\n\n\n31.4.4 Summarizing the Effect Sizes from modA_cph\nWe can generate the usual summaries of effect size in this context, too.\n\nsummary(modA_cph)\n\n             Effects              Response : Surv(months, alive == 0) \n\n Factor        Low High Diff. Effect S.E.    Lower 0.95 Upper 0.95\n age           35  61   26    0.8422 0.24755 0.35701    1.3274    \n  Hazard Ratio 35  61   26    2.3215      NA 1.42910    3.7712    \n\nplot(summary(modA_cph))\n\n\n\n\nAs with all rms package effect estimates, this quantitative predictor (age) yields an effect comparing age at the 25th percentile of the sample (age = 35) to age at the 75th percentile (age = 61). So the hazard ratio is 2.32, with 95% CI (1.43, 3.77) for the effect of moving 26 years. Our coxph version of this same model showed a hazard ratio for the effect of moving just a single year.\n\n\n31.4.5 Validating the Cox Model Summary Statistics\n\nset.seed(432410); validate(modA_cph)\n\n      index.orig training   test optimism index.corrected  n\nDxy       0.3007   0.2950 0.3007  -0.0057          0.3064 40\nR2        0.2081   0.2226 0.2081   0.0145          0.1936 40\nSlope     1.0000   1.0000 1.0253  -0.0253          1.0253 40\nD         0.0379   0.0425 0.0379   0.0045          0.0334 40\nU        -0.0070  -0.0070 0.0037  -0.0107          0.0037 40\nQ         0.0449   0.0495 0.0343   0.0152          0.0297 40\ng         0.6210   0.6492 0.6210   0.0282          0.5928 40\n\n\nThe \\(R^2\\) statistic barely moves, and neither does the Somers’ d estimate, so at least in this simple model, the nominal summary statistics are likely to hold up pretty well in new data.\n\n\n31.4.6 Looking for Influential Points\nThis plot shows the influence of each point, in terms of DFBETA - the impact on the coefficient of age in the model were that specific point to be removed from the data set. We can also identify the row numbers of the largest (positive and negative) DFBETAs.\n\nplot(residuals(modA_cph, type=\"dfbeta\", \n               collapse = leukem$id) ~ \n       leukem$id, main=\"Index Plot of DFBETA for Age\", \n     type=\"h\", ylab=\"DFBETA in modelA_cph\")\n\n\n\nwhich.max(residuals(modA_cph, type=\"dfbeta\"))\n\n8 \n8 \n\nwhich.min(residuals(modA_cph, type=\"dfbeta\"))\n\n50 \n50 \n\n\nThe DFBETAs look very small here. Changes in the \\(\\beta\\) estimates as large as 0.002 don’t have a meaningful impact in this case, so I don’t see anything particularly influential.\n\n\n31.4.7 Checking the Proportional Hazards Assumption\nAs before, we can check the proportional hazards assumption with a test, or plot.\n\ncox.zph(modA_cph)\n\n       chisq df    p\nage     1.05  1 0.31\nGLOBAL  1.05  1 0.31\n\nplot(cox.zph(modA_cph))\n\n\n\n\nStill no serious signs of trouble, of course. We’ll see what happens when we fit a bigger model."
  },
  {
    "objectID": "cox2.html#model-b-fitting-a-5-predictor-model-with-coxph",
    "href": "cox2.html#model-b-fitting-a-5-predictor-model-with-coxph",
    "title": "31  Cox Regression Models, Part 2",
    "section": "31.5 Model B: Fitting a 5-Predictor Model with coxph",
    "text": "31.5 Model B: Fitting a 5-Predictor Model with coxph\nNext, we use the coxph function from the survival package to apply a Cox regression model to predict the survival time using the main effects of the five predictors: age, pblasts, pinf, plab and maxtemp.\n\nmodB <- coxph(Surv(months, alive==0) ~ \n          age + pblasts + pinf + plab + maxtemp, data=leukem)\nmodB\n\nCall:\ncoxph(formula = Surv(months, alive == 0) ~ age + pblasts + pinf + \n    plab + maxtemp, data = leukem)\n\n             coef exp(coef)  se(coef)      z       p\nage      0.033080  1.033633  0.010163  3.255 0.00113\npblasts  0.009452  1.009497  0.013959  0.677 0.49831\npinf    -0.017102  0.983043  0.012244 -1.397 0.16248\nplab    -0.066000  0.936131  0.038651 -1.708 0.08771\nmaxtemp  0.155448  1.168182  0.111978  1.388 0.16507\n\nLikelihood ratio test=18.48  on 5 df, p=0.002405\nn= 51, number of events= 45 \n\n\nThe Wald tests suggest that age and perhaps plab are the only terms which appear to be significant after accounting for the other predictors in the model.\n\n31.5.1 Interpreting the Results from Model B\n\nsummary(modB)\n\nCall:\ncoxph(formula = Surv(months, alive == 0) ~ age + pblasts + pinf + \n    plab + maxtemp, data = leukem)\n\n  n= 51, number of events= 45 \n\n             coef exp(coef)  se(coef)      z Pr(>|z|)   \nage      0.033080  1.033633  0.010163  3.255  0.00113 **\npblasts  0.009452  1.009497  0.013959  0.677  0.49831   \npinf    -0.017102  0.983043  0.012244 -1.397  0.16248   \nplab    -0.066000  0.936131  0.038651 -1.708  0.08771 . \nmaxtemp  0.155448  1.168182  0.111978  1.388  0.16507   \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n        exp(coef) exp(-coef) lower .95 upper .95\nage        1.0336     0.9675    1.0132     1.054\npblasts    1.0095     0.9906    0.9823     1.037\npinf       0.9830     1.0172    0.9597     1.007\nplab       0.9361     1.0682    0.8678     1.010\nmaxtemp    1.1682     0.8560    0.9380     1.455\n\nConcordance= 0.705  (se = 0.039 )\nLikelihood ratio test= 18.48  on 5 df,   p=0.002\nWald test            = 17.62  on 5 df,   p=0.003\nScore (logrank) test = 18.96  on 5 df,   p=0.002\n\n\nAgain, it appears that only age has a statistically significant effect, as last predictor in.\n\nanova(modB)\n\nAnalysis of Deviance Table\n Cox model: response is Surv(months, alive == 0)\nTerms added sequentially (first to last)\n\n         loglik   Chisq Df Pr(>|Chi|)    \nNULL    -142.94                          \nage     -137.02 11.8488  1   0.000577 ***\npblasts -136.64  0.7537  1   0.385319    \npinf    -135.77  1.7308  1   0.188314    \nplab    -134.60  2.3370  1   0.126334    \nmaxtemp -133.70  1.8057  1   0.179022    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nAt least taken in this order, none of the variables appear to add significant predictive value, given that we have already accounted for the preceding variables.\n\n\n31.5.2 Plotting the Survival Curve implied by Model B\n\nplot(survfit(modB), ylab=\"Probability of Survival\", \n     xlab=\"Months in Study\", col=c(\"red\", \"black\", \"black\"))\n\n\n\n\nThe crosses in the plot indicate censoring points, while the drops indicate people who have died, and are thus no longer at risk.\n\n\n31.5.3 Testing the Proportional Hazards Assumption\n\ncox.zph(modB, transform=\"km\", global=TRUE)\n\n        chisq df     p\nage      1.87  1 0.171\npblasts  4.37  1 0.037\npinf     3.51  1 0.061\nplab     1.19  1 0.275\nmaxtemp  1.53  1 0.216\nGLOBAL   9.22  5 0.101\n\n\nNote that we get a global test, and a separate test for each predictor. None show significant problems. We can plot the scaled Schoenfeld residuals directly with ggcoxzph from the survminer package.\n\nggcoxzph(cox.zph(modB))\n\n\n\n\n\n\n31.5.4 Assessing Collinearity\nPerhaps we have some collinearity here, which might imply that we could sensibly fit a smaller model, which would be appealing anyway, with only 45 actual events - we should probably be sticking to a model with no more than 2 or perhaps as many as 3 coefficients to be estimated.\n\nrms::vif(modB)\n\n     age  pblasts     pinf     plab  maxtemp \n1.081775 3.029862 3.000944 1.035400 1.045249 \n\n\nThe variance inflation factors don’t look enormous - it may be that removing one of these variables will help make the others look more significant. Let’s consider a stepwise variable selection algorithm to see what results…\n\nNote that the leaps library, which generates best subsets output, is designed for linear regression, as is the lars library, which generates the lasso. Either could be used here for some guidance, but not with the survival object stime = Surv(months, age) as the response, but instead only with months as the outcome, which ignores the censoring. The step procedure can be used on the survival object, though."
  },
  {
    "objectID": "cox2.html#model-b2-a-stepwise-reduction-of-model-b",
    "href": "cox2.html#model-b2-a-stepwise-reduction-of-model-b",
    "title": "31  Cox Regression Models, Part 2",
    "section": "31.6 Model B2: A Stepwise Reduction of Model B",
    "text": "31.6 Model B2: A Stepwise Reduction of Model B\n\nstats::step(modB)\n\nStart:  AIC=277.4\nSurv(months, alive == 0) ~ age + pblasts + pinf + plab + maxtemp\n\n          Df    AIC\n- pblasts  1 275.85\n- pinf     1 277.17\n- maxtemp  1 277.21\n<none>       277.40\n- plab     1 278.42\n- age      1 286.47\n\nStep:  AIC=275.85\nSurv(months, alive == 0) ~ age + pinf + plab + maxtemp\n\n          Df    AIC\n- maxtemp  1 275.63\n<none>       275.85\n- pinf     1 275.89\n- plab     1 276.86\n- age      1 284.47\n\nStep:  AIC=275.63\nSurv(months, alive == 0) ~ age + pinf + plab\n\n       Df    AIC\n<none>    275.63\n- pinf  1 275.69\n- plab  1 275.95\n- age   1 285.52\n\n\nCall:\ncoxph(formula = Surv(months, alive == 0) ~ age + pinf + plab, \n    data = leukem)\n\n          coef exp(coef)  se(coef)      z        p\nage   0.033171  1.033727  0.009733  3.408 0.000655\npinf -0.010147  0.989905  0.007088 -1.432 0.152246\nplab -0.057558  0.944067  0.038476 -1.496 0.134662\n\nLikelihood ratio test=16.25  on 3 df, p=0.001008\nn= 51, number of events= 45 \n\n\nThe stepwise procedure lands on a model with three predictors. How does this result look, practically?\n\nmodB2 <- coxph(Surv(months, alive==0) ~ age + pinf + plab, data=leukem)\nsummary(modB2)\n\nCall:\ncoxph(formula = Surv(months, alive == 0) ~ age + pinf + plab, \n    data = leukem)\n\n  n= 51, number of events= 45 \n\n          coef exp(coef)  se(coef)      z Pr(>|z|)    \nage   0.033171  1.033727  0.009733  3.408 0.000655 ***\npinf -0.010147  0.989905  0.007088 -1.432 0.152246    \nplab -0.057558  0.944067  0.038476 -1.496 0.134662    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n     exp(coef) exp(-coef) lower .95 upper .95\nage     1.0337     0.9674    1.0142     1.054\npinf    0.9899     1.0102    0.9762     1.004\nplab    0.9441     1.0592    0.8755     1.018\n\nConcordance= 0.676  (se = 0.046 )\nLikelihood ratio test= 16.25  on 3 df,   p=0.001\nWald test            = 15.28  on 3 df,   p=0.002\nScore (logrank) test = 16.21  on 3 df,   p=0.001\n\n\n\n31.6.1 The Survival Curve implied by Model B2\n\nplot(survfit(modB2), ylab=\"Probability of Survival\", xlab=\"Months in Study\", \n     col=c(\"red\", \"black\", \"black\"))\n\n\n\n\n\n\n31.6.2 Checking Proportional Hazards for Model B2\n\ncox.zph(modB2, transform=\"km\", global=TRUE)\n\n       chisq df     p\nage     1.66  1 0.197\npinf    3.20  1 0.074\nplab    2.17  1 0.141\nGLOBAL  6.07  3 0.108\n\nggcoxzph(cox.zph(modB2))"
  },
  {
    "objectID": "cox2.html#model-c-using-a-spearman-plot-to-pick-a-model",
    "href": "cox2.html#model-c-using-a-spearman-plot-to-pick-a-model",
    "title": "31  Cox Regression Models, Part 2",
    "section": "31.7 Model C: Using a Spearman Plot to pick a model",
    "text": "31.7 Model C: Using a Spearman Plot to pick a model\nIf we want to use the Spearman \\(\\rho^2\\) plot to consider how we might perhaps incorporate non-linear terms describing any or all of the five potential predictors (age, pblasts, pinf, plab and maxtemp) for survival time, we need to do so on the raw months variable, rather than the survival object (stime = Surv(months, alive==0)) which accounts for censoring…\n\nplot(spearman2(months ~ age + pblasts + pinf + plab + maxtemp, data=leukem))\n\n\n\n\nRecognizing that we can probably only fit a small model safely (since we observe only 45 actual [uncensored] survival times) I will consider a non-linear term in age (specifically a restricted cubic spline with 3 knots), along with linear terms for plab and maxtemp. I’m mostly just looking for a new model to study for this example.\n\n31.7.1 Fitting Model C\n\n# still have datadist set up for leukem\nmodC <- cph(Surv(months, alive==0) ~ rcs(age, 3) + plab + maxtemp, \n            data=leukem, x=TRUE, y=TRUE, surv=TRUE, time.inc=12)\nmodC\n\nCox Proportional Hazards Model\n \n cph(formula = Surv(months, alive == 0) ~ rcs(age, 3) + plab + \n     maxtemp, data = leukem, x = TRUE, y = TRUE, surv = TRUE, \n     time.inc = 12)\n \n                         Model Tests    Discrimination    \n                                               Indexes    \n Obs         51    LR chi2     19.75    R2       0.322    \n Events      45    d.f.            4    R2(4,51) 0.266    \n Center 20.3856    Pr(> chi2) 0.0006    R2(4,45) 0.295    \n                   Score chi2  18.66    Dxy      0.438    \n                   Pr(> chi2) 0.0009                      \n \n         Coef    S.E.   Wald Z Pr(>|Z|)\n age      0.0804 0.0283  2.84  0.0045  \n age'    -0.0629 0.0332 -1.90  0.0580  \n plab    -0.0736 0.0381 -1.93  0.0536  \n maxtemp  0.1788 0.1145  1.56  0.1184  \n \n\n\n\n\n31.7.2 ANOVA for Model C\n\nanova(modC)\n\n                Wald Statistics          Response: Surv(months, alive == 0) \n\n Factor     Chi-Square d.f. P     \n age        11.55      2    0.0031\n  Nonlinear  3.59      1    0.0580\n plab        3.73      1    0.0536\n maxtemp     2.44      1    0.1184\n TOTAL      17.13      4    0.0018\n\n\n\n\n31.7.3 Summarizing Model C Effect Sizes\n\nsummary(modC)\n\n             Effects              Response : Surv(months, alive == 0) \n\n Factor        Low  High  Diff. Effect   S.E.    Lower 0.95 Upper 0.95\n age           35.0  61.0 26.0   1.06010 0.31559  0.44152   1.6786000 \n  Hazard Ratio 35.0  61.0 26.0   2.88650      NA  1.55510   5.3580000 \n plab           6.5  13.5  7.0  -0.51511 0.26686 -1.03820   0.0079317 \n  Hazard Ratio  6.5  13.5  7.0   0.59744      NA  0.35411   1.0080000 \n maxtemp       98.6 100.5  1.9   0.33979 0.21758 -0.08666   0.7662400 \n  Hazard Ratio 98.6 100.5  1.9   1.40470      NA  0.91699   2.1517000 \n\nplot(summary(modC))\n\n\n\n\n\n\n31.7.4 Plotting the diagnosis age effect in Model C\nOf course, we’re no longer assuming that the log relative hazard is linear in age, once we include a restricted cubic spline for age in our Model C. So our hazard ratio and confidence intervals for age are a bit trickier to understand.\n\nexp(coef(modC))\n\n      age      age'      plab   maxtemp \n1.0837479 0.9390008 0.9290553 1.1958265 \n\nexp(confint(modC))\n\n            2.5 %   97.5 %\nage     1.0252687 1.145563\nage'    0.8798328 1.002148\nplab    0.8621662 1.001134\nmaxtemp 0.9554141 1.496734\n\n\nWe can use ggplot and the Predict function to produce plots of the log Relative Hazard associated with any of our predictors, while holding the others constant at their medians. The effects of maxtemp and plab in our Model C are linear in the log Relative Hazard, but age, thanks to our use of a restricted cubic spline with 3 knots, shows a single bend.\n\nggplot(Predict(modC, age))\n\n\n\n\n\n\n31.7.5 Survival Plot associated with Model C\nLet’s look at a survival plot associated with Model C for a subject with median values of our three predictors.\n\nsurvplot(modC, age=median(leukem$age), conf.int=0.95, col=\"blue\", \n         time.inc=12, n.risk=TRUE, conf='bands',\n         xlab=\"Study Time in Months\")\n\n\n\n\nAs before, we could fit such a plot to compare results across multiple age values, if desired.\n\n\n31.7.6 Checking the Proportional Hazards Assumption\n\ncox.zph(modC, transform=\"km\", global=TRUE)\n\n            chisq df     p\nrcs(age, 3)  2.39  2 0.303\nplab         1.55  1 0.214\nmaxtemp      1.14  1 0.285\nGLOBAL       8.27  4 0.082\n\nggcoxzph(cox.zph(modC))\n\n\n\n\n\n\n31.7.7 Model C Nomogram\n\nsv <- Survival(modC)\nsurv12 <- function(x) sv(12, lp=x)\nsurv24 <- function(x) sv(24, lp=x)\n\nplot(nomogram(modC, fun=list(surv12, surv24), \n              funlabel=c('12-month survival', '24-month survival')))\n\n\n\n\n\n\n31.7.8 Validating Model C’s Summary Statistics\nWe can validate the model for Somers’ \\(D_{xy}\\), which is the rank correlation between the predicted log hazard and observed survival time, and for slope shrinkage.\n\nset.seed(43234); validate(modC, B=100)\n\n      index.orig training   test optimism index.corrected   n\nDxy       0.4385   0.4749 0.4144   0.0605          0.3780 100\nR2        0.3222   0.3716 0.2785   0.0931          0.2292 100\nSlope     1.0000   1.0000 0.8101   0.1899          0.8101 100\nD         0.0656   0.0814 0.0547   0.0266          0.0390 100\nU        -0.0070  -0.0070 0.0101  -0.0171          0.0101 100\nQ         0.0726   0.0884 0.0446   0.0437          0.0288 100\ng         0.9466   1.0751 0.8364   0.2387          0.7078 100\n\n\n\n\n31.7.9 Calibration of Model C (12-month survival estimates)\nFinally, we validate the model for calibration accuracy in predicting the probability of surviving one year.\nQuoting Harrell (page 529, RMS second edition):\n\nThe bootstrap is used to estimate the optimism in how well predicted [one]-year survival from the final Cox model tracks flexible smooth estimates, without any binning of predicted survival probabilities or assuming proportional hazards.\n\nThe u variable specifies the length of time at which we look at the calibration. I’ve specified the units earlier to be months.\n\nset.seed(43233); plot(rms::calibrate(modC, B = 10, u = 12))\n\nUsing Cox survival estimates at 12 months\n\n\n\n\n\nThe model seems neither especially well calibrated nor especially poorly so - looking at the comparison of the blue curve to the gray, our predictions basically aren’t aggressive enough - more people are surviving to a year in our low predicted probability of 12 month survival group, and fewer people are surviving on the high end of the x-axis than should be the case."
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Barnett, Peggy A., Simon Roman-Golstein, Fred Ramsey, et al. 1995.\n“Differential Permeability and Quantitative MR Imaging of a Human\nLung Carcinoma Brain Xenograft in the Nude Rat.” American\nJournal of Pathology 146(2): 436–49. https://www.ncbi.nlm.nih.gov/pmc/articles/PMC1869863/.\n\n\nBerkhemer, Olvert A., Puck S. S. Fransen, Debbie Buemer, et al. 2015.\n“A Randomized Trial of Intraarterial Treatment for Acute Ischemic\nStroke.” New England Journal of Medicine 372: 11–20. http://www.nejm.org/doi/full/10.1056/NEJMoa1411587.\n\n\nFaraway, Julian J. 2006. Extending the Linear Model with r.\nBoca Raton, FL: CRC Press.\n\n\nGelman, Andrew, and Jennifer Hill. 2007. Data Analysis Using\nRegression and Multilevel/Hierarchical Models. New York: Cambridge\nUniversity Press.\n\n\nHarrell, Frank E. 2001. Regression Modeling Strategies. New\nYork: Springer.\n\n\n———. 2018. Regression Modeling Strategies Course Notes. http://www.fharrell.com/#links.\n\n\nKim, Hae-Young. 2014. “Statistical Notes for Clinical Researchers:\nTwo-Way Analysis of Variance (ANOVA) - Exploring Possible Interaction\nBetween Factors.” Restorative Dentistry &\nEndodontics 39(2): 143–47. https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3978106/.\n\n\nLong, J. Scott. 1997. Regression Models for Categorical and Limited\nDependent Variables. Thousand Oaks, CA: Sage Publications.\n\n\nMcDonald, Gary C., and Richard C. Schwing. 1973. “Instabilities of\nRegression Estimates Relating Air Pollution to Mortality.”\nTechnometrics 15 (3): 463–81.\n\n\nPeduzzi, Peter, John Concato, Elizabeth Kemper, Theodore R. Holford, and\nAlvan R. Feinstein. 1996. “A Simulation Study of the Number of\nEvents Per Variable in Logistic Regression Analysis.” Journal\nof Clinical Epidemiology 49 (12): 1373–79.\n\n\nRamsey, Fred L., and Daniel W. Schafer. 2002. The Statistical\nSleuth: A Course in Methods of Data Analysis. Second Edition.\nPacific Grove, CA: Duxbury.\n\n\nRiffenburgh, Robert H. 2006. Statistics in Medicine. Second\nEdition. Burlington, MA: Elsevier Academic Press.\n\n\nRosenbaum, Paul R. 2017. Observation and Experiment: An Introduction\nto Causal Inference. Cambridge, MA: Harvard University Press.\n\n\nRoy, Denis, Mario Talajic, Stanley Nattel, et al. 2008. “Rhythm\nControl Versus Rate Control for Atrial Fibrillation and Heart\nFailure.” New England Journal of Medicine 358: 2667–77.\nhttp://www.nejm.org/doi/full/10.1056/NEJMoa0708789.\n\n\nStamey, J. N. Kabalin, T. A. et al. 1989. “Prostate Specific\nAntigen in the Diagnosis and Treatment of Adenocarcinoma of the\nProstate: II. Radical Prostatectomy Treated Patients.”\nJournal of Urology 141(5): 1076–83. https://www.ncbi.nlm.nih.gov/pubmed/2468795.\n\n\nTolaney, Sara M, William T. Barry, T. Dang Chau, et al. 2015.\n“Adjuvant Paclitaxel and Trastuzumab for Node-Negative,\nHER2-Positive Breast Cancer.” New England Journal of\nMedicine 372: 134–41. http://www.nejm.org/doi/full/10.1056/NEJMoa1406281.\n\n\nVittinghoff, Eric, David V. Glidden, Stephen C. Shiboski, and Charles E.\nMcCulloch. 2012. Regression Methods in Biostatistics: Linear,\nLogistic, Survival, and Repeated Measures Models. Second Edition.\nSpringer-Verlag, Inc. http://www.biostat.ucsf.edu/vgsm/."
  }
]