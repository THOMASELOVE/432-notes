[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Data Science for Biological, Medical and Health Research",
    "section": "",
    "text": "Introduction\nThese Notes provide a series of examples using R to work through issues that are likely to come up in PQHS/CRSP/MPHP 432.\nWhile these Notes share some of the features of a textbook, they are neither comprehensive nor completely original. The main purpose is to give students in 432 a set of common materials on which to draw during the course. In class, we will sometimes:\n\nreiterate points made in this document,\namplify what is here,\nsimplify the presentation of things done here,\nuse new examples to show some of the same techniques,\nrefer to issues not mentioned in this document,\n\nbut what we don’t (always) do is follow these notes very precisely. We assume instead that you will read the materials and try to learn from them, just as you will attend classes and try to learn from them. We welcome feedback of all kinds on this document or anything else via our Campuswire discussion forum.\nWhat you will mostly find are brief explanations of a key idea or summary, accompanied (most of the time) by R code and a demonstration of the results of applying that code.\nEverything you see here is available to you in this HTML document. You will also have access to the Quarto files, which contain the code which generates everything in the document, including all of the R results. We will demonstrate the use of Quarto and R Studio (the “program” which we use to interface with the R language) in class. At the end of the semester, I hope to be able to get the PDF version of this document posted, but that may be a challenge.\nTo download the data and R code related to these notes, visit the 432-data page."
  },
  {
    "objectID": "setup.html#general-theme-for-ggplot-work",
    "href": "setup.html#general-theme-for-ggplot-work",
    "title": "R Setup",
    "section": "General Theme for ggplot work",
    "text": "General Theme for ggplot work\nDr. Love prefers theme_bw() to the default choice.\n\ntheme_set(theme_bw())"
  },
  {
    "objectID": "setup.html#data-used-in-these-notes",
    "href": "setup.html#data-used-in-these-notes",
    "title": "R Setup",
    "section": "Data used in these notes",
    "text": "Data used in these notes\nAll data sets used in these notes are available on our 432-data website.\nDr. Love is in the process of moving all of the data loads below to their individual chapters.\n\nprost <- read_csv(\"data/prost.csv\", show_col_types = FALSE) \npollution <- read_csv(\"data/pollution.csv\", show_col_types = FALSE) \n\nemphysema <- read_csv(\"data/emphysema.csv\", show_col_types = FALSE) \nresect <- read_csv(\"data/resect.csv\", show_col_types = FALSE) \ncolscr <- read_csv(\"data/screening.csv\", show_col_types = FALSE) \ncolscr2 <- read_csv(\"data/screening2.csv\", show_col_types = FALSE) \nauthorship <- read_csv(\"data/authorship.csv\", show_col_types = FALSE) \nhem <- read_csv(\"data/hem.csv\", show_col_types = FALSE) \nleukem <- read_csv(\"data/leukem.csv\", show_col_types = FALSE)"
  },
  {
    "objectID": "01-nhanes.html#r-setup",
    "href": "01-nhanes.html#r-setup",
    "title": "1  Building the nh432 example",
    "section": "1.1 R Setup",
    "text": "1.1 R Setup\n\nknitr::opts_chunk$set(comment = NA)\n\nlibrary(janitor) \nlibrary(gt)\nlibrary(gtsummary)\nlibrary(Hmisc)\nlibrary(naniar)\nlibrary(nhanesA)\n\nlibrary(tidyverse) \n\ntheme_set(theme_bw())"
  },
  {
    "objectID": "01-nhanes.html#selecting-nhanes-variables",
    "href": "01-nhanes.html#selecting-nhanes-variables",
    "title": "1  Building the nh432 example",
    "section": "1.2 Selecting NHANES Variables",
    "text": "1.2 Selecting NHANES Variables\nWe’ll focus on NHANES data describing\n\nparticipating adults ages 30-59 years who\ncompleted both an NHANES interview and medical examination, and who\nalso completed an oral health examination, and who\nalso reported their overall health as either Excellent, Very Good, Good, Fair, or Poor\n\nWe will pull the following NHANES data elements using the nhanesA package:\n\n1.2.1 Demographics and Sample Weights\nFrom the Demographic Variables and Sample Weights database (P_DEMO), we collect the following variables.\n\n\n\n\n\n\n\n\nVariable\nDescription\nValues\n\n\n\n\nSEQN\nRespondent Sequence Number  (participant ID)\nlink across databases\n\n\nRIDSTATR\nInterview and MEC exam status\n1 = Interviewed only  2 = Interviewed & MEC examined\n\n\nRIDAGEYR\nAge at screening (years)  We will require ages 30-59.\nTop coded at 801\n\n\nRIDRETH3\nRace/Hispanic origin\n1 = Mexican American  2 = Other Hispanic  3 = Non-Hispanic White  4 = Non-Hispanic Black  6 = Non-Hispanic Asian  7 = Other Race, including Multi-Racial\n\n\nDMDEDUC2\nEducation Level\n1 = Less than 9th grade  2 = 9-11th grade  3 = High school graduate  4 = Some college or AA  5 = College graduate or above  7 = Refused (will treat as NA)  9 = Don’t Know (will treat as NA)\n\n\nRIAGENDR\nSex\n1 = Male, 2 = Female\n\n\nWTINTPRP\nFull sample interview weight\nSampling weight\n\n\nWTMECPRP\nFull sample MEC examination weight\nSampling weight\n\n\n\nNote As part of our inclusion criteria, we will require that all participants in our analytic data have RIDAGEYR values of 30-59 and thus drop the participants whose responses to that item are missing or outside that range.\nHere’s my code to select these variables from the P_DEMO data.\n\np_demo <- nhanes('P_DEMO') |>\n  select(SEQN, RIDSTATR, RIDAGEYR, RIDRETH3, DMDEDUC2, RIAGENDR,\n         WTINTPRP, WTMECPRP) \n\ndim(p_demo) # gives number of rows (participants) and columns (variables)\n\n[1] 15560     8\n\n\nDo we have any duplicate SEQN values?\n\np_demo |> get_dupes(SEQN)\n\nNo duplicate combinations found of: SEQN\n\n\n[1] SEQN       dupe_count RIDSTATR   RIDAGEYR   RIDRETH3   DMDEDUC2   RIAGENDR  \n[8] WTINTPRP   WTMECPRP  \n<0 rows> (or 0-length row.names)\n\n\nGood.\n\n\n1.2.2 Oral Health\nFrom the Oral Health - Recommendation of Care (P_OHXREF), we collect the following variables.\n\n\n\n\n\n\n\n\nVariable\nDescription\nValues\n\n\n\n\nSEQN\nRespondent Sequence Number  (participant ID)\nlink across databases\n\n\nOHDEXSTS\nOverall Oral Health Exam Status\n1 = Complete  2 = Partial  3 = Not Done\n\n\nOHAREC\nOverall Recommendation for Dental Care\n1 = See a dentist immediately  2 = See a dentist within the next 2 weeks  3 = See a dentist at your earliest convenience  4 = Continue your regular routine care\n\n\n\nNote In addition to requiring that all participants in our analytic data have OHDEXSTS = 1, we will (later) collapse values 1 and 2 in OHAREC because there are only a few participants with code 1 in OHAREC.\nHere’s my code to select these variables from the P_OHXREF data.\n\np_ohxref <- nhanes('P_OHXREF') |>\n  select(SEQN, OHDEXSTS, OHAREC)\n\ndim(p_ohxref)\n\n[1] 13772     3\n\n\n\n\n1.2.3 Hospital Utilization & Access to Care\nFrom the Questionnaire on Hospital Utilization & Access to Care (P_HUQ) we collect the following variables.\n\n\n\n\n\n\n\n\nVariable\nDescription\nValues\n\n\n\n\nSEQN\nRespondent Sequence Number  (participant ID)\nlink across databases\n\n\nHUQ010\nGeneral health condition  we require a 1-5 response\n1 = Excellent  2 = Very Good  3 = Good  4 = Fair  5 = Poor  7 = Refused (to be dropped)  9 = Don’t Know (to be dropped)\n\n\nHUQ071\nOvernight hospital patient in past 12 months\n1 = Yes, 2 = No  7 = Refused (will treat as NA)  9 = Don’t Know (will treat as NA)\n\n\nHUQ090\nSeen mental health professional in past 12 months\n1 = Yes, 2 = No  7 = Refused (will treat as NA)  9 = Don’t Know (will treat as NA)\n\n\n\nNote As part of our inclusion criteria, we will require that all participants in our analytic data have HUQ010 values of 1-5 and drop participants whose responses are missing, Refused or Don’t Know for that item.\nHere’s my code to select these variables from the P_HUQ data.\n\np_huq <- nhanes('P_HUQ') |>\n  select(SEQN, HUQ010, HUQ071, HUQ090)\n\ndim(p_huq)\n\n[1] 15560     4\n\n\n\n\n1.2.4 Body Measures\nFrom the Body Measures database (P_BMX), we collect the following variables.\n\n\n\n\n\n\n\n\nVariable\nDescription\nValues\n\n\n\n\nSEQN\nRespondent Sequence Number  (participant ID)\nlink across databases\n\n\nBMXWT\nBody weight (kg)\nMeasured in examination\n\n\nBMXHT\nStanding height (cm)\nMeasured in examination\n\n\nBMXWAIST\nWaist Circumference (cm)\nMeasured in examination\n\n\n\nHere’s my code to select these variables from the P_BMX data.\n\np_bmx <- nhanes('P_BMX') |>\n  select(SEQN, BMXWT, BMXHT, BMXWAIST)\n\ndim(p_bmx)\n\n[1] 14300     4\n\n\n\n\n1.2.5 Blood Pressure\nFrom the Blood Pressure - Oscillometric Measurement (P_BPXO), we collect the following variables.\n\n\n\n\n\n\n\n\nVariable\nDescription\nValues\n\n\n\n\nSEQN\nRespondent Sequence Number  (participant ID)\nlink across databases\n\n\nBPXOSY2\nSystolic BP  (2nd reading, in mm Hg)\nMeasured in examination\n\n\nBPXODI2\nDiastolic BP  (2nd reading, in mm Hg)\nMeasured in examination\n\n\nBPXOPLS1\nPulse  (1st reading, beats/minute)\nMeasured in examination\n\n\nBPXOPLS2\nPulse  (2nd reading, beats/minute)\nMeasured in examination\n\n\n\n\nA “normal” blood pressure for most adults is < 120 systolic and < 80 diastolic.\nA “normal” resting pulse rate for most adults is between 60 and 100 beats/minute.\n\nHere’s my code to select these variables from the P_BPXO data.\n\np_bpxo <- nhanes('P_BPXO') |>\n  select(SEQN, BPXOSY2, BPXODI2, BPXOPLS1, BPXOPLS2)\n\ndim(p_bpxo)\n\n[1] 11656     5\n\n\n\n\n1.2.6 Complete Blood Count\nFrom the Complete Blood Count with 5-Part Differential in Whole Blood (P_CBC), we collect the following variables.\n\n\n\n\n\n\n\n\nVariable\nDescription\nValues\n\n\n\n\nSEQN\nRespondent Sequence Number  (participant ID)\nlink across databases\n\n\nLBXWBCSI\nWhite blood cell count  in 1000 cells/uL\nnormal range: 4.5 - 11\n\n\nLBXPLTSI\nPlatelet count  in 1000 cells/uL\nnormal range: 150-450\n\n\n\nHere’s my code to select these variables from the P_CBC data.\n\np_cbc <- nhanes('P_CBC') |>\n  select(SEQN, LBXWBCSI, LBXPLTSI)\n\ndim(p_cbc)\n\n[1] 13772     3\n\n\n\n\n1.2.7 C-Reactive Protein\nFrom the High-Sensitivity C-Reactive Protein (P_HSCRP) we collect the following variables.\n\n\n\n\n\n\n\n\nVariable\nDescription\nValues\n\n\n\n\nSEQN\nRespondent Sequence Number  (participant ID)\nlink across databases\n\n\nLBXHSCRP\nHigh-Sensitivity  C-Reactive Protein (mg/L)\nnormal range: 1.0 - 3.0\n\n\n\nHere’s my code to select these variables from the P_HSCRP data.\n\np_hscrp <- nhanes('P_HSCRP') |>\n  select(SEQN, LBXHSCRP)\n\ndim(p_hscrp)\n\n[1] 13772     2\n\n\n\n\n1.2.8 Alcohol Use\nFrom the Questionnaire on Alcohol Use (P_ALQ) we collect the following variables.\n\n\n\n\n\n\n\n\nVariable\nDescription\nValues\n\n\n\n\nSEQN\nRespondent Sequence Number  (participant ID)\nlink across databases\n\n\nALQ111\nEver had a drink of alcohol?\n1 = Yes, 2 = No  7 = Refused (treat as NA)  9 = Don’t Know (treat as NA)\n\n\nALQ130\nAverage drinks per day  in past 12 months  (Top coded at 152)\ncount (set to 0 if ALQ111 is No)  777 = Refused (treat as NA)  999 = Don’t Know (treat as NA)\n\n\n\nHere’s my code to select these variables from the P_ALQ data.\n\np_alq <- nhanes('P_ALQ') |>\n  select(SEQN, ALQ111, ALQ130)\n\ndim(p_alq)\n\n[1] 8965    3\n\n\nAs noted above, we set the value of ALQ130 to be 0 if the response to ALQ111 is 2 (No).\n\np_alq <- p_alq |>\n  mutate(ALQ130 = ifelse(ALQ111 == 2, 0, ALQ130))\n\np_alq |> count(ALQ130, ALQ111)\n\n   ALQ130 ALQ111    n\n1       0      2  867\n2       1      1 2126\n3       2      1 1769\n4       3      1  843\n5       4      1  431\n6       5      1  231\n7       6      1  201\n8       7      1   44\n9       8      1   65\n10      9      1   13\n11     10      1   42\n12     11      1    3\n13     12      1   53\n14     13      1    3\n15     15      1   29\n16    777      1    1\n17    999      1    9\n18     NA      1 1640\n19     NA     NA  595\n\n\n\n\n1.2.9 Dermatology\nFrom the Questionnaire on Dermatology (P_DEQ) we collect the following variables.\n\n\n\n\n\n\n\n\nVariable\nDescription\nValues\n\n\n\n\nSEQN\nRespondent Sequence Number  (participant ID)\nlink across databases\n\n\nDEQ034D\nUse sunscreen if outside on very sunny day?\n1 = Always  2 = Most of the time  3 = Sometimes  4 = Rarely  5 = Never  7 = Refused (treat as NA)  9 = Don’t Know (treat as NA)\n\n\n\nHere’s my code to select these variables from the P_DEQ data.\n\np_deq <- nhanes('P_DEQ') |>\n  select(SEQN, DEQ034D)\n\ndim(p_deq)\n\n[1] 5810    2\n\n\n\n\n1.2.10 Depression Screener\nFrom the Questionnaire on Mental Health - Depression Screener (P_DPQ) we collect the following variables.\n\n\n\n\n\n\n\n\nVariable\nDescription\nValues\n\n\n\n\nSEQN\nRespondent Sequence Number  (participant ID)\nlink across databases\n\n\nDPQ010\nHave little interest in doing things\n0-3: codes below\n\n\nDPQ020\nFeeling down, depressed or hopeless\n0-3: codes below\n\n\nDPQ030\nTrouble sleeping or sleeping too much\n0-3: codes below\n\n\nDPQ040\nFeeling tired or having little energy\n0-3: codes below\n\n\nDPQ050\nPoor appetite or overeating\n0-3: codes below\n\n\nDPQ060\nFeeling bad about yourself\n0-3: codes below\n\n\nDPQ070\nTrouble concentrating on things\n0-3: codes below\n\n\nDPQ080\nMoving or speaking slowly or too fast\n0-3: codes below\n\n\nDPQ090\nThoughts you would be better off dead\n0-3: codes below\n\n\nDPQ100\nDifficulty these problems have caused\n0-3: codes below\n\n\n\n\nFor DPQ010 - DPQ090, the codes are 0 = Not at all, 1 = Several days in the past two weeks, 2 = More than half the days in the past two weeks, 3 = Nearly every day in the past two weeks, with 7 = Refused and 9 = Don’t Know which we will treat as NA.\nFor DPQ100, the codes are 0 = Not at all difficult, 1 = Somewhat difficult, 2 = Very difficult, 3 = Extremely difficult, with 7 = Refused and 9 = Don’t Know which we will treat as NA. Also, the DPQ100 score should be 0 if the scores on DPQ010 through DPQ090 are all zero.\n\nLater, we will sum the scores in DPQ010 - DPQ090 to produce a PHQ-9 score for each participant.\nHere’s my code to select these variables from the P_DPQ data.\n\np_dpq <- nhanes('P_DPQ') # we're actually pulling all available variables\n\ndim(p_dpq)\n\n[1] 8965   11\n\n\n\n\n1.2.11 Diet Behavior\nFrom the Questionnaire on Diet Behavior and Nutrition (P_DBQ) we collect the following variables.\n\n\n\n\n\n\n\n\nVariable\nDescription\nValues\n\n\n\n\nSEQN\nRespondent Sequence Number  (participant ID)\nlink across databases\n\n\nDBQ700\nHow healthy is your diet?\n1 = Excellent  2 = Very Good  3 = Good  4 = Fair  5 = Poor  7 = Refused (treat as NA)  9 = Don’t Know (treat as NA)\n\n\n\nHere’s my code to select these variables from the P_DBQ data.\n\np_dbq <- nhanes('P_DBQ') |>\n  select(SEQN, DBQ700)\n\ndim(p_dbq)\n\n[1] 15560     2\n\n\n\n\n1.2.12 Food Security\nFrom the Questionnaire on Food Security (P_FSQ) we collect the following variables.\n\n\n\n\n\n\n\n\nVariable\nDescription\nValues\n\n\n\n\nSEQN\nRespondent Sequence Number  (participant ID)\nlink across databases\n\n\nFSDAD\nAdult food security category for last 12m\n1 = Full food security  2 = Marginal  3 = Low  4 = Very low\n\n\n\nHere’s my code to select these variables from the P_FSQ data.\n\np_fsq <- nhanes('P_FSQ') |>\n  select(SEQN, FSDAD)\n\ndim(p_fsq)\n\n[1] 15560     2\n\n\n\n\n1.2.13 Health Insurance\nFrom the Questionnaire on Health Insurance (P_HIQ) we collect the following variables.\n\n\n\n\n\n\n\n\nVariable\nDescription\nValues\n\n\n\n\nSEQN\nRespondent Sequence Number  (participant ID)\nlink across databases\n\n\nHIQ011\nCovered by health insurance now?\n1 = Yes, 2 = No  7 = Refused (treat as NA)  9 = Don’t Know (treat as NA)\n\n\nHIQ210\nTime when no insurance in past year?\n1 = Yes, 2 = No  7 = Refused (treat as NA)  9 = Don’t Know (treat as NA)  (set to Yes if HIQ011 is No.)\n\n\n\nHere’s my code to select these variables from the P_HIQ data.\n\np_hiq <- nhanes('P_HIQ') |>\n  select(SEQN, HIQ011, HIQ210)\n\ndim(p_hiq)\n\n[1] 15560     3\n\n\nAs noted above, we set the value of HIQ210 to be 1 (Yes) if HIQ011 is 2 (No).\n\np_hiq <- p_hiq |>\n  mutate(HIQ210 = ifelse(HIQ011 == 2, 1, HIQ210))\n\np_hiq |> count(HIQ210, HIQ011)\n\n  HIQ210 HIQ011     n\n1      1      1   960\n2      1      2  1852\n3      2      1 12682\n4      7      1     2\n5      9      1    25\n6     NA      1     2\n7     NA      7     8\n8     NA      9    29\n\n\n\n\n1.2.14 Medical Conditions\nFrom the Questionnaire on Medical Conditions (P_MCQ) we collect the following variables.\n\n\n\n\n\n\n\n\nVariable\nDescription\nValues\n\n\n\n\nSEQN\nRespondent Sequence Number  (participant ID)\nlink across databases\n\n\nMCQ366A\nDoctor told you to control/lose weight in the past 12 months?\n1 = Yes, 2 = No  7 = Refused (treat as NA)  9 = Don’t Know (treat as NA)\n\n\nMCQ366B\nDoctor told you to exercise  in the past 12 months?\n1 = Yes, 2 = No  7 = Refused (treat as NA)  9 = Don’t Know (treat as NA)\n\n\nMCQ371A\nAre you now controlling or losing weight?\n1 = Yes, 2 = No  7 = Refused (treat as NA)  9 = Don’t Know (treat as NA)\n\n\nMCQ371B\nAre you now increasing exercise?\n1 = Yes, 2 = No  7 = Refused (treat as NA)  9 = Don’t Know (treat as NA)\n\n\n\nHere’s my code to select these variables from the P_MCQ data.\n\np_mcq <- nhanes('P_MCQ') |>\n  select(SEQN, MCQ366A, MCQ366B, MCQ371A, MCQ371B)\n\ndim(p_mcq)\n\n[1] 14986     5\n\n\n\n\n1.2.15 Oral Health\nFrom the Questionnaire on Oral Health (P_OHQ) we collect the following variables.\n\n\n\n\n\n\n\n\nVariable\nDescription\nValues\n\n\n\n\nSEQN\nRespondent Sequence Number  (participant ID)\nlink across databases\n\n\nOHQ870\nDays using dental floss  (in the last week)\ncount (0-7)  9 = Unknown (treat as NA)  99 = Don’t Know (treat as NA)\n\n\n\nHere’s my code to select these variables from the P_OHQ data.\n\np_ohq <- nhanes('P_OHQ') |>\n  select(SEQN, OHQ870)\n\ndim(p_ohq)\n\n[1] 14986     2\n\n\n\n\n1.2.16 Physical Activity\nFrom the Questionnaire on Physical Activity (P_PAQ) we collect the following variables.\n\n\n\n\n\n\n\n\nVariable\nDescription\nValues\n\n\n\n\nSEQN\nRespondent Sequence Number  (participant ID)\nlink across databases\n\n\nPAQ605\nVigorous work activity for 10 min/week?\n1 = Yes, 2 = No  7 = Refused (treat as NA)  9 = Don’t Know (treat as NA)\n\n\nPAQ610\n# of days of vigorous work activity  in past week\ncount (1-7)  77 = Refused (treat as NA)  99 = Don’t Know (treat as NA)  (set to 0 if PAQ605 is No.)\n\n\nPAQ650\nVigorous recreational activity for 10 min/week?\n1 = Yes, 2 = No  7 = Refused (treat as NA)  9 = Don’t Know (treat as NA)\n\n\nPAQ655\n# of days of vigorous recreational  activity in past week\ncount (1-7)  77 = Refused (treat as NA)  99 = Don’t Know (treat as NA)  (set to 0 if PAQ650 is No.)\n\n\nPAD680\nMinutes of sedentary activity (min/day)\nexcludes sleeping  7777 = Refused (treat as NA)  9999 = Don’t Know (treat as NA)\n\n\n\nHere’s my code to select these variables from the P_PAQ data.\n\np_paq <- nhanes('P_PAQ') |>\n  select(SEQN, PAQ605, PAQ610, PAQ650, PAQ655, PAD680)\n\ndim(p_paq)\n\n[1] 9693    6\n\n\nNow, let’s set the value of PAQ610 to be 0 if PAQ605 is 2 (No).\n\np_paq <- p_paq |>\n  mutate(PAQ610 = ifelse(PAQ605 == 2, 0, PAQ610))\n\nFinally, we set the value of PAQ655 to be 0 if PAQ650 is 2 (No).\n\np_paq <- p_paq |>\n  mutate(PAQ655 = ifelse(PAQ650 == 2, 0, PAQ655))\n\n\n\n1.2.17 Reproductive Health\nFrom the Questionnaire on Reproductive Health (P_RHQ) we collect the following variables.\n\n\n\n\n\n\n\n\nVariable\nDescription\nValues\n\n\n\n\nSEQN\nRespondent Sequence Number  (participant ID)\nlink across databases\n\n\nRHQ131\nEver been pregnant?\n1 = Yes, 2 = No  7 = Refused (treat as NA)  9 = Don’t Know (treat as NA)\n\n\nRHQ160\nHow many times have you been pregnant?\ncount (1-11)  77 = Refused (treat as NA)  99 = Don’t Know (treat as NA)  (set to 0 if RHQ131 is No.)\n\n\n\nHere’s my code to select these variables from the P_RHQ data.\n\np_rhq <- nhanes('P_RHQ') |>\n  select(SEQN, RHQ131, RHQ160)\n\ndim(p_rhq)\n\n[1] 5314    3\n\n\nNow, let’s set the value of RHQ160 to be 0 if RHQ131 is 2 (No).\n\np_rhq <- p_rhq |>\n  mutate(RHQ160 = ifelse(RHQ131 == 2, 0, RHQ160))\n\n\n\n1.2.18 Sleep Disorders\nFrom the Questionnaire on Sleep Disorders (P_SLQ) we collect the following variables.\n\n\n\n\n\n\n\n\nVariable\nDescription\nValues\n\n\n\n\nSEQN\nRespondent Sequence Number  (participant ID)\nlink across databases\n\n\nSLD012\nUsual hours of sleep (weekdays)\nhours limited to 2-14\n\n\nSLD013\nUsual hours of sleep (weekends)\nhours limited to 2-14\n\n\nSLQ030\nHow often do you snore in the past 12 months?\n0 = Never  1 = Rarely (1-2 nights/week)  2 = Occasionally (3-4 nights/week)  3 = Frequently (5+ nights/week)  7 = Refused (treat as NA)  9 = Don’t Know (treat as NA)\n\n\nSLQ050\nHave you ever told a doctor you had trouble sleeping?\n1 = Yes, 2 = No  7 = Refused (treat as NA)  9 = Don’t Know (treat as NA)\n\n\n\nHere’s my code to select these variables from the P_SLQ data.\n\np_slq <- nhanes('P_SLQ') |>\n  select(SEQN, SLD012, SLD013, SLQ030, SLQ050)\n\ndim(p_slq)\n\n[1] 10195     5\n\n\n\n\n1.2.19 Smoking Cigarettes\nFrom the Questionnaire on Smoking - Cigarette Use (P_SMQ) we collect the following variables.\n\n\n\n\n\n\n\n\nVariable\nDescription\nValues\n\n\n\n\nSEQN\nRespondent Sequence Number  (participant ID)\nlink across databases\n\n\nSMQ020\nSmoked at least 100 cigarettes in your life?\n1 = Yes, 2 = No  7 = Refused (treat as NA)  9 = Don’t Know (treat as NA)\n\n\nSMD641\nDays (in past 30) when you smoked a cigarette?\ncount (0-30)  77 = Refused (treat as NA)  99 = Don’t Know (treat as NA)  (set to 0 if SMQ020 is No.)\n\n\n\nHere’s my code to select these variables from the P_SMQ data.\n\np_smq <- nhanes('P_SMQ') |>\n  select(SEQN, SMQ020, SMD641)\n\ndim(p_smq)\n\n[1] 11137     3\n\n\nNow, let’s set the value of SMD641 to be 0 if SMQ020 is 2 (No).\n\np_smq <- p_smq |>\n  mutate(SMD641 = ifelse(SMQ020 == 2, 0, SMD641))\n\n\n\n1.2.20 Secondhand Smoke\nFrom the Questionnaire on Smoking - Secondhand Smoke Exposure (P_SMQSHS) we collect the following variables.\n\n\n\n\n\n\n\n\nVariable\nDescription\nValues\n\n\n\n\nSEQN\nRespondent Sequence Number  (participant ID)\nlink across databases\n\n\nSMQ856\nLast 7 days worked at a job not at home?\n1 = Yes, 2 = No  7 = Refused (treat as NA)  9 = Don’t Know (treat as NA)\n\n\nSMQ860\nLast 7 days spent time in a restaurant?\n1 = Yes, 2 = No  7 = Refused (treat as NA)  9 = Don’t Know (treat as NA)\n\n\nSMQ866\nLast 7 days spent time in a bar?\n1 = Yes, 2 = No  7 = Refused (treat as NA)  9 = Don’t Know (treat as NA)\n\n\n\nHere’s my code to select these variables from the P_SMQSHS data.\n\np_smqshs <- nhanes('P_SMQSHS') |>\n  select(SEQN, SMQ856, SMQ860, SMQ866)\n\ndim(p_smqshs)\n\n[1] 15560     4\n\n\n\n\n1.2.21 Weight History\nFrom the Questionnaire on Weight History (P_WHQ) we collect the following variables.\n\n\n\n\n\n\n\n\nVariable\nDescription\nValues\n\n\n\n\nSEQN\nRespondent Sequence Number  (participant ID)\nlink across databases\n\n\nWHD010\nCurrent self-reported height (in inches)\n49 to 82  7777 = Refused (treat as NA)  9999 = Don’t Know (treat as NA)\n\n\nWHD020\nCurrent self-reported weight (in pounds)\n67 to 578  7777 = Refused (treat as NA)  9999 = Don’t Know (treat as NA)\n\n\nWHQ040\nLike to weigh more, less, or same\n1 = More  2 = Less  3 = Stay about the same  7 = Refused (treat as NA)  9 = Don’t Know (treat as NA)\n\n\n\nHere’s my code to select these variables from the P_WHQ data.\n\np_whq <- nhanes('P_WHQ') |>\n  select(SEQN, WHD010, WHD020, WHQ040)\n\ndim(p_whq)\n\n[1] 10195     4"
  },
  {
    "objectID": "01-nhanes.html#filtering-for-inclusion",
    "href": "01-nhanes.html#filtering-for-inclusion",
    "title": "1  Building the nh432 example",
    "section": "1.3 Filtering for Inclusion",
    "text": "1.3 Filtering for Inclusion\nFirst, I’ll filter the demographic data (p_demo) to the participants with known ages (RIDAGEYR here) between 30 and 59 years (inclusive), and to those who were both interviewed and examined (so RIDSTATR is 2) to match our inclusion criteria.\n\np_demo <- p_demo |>\n  filter(RIDAGEYR >= 30 & RIDAGEYR <= 59,\n         RIDSTATR == 2)\n\ndim(p_demo)\n\n[1] 4133    8\n\n\nSecond, I’ll restrict the p_ohxref sample to the participants who had a complete oral health exam (so OHDEXSTS is 1) which is also part of our inclusion criteria.\n\np_ohxref <- p_ohxref |>\n  filter(OHDEXSTS == 1)\n\ndim(p_ohxref)\n\n[1] 13271     3\n\n\nThird, I’ll restrict the p_hug sample to the participants who gave one of our five available responses (codes 1-5) to the general health condition question in HUQ010, which is the final element of our inclusion criteria.\n\np_huq <- p_huq |>\n  filter(HUQ010 <= 5)\n\ndim(p_huq)\n\n[1] 15550     4\n\n\nSubjects that meet all of these requirements will be included in our analytic data. To achieve that end, we’ll begin merging the individual data bases."
  },
  {
    "objectID": "01-nhanes.html#merging-the-data",
    "href": "01-nhanes.html#merging-the-data",
    "title": "1  Building the nh432 example",
    "section": "1.4 Merging the Data",
    "text": "1.4 Merging the Data\n\n1.4.1 Merging Two Data Frames at a Time\nWe have two ways to merge our data. We can merge data sets two at a time. In this case, we’ll use inner_join() from the dplyr package to include only those participants with data in each of the two data frames we’re merging. For example, we’ll create temp01 to include data from both p_demo and p_ohxref for all participants (identified by their SEQN) that appear in each of those two data frames. Then, we’ll merge the resulting temp01 with p_huq to create temp02 in a similar way.\n\ntemp01 <- inner_join(p_demo, p_ohxref, by = \"SEQN\")\ntemp02 <- inner_join(temp01, p_huq, by = \"SEQN\")\n\ndim(temp02)\n\n[1] 3931   13\n\n\nNote that we now have 3931 participants in our data, and this should be the case after we merge in all of the other data sets, too. Rather than using inner_join() we will switch now to using left_join() many more times so that we always add new information only on those subjects who meet our inclusion criteria (as identified in temp02. For more on the various types of joins we can use from the dplyr package, visit <https://dplyr.tidyverse.org/reference/mutate-joins.html. The problem is that that approach would force us to create lots of new temporary files as we add in each new variable.\n\n\n1.4.2 Merging Many Data Frames Together\nA better approach is to use the reduce() function in the purrr package3, which will let us join this temp02 data frame with our remaining 17 data frames using left_join() in a much more streamlined way. We’ll also ensure that the final result (which we’ll call nh_raw) is a tibble, rather than just a data frame.\n\ndf_list <- list(temp02, p_bmx, p_bpxo, p_cbc, p_hscrp, \n                p_alq, p_deq, p_dpq, p_dbq, p_fsq, \n                p_hiq, p_mcq, p_ohq, p_paq, p_rhq,\n                p_slq, p_smqshs, p_smq, p_whq)\n\nnh_raw <- df_list |> \n  reduce(left_join, by = 'SEQN') |>\n  as_tibble()\n\ndim(nh_raw)\n\n[1] 3931   64"
  },
  {
    "objectID": "01-nhanes.html#the-raw-data",
    "href": "01-nhanes.html#the-raw-data",
    "title": "1  Building the nh432 example",
    "section": "1.5 The “Raw” Data",
    "text": "1.5 The “Raw” Data\nWhat does the data in nh_raw look like? Normally, I wouldn’t include this sort of intermediate description in a published bit of work, but it may be helpful to compare this description to the one we’ll generate at the end of the cleaning process in this case.\n\nsummary(nh_raw)\n\n      SEQN           RIDSTATR    RIDAGEYR        RIDRETH3        DMDEDUC2   \n Min.   :109271   Min.   :2   Min.   :30.00   Min.   :1.000   Min.   :1.00  \n 1st Qu.:113103   1st Qu.:2   1st Qu.:37.00   1st Qu.:3.000   1st Qu.:3.00  \n Median :117059   Median :2   Median :45.00   Median :3.000   Median :4.00  \n Mean   :117074   Mean   :2   Mean   :44.79   Mean   :3.561   Mean   :3.64  \n 3rd Qu.:121040   3rd Qu.:2   3rd Qu.:53.00   3rd Qu.:4.000   3rd Qu.:5.00  \n Max.   :124818   Max.   :2   Max.   :59.00   Max.   :7.000   Max.   :7.00  \n                                                                            \n    RIAGENDR        WTINTPRP         WTMECPRP         OHDEXSTS     OHAREC     \n Min.   :1.000   Min.   :  2467   Min.   :  2589   Min.   :1   Min.   :1.000  \n 1st Qu.:1.000   1st Qu.: 10615   1st Qu.: 11365   1st Qu.:1   1st Qu.:3.000  \n Median :2.000   Median : 17358   Median : 18422   Median :1   Median :4.000  \n Mean   :1.533   Mean   : 28434   Mean   : 30353   Mean   :1   Mean   :3.455  \n 3rd Qu.:2.000   3rd Qu.: 31476   3rd Qu.: 33155   3rd Qu.:1   3rd Qu.:4.000  \n Max.   :2.000   Max.   :311265   Max.   :321574   Max.   :1   Max.   :4.000  \n                                                                              \n     HUQ010          HUQ071          HUQ090          BMXWT       \n Min.   :1.000   Min.   :1.000   Min.   :1.000   Min.   : 36.90  \n 1st Qu.:2.000   1st Qu.:2.000   1st Qu.:2.000   1st Qu.: 69.30  \n Median :3.000   Median :2.000   Median :2.000   Median : 82.10  \n Mean   :2.741   Mean   :1.913   Mean   :1.883   Mean   : 86.31  \n 3rd Qu.:3.000   3rd Qu.:2.000   3rd Qu.:2.000   3rd Qu.: 99.10  \n Max.   :5.000   Max.   :2.000   Max.   :9.000   Max.   :254.30  \n                                                 NA's   :28      \n     BMXHT          BMXWAIST        BPXOSY2         BPXODI2      \n Min.   :135.3   Min.   : 57.9   Min.   : 69.0   Min.   : 31.00  \n 1st Qu.:160.0   1st Qu.: 89.1   1st Qu.:110.0   1st Qu.: 69.00  \n Median :166.9   Median : 99.2   Median :120.0   Median : 76.00  \n Mean   :167.4   Mean   :101.5   Mean   :121.5   Mean   : 77.03  \n 3rd Qu.:174.7   3rd Qu.:111.7   3rd Qu.:131.0   3rd Qu.: 84.00  \n Max.   :198.7   Max.   :178.0   Max.   :222.0   Max.   :136.00  \n NA's   :30      NA's   :149     NA's   :346     NA's   :346     \n    BPXOPLS1        BPXOPLS2         LBXWBCSI         LBXPLTSI    \n Min.   : 38.0   Min.   : 37.00   Min.   : 2.300   Min.   : 47.0  \n 1st Qu.: 62.0   1st Qu.: 63.00   1st Qu.: 5.700   1st Qu.:210.0  \n Median : 69.0   Median : 70.00   Median : 6.900   Median :246.0  \n Mean   : 70.3   Mean   : 70.96   Mean   : 7.254   Mean   :253.3  \n 3rd Qu.: 77.0   3rd Qu.: 78.00   3rd Qu.: 8.400   3rd Qu.:290.0  \n Max.   :126.0   Max.   :121.00   Max.   :22.800   Max.   :818.0  \n NA's   :615     NA's   :617      NA's   :176      NA's   :176    \n    LBXHSCRP           ALQ111          ALQ130          DEQ034D     \n Min.   :  0.110   Min.   :1.000   Min.   : 0.000   Min.   :1.000  \n 1st Qu.:  0.890   1st Qu.:1.000   1st Qu.: 1.000   1st Qu.:3.000  \n Median :  2.090   Median :1.000   Median : 2.000   Median :4.000  \n Mean   :  4.326   Mean   :1.089   Mean   : 2.345   Mean   :3.675  \n 3rd Qu.:  4.740   3rd Qu.:1.000   3rd Qu.: 3.000   3rd Qu.:5.000  \n Max.   :182.820   Max.   :2.000   Max.   :15.000   Max.   :5.000  \n NA's   :267       NA's   :205     NA's   :789      NA's   :19     \n     DPQ010           DPQ020           DPQ030           DPQ040      \n Min.   :0.0000   Min.   :0.0000   Min.   :0.0000   Min.   :0.0000  \n 1st Qu.:0.0000   1st Qu.:0.0000   1st Qu.:0.0000   1st Qu.:0.0000  \n Median :0.0000   Median :0.0000   Median :0.0000   Median :1.0000  \n Mean   :0.3907   Mean   :0.3732   Mean   :0.6529   Mean   :0.7612  \n 3rd Qu.:1.0000   3rd Qu.:1.0000   3rd Qu.:1.0000   3rd Qu.:1.0000  \n Max.   :9.0000   Max.   :7.0000   Max.   :9.0000   Max.   :9.0000  \n NA's   :212      NA's   :212      NA's   :212      NA's   :212     \n     DPQ050           DPQ060           DPQ070           DPQ080      \n Min.   :0.0000   Min.   :0.0000   Min.   :0.0000   Min.   :0.0000  \n 1st Qu.:0.0000   1st Qu.:0.0000   1st Qu.:0.0000   1st Qu.:0.0000  \n Median :0.0000   Median :0.0000   Median :0.0000   Median :0.0000  \n Mean   :0.4117   Mean   :0.2504   Mean   :0.2924   Mean   :0.1622  \n 3rd Qu.:1.0000   3rd Qu.:0.0000   3rd Qu.:0.0000   3rd Qu.:0.0000  \n Max.   :9.0000   Max.   :3.0000   Max.   :3.0000   Max.   :3.0000  \n NA's   :212      NA's   :213      NA's   :213      NA's   :213     \n     DPQ090          DPQ100           DBQ700         FSDAD      \n Min.   :0.000   Min.   :0.0000   Min.   :1.00   Min.   :1.000  \n 1st Qu.:0.000   1st Qu.:0.0000   1st Qu.:2.00   1st Qu.:1.000  \n Median :0.000   Median :0.0000   Median :3.00   Median :1.000  \n Mean   :0.053   Mean   :0.3447   Mean   :3.11   Mean   :1.737  \n 3rd Qu.:0.000   3rd Qu.:1.0000   3rd Qu.:4.00   3rd Qu.:2.000  \n Max.   :3.000   Max.   :3.0000   Max.   :9.00   Max.   :4.000  \n NA's   :214     NA's   :1448                    NA's   :231    \n     HIQ011          HIQ210         MCQ366A         MCQ366B     \n Min.   :1.000   Min.   :1.000   Min.   :1.000   Min.   :1.000  \n 1st Qu.:1.000   1st Qu.:1.000   1st Qu.:1.000   1st Qu.:1.000  \n Median :1.000   Median :2.000   Median :2.000   Median :2.000  \n Mean   :1.211   Mean   :1.742   Mean   :1.699   Mean   :1.574  \n 3rd Qu.:1.000   3rd Qu.:2.000   3rd Qu.:2.000   3rd Qu.:2.000  \n Max.   :9.000   Max.   :9.000   Max.   :9.000   Max.   :9.000  \n                 NA's   :12                                     \n    MCQ371A         MCQ371B          OHQ870           PAQ605     \n Min.   :1.000   Min.   :1.000   Min.   : 0.000   Min.   :1.000  \n 1st Qu.:1.000   1st Qu.:1.000   1st Qu.: 0.000   1st Qu.:1.000  \n Median :1.000   Median :1.000   Median : 3.000   Median :2.000  \n Mean   :1.371   Mean   :1.399   Mean   : 3.503   Mean   :1.723  \n 3rd Qu.:2.000   3rd Qu.:2.000   3rd Qu.: 7.000   3rd Qu.:2.000  \n Max.   :9.000   Max.   :9.000   Max.   :99.000   Max.   :9.000  \n                                 NA's   :1                       \n     PAQ610           PAQ650          PAQ655            PAD680      \n Min.   : 0.000   Min.   :1.000   Min.   : 0.0000   Min.   :   2.0  \n 1st Qu.: 0.000   1st Qu.:1.000   1st Qu.: 0.0000   1st Qu.: 180.0  \n Median : 0.000   Median :2.000   Median : 0.0000   Median : 300.0  \n Mean   : 1.244   Mean   :1.731   Mean   : 0.9201   Mean   : 363.7  \n 3rd Qu.: 2.000   3rd Qu.:2.000   3rd Qu.: 1.0000   3rd Qu.: 480.0  \n Max.   :99.000   Max.   :2.000   Max.   :99.0000   Max.   :9999.0  \n NA's   :4                                          NA's   :11      \n     RHQ131          RHQ160           SLD012           SLD013      \n Min.   :1.000   Min.   : 0.000   Min.   : 2.000   Min.   : 2.000  \n 1st Qu.:1.000   1st Qu.: 2.000   1st Qu.: 6.500   1st Qu.: 7.000  \n Median :1.000   Median : 3.000   Median : 7.500   Median : 8.000  \n Mean   :1.114   Mean   : 3.159   Mean   : 7.359   Mean   : 8.231  \n 3rd Qu.:1.000   3rd Qu.: 4.000   3rd Qu.: 8.000   3rd Qu.: 9.000  \n Max.   :7.000   Max.   :77.000   Max.   :14.000   Max.   :14.000  \n NA's   :1970    NA's   :1972     NA's   :34       NA's   :34      \n     SLQ030          SLQ050          SMQ856          SMQ860     \n Min.   :0.000   Min.   :1.000   Min.   :1.000   Min.   :1.000  \n 1st Qu.:1.000   1st Qu.:1.000   1st Qu.:1.000   1st Qu.:1.000  \n Median :2.000   Median :2.000   Median :1.000   Median :1.000  \n Mean   :2.015   Mean   :1.711   Mean   :1.323   Mean   :1.423  \n 3rd Qu.:3.000   3rd Qu.:2.000   3rd Qu.:2.000   3rd Qu.:2.000  \n Max.   :9.000   Max.   :9.000   Max.   :2.000   Max.   :9.000  \n                                                                \n     SMQ866          SMQ020        SMD641           WHD010         WHD020      \n Min.   :1.000   Min.   :1.0   Min.   : 0.000   Min.   :  50   Min.   :  86.0  \n 1st Qu.:2.000   1st Qu.:1.0   1st Qu.: 0.000   1st Qu.:  63   1st Qu.: 152.0  \n Median :2.000   Median :2.0   Median : 0.000   Median :  66   Median : 180.0  \n Mean   :1.846   Mean   :1.6   Mean   : 6.859   Mean   : 247   Mean   : 353.9  \n 3rd Qu.:2.000   3rd Qu.:2.0   3rd Qu.: 5.000   3rd Qu.:  70   3rd Qu.: 220.0  \n Max.   :2.000   Max.   :7.0   Max.   :99.000   Max.   :9999   Max.   :9999.0  \n                               NA's   :724      NA's   :24                     \n     WHQ040    \n Min.   :1.00  \n 1st Qu.:2.00  \n Median :2.00  \n Mean   :2.18  \n 3rd Qu.:2.00  \n Max.   :9.00"
  },
  {
    "objectID": "01-nhanes.html#cleaning-tasks",
    "href": "01-nhanes.html#cleaning-tasks",
    "title": "1  Building the nh432 example",
    "section": "1.6 Cleaning Tasks",
    "text": "1.6 Cleaning Tasks\nWe now have a tibble called nh_raw containing 3931 NHANES participants in the rows and 64 variables in the columns. What must we do to clean up the data?\n\nCheck that every identifier (here, SEQN) is unique.\nEnsure that all coded values for “Refused”, “Don’t Know” or “Missing” are interpreted as missing values by R.\nBe sure all quantitative variables have plausible minimum and maximum values.\nReplace the RIAGENDR variable with a new factor variable called SEX with levels Male and Female.\nConvert all binary Yes/No variables to 1/0 numeric variables where 1 = Yes, 0 = No.\nCreate the PHQ-9 score from the nine relevant items in the depression screener (P_DPQ).\nUse meaningful level names for all multi-categorical variables, and be sure R uses factors to represent them.\nClean and adjust the names of the variables to something more useful, as desired. (Usually, I will do this first, but in this case, I’ve decided to do it last.)\n\nOnce we’ve accomplished these cleaning tasks, we’ll save the resulting tibble as an R data set we can use later, and we’ll summarize our final analytic variables in a proper codebook."
  },
  {
    "objectID": "01-nhanes.html#our-identifying-variable",
    "href": "01-nhanes.html#our-identifying-variable",
    "title": "1  Building the nh432 example",
    "section": "1.7 Our identifying variable",
    "text": "1.7 Our identifying variable\nSEQN is meant to identify the rows (participants) in these data, with one row per SEQN. Is every SEQN unique?\n\nnrow(nh_raw)\n\n[1] 3931\n\nn_distinct(nh_raw$SEQN)\n\n[1] 3931\n\n\nIt looks like the number of rows in our tibble matches the number of unique (distinct) SEQN values, so we’re OK. I prefer to specify that the SEQN be maintained by R as a character variable, which reduces the chance of my accidentally including it in a model as if it were something meaningful.\n\nnh_fixing <- nh_raw |> mutate(SEQN = as.character(SEQN))"
  },
  {
    "objectID": "01-nhanes.html#refused-dont-know",
    "href": "01-nhanes.html#refused-dont-know",
    "title": "1  Building the nh432 example",
    "section": "1.8 “Refused” & “Don’t Know”",
    "text": "1.8 “Refused” & “Don’t Know”\nSome of our variables have “hidden” missing values coded as “Refused” or “Don’t Know”. We must ensure that R sees these values as missing.\n\nThe following variables use code 7 for Refused and 9 for Don’t Know:\n\nDMDEDUC2, HUQ071, HUQ090, ALQ111, DEQ034D,\nDPQ010, DPQ020, DPQ030, DPQ040, DPQ050,\nDPQ060, DPQ070, DPQ080, DPQ090, DPQ100,\nDBQ700, HIQ011, HIQ210, MCQ366A, MCQ366B,\nMCQ371A, MCQ371B, PAQ605, PAQ650, RHQ131,\nSLQ030, SLQ050, SMQ020, SMQ856, SMQ860,\nSMQ866, WHQ040\n\nThe following variables use code 9 for Unknown and 99 for Don’t Know:\n\nOHQ870\n\nThe following variables use code 77 for Refused and 99 for Unknown:\n\nPAQ610, PAQ655, RHQ160, SMD641\n\nThe following variables use code 777 for Refused and 999 for Don’t Know:\n\nALQ130\n\nThe following variables use code 7777 for Refused and 9999 for Don’t Know:\n\nPAD680, WHD010, WHD020\n\n\nThe replace_with_na() set of functions from the naniar package can be very helpful here4.\n\nnh_fixing <- nh_fixing %>%\n  replace_with_na_at(.vars = c(\"DMDEDUC2\", \"HUQ071\", \"HUQ090\", \"ALQ111\", \n                               \"DEQ034D\", \"DPQ010\", \"DPQ020\", \"DPQ030\",\n                               \"DPQ040\", \"DPQ050\", \"DPQ060\", \"DPQ070\",\n                               \"DPQ080\", \"DPQ090\", \"DPQ100\", \"DBQ700\",\n                               \"HIQ011\", \"HIQ210\", \"MCQ366A\", \"MCQ366B\",\n                               \"MCQ371A\", \"MCQ371B\", \"PAQ605\", \"PAQ650\", \n                               \"RHQ131\", \"SLQ030\", \"SLQ050\", \"SMQ020\",\n                               \"SMQ856\", \"SMQ860\", \"SMQ866\", \"WHQ040\"),\n                     condition = ~.x %in% c(7, 9)) %>%\n  replace_with_na_at(.vars = c(\"OHQ870\"),\n                     condition = ~.x %in% c(9, 99)) %>%\n  replace_with_na_at(.vars = c(\"PAQ610\", \"PAQ655\", \"RHQ160\", \"SMD641\"),\n                     condition = ~.x %in% c(77, 99)) %>%\n  replace_with_na_at(.vars = c(\"ALQ130\"),\n                     condition = ~.x %in% c(777, 999)) %>%\n  replace_with_na_at(.vars = c(\"PAD680\", \"WHD010\", \"WHD020\"),\n                     condition = ~.x %in% c(7777, 9999))"
  },
  {
    "objectID": "01-nhanes.html#variables-without-variation",
    "href": "01-nhanes.html#variables-without-variation",
    "title": "1  Building the nh432 example",
    "section": "1.9 Variables without Variation",
    "text": "1.9 Variables without Variation\nNote first that we have two variables which now have the same value for all participants.\n\n\n\nVariable\nDescription\nCodes\n\n\n\n\nRIDSTATR\nInterview and examination status\n2 = Both\n\n\nOHDEXSTS\nComplete oral health exam?\n1 = Yes\n\n\n\n\nnh_fixing |> count(RIDSTATR, OHDEXSTS)\n\n# A tibble: 1 × 3\n  RIDSTATR OHDEXSTS     n\n     <dbl>    <dbl> <int>\n1        2        1  3931\n\n\nWe won’t use these variables in our analyses, now that we’ve verified them."
  },
  {
    "objectID": "01-nhanes.html#quantitative-variables",
    "href": "01-nhanes.html#quantitative-variables",
    "title": "1  Building the nh432 example",
    "section": "1.10 Quantitative Variables",
    "text": "1.10 Quantitative Variables\nHere are our quantitative variables, and some key information about the values we observe, along with their units of measurement. Our job here is to check the ranges of these variables, and be sure we have no unreasonable values. It’s also helpful to keep an eye on how much missingness we might have to deal with.\nWe’re also going to rename each of these variables, as indicated.\n\n\n\n\n\n\n\n\n\n\n\nNew  Name\nNHANES  Name\nDescription\nUnits\n# NA\nRange\n\n\n\n\nAGE\nRIDAGEYR\nAge at screening\nyears\n0\n30, 59\n\n\nWEIGHT\nBMXWT\nBody weight\nkg\n28\n36.9, 254.3\n\n\nHEIGHT\nBMXHT\nStanding height\ncm\n30\n135.3, 198.7\n\n\nWAIST\nBMXWAIST\nWaist Circumference\ncm\n149\n57.9, 178\n\n\nSBP\nBPXOSY2\nSystolic BP (2nd reading)\nmm Hg\n346\n69, 222\n\n\nDBP\nBPXODI2\nDiastolic BP (2nd reading)\nmm Hg\n346\n31, 136\n\n\nPULSE1\nBPXOPLS1\nPulse (1st reading)\n\\(\\frac{beats}{minute}\\)\n615\n38, 126\n\n\nPULSE2\nBPXOPLS2\nPulse (2nd reading)\n\\(\\frac{beats}{minute}\\)\n617\n37, 121\n\n\nWBC\nLBXWBCSI\nWhite blood cell count\n\\(\\frac{\\mbox{1000 cells}}{uL}\\)\n176\n2.3, 22.8\n\n\nPLATELET\nLBXPLTSI\nPlatelet count\n\\(\\frac{\\mbox{1000 cells}}{uL}\\)\n176\n47, 818\n\n\nHSCRP\nLBXHSCRP\nHigh-Sensitivity  C-Reactive Protein\n\\(mg/L\\)\n267\n0.11, 182.82\n\n\nDRINKS\nALQ130\nAverage daily  Alcoholic drinks\ndrinks\n789\n0, 15\n\n\nFLOSS\nOHQ870\nDays using dental floss  in past week\ndays\n4\n0, 7\n\n\nVIGWK_D\nPAQ610\nAverage days per week  with Vigorous work\ndays\n5\n0, 7\n\n\nVIGREC_D\nPAQ655\nAverage days per week  with Vigorous recreation\ndays\n1\n0, 7\n\n\nSEDATE\nPAD680\nAverage daily  Sedentary activity\nminutes\n24\n2, 1320\n\n\nPREGNANT\nRHQ160\nPregnancies\ntimes\n1975\n0, 11\n\n\nSLPWKDAY\nSLD012\nUsual sleep (weekdays)\nhours\n34\n2, 14\n\n\nSLPWKEND\nSLD013\nUsual sleep (weekends)\nhours\n34\n2, 14\n\n\nSMOKE30\nSMD641\nDays in past 30 when  you smoked a cigarette\ndays\n726\n0, 30\n\n\nESTHT\nWHD010\nSelf-reported height\ninches\n95\n50, 81\n\n\nESTWT\nWHD020\nSelf-reported weight\npounds\n68\n86, 578\n\n\n\nTo insert the number of missing values and range (minimum, maximum among non-missing values) into the table, I used inline R code like this:\n\nn_miss(nh_fixing$BMXWT)\n\n[1] 28\n\nrange(nh_fixing$BMXWT, na.rm = TRUE)\n\n[1]  36.9 254.3\n\n\n\n1.10.1 Renaming the Quantities\nHere’s the renaming code:\n\nnh_fixing <- nh_fixing |>\n  rename(AGE = RIDAGEYR, WEIGHT = BMXWT, HEIGHT = BMXHT,\n         WAIST = BMXWAIST, SBP = BPXOSY2, DBP = BPXODI2,\n         PULSE1 = BPXOPLS1, PULSE2 = BPXOPLS2, WBC = LBXWBCSI,\n         PLATELET = LBXPLTSI, HSCRP = LBXHSCRP, DRINKS = ALQ130,\n         FLOSS = OHQ870, VIGWK_D = PAQ610, VIGREC_D = PAQ655,\n         SEDATE = PAD680, PREGS = RHQ160, SLPWKDAY = SLD012, \n         SLPWKEND = SLD013, SMOKE30 = SMD641, \n         ESTHT = WHD010, ESTWT = WHD020)\n\n\n\n1.10.2 Sampling Weights\nHere are the sampling weights, which I think of as unitless, typically, though they represent people.\n\n\n\nVariable\nDescription\n# NA\nRange\n\n\n\n\nWTINTPRP\nSampling Weight (interviews)\n0\n2467.1, 311265.2\n\n\nWTMECPRP\nSampling Weight (examinations)\n0\n2589.2, 321573.5\n\n\n\nNote that to obtain these ranges formatted like this, I had to use some additional code in the table:\n\nformat(round_half_up(range(nh_fixing$WTINTPRP, na.rm = TRUE),1), scientific = FALSE)\n\n[1] \"  2467.1\" \"311265.2\""
  },
  {
    "objectID": "01-nhanes.html#binary-variables",
    "href": "01-nhanes.html#binary-variables",
    "title": "1  Building the nh432 example",
    "section": "1.11 Binary Variables",
    "text": "1.11 Binary Variables\n\n1.11.1 Sex (RIAGENDR)\nTo start, let’s do something about the variable describing the participant’s biological sex (so we’ll rename it to a more useful name), and then we’ll recode the values of the SEX variable to more useful choices.\n\n\n\nNew Name\nNHANES Name\nDescription\n\n\n\n\nSEX\nRIAGENDR\nSex\n\n\n\nNote that we have more female than male subjects, and no missing values, as it turns out.\n\nnh_fixing |> count(RIAGENDR)\n\n# A tibble: 2 × 2\n  RIAGENDR     n\n     <dbl> <int>\n1        1  1837\n2        2  2094\n\n\nNow, let’s convert the information in RIAGENDR to SEX.\n\nnh_fixing <- nh_fixing |>\n  rename(SEX = RIAGENDR) |>\n  mutate(SEX = factor(ifelse(SEX == 1, \"Male\", \"Female\")))\n\nAnd we’ll run a little “sanity check” here to ensure that we’ve recoded this variable properly5.\n\nnh_fixing |> count(SEX)\n\n# A tibble: 2 × 2\n  SEX        n\n  <fct>  <int>\n1 Female  2094\n2 Male    1837\n\n\n\n\n1.11.2 Yes/No variables\nNow, let’s tackle the variables with code 1 = Yes, and 2 = No, and (potentially) some missing values. I’ll summarize each with the percentage of “Yes” responses (out of those with code 1 or 2) and the number of missing values.\n\n\n\n\n\n\n\n\n\n\nNew NAME\nNHANES NAME\nDescription\n% Yes\n# NA\n\n\n\n\nHOSPITAL\nHUQ071\nOvernight hospital patient in past 12m?\n8.7\n0\n\n\nMENTALH\nHUQ090\nSeen mental health professional past 12m?\n12.1\n2\n\n\nEVERALC\nALQ111\nEver had a drink of alcohol?\n91.1\n205\n\n\nINSURNOW\nHIQ011\nCovered by health insurance now?\n80.8\n10\n\n\nNOINSUR\nHIQ210\nTime when no insurance in past year?\n26.6\n16\n\n\nDR_LOSE\nMCQ366A\nDoctor told you to control/lose weight  in the past 12 months?\n30.3\n1\n\n\nDR_EXER\nMCQ366B\nDoctor told you to exercise  in the past 12 months?\n42.7\n1\n\n\nNOW_LOSE\nMCQ371A\nAre you now controlling or losing weight?\n63.2\n2\n\n\nNOW_EXER\nMCQ371B\nAre you now increasing exercise?\n60.3\n1\n\n\nWORK_V\nPAQ605\nVigorous work activity for 10 min/week?\n28.4\n4\n\n\nREC_V\nPAQ650\nVigorous recreational activity for 10 min/week?\n26.9\n0\n\n\nEVERPREG\nRHQ131\nEver been pregnant?\n89.2\n1972\n\n\nSLPTROUB\nSLQ050\nEver told a doctor you had trouble sleeping?\n29.5\n3\n\n\nCIG100\nSMQ020\nSmoked at least 100 cigarettes in your life?\n40.1\n1\n\n\nAWAYWORK\nSMQ856\nLast 7 days worked at a job not at home?\n67.7\n0\n\n\nAWAYREST\nSMQ860\nLast 7 days spent time in a restaurant?\n58.1\n2\n\n\nAWAYBAR\nSMQ866\nLast 7 days spent time in a bar?\n15.4\n0\n\n\n\nThe inline code I used in the tables was, for example:\n\nround_half_up(100 * sum(nh_fixing$HUQ090 == \"1\", na.rm = TRUE) / \n                sum(nh_fixing$HUQ090 %in% c(\"1\",\"2\"), na.rm = TRUE), 1)\n\n[1] 12.1\n\nn_miss(nh_fixing$HUQ090)\n\n[1] 2\n\n\nTo clean these (1 = Yes, 2 = No) variables, I’ll subtract the values from 2, to obtain variables where 1 = Yes and 0 = No. I’ll use the across() function within my mutate() statement so as to avoid having to type out each change individually6.\n\nnh_fixing <- nh_fixing |>\n  mutate(across(c(HUQ071, HUQ090, ALQ111, HIQ011, HIQ210,\n                  MCQ366A, MCQ366B, MCQ371A, MCQ371B, PAQ605,\n                  PAQ650, RHQ131, SLQ050, SMQ020, SMQ856,\n                  SMQ860, SMQ866), \n                ~ 2 - .x))\n\nLet’s do just one of the relevant sanity checks here. In addition to verifying that our new variable has the values 0 and 1 (instead of 2 and 1), we want to be certain that we’ve maintained any missing values.\n\nnh_fixing |> count(SLQ050)\n\n# A tibble: 3 × 2\n  SLQ050     n\n   <dbl> <int>\n1      0  2769\n2      1  1159\n3     NA     3\n\n\n\n\n1.11.3 Renaming Binary Variables\nHere’s the renaming code.\n\nnh_fixing <- nh_fixing |>\n  rename(HOSPITAL = HUQ071, MENTALH = HUQ090, EVERALC = ALQ111,\n         INSURNOW = HIQ011, NOINSUR = HIQ210, DR_LOSE = MCQ366A,\n         DR_EXER = MCQ366B, NOW_LOSE = MCQ371A, NOW_EXER = MCQ371B,\n         WORK_V = PAQ605, REC_V = PAQ650, EVERPREG = RHQ131, \n         SLPTROUB = SLQ050, CIG100 =  SMQ020, AWAYWORK = SMQ856,\n         AWAYREST = SMQ860, AWAYBAR = SMQ866)"
  },
  {
    "objectID": "01-nhanes.html#create-phq-9-scores",
    "href": "01-nhanes.html#create-phq-9-scores",
    "title": "1  Building the nh432 example",
    "section": "1.12 Create PHQ-9 Scores",
    "text": "1.12 Create PHQ-9 Scores\nThe questions below are asked to assess depression severity, following the prompt “Over the last two weeks, how often have you been bothered by any of the following problems?”\n\n\n\n\n\n\n\n\nVariable\nDescription\nValues\n\n\n\n\nSEQN\nRespondent Sequence Number  (participant ID)\nlink across databases\n\n\nDPQ010\nHave little interest in doing things\n0-3: see below\n\n\nDPQ020\nFeeling down, depressed or hopeless\n0-3: see below\n\n\nDPQ030\nTrouble sleeping or sleeping too much\n0-3: see below\n\n\nDPQ040\nFeeling tired or having little energy\n0-3: see below\n\n\nDPQ050\nPoor appetite or overeating\n0-3: see below\n\n\nDPQ060\nFeeling bad about yourself\n0-3: see below\n\n\nDPQ070\nTrouble concentrating on things\n0-3: see below\n\n\nDPQ080\nMoving or speaking slowly or too fast\n0-3: see below\n\n\nDPQ090\nThoughts you would be better off dead\n0-3: see below\n\n\n\n\nFor DPQ010 - DPQ090, the codes are 0 = Not at all, 1 = Several days in the past two weeks, 2 = More than half the days in the past two weeks, 3 = Nearly every day in the past two weeks.\n\n\n1.12.1 Forming the PHQ-9 Score\nOne way to use this information is to sum the scores from items DPQ010 through DPQ090 to obtain a result on a scale from 0 - 27. Cutoffs of 5, 10, 15, and 20 then represent mild, moderate, moderately severe, and severe levels of depressive symptoms, respectively7. If we had no missing values in our responses, then this would be relatively straightforward.\n\ntemp <- nh_fixing |>\n  mutate(PHQ9 = DPQ010 + DPQ020 + DPQ030 + DPQ040 + DPQ050 +\n           DPQ060 + DPQ070 + DPQ080 + DPQ090)\n\ntemp |> count(PHQ9) |> tail()\n\n# A tibble: 6 × 2\n   PHQ9     n\n  <dbl> <int>\n1    22     9\n2    23     5\n3    24     2\n4    25     2\n5    26     3\n6    NA   221\n\n\nIt turns out that this formulation of PHQ9 regards as missing the result for any participant who failed to answer all 9 questions. A common approach to dealing with missing data in creating PHQ-9 scores8 is to score all questionnaires with up to two missing values, replacing any missing values with the average score of the completed items.\nSo how many of our subjects are missing only one or two of the 9 items?\n\ntemp2 <- temp |>\n  select(SEQN, DPQ010, DPQ020, DPQ030, DPQ040, DPQ050, DPQ060, \n         DPQ070, DPQ080, DPQ090) \n\nmiss_case_table(temp2)\n\n# A tibble: 5 × 3\n  n_miss_in_case n_cases pct_cases\n           <int>   <int>     <dbl>\n1              0    3710   94.4   \n2              1       5    0.127 \n3              2       3    0.0763\n4              4       1    0.0254\n5              9     212    5.39  \n\n\nWith a little googling I found an R script online that will perform this task, and create three new variables:\n\nnvalid_phq9 = Number of Valid Responses (out of 9) to the PHQ-9 items\nPHQ9 = PHQ-9 score (0-27 scale, higher values indicate more depression)\nPHQ9_CAT = factor describing PHQ-9 score\n\nPHQ9 > 20 means PHQ9_CAT is”severe”,\n15-19 = “moderately severe”,\n10-14 = “moderate”\n5-9 = “mild”\n0-4 = “minimal”\n\n\n\nscoring_phq9 <- function(data, items.phq9) {\n  data %>%\n    mutate(nvalid_phq9 = rowSums(!is.na(select(., items.phq9))),\n           nvalid_phq9 = as.integer(nvalid_phq9),\n           mean.temp = rowSums(select(., items.phq9), na.rm = TRUE)/nvalid_phq9,\n           phq.01.temp = as.integer(unlist(data[items.phq9[1]])),\n           phq.02.temp = as.integer(unlist(data[items.phq9[2]])),\n           phq.03.temp = as.integer(unlist(data[items.phq9[3]])),\n           phq.04.temp = as.integer(unlist(data[items.phq9[4]])),\n           phq.05.temp = as.integer(unlist(data[items.phq9[5]])),\n           phq.06.temp = as.integer(unlist(data[items.phq9[6]])),\n           phq.07.temp = as.integer(unlist(data[items.phq9[7]])),\n           phq.08.temp = as.integer(unlist(data[items.phq9[8]])),\n           phq.09.temp = as.integer(unlist(data[items.phq9[9]]))) %>%\n    mutate_at(vars(phq.01.temp:phq.09.temp),\n              funs(ifelse(is.na(.), round(mean.temp), .))) %>%\n    mutate(score.temp = rowSums(select(., phq.01.temp:phq.09.temp), na.rm = TRUE),\n           PHQ9 = ifelse(nvalid_phq9 >= 7, as.integer(round(score.temp)), NA),\n           PHQ9_CAT = case_when(\n             PHQ9 >= 20 ~ 'severe',\n             PHQ9 >= 15 ~ 'moderately severe',\n             PHQ9 >= 10 ~ 'moderate',\n             PHQ9 >= 5 ~ 'mild',\n             PHQ9 < 5 ~ 'minimal'),\n             PHQ9_CAT = factor(PHQ9_CAT, levels = c('minimal', 'mild',\n                                                          'moderate', 'moderately severe',\n                                                          'severe'))) %>%\n    select(-ends_with(\"temp\"))\n \n}\n\nApplying this script to our nh_fixing data, our result is:\n\nitems.phq9 <- c(\"DPQ010\", \"DPQ020\", \"DPQ030\", \"DPQ040\", \"DPQ050\",\n                \"DPQ060\", \"DPQ070\", \"DPQ080\", \"DPQ090\")\nnh_fixing <- nh_fixing %>% scoring_phq9(., all_of(items.phq9))\n\nnh_fixing |> count(nvalid_phq9, PHQ9, PHQ9_CAT)\n\n# A tibble: 36 × 4\n   nvalid_phq9  PHQ9 PHQ9_CAT     n\n         <int> <int> <fct>    <int>\n 1           0    NA <NA>       212\n 2           5    NA <NA>         1\n 3           7     1 minimal      1\n 4           7     2 minimal      1\n 5           7     7 mild         1\n 6           8     1 minimal      2\n 7           8     2 minimal      1\n 8           8     3 minimal      1\n 9           8     8 mild         1\n10           9     0 minimal   1233\n# … with 26 more rows\n\n\n\n\n1.12.2 Distribution of PHQ-9 Score\nHere’s a quick look at the distribution of PHQ-9 scores in our nh_fixing data.\n\nnh_fixing |> filter(complete.cases(PHQ9, PHQ9_CAT)) %>%\n  ggplot(., aes(x = PHQ9, fill = PHQ9_CAT)) +\n  geom_histogram(binwidth = 1) +\n  scale_fill_viridis_d() + \n  labs(title = \"PHQ-9 Scores for subjects in `nh_fixing`\")\n\n\n\n\n\n\n1.12.3 Fixing the DPQ100 variable\nThe DPQ100 variable should be 0 (Not at all difficult) if the PHQ-9 score is zero. We need to fix this, because NHANES participants who answered 0 (Not at all) to each of the nine elements contained in the PHQ-9 were not asked the DPQ100 question. So, we set the value of DPQ100 to be 0 if PHQ9 is 0.\n\nnh_fixing <- nh_fixing |>\n  mutate(DPQ100 = ifelse(PHQ9 == 0, 0, DPQ100))\n\nThis will eliminate the “automatic missing” values in DPQ100.\n\nnh_fixing |> tabyl(DPQ100)\n\n DPQ100    n    percent valid_percent\n      0 3039 0.77308573    0.81781485\n      1  541 0.13762401    0.14558665\n      2   93 0.02365810    0.02502691\n      3   43 0.01093869    0.01157158\n     NA  215 0.05469346            NA"
  },
  {
    "objectID": "01-nhanes.html#multi-categorical-variables",
    "href": "01-nhanes.html#multi-categorical-variables",
    "title": "1  Building the nh432 example",
    "section": "1.13 Multi-Categorical Variables",
    "text": "1.13 Multi-Categorical Variables\nOur remaining categorical variables with more than two levels are:\n\n\n\n\n\n\n\n\n\nVariable\nDescription\nCodes\n# NAs\n\n\n\n\nRIDRETH3\nRace/Hispanic origin\n1, 2, 3, 4, 6, 7\n0\n\n\nDMDEDUC2\nEducation Level\n1, 2, 3, 4, 5\n1\n\n\nOHAREC\nOverall Recommendation for Care\n1, 2, 3, 4\n0\n\n\nHUQ010\nGeneral health condition\n1, 2, 3, 4, 5\n0\n\n\nDEQ034D\nUse sunscreen if outside on very sunny day?\n1, 2, 3, 4, 5\n19\n\n\nDPQ100\nDifficulty depression problems have caused\n0, 1, 2, 3\n215\n\n\nDBQ700\nHow healthy is your diet?\n1, 2, 3, 4, 5\n1\n\n\nFSDAD\nAdult food security in last 12m\n1, 2, 3, 4\n231\n\n\nSLQ030\nHow often do you snore?\n0, 1, 2, 3\n219\n\n\nWHQ040\nLike to weigh more, less, or same?\n1, 2, 3\n3\n\n\n\n\n1.13.1 Creating RACEETH from RIDRETH3\nAt the moment, our RIDRETH3 data look like this:\n\nt_ridreth3 <- nh_fixing |> tabyl(RIDRETH3) |> adorn_pct_formatting() |>\n  mutate(Code = c(\"Mexican American\", \"Other Hispanic\", \"Non-Hispanic White\", \n                  \"Non-Hispanic Black\", \"Non-Hispanic Asian\", \"Other Race\"))\n\ngt(t_ridreth3)\n\n\n\n\n\n  \n  \n    \n      RIDRETH3\n      n\n      percent\n      Code\n    \n  \n  \n    1\n500\n12.7%\nMexican American\n    2\n403\n10.3%\nOther Hispanic\n    3\n1192\n30.3%\nNon-Hispanic White\n    4\n1049\n26.7%\nNon-Hispanic Black\n    6\n588\n15.0%\nNon-Hispanic Asian\n    7\n199\n5.1%\nOther Race\n  \n  \n  \n\n\n\n\nNow, we’ll turn this RIDRETH3 variable into a new factor called RACEETH with meaningful levels, and then sort those levels by their frequency in the data. We’ll also collapse together the Mexican American and Other Hispanic levels, not because the distinction is irrelevant, but more to demonstrate how this might be done.\n\nnh_fixing <- nh_fixing |>\n  mutate(RACEETH = \n           fct_recode(\n             factor(RIDRETH3), \n             \"Hispanic\" = \"1\", \n             \"Hispanic\" = \"2\", \n             \"Non-H White\" = \"3\",\n             \"Non-H Black\" = \"4\",\n             \"Non-H Asian\" = \"6\",\n             \"Other Race\" = \"7\")) |>\n  mutate(RACEETH = fct_infreq(RACEETH))\n\nI’m using fct_infreq() here to sort the (nominal) Race and Ethnicity data so that the most common column appears first, and will thus be treated as the “baseline” level in models. Here is the resulting order.\n\nnh_fixing |> count(RACEETH)\n\n# A tibble: 5 × 2\n  RACEETH         n\n  <fct>       <int>\n1 Non-H White  1192\n2 Non-H Black  1049\n3 Hispanic      903\n4 Non-H Asian   588\n5 Other Race    199\n\n\nNow, let’s check9 to see if RACEETH and RIDRETH3 include the same information (after collapsing the Mexican American and Other Hispanic categories.)\n\nnh_fixing |> count(RACEETH, RIDRETH3)\n\n# A tibble: 6 × 3\n  RACEETH     RIDRETH3     n\n  <fct>          <dbl> <int>\n1 Non-H White        3  1192\n2 Non-H Black        4  1049\n3 Hispanic           1   500\n4 Hispanic           2   403\n5 Non-H Asian        6   588\n6 Other Race         7   199\n\n\n\n\n1.13.2 Creating EDUC from DMDEDUC2\nAt the moment, our DMDEDUC2 data look like this:\n\nt_dmdeduc2 <- nh_fixing |> tabyl(DMDEDUC2) |> adorn_pct_formatting() |>\n  mutate(Code = c(\"Less than 9th grade\", \"9th-11th grade\", \"High School Grad\", \n                  \"Some College / AA\", \"College Grad\", \"Missing\"))\n\ngt(t_dmdeduc2)\n\n\n\n\n\n  \n  \n    \n      DMDEDUC2\n      n\n      percent\n      valid_percent\n      Code\n    \n  \n  \n    1\n272\n6.9%\n6.9%\nLess than 9th grade\n    2\n424\n10.8%\n10.8%\n9th-11th grade\n    3\n850\n21.6%\n21.6%\nHigh School Grad\n    4\n1287\n32.7%\n32.7%\nSome College / AA\n    5\n1097\n27.9%\n27.9%\nCollege Grad\n    NA\n1\n0.0%\n-\nMissing\n  \n  \n  \n\n\n\n\nNow, we’ll turn this DMDEDUC2 variable into a new factor called EDUC with meaningful levels.\n\nnh_fixing <- nh_fixing |>\n  mutate(EDUC = \n           fct_recode(\n             factor(DMDEDUC2), \n             \"Less than 9th Grade\" = \"1\", \n             \"9th - 11th Grade\" = \"2\", \n             \"High School Grad\" = \"3\",\n             \"Some College / AA\" = \"4\",\n             \"College Grad\" = \"5\")) \n\nOnce again, checking our work…\n\nnh_fixing |> tabyl(EDUC, DMDEDUC2) |> gt()\n\n\n\n\n\n  \n  \n    \n      EDUC\n      1\n      2\n      3\n      4\n      5\n      NA_\n    \n  \n  \n    Less than 9th Grade\n272\n0\n0\n0\n0\n0\n    9th - 11th Grade\n0\n424\n0\n0\n0\n0\n    High School Grad\n0\n0\n850\n0\n0\n0\n    Some College / AA\n0\n0\n0\n1287\n0\n0\n    College Grad\n0\n0\n0\n0\n1097\n0\n    NA\n0\n0\n0\n0\n0\n1\n  \n  \n  \n\n\n\n\n\n\n1.13.3 Creating DENTAL from OHAREC\n\nt_oharec <- nh_fixing |> tabyl(OHAREC) |> adorn_pct_formatting() |>\n  mutate(Code = c(\"See a dentist immediately\", \"See a dentist within the next 2 weeks\", \"See a dentist at your earliest convenience\", \"Continue your regular routine care\"))\n\ngt(t_oharec)\n\n\n\n\n\n  \n  \n    \n      OHAREC\n      n\n      percent\n      Code\n    \n  \n  \n    1\n4\n0.1%\nSee a dentist immediately\n    2\n230\n5.9%\nSee a dentist within the next 2 weeks\n    3\n1671\n42.5%\nSee a dentist at your earliest convenience\n    4\n2026\n51.5%\nContinue your regular routine care\n  \n  \n  \n\n\n\n\nWe’ll collapse categories 1 and 2 together since they are quite small.\n\nnh_fixing <- nh_fixing |>\n  mutate(DENTAL = \n           fct_recode(\n             factor(OHAREC), \n             \"See dentist urgently\" = \"1\", \n             \"See dentist urgently\" = \"2\", \n             \"See dentist soon\" = \"3\",\n             \"Regular Routine\" = \"4\")) \n\nOnce again, checking our work…\n\nnh_fixing |> tabyl(DENTAL, OHAREC) |> gt()\n\n\n\n\n\n  \n  \n    \n      DENTAL\n      1\n      2\n      3\n      4\n    \n  \n  \n    See dentist urgently\n4\n230\n0\n0\n    See dentist soon\n0\n0\n1671\n0\n    Regular Routine\n0\n0\n0\n2026\n  \n  \n  \n\n\n\n\n\n\n1.13.4 Creating SROH from HUQ010\n\n\n\n\n\n\n\n\n\nVariable\nDescription\nCodes\n# NAs\n\n\n\n\nHUQ010\nGeneral health condition\n1 = Excellent  2 = Very Good  3 = Good  4 = Fair  5 = Poor\n0\n\n\n\n\nnh_fixing <- nh_fixing |>\n  mutate(SROH = \n           fct_recode(\n             factor(HUQ010), \n             \"Excellent\" = \"1\", \n             \"Very Good\" = \"2\", \n             \"Good\" = \"3\",\n             \"Fair\" = \"4\",\n             \"Poor\" = \"5\"))\n\nChecking our work…\n\nnh_fixing |> tabyl(SROH, HUQ010) |> gt()\n\n\n\n\n\n  \n  \n    \n      SROH\n      1\n      2\n      3\n      4\n      5\n    \n  \n  \n    Excellent\n495\n0\n0\n0\n0\n    Very Good\n0\n1071\n0\n0\n0\n    Good\n0\n0\n1462\n0\n0\n    Fair\n0\n0\n0\n765\n0\n    Poor\n0\n0\n0\n0\n138\n  \n  \n  \n\n\n\n\n\n\n1.13.5 Creating SUNSCR from DEQ034D\n\n\n\n\n\n\n\n\n\nVariable\nDescription\nCodes\n# NAs\n\n\n\n\nDEQ034D\nUse sunscreen if outside on very sunny day?\n1 = Always  2 = Most of the time  3 = Sometimes  4 = Rarely  5 = Never\n19\n\n\n\n\nnh_fixing <- nh_fixing |>\n  mutate(SUNSCR = \n           fct_recode(\n             factor(DEQ034D), \n             \"Always\" = \"1\", \n             \"Most of the time\" = \"2\", \n             \"Sometimes\" = \"3\",\n             \"Rarely\" = \"4\",\n             \"Never\" = \"5\")) \n\n\nnh_fixing |> tabyl(SUNSCR, DEQ034D) |> gt()\n\n\n\n\n\n  \n  \n    \n      SUNSCR\n      1\n      2\n      3\n      4\n      5\n      NA_\n    \n  \n  \n    Always\n351\n0\n0\n0\n0\n0\n    Most of the time\n0\n485\n0\n0\n0\n0\n    Sometimes\n0\n0\n831\n0\n0\n0\n    Rarely\n0\n0\n0\n662\n0\n0\n    Never\n0\n0\n0\n0\n1583\n0\n    NA\n0\n0\n0\n0\n0\n19\n  \n  \n  \n\n\n\n\n\n\n1.13.6 Creating DEPRDIFF from DPQ100\n\n\n\n\n\n\n\n\n\nVariable\nDescription\nCodes\n# NAs\n\n\n\n\nDPQ100\nDifficulty depression problems have caused\n0 = Not at all difficult  1 = Somewhat difficult  2 = Very difficult  3 = Extremely difficult\n215\n\n\n\n\nnh_fixing <- nh_fixing |>\n  mutate(DEPRDIFF = \n           fct_recode(\n             factor(DPQ100), \n             \"Not at all\" = \"0\", \n             \"Somewhat\" = \"1\", \n             \"Very\" = \"2\",\n             \"Extremely\" = \"3\")) \n\n\nnh_fixing |> tabyl(DEPRDIFF, DPQ100) |> gt()\n\n\n\n\n\n  \n  \n    \n      DEPRDIFF\n      0\n      1\n      2\n      3\n      NA_\n    \n  \n  \n    Not at all\n3039\n0\n0\n0\n0\n    Somewhat\n0\n541\n0\n0\n0\n    Very\n0\n0\n93\n0\n0\n    Extremely\n0\n0\n0\n43\n0\n    NA\n0\n0\n0\n0\n215\n  \n  \n  \n\n\n\n\n\n\n1.13.7 Creating DIETQUAL from DBQ700\n\n\n\n\n\n\n\n\n\nVariable\nDescription\nCodes\n# NAs\n\n\n\n\nDBQ700\nHow healthy is your diet?\n1 = Excellent  2 = Very Good  3 = Good  4 = Fair  5 = Poor\n1\n\n\n\n\nnh_fixing <- nh_fixing |>\n  mutate(DIETQUAL = \n           fct_recode(\n             factor(DBQ700), \n             \"Excellent\" = \"1\", \n             \"Very Good\" = \"2\", \n             \"Good\" = \"3\",\n             \"Fair\" = \"4\",\n             \"Poor\" = \"5\")) \n\n\nnh_fixing |> tabyl(DIETQUAL, DBQ700) |> gt()\n\n\n\n\n\n  \n  \n    \n      DIETQUAL\n      1\n      2\n      3\n      4\n      5\n      NA_\n    \n  \n  \n    Excellent\n260\n0\n0\n0\n0\n0\n    Very Good\n0\n758\n0\n0\n0\n0\n    Good\n0\n0\n1519\n0\n0\n0\n    Fair\n0\n0\n0\n1082\n0\n0\n    Poor\n0\n0\n0\n0\n311\n0\n    NA\n0\n0\n0\n0\n0\n1\n  \n  \n  \n\n\n\n\n\n\n1.13.8 Creating FOODSEC from FSDAD\n\n\n\n\n\n\n\n\n\nVariable\nDescription\nCodes\n# NAs\n\n\n\n\nFSDAD\nAdult food security category for last 12m\n1 = Full food security  2 = Marginal food security  3 = Low food security  4 = Very low food security\n231\n\n\n\n\nnh_fixing <- nh_fixing |>\n  mutate(FOODSEC = \n           fct_recode(\n             factor(FSDAD), \n             \"Full\" = \"1\", \n             \"Marginal\" = \"2\", \n             \"Low\" = \"3\",\n             \"Very Low\" = \"4\")) \n\n\nnh_fixing |> tabyl(FOODSEC, FSDAD) |> gt()\n\n\n\n\n\n  \n  \n    \n      FOODSEC\n      1\n      2\n      3\n      4\n      NA_\n    \n  \n  \n    Full\n2247\n0\n0\n0\n0\n    Marginal\n0\n565\n0\n0\n0\n    Low\n0\n0\n503\n0\n0\n    Very Low\n0\n0\n0\n385\n0\n    NA\n0\n0\n0\n0\n231\n  \n  \n  \n\n\n\n\n\n\n1.13.9 Creating SNORE from SLQ030\n\n\n\n\n\n\n\n\n\nVariable\nDescription\nCodes\n# NAs\n\n\n\n\nSLQ030\nHow often do you snore?\n0 = Never  1 = Rarely (1-2 nights/week)  2 = Occasionally (3-4 nights/week)  3 = Frequently (5+ nights/week)\n219\n\n\n\n\nnh_fixing <- nh_fixing |>\n  mutate(SNORE = \n           fct_recode(\n             factor(SLQ030), \n             \"Never\" = \"0\", \n             \"Rarely\" = \"1\", \n             \"Occasionally\" = \"2\",\n             \"Frequently\" = \"3\")) \n\n\nnh_fixing |> tabyl(SNORE, SLQ030) |> gt()\n\n\n\n\n\n  \n  \n    \n      SNORE\n      0\n      1\n      2\n      3\n      NA_\n    \n  \n  \n    Never\n855\n0\n0\n0\n0\n    Rarely\n0\n959\n0\n0\n0\n    Occasionally\n0\n0\n700\n0\n0\n    Frequently\n0\n0\n0\n1198\n0\n    NA\n0\n0\n0\n0\n219\n  \n  \n  \n\n\n\n\n\n\n1.13.10 Creating WTGOAL from WHQ040\n\n\n\n\n\n\n\n\n\nVariable\nDescription\nCodes\n# NAs\n\n\n\n\nWHQ040\nLike to weigh more, less, or same?\n1 = More  2 = Less  3 = Stay about the same\n3\n\n\n\nSince there’s a natural ordering here (more then same then less) I’ll adapt it using the fct_relevel() function from the forcats package10 for this variable.\n\nnh_fixing <- nh_fixing |>\n  mutate(WTGOAL = \n           fct_recode(\n             factor(WHQ040), \n             \"More\" = \"1\", \n             \"Less\" = \"2\", \n             \"Same\" = \"3\")) |>\n  mutate(WTGOAL = fct_relevel(WTGOAL, \"More\", \"Same\", \"Less\"))\n\n\nnh_fixing |> tabyl(WTGOAL, WHQ040) |> gt()\n\n\n\n\n\n  \n  \n    \n      WTGOAL\n      1\n      2\n      3\n      NA_\n    \n  \n  \n    More\n289\n0\n0\n0\n    Same\n0\n0\n974\n0\n    Less\n0\n2665\n0\n0\n    NA\n0\n0\n0\n3"
  },
  {
    "objectID": "01-nhanes.html#dropping-variables",
    "href": "01-nhanes.html#dropping-variables",
    "title": "1  Building the nh432 example",
    "section": "1.14 Dropping Variables",
    "text": "1.14 Dropping Variables\nWe’ll drop the following variables before saving an analytic tibble.\n\nOur two variables with no variation\n\nRIDSTATR, OHDEXSTS\n\nElements of the PHQ-9 we no longer need\n\nnvalid_phq, DPQ010, DPQ020, DPQ030, DPQ040\nDPQ050, DPQ060, DPQ070, DPQ080, DPQ090\n\nMulti-categorical variables that we renamed\n\nRIDRETH3, DMDEDUC2, OHAREC, HUQ010, DEQ034D\nDPQ100, DBQ700, FSDAD, SLQ030, WHQ040\n\n\n\nnh_fixing <- nh_fixing |>\n  select(-c(RIDSTATR, OHDEXSTS, nvalid_phq9, DPQ010, \n            DPQ020, DPQ030, DPQ040, DPQ050, DPQ060, \n            DPQ070, DPQ080, DPQ090, RIDRETH3, DMDEDUC2, \n            OHAREC, HUQ010, DEQ034D, DPQ100, DBQ700, \n            FSDAD, SLQ030, WHQ040))"
  },
  {
    "objectID": "01-nhanes.html#resorting-variables",
    "href": "01-nhanes.html#resorting-variables",
    "title": "1  Building the nh432 example",
    "section": "1.15 Resorting Variables",
    "text": "1.15 Resorting Variables\nI’d like to have the variables in the following order:\n\nnh432 <- nh_fixing |>\n  select(SEQN, AGE, RACEETH, EDUC, SEX, INSURNOW, \n         NOINSUR, SROH, WEIGHT, HEIGHT, WAIST, \n         SBP, DBP, PULSE1, PULSE2, WBC, PLATELET, HSCRP, \n         DR_LOSE, DR_EXER, NOW_LOSE, NOW_EXER,\n         ESTHT, ESTWT, WTGOAL, DIETQUAL, FOODSEC, \n         WORK_V, VIGWK_D, REC_V, VIGREC_D, SEDATE, \n         PHQ9, PHQ9_CAT, DEPRDIFF, MENTALH, \n         SLPWKDAY, SLPWKEND, SLPTROUB, SNORE,\n         HOSPITAL, EVERALC, DRINKS, CIG100, SMOKE30,\n         AWAYWORK, AWAYREST, AWAYBAR, DENTAL, FLOSS, \n         EVERPREG, PREGS, SUNSCR, WTINTPRP, WTMECPRP)"
  },
  {
    "objectID": "01-nhanes.html#nh432-analytic-tibble",
    "href": "01-nhanes.html#nh432-analytic-tibble",
    "title": "1  Building the nh432 example",
    "section": "1.16 nh432 analytic tibble",
    "text": "1.16 nh432 analytic tibble\n\nnh432\n\n# A tibble: 3,931 × 55\n   SEQN     AGE RACEETH    EDUC  SEX   INSUR…¹ NOINSUR SROH  WEIGHT HEIGHT WAIST\n   <chr>  <dbl> <fct>      <fct> <fct>   <dbl>   <dbl> <fct>  <dbl>  <dbl> <dbl>\n 1 109271    49 Non-H Whi… 9th … Male        1       0 Fair    98.8   182. 120. \n 2 109273    36 Non-H Whi… Some… Male        1       1 Good    74.3   184.  86.8\n 3 109284    44 Hispanic   9th … Fema…       0       1 Fair    91.1   153. 103. \n 4 109291    42 Non-H Asi… Coll… Fema…       1       0 Fair    81.4   161.  NA  \n 5 109292    58 Hispanic   High… Male        1       0 Very…   86     168. 108. \n 6 109293    44 Non-H Whi… High… Male        1       0 Good    99.4   182. 107  \n 7 109295    54 Hispanic   Less… Fema…       1       0 Good    61.7   157.  90.5\n 8 109297    30 Non-H Asi… Some… Fema…       1       0 Very…   55.4   155.  73.2\n 9 109300    54 Non-H Asi… Coll… Fema…       1       0 Exce…   62     145.  84.8\n10 109305    55 Non-H Asi… Coll… Male        1       0 Good    64     175.  82.5\n# … with 3,921 more rows, 44 more variables: SBP <dbl>, DBP <dbl>,\n#   PULSE1 <dbl>, PULSE2 <dbl>, WBC <dbl>, PLATELET <dbl>, HSCRP <dbl>,\n#   DR_LOSE <dbl>, DR_EXER <dbl>, NOW_LOSE <dbl>, NOW_EXER <dbl>, ESTHT <dbl>,\n#   ESTWT <dbl>, WTGOAL <fct>, DIETQUAL <fct>, FOODSEC <fct>, WORK_V <dbl>,\n#   VIGWK_D <dbl>, REC_V <dbl>, VIGREC_D <dbl>, SEDATE <dbl>, PHQ9 <int>,\n#   PHQ9_CAT <fct>, DEPRDIFF <fct>, MENTALH <dbl>, SLPWKDAY <dbl>,\n#   SLPWKEND <dbl>, SLPTROUB <dbl>, SNORE <fct>, HOSPITAL <dbl>, …\n\n\n\n1.16.1 Saving the tibble as nh432.Rds\n\nwrite_rds(nh432, \"data/nh432.Rds\")"
  },
  {
    "objectID": "02-nh432cb.html#r-setup",
    "href": "02-nh432cb.html#r-setup",
    "title": "2  Codebook for nh432",
    "section": "2.1 R Setup",
    "text": "2.1 R Setup\n\nknitr::opts_chunk$set(comment = NA)\n\nlibrary(janitor) \nlibrary(gt)\nlibrary(gtsummary)\nlibrary(Hmisc)\nlibrary(mosaic)\nlibrary(naniar)\nlibrary(tidyverse) \n\ntheme_set(theme_bw())\n\n\n2.1.1 Data Load\n\nnh432 <- read_rds(\"data/nh432.Rds\")"
  },
  {
    "objectID": "02-nh432cb.html#quantitative-variables-in-nh432",
    "href": "02-nh432cb.html#quantitative-variables-in-nh432",
    "title": "2  Codebook for nh432",
    "section": "2.2 Quantitative Variables in nh432",
    "text": "2.2 Quantitative Variables in nh432\n\nt1_quantitative <- df_stats(~ AGE + WEIGHT + HEIGHT + WAIST + SBP + DBP +\n           PULSE1 + PULSE2 + WBC + PLATELET + HSCRP +\n           ESTHT + ESTWT + VIGWK_D + VIGREC_D + SEDATE + PHQ9 +\n           SLPWKDAY + SLPWKEND + DRINKS + SMOKE30 + \n           FLOSS + PREGS, data = nh432) |>\n  mutate(across(.cols = -c(response, n, missing), \n              round_half_up, digits = 1)) |> \n  rename(med = median, \"NA\" = missing)\n\nt1_quantitative |>\n  mutate(description = \n           c(\"Age (years)\", \"Weight (kg)\", \"Height (cm)\", \n             \"Waist circumference (cm)\", \"Systolic BP (mm Hg)\", \n             \"Diastolic BP (mm Hg)\", \"1st Pulse (beats/min)\", \n             \"2nd Pulse (beats/min)\", \"White Blood Cell Count (1000 cells/uL)\",\n             \"Platelets (1000 cells/uL)\", \n             \"High-Sensitivity C-Reactive Protein (mg/L)\",\n             \"Self Estimate: Height (in)\", \"Self-Estimate: Weight (lb)\",\n             \"Vigorous Work per week (days)\", \n             \"Vigorous Recreation per week (days)\",\n             \"Sedentary Activity per day (minutes)\",\n             \"PHQ-9 Depression Screener Score (points)\",\n             \"Average weekday sleep (hours)\", \"Average weekend sleep (hours)\",\n             \"Average Alcohol per day (drinks)\", \n             \"Days smoked cigarette in last 30\",\n             \"Days Flossed in last 7\", \"Pregnancies\")) |>\n  select(response, description, everything()) |>\n  gt() |>\n  tab_header(title = \"Quantitative Variables in nh432\")\n\n\n\n\n\n  \n    \n      Quantitative Variables in nh432\n    \n    \n  \n  \n    \n      response\n      description\n      min\n      Q1\n      med\n      Q3\n      max\n      mean\n      sd\n      n\n      NA\n    \n  \n  \n    AGE\nAge (years)\n30.0\n37.0\n45.0\n53.0\n59.0\n44.8\n8.7\n3931\n0\n    WEIGHT\nWeight (kg)\n36.9\n69.3\n82.1\n99.1\n254.3\n86.3\n24.6\n3903\n28\n    HEIGHT\nHeight (cm)\n135.3\n160.0\n166.9\n174.7\n198.7\n167.4\n10.1\n3901\n30\n    WAIST\nWaist circumference (cm)\n57.9\n89.1\n99.2\n111.7\n178.0\n101.5\n17.7\n3782\n149\n    SBP\nSystolic BP (mm Hg)\n69.0\n110.0\n120.0\n131.0\n222.0\n121.5\n17.0\n3585\n346\n    DBP\nDiastolic BP (mm Hg)\n31.0\n69.0\n76.0\n84.0\n136.0\n77.0\n11.7\n3585\n346\n    PULSE1\n1st Pulse (beats/min)\n38.0\n62.0\n69.0\n77.0\n126.0\n70.3\n11.6\n3316\n615\n    PULSE2\n2nd Pulse (beats/min)\n37.0\n63.0\n70.0\n78.0\n121.0\n71.0\n11.6\n3314\n617\n    WBC\nWhite Blood Cell Count (1000 cells/uL)\n2.3\n5.7\n6.9\n8.4\n22.8\n7.3\n2.2\n3755\n176\n    PLATELET\nPlatelets (1000 cells/uL)\n47.0\n210.0\n246.0\n290.0\n818.0\n253.3\n66.4\n3755\n176\n    HSCRP\nHigh-Sensitivity C-Reactive Protein (mg/L)\n0.1\n0.9\n2.1\n4.7\n182.8\n4.3\n8.3\n3664\n267\n    ESTHT\nSelf Estimate: Height (in)\n50.0\n63.0\n66.0\n69.0\n81.0\n66.5\n4.2\n3836\n95\n    ESTWT\nSelf-Estimate: Weight (lb)\n86.0\n150.0\n180.0\n216.0\n578.0\n188.1\n52.2\n3863\n68\n    VIGWK_D\nVigorous Work per week (days)\n0.0\n0.0\n0.0\n2.0\n7.0\n1.2\n2.1\n3926\n5\n    VIGREC_D\nVigorous Recreation per week (days)\n0.0\n0.0\n0.0\n1.0\n7.0\n0.9\n1.7\n3930\n1\n    SEDATE\nSedentary Activity per day (minutes)\n2.0\n180.0\n300.0\n480.0\n1320.0\n332.7\n210.2\n3907\n24\n    PHQ9\nPHQ-9 Depression Screener Score (points)\n0.0\n0.0\n2.0\n5.0\n26.0\n3.3\n4.3\n3718\n213\n    SLPWKDAY\nAverage weekday sleep (hours)\n2.0\n6.5\n7.5\n8.0\n14.0\n7.4\n1.6\n3897\n34\n    SLPWKEND\nAverage weekend sleep (hours)\n2.0\n7.0\n8.0\n9.0\n14.0\n8.2\n1.8\n3897\n34\n    DRINKS\nAverage Alcohol per day (drinks)\n0.0\n1.0\n2.0\n3.0\n15.0\n2.3\n2.2\n3142\n789\n    SMOKE30\nDays smoked cigarette in last 30\n0.0\n0.0\n0.0\n5.0\n30.0\n6.8\n12.1\n3205\n726\n    FLOSS\nDays Flossed in last 7\n0.0\n0.0\n3.0\n7.0\n7.0\n3.5\n2.9\n3927\n4\n    PREGS\nPregnancies\n0.0\n2.0\n3.0\n4.0\n11.0\n3.0\n2.1\n1956\n1975"
  },
  {
    "objectID": "02-nh432cb.html#two-category-10-variables-in-nh432",
    "href": "02-nh432cb.html#two-category-10-variables-in-nh432",
    "title": "2  Codebook for nh432",
    "section": "2.3 Two-Category (1/0) Variables in nh432",
    "text": "2.3 Two-Category (1/0) Variables in nh432\n\nnh_dich_vars <- nh432 |>\n  select(HOSPITAL, MENTALH, EVERALC, INSURNOW, NOINSUR, DR_LOSE,\n         DR_EXER, NOW_LOSE, NOW_EXER, WORK_V, REC_V, EVERPREG,\n         SLPTROUB, CIG100, AWAYWORK, AWAYREST, AWAYBAR) \n\ntemp1 <- nh_dich_vars |>  summarise(across(.cols = everything(), \n                                           ~ sum(.x, na.rm = TRUE)))\n\ntemp2 <- nh_dich_vars |>  summarise(across(.cols = everything(), \n                           ~ round_half_up(100*mean(.x, na.rm = TRUE), 1)))\n\ntemp3 <- nh_dich_vars |>  summarise(across(.cols = everything(), \n                                           ~ n_miss(.x)))\n\nnh_dichotomous_summary <- bind_rows(temp1, temp2, temp3) |>\n  mutate(summary = c(\"Yes\", \"% Yes\", \"# NA\")) |>\n  relocate(summary) |>\n  pivot_longer(!summary, names_to = \"variable\") |>\n  pivot_wider(names_from = summary) |>\n  mutate(Description = \n           c(\"Overnight hospital patient in past 12m?\",\n             \"Seen mental health professional past 12m?\",\n             \"Ever had a drink of alcohol?\",\n             \"Covered by health insurance now?\",\n             \"Time when no insurance in past year?\",\n             \"Doctor said to control/lose weight past 12m?\",\n             \"Doctor said to exercise in past 12m?\",\n             \"Are you now controlling or losing weight?\",\n             \"Are you now increasing exercise?\",\n             \"Vigorous work activity for 10 min/week?\",\n             \"Vigorous recreational activity for 10 min/week?\",\n             \"Ever been pregnant?\",\n             \"Ever told a doctor you had trouble sleeping?\",\n             \"Smoked at least 100 cigarettes in your life?\",\n             \"Last 7 days worked at a job not at home?\",\n             \"Last 7 days spent time in a restaurant?\",\n             \"Last 7 days spent time in a bar?\"))\n\nnh_dichotomous_summary |> \n  gt()\n\n\n\n\n\n  \n  \n    \n      variable\n      Yes\n      % Yes\n      # NA\n      Description\n    \n  \n  \n    HOSPITAL\n343\n8.7\n0\nOvernight hospital patient in past 12m?\n    MENTALH\n475\n12.1\n2\nSeen mental health professional past 12m?\n    EVERALC\n3393\n91.1\n205\nEver had a drink of alcohol?\n    INSURNOW\n3169\n80.8\n10\nCovered by health insurance now?\n    NOINSUR\n1041\n26.6\n16\nTime when no insurance in past year?\n    DR_LOSE\n1189\n30.3\n1\nDoctor said to control/lose weight past 12m?\n    DR_EXER\n1680\n42.7\n1\nDoctor said to exercise in past 12m?\n    NOW_LOSE\n2485\n63.2\n2\nAre you now controlling or losing weight?\n    NOW_EXER\n2369\n60.3\n1\nAre you now increasing exercise?\n    WORK_V\n1116\n28.4\n4\nVigorous work activity for 10 min/week?\n    REC_V\n1056\n26.9\n0\nVigorous recreational activity for 10 min/week?\n    EVERPREG\n1747\n89.2\n1972\nEver been pregnant?\n    SLPTROUB\n1159\n29.5\n3\nEver told a doctor you had trouble sleeping?\n    CIG100\n1576\n40.1\n1\nSmoked at least 100 cigarettes in your life?\n    AWAYWORK\n2663\n67.7\n0\nLast 7 days worked at a job not at home?\n    AWAYREST\n2283\n58.1\n2\nLast 7 days spent time in a restaurant?\n    AWAYBAR\n605\n15.4\n0\nLast 7 days spent time in a bar?"
  },
  {
    "objectID": "02-nh432cb.html#factor-variables-in-nh432",
    "href": "02-nh432cb.html#factor-variables-in-nh432",
    "title": "2  Codebook for nh432",
    "section": "2.4 Factor Variables in nh432",
    "text": "2.4 Factor Variables in nh432\n\nnh_factor_vars <- nh432 |>\n  select(where(~ is.factor(.x)))\n\ntbl_summary(nh_factor_vars,\n            label = c(RACEETH = \"RACEETH: Race/Ethnicity\",\n                      EDUC = \"EDUC: Educational Attainment\",\n                      SROH = \"SROH: Self-reported Overall Health\",\n                      WTGOAL = \"WTGOAL: Like to weigh more/less/the same?\",\n                      DIETQUAL = \"DIETQUAL: How healthy is your diet?\",\n                      FOODSEC = \"FOODSEC: Adult food security (last 12m)\",\n                      PHQ9_CAT = \"PHQ9_CAT: Depression Screen Category\",\n                      DEPRDIFF = \"DEPRDIFF: Difficulty with Depression?\",\n                      SNORE = \"SNORE: How often do you snore?\",\n                      DENTAL = \"DENTAL: Recommendation for Dental Care?\",\n                      SUNSCR = \"SUNSCR: Use sunscreen on very sunny day?\"),\n            missing_text = \"(# NA)\")\n\n\n\n\n\n  \n  \n    \n      Characteristic\n      N = 3,9311\n    \n  \n  \n    RACEETH: Race/Ethnicity\n\n        Non-H White\n1,192 (30%)\n        Non-H Black\n1,049 (27%)\n        Hispanic\n903 (23%)\n        Non-H Asian\n588 (15%)\n        Other Race\n199 (5.1%)\n    EDUC: Educational Attainment\n\n        Less than 9th Grade\n272 (6.9%)\n        9th - 11th Grade\n424 (11%)\n        High School Grad\n850 (22%)\n        Some College / AA\n1,287 (33%)\n        College Grad\n1,097 (28%)\n        (# NA)\n1\n    SEX\n\n        Female\n2,094 (53%)\n        Male\n1,837 (47%)\n    SROH: Self-reported Overall Health\n\n        Excellent\n495 (13%)\n        Very Good\n1,071 (27%)\n        Good\n1,462 (37%)\n        Fair\n765 (19%)\n        Poor\n138 (3.5%)\n    WTGOAL: Like to weigh more/less/the same?\n\n        More\n289 (7.4%)\n        Same\n974 (25%)\n        Less\n2,665 (68%)\n        (# NA)\n3\n    DIETQUAL: How healthy is your diet?\n\n        Excellent\n260 (6.6%)\n        Very Good\n758 (19%)\n        Good\n1,519 (39%)\n        Fair\n1,082 (28%)\n        Poor\n311 (7.9%)\n        (# NA)\n1\n    FOODSEC: Adult food security (last 12m)\n\n        Full\n2,247 (61%)\n        Marginal\n565 (15%)\n        Low\n503 (14%)\n        Very Low\n385 (10%)\n        (# NA)\n231\n    PHQ9_CAT: Depression Screen Category\n\n        minimal\n2,748 (74%)\n        mild\n621 (17%)\n        moderate\n220 (5.9%)\n        moderately severe\n91 (2.4%)\n        severe\n38 (1.0%)\n        (# NA)\n213\n    DEPRDIFF: Difficulty with Depression?\n\n        Not at all\n3,039 (82%)\n        Somewhat\n541 (15%)\n        Very\n93 (2.5%)\n        Extremely\n43 (1.2%)\n        (# NA)\n215\n    SNORE: How often do you snore?\n\n        Never\n855 (23%)\n        Rarely\n959 (26%)\n        Occasionally\n700 (19%)\n        Frequently\n1,198 (32%)\n        (# NA)\n219\n    DENTAL: Recommendation for Dental Care?\n\n        See dentist urgently\n234 (6.0%)\n        See dentist soon\n1,671 (43%)\n        Regular Routine\n2,026 (52%)\n    SUNSCR: Use sunscreen on very sunny day?\n\n        Always\n351 (9.0%)\n        Most of the time\n485 (12%)\n        Sometimes\n831 (21%)\n        Rarely\n662 (17%)\n        Never\n1,583 (40%)\n        (# NA)\n19\n  \n  \n  \n    \n      1 n (%)"
  },
  {
    "objectID": "02-nh432cb.html#detailed-numerical-description-for-nh432",
    "href": "02-nh432cb.html#detailed-numerical-description-for-nh432",
    "title": "2  Codebook for nh432",
    "section": "2.5 Detailed Numerical Description for nh432",
    "text": "2.5 Detailed Numerical Description for nh432\n\ndescribe(nh432)\n\nnh432 \n\n 55  Variables      3931  Observations\n--------------------------------------------------------------------------------\nSEQN \n       n  missing distinct \n    3931        0     3931 \n\nlowest : 109271 109273 109284 109291 109292, highest: 124807 124810 124813 124815 124818\n--------------------------------------------------------------------------------\nAGE \n       n  missing distinct     Info     Mean      Gmd      .05      .10 \n    3931        0       30    0.999    44.79    10.09       31       33 \n     .25      .50      .75      .90      .95 \n      37       45       53       57       58 \n\nlowest : 30 31 32 33 34, highest: 55 56 57 58 59\n--------------------------------------------------------------------------------\nRACEETH \n       n  missing distinct \n    3931        0        5 \n\nlowest : Non-H White Non-H Black Hispanic    Non-H Asian Other Race \nhighest: Non-H White Non-H Black Hispanic    Non-H Asian Other Race \n                                                                      \nValue      Non-H White Non-H Black    Hispanic Non-H Asian  Other Race\nFrequency         1192        1049         903         588         199\nProportion       0.303       0.267       0.230       0.150       0.051\n--------------------------------------------------------------------------------\nEDUC \n       n  missing distinct \n    3930        1        5 \n\nlowest : Less than 9th Grade 9th - 11th Grade    High School Grad    Some College / AA   College Grad       \nhighest: Less than 9th Grade 9th - 11th Grade    High School Grad    Some College / AA   College Grad       \n                                                                      \nValue      Less than 9th Grade    9th - 11th Grade    High School Grad\nFrequency                  272                 424                 850\nProportion               0.069               0.108               0.216\n                                                  \nValue        Some College / AA        College Grad\nFrequency                 1287                1097\nProportion               0.327               0.279\n--------------------------------------------------------------------------------\nSEX \n       n  missing distinct \n    3931        0        2 \n                        \nValue      Female   Male\nFrequency    2094   1837\nProportion  0.533  0.467\n--------------------------------------------------------------------------------\nINSURNOW \n       n  missing distinct     Info      Sum     Mean      Gmd \n    3921       10        2    0.465     3169   0.8082   0.3101 \n\n--------------------------------------------------------------------------------\nNOINSUR \n       n  missing distinct     Info      Sum     Mean      Gmd \n    3915       16        2    0.586     1041   0.2659   0.3905 \n\n--------------------------------------------------------------------------------\nSROH \n       n  missing distinct \n    3931        0        5 \n\nlowest : Excellent Very Good Good      Fair      Poor     \nhighest: Excellent Very Good Good      Fair      Poor     \n                                                            \nValue      Excellent Very Good      Good      Fair      Poor\nFrequency        495      1071      1462       765       138\nProportion     0.126     0.272     0.372     0.195     0.035\n--------------------------------------------------------------------------------\nWEIGHT \n       n  missing distinct     Info     Mean      Gmd      .05      .10 \n    3903       28      969        1    86.31    26.59    54.20    58.82 \n     .25      .50      .75      .90      .95 \n   69.30    82.10    99.10   119.30   131.49 \n\nlowest :  36.9  39.4  39.6  39.8  39.9, highest: 204.4 204.6 210.8 242.6 254.3\n--------------------------------------------------------------------------------\nHEIGHT \n       n  missing distinct     Info     Mean      Gmd      .05      .10 \n    3901       30      484        1    167.4    11.45    152.0    154.8 \n     .25      .50      .75      .90      .95 \n   160.0    166.9    174.7    180.8    184.6 \n\nlowest : 135.3 138.3 139.7 141.4 141.9, highest: 195.8 195.9 196.6 198.3 198.7\n--------------------------------------------------------------------------------\nWAIST \n       n  missing distinct     Info     Mean      Gmd      .05      .10 \n    3782      149      781        1    101.5    19.68     75.9     80.4 \n     .25      .50      .75      .90      .95 \n    89.1     99.2    111.7    125.4    134.5 \n\nlowest :  57.9  62.7  63.2  64.5  64.9, highest: 166.0 167.1 170.8 173.1 178.0\n--------------------------------------------------------------------------------\nSBP \n       n  missing distinct     Info     Mean      Gmd      .05      .10 \n    3585      346      116        1    121.5    18.61       98      102 \n     .25      .50      .75      .90      .95 \n     110      120      131      143      152 \n\nlowest :  69  72  77  79  80, highest: 199 200 211 219 222\n--------------------------------------------------------------------------------\nDBP \n       n  missing distinct     Info     Mean      Gmd      .05      .10 \n    3585      346       81    0.999    77.03    13.01     59.2     63.0 \n     .25      .50      .75      .90      .95 \n    69.0     76.0     84.0     92.0     97.0 \n\nlowest :  31  44  45  46  47, highest: 121 122 126 127 136\n--------------------------------------------------------------------------------\nPULSE1 \n       n  missing distinct     Info     Mean      Gmd      .05      .10 \n    3316      615       76    0.999     70.3    12.92       53       57 \n     .25      .50      .75      .90      .95 \n      62       69       77       86       91 \n\nlowest :  38  40  41  42  44, highest: 114 115 120 121 126\n--------------------------------------------------------------------------------\nPULSE2 \n       n  missing distinct     Info     Mean      Gmd      .05      .10 \n    3314      617       80    0.999    70.96    12.92       54       57 \n     .25      .50      .75      .90      .95 \n      63       70       78       86       91 \n\nlowest :  37  39  40  41  42, highest: 117 118 119 120 121\n--------------------------------------------------------------------------------\nWBC \n       n  missing distinct     Info     Mean      Gmd      .05      .10 \n    3755      176      136        1    7.254    2.387      4.3      4.8 \n     .25      .50      .75      .90      .95 \n     5.7      6.9      8.4     10.1     11.3 \n\nlowest :  2.3  2.5  2.6  2.7  2.8, highest: 17.2 17.4 17.6 20.6 22.8\n--------------------------------------------------------------------------------\nPLATELET \n       n  missing distinct     Info     Mean      Gmd      .05      .10 \n    3755      176      372        1    253.3    72.04      159      179 \n     .25      .50      .75      .90      .95 \n     210      246      290      337      371 \n\nlowest :  47  48  54  57  61, highest: 583 602 638 662 818\n--------------------------------------------------------------------------------\nHSCRP \n       n  missing distinct     Info     Mean      Gmd      .05      .10 \n    3664      267     1065        1    4.326    5.271    0.350    0.470 \n     .25      .50      .75      .90      .95 \n   0.890    2.090    4.740    9.217   13.630 \n\nlowest :   0.11   0.16   0.17   0.18   0.19, highest: 102.94 104.48 109.81 138.81 182.82\n--------------------------------------------------------------------------------\nDR_LOSE \n       n  missing distinct     Info      Sum     Mean      Gmd \n    3930        1        2    0.633     1189   0.3025   0.4221 \n\n--------------------------------------------------------------------------------\nDR_EXER \n       n  missing distinct     Info      Sum     Mean      Gmd \n    3930        1        2    0.734     1680   0.4275   0.4896 \n\n--------------------------------------------------------------------------------\nNOW_LOSE \n       n  missing distinct     Info      Sum     Mean      Gmd \n    3929        2        2    0.697     2485   0.6325    0.465 \n\n--------------------------------------------------------------------------------\nNOW_EXER \n       n  missing distinct     Info      Sum     Mean      Gmd \n    3930        1        2    0.718     2369   0.6028    0.479 \n\n--------------------------------------------------------------------------------\nESTHT \n       n  missing distinct     Info     Mean      Gmd      .05      .10 \n    3836       95       29    0.995    66.46    4.722       60       61 \n     .25      .50      .75      .90      .95 \n      63       66       69       72       74 \n\nlowest : 50 53 54 55 56, highest: 76 77 78 79 81\n--------------------------------------------------------------------------------\nESTWT \n       n  missing distinct     Info     Mean      Gmd      .05      .10 \n    3863       68      255        1    188.1    56.51    120.0    130.0 \n     .25      .50      .75      .90      .95 \n   150.0    180.0    216.0    258.0    281.9 \n\nlowest :  86  88  90  93  95, highest: 416 434 450 457 578\n--------------------------------------------------------------------------------\nWTGOAL \n       n  missing distinct \n    3928        3        3 \n                            \nValue       More  Same  Less\nFrequency    289   974  2665\nProportion 0.074 0.248 0.678\n--------------------------------------------------------------------------------\nDIETQUAL \n       n  missing distinct \n    3930        1        5 \n\nlowest : Excellent Very Good Good      Fair      Poor     \nhighest: Excellent Very Good Good      Fair      Poor     \n                                                            \nValue      Excellent Very Good      Good      Fair      Poor\nFrequency        260       758      1519      1082       311\nProportion     0.066     0.193     0.387     0.275     0.079\n--------------------------------------------------------------------------------\nFOODSEC \n       n  missing distinct \n    3700      231        4 \n                                              \nValue          Full Marginal      Low Very Low\nFrequency      2247      565      503      385\nProportion    0.607    0.153    0.136    0.104\n--------------------------------------------------------------------------------\nWORK_V \n       n  missing distinct     Info      Sum     Mean      Gmd \n    3927        4        2     0.61     1116   0.2842    0.407 \n\n--------------------------------------------------------------------------------\nVIGWK_D \n       n  missing distinct     Info     Mean      Gmd \n    3926        5        8    0.632     1.22    1.904 \n\nlowest : 0 1 2 3 4, highest: 3 4 5 6 7\n                                                          \nValue          0     1     2     3     4     5     6     7\nFrequency   2811    79   127   184   112   352   132   129\nProportion 0.716 0.020 0.032 0.047 0.029 0.090 0.034 0.033\n--------------------------------------------------------------------------------\nREC_V \n       n  missing distinct     Info      Sum     Mean      Gmd \n    3931        0        2    0.589     1056   0.2686    0.393 \n\n--------------------------------------------------------------------------------\nVIGREC_D \n       n  missing distinct     Info     Mean      Gmd \n    3930        1        8    0.608   0.8952    1.436 \n\nlowest : 0 1 2 3 4, highest: 3 4 5 6 7\n                                                          \nValue          0     1     2     3     4     5     6     7\nFrequency   2875   126   211   301   166   154    46    51\nProportion 0.732 0.032 0.054 0.077 0.042 0.039 0.012 0.013\n--------------------------------------------------------------------------------\nSEDATE \n       n  missing distinct     Info     Mean      Gmd      .05      .10 \n    3907       24       44     0.99    332.7    232.2       60      120 \n     .25      .50      .75      .90      .95 \n     180      300      480      600      720 \n\nlowest :    2    3    5    8    9, highest:  960 1020 1080 1200 1320\n--------------------------------------------------------------------------------\nPHQ9 \n       n  missing distinct     Info     Mean      Gmd      .05      .10 \n    3718      213       27    0.958    3.324    4.201        0        0 \n     .25      .50      .75      .90      .95 \n       0        2        5        9       13 \n\nlowest :  0  1  2  3  4, highest: 22 23 24 25 26\n--------------------------------------------------------------------------------\nPHQ9_CAT \n       n  missing distinct \n    3718      213        5 \n\nlowest : minimal           mild              moderate          moderately severe severe           \nhighest: minimal           mild              moderate          moderately severe severe           \n                                                                \nValue                minimal              mild          moderate\nFrequency               2748               621               220\nProportion             0.739             0.167             0.059\n                                              \nValue      moderately severe            severe\nFrequency                 91                38\nProportion             0.024             0.010\n--------------------------------------------------------------------------------\nDEPRDIFF \n       n  missing distinct \n    3716      215        4 \n                                                      \nValue      Not at all   Somewhat       Very  Extremely\nFrequency        3039        541         93         43\nProportion      0.818      0.146      0.025      0.012\n--------------------------------------------------------------------------------\nMENTALH \n       n  missing distinct     Info      Sum     Mean      Gmd \n    3929        2        2    0.319      475   0.1209   0.2126 \n\n--------------------------------------------------------------------------------\nSLPWKDAY \n       n  missing distinct     Info     Mean      Gmd      .05      .10 \n    3897       34       22    0.984    7.359    1.735      5.0      5.5 \n     .25      .50      .75      .90      .95 \n     6.5      7.5      8.0      9.0     10.0 \n\nlowest :  2.0  3.0  3.5  4.0  4.5, highest: 11.0 11.5 12.0 13.0 14.0\n--------------------------------------------------------------------------------\nSLPWKEND \n       n  missing distinct     Info     Mean      Gmd      .05      .10 \n    3897       34       24    0.983    8.231    1.928        5        6 \n     .25      .50      .75      .90      .95 \n       7        8        9       10       11 \n\nlowest :  2.0  3.0  3.5  4.0  4.5, highest: 12.0 12.5 13.0 13.5 14.0\n--------------------------------------------------------------------------------\nSLPTROUB \n       n  missing distinct     Info      Sum     Mean      Gmd \n    3928        3        2    0.624     1159   0.2951   0.4161 \n\n--------------------------------------------------------------------------------\nSNORE \n       n  missing distinct \n    3712      219        4 \n                                                              \nValue             Never       Rarely Occasionally   Frequently\nFrequency           855          959          700         1198\nProportion        0.230        0.258        0.189        0.323\n--------------------------------------------------------------------------------\nHOSPITAL \n       n  missing distinct     Info      Sum     Mean      Gmd \n    3931        0        2    0.239      343  0.08726   0.1593 \n\n--------------------------------------------------------------------------------\nEVERALC \n       n  missing distinct     Info      Sum     Mean      Gmd \n    3726      205        2    0.244     3393   0.9106   0.1628 \n\n--------------------------------------------------------------------------------\nDRINKS \n       n  missing distinct     Info     Mean      Gmd      .05      .10 \n    3142      789       15    0.948    2.345    2.051        0        0 \n     .25      .50      .75      .90      .95 \n       1        2        3        5        6 \n\nlowest :  0  1  2  3  4, highest: 10 11 12 13 15\n                                                                            \nValue          0     1     2     3     4     5     6     7     8     9    10\nFrequency    333   912   903   412   228   123   105    21    37     4    19\nProportion 0.106 0.290 0.287 0.131 0.073 0.039 0.033 0.007 0.012 0.001 0.006\n                                  \nValue         11    12    13    15\nFrequency      1    26     2    16\nProportion 0.000 0.008 0.001 0.005\n--------------------------------------------------------------------------------\nCIG100 \n       n  missing distinct     Info      Sum     Mean      Gmd \n    3930        1        2    0.721     1576    0.401   0.4805 \n\n--------------------------------------------------------------------------------\nSMOKE30 \n       n  missing distinct     Info     Mean      Gmd      .05      .10 \n    3205      726       26    0.594    6.808     10.5        0        0 \n     .25      .50      .75      .90      .95 \n       0        0        5       30       30 \n\nlowest :  0  1  2  3  4, highest: 26 27 28 29 30\n--------------------------------------------------------------------------------\nAWAYWORK \n       n  missing distinct     Info      Sum     Mean      Gmd \n    3931        0        2    0.656     2663   0.6774   0.4371 \n\n--------------------------------------------------------------------------------\nAWAYREST \n       n  missing distinct     Info      Sum     Mean      Gmd \n    3929        2        2     0.73     2283   0.5811    0.487 \n\n--------------------------------------------------------------------------------\nAWAYBAR \n       n  missing distinct     Info      Sum     Mean      Gmd \n    3931        0        2    0.391      605   0.1539   0.2605 \n\n--------------------------------------------------------------------------------\nDENTAL \n       n  missing distinct \n    3931        0        3 \n                                                                         \nValue      See dentist urgently     See dentist soon      Regular Routine\nFrequency                   234                 1671                 2026\nProportion                0.060                0.425                0.515\n--------------------------------------------------------------------------------\nFLOSS \n       n  missing distinct     Info     Mean      Gmd \n    3927        4        8    0.934    3.476    3.248 \n\nlowest : 0 1 2 3 4, highest: 3 4 5 6 7\n                                                          \nValue          0     1     2     3     4     5     6     7\nFrequency   1104   288   395   329   230   179    44  1358\nProportion 0.281 0.073 0.101 0.084 0.059 0.046 0.011 0.346\n--------------------------------------------------------------------------------\nEVERPREG \n       n  missing distinct     Info      Sum     Mean      Gmd \n    1959     1972        2     0.29     1747   0.8918   0.1931 \n\n--------------------------------------------------------------------------------\nPREGS \n       n  missing distinct     Info     Mean      Gmd      .05      .10 \n    1956     1975       12    0.973    3.046     2.24        0        0 \n     .25      .50      .75      .90      .95 \n       2        3        4        6        7 \n\nlowest :  0  1  2  3  4, highest:  7  8  9 10 11\n                                                                            \nValue          0     1     2     3     4     5     6     7     8     9    10\nFrequency    212   205   421   420   297   191    95    58    22    10     9\nProportion 0.108 0.105 0.215 0.215 0.152 0.098 0.049 0.030 0.011 0.005 0.005\n                \nValue         11\nFrequency     16\nProportion 0.008\n--------------------------------------------------------------------------------\nSUNSCR \n       n  missing distinct \n    3912       19        5 \n\nlowest : Always           Most of the time Sometimes        Rarely           Never           \nhighest: Always           Most of the time Sometimes        Rarely           Never           \n                                                                              \nValue                Always Most of the time        Sometimes           Rarely\nFrequency               351              485              831              662\nProportion            0.090            0.124            0.212            0.169\n                           \nValue                 Never\nFrequency              1583\nProportion            0.405\n--------------------------------------------------------------------------------\nWTINTPRP \n       n  missing distinct     Info     Mean      Gmd      .05      .10 \n    3931        0     3677        1    28434    27437     5911     7199 \n     .25      .50      .75      .90      .95 \n   10615    17358    31476    65098    94422 \n\nlowest :   2467.054   2779.464   2833.287   2917.413   2967.271\nhighest: 246249.502 248091.496 264719.137 282883.648 311265.152\n--------------------------------------------------------------------------------\nWTMECPRP \n       n  missing distinct     Info     Mean      Gmd      .05      .10 \n    3931        0     3701        1    30353    29409     6217     7634 \n     .25      .50      .75      .90      .95 \n   11365    18422    33155    68569   102038 \n\nlowest :   2589.175   2782.738   3003.518   3009.532   3016.643\nhighest: 267064.352 268878.570 273958.374 308014.509 321573.519\n--------------------------------------------------------------------------------"
  },
  {
    "objectID": "02-nh432cb.html#missingness-in-nh432",
    "href": "02-nh432cb.html#missingness-in-nh432",
    "title": "2  Codebook for nh432",
    "section": "2.6 Missingness in nh432",
    "text": "2.6 Missingness in nh432\n\nmiss_case_table(nh432) |> gt()\n\n\n\n\n\n  \n  \n    \n      n_miss_in_case\n      n_cases\n      pct_cases\n    \n  \n  \n    0\n907\n23.0730094\n    1\n533\n13.5588909\n    2\n1030\n26.2019842\n    3\n591\n15.0343424\n    4\n307\n7.8097176\n    5\n161\n4.0956500\n    6\n87\n2.2131773\n    7\n106\n2.6965149\n    8\n68\n1.7298397\n    9\n18\n0.4578988\n    10\n20\n0.5087764\n    11\n39\n0.9921140\n    12\n27\n0.6868481\n    13\n14\n0.3561435\n    14\n9\n0.2289494\n    15\n14\n0.3561435\n  \n  \n  \n\n\n\n\n\ngg_miss_var(nh432)\n\n\n\n\n\nmiss_var_summary(nh432) |> gt()\n\n\n\n\n\n  \n  \n    \n      variable\n      n_miss\n      pct_miss\n    \n  \n  \n    PREGS\n1975\n50.24166879\n    EVERPREG\n1972\n50.16535233\n    DRINKS\n789\n20.07122869\n    SMOKE30\n726\n18.46858306\n    PULSE2\n617\n15.69575172\n    PULSE1\n615\n15.64487408\n    SBP\n346\n8.80183160\n    DBP\n346\n8.80183160\n    HSCRP\n267\n6.79216484\n    FOODSEC\n231\n5.87636734\n    SNORE\n219\n5.57110150\n    DEPRDIFF\n215\n5.46934622\n    PHQ9\n213\n5.41846858\n    PHQ9_CAT\n213\n5.41846858\n    EVERALC\n205\n5.21495803\n    WBC\n176\n4.47723226\n    PLATELET\n176\n4.47723226\n    WAIST\n149\n3.79038413\n    ESTHT\n95\n2.41668787\n    ESTWT\n68\n1.72983974\n    SLPWKDAY\n34\n0.86491987\n    SLPWKEND\n34\n0.86491987\n    HEIGHT\n30\n0.76316459\n    WEIGHT\n28\n0.71228695\n    SEDATE\n24\n0.61053167\n    SUNSCR\n19\n0.48333757\n    NOINSUR\n16\n0.40702111\n    INSURNOW\n10\n0.25438820\n    VIGWK_D\n5\n0.12719410\n    WORK_V\n4\n0.10175528\n    FLOSS\n4\n0.10175528\n    WTGOAL\n3\n0.07631646\n    SLPTROUB\n3\n0.07631646\n    NOW_LOSE\n2\n0.05087764\n    MENTALH\n2\n0.05087764\n    AWAYREST\n2\n0.05087764\n    EDUC\n1\n0.02543882\n    DR_LOSE\n1\n0.02543882\n    DR_EXER\n1\n0.02543882\n    NOW_EXER\n1\n0.02543882\n    DIETQUAL\n1\n0.02543882\n    VIGREC_D\n1\n0.02543882\n    CIG100\n1\n0.02543882\n    SEQN\n0\n0.00000000\n    AGE\n0\n0.00000000\n    RACEETH\n0\n0.00000000\n    SEX\n0\n0.00000000\n    SROH\n0\n0.00000000\n    REC_V\n0\n0.00000000\n    HOSPITAL\n0\n0.00000000\n    AWAYWORK\n0\n0.00000000\n    AWAYBAR\n0\n0.00000000\n    DENTAL\n0\n0.00000000\n    WTINTPRP\n0\n0.00000000\n    WTMECPRP\n0\n0.00000000\n  \n  \n  \n\n\n\n\n\nmiss_var_table(nh432) |> gt()\n\n\n\n\n\n  \n  \n    \n      n_miss_in_var\n      n_vars\n      pct_vars\n    \n  \n  \n    0\n12\n21.818182\n    1\n7\n12.727273\n    2\n3\n5.454545\n    3\n2\n3.636364\n    4\n2\n3.636364\n    5\n1\n1.818182\n    10\n1\n1.818182\n    16\n1\n1.818182\n    19\n1\n1.818182\n    24\n1\n1.818182\n    28\n1\n1.818182\n    30\n1\n1.818182\n    34\n2\n3.636364\n    68\n1\n1.818182\n    95\n1\n1.818182\n    149\n1\n1.818182\n    176\n2\n3.636364\n    205\n1\n1.818182\n    213\n2\n3.636364\n    215\n1\n1.818182\n    219\n1\n1.818182\n    231\n1\n1.818182\n    267\n1\n1.818182\n    346\n2\n3.636364\n    615\n1\n1.818182\n    617\n1\n1.818182\n    726\n1\n1.818182\n    789\n1\n1.818182\n    1972\n1\n1.818182\n    1975\n1\n1.818182"
  },
  {
    "objectID": "03-431review1.html#r-setup",
    "href": "03-431review1.html#r-setup",
    "title": "3  431 Review: Comparing Means",
    "section": "3.1 R Setup",
    "text": "3.1 R Setup\n\nknitr::opts_chunk$set(comment = NA)\n\nlibrary(janitor) \nlibrary(broom)\nlibrary(car)\nlibrary(glue)\nlibrary(gt)\nlibrary(gtsummary)\nlibrary(lmboot)\nlibrary(MKinfer)\nlibrary(mosaic)\nlibrary(naniar)\nlibrary(patchwork)\nlibrary(rstatix)\n\nlibrary(tidyverse) \n\ntheme_set(theme_bw())"
  },
  {
    "objectID": "03-431review1.html#data-ingest",
    "href": "03-431review1.html#data-ingest",
    "title": "3  431 Review: Comparing Means",
    "section": "3.2 Data Ingest",
    "text": "3.2 Data Ingest\nSince we’ve already got the nh432 file formatted as an R data set, we’ll use that.\n\nnh432 <- read_rds(\"data/nh432.Rds\")"
  },
  {
    "objectID": "03-431review1.html#testing-or-summarizing-normality",
    "href": "03-431review1.html#testing-or-summarizing-normality",
    "title": "3  431 Review: Comparing Means",
    "section": "3.3 Testing or Summarizing Normality?",
    "text": "3.3 Testing or Summarizing Normality?\nAs we’ll see, the two most useful strategies for dealing with problematic non-Normality when comparing means are (1) transformation of the outcome to make the assumption of Normality more tenable, and (2) alternate inference approaches (for example, using a bootstrap or rank-based procedure instead of a t test.)\nWhile it is possible to obtain numerical summaries of deviations from Normality, perhaps a measure of skewness (asymmetry) or kurtosis (heavy-tailed behavior), in practical work, I never use such summaries to overrule my assessment of the plots. It’s critical instead to focus on the pictures of a distribution, most especially Normal Q-Q plots.\nPerhaps the simplest skewness summary is \\(skew_1\\) = (mean-median)/(standard deviation), where values below -0.2 are meant to indicate (meaningful) left skew, and values above +0.2 indicate (meaningful) right skew. Unfortunately, this approach works poorly with many distributions (for example, multimodal distributions) and so do many other (more sophisticated) measures1.\nIt is also possible to develop hypothesis tests of whether a particular batch of data follows a Normal distribution, for example, the Kolmogorov-Smirnov test2, or the Shapiro-Wilk test3, but again, I find these to be without value in practical work and cannot recommend their use."
  },
  {
    "objectID": "03-431review1.html#comparing-two-means-using-paired-samples",
    "href": "03-431review1.html#comparing-two-means-using-paired-samples",
    "title": "3  431 Review: Comparing Means",
    "section": "3.4 Comparing Two Means using Paired Samples",
    "text": "3.4 Comparing Two Means using Paired Samples\nNow, we’ll demonstrate some approaches to comparing two means coming from paired samples. This will include:\n\na paired t test (one-sample t test on the paired differences), which we can obtain from a linear model, or from t.test()\n\nThese procedures based on the t distribution for paired samples require that the distribution of the sample paired differences is well-approximated by a Normal model. As an alternative without that requirement, we’ll focus primarily on a bootstrap comparison (not assuming Normality) from boot.t.test(), which comes from the MKinfer package. It is also possible to generate rank-based inference, such as using the Wilcoxon signed rank approach, but this introduces the major weakness of not estimating the population mean (or even the population median.)\nWe’ll assume a Missing Completely at Random (MCAR) mechanism for missing data, so that a complete case analysis makes sense, and we’ll also use functions from the broom package to tidy our output, and from the gt package to help present it in an attractive table."
  },
  {
    "objectID": "03-431review1.html#comparing-pulse1-to-pulse2",
    "href": "03-431review1.html#comparing-pulse1-to-pulse2",
    "title": "3  431 Review: Comparing Means",
    "section": "3.5 Comparing PULSE1 to PULSE2",
    "text": "3.5 Comparing PULSE1 to PULSE2\nWe have two measurements of pulse rate (in beats per minute) in nh432 for each participant. Let’s compare the two for all participants with two PULSE readings. Since we have a value of PULSE1 and PULSE2 for each participant, it makes sense to treat these as paired samples, and study the paired differences in pulse rate.\n\ndat1 <- nh432 |> select(SEQN, PULSE1, PULSE2) |>\n  drop_na() |>\n  mutate(PULSEDIFF = PULSE2 - PULSE1)\n\nsummary(dat1 |> select(-SEQN))\n\n     PULSE1          PULSE2         PULSEDIFF       \n Min.   : 38.0   Min.   : 37.00   Min.   :-22.0000  \n 1st Qu.: 62.0   1st Qu.: 63.00   1st Qu.: -1.0000  \n Median : 69.0   Median : 70.00   Median :  1.0000  \n Mean   : 70.3   Mean   : 70.96   Mean   :  0.6533  \n 3rd Qu.: 77.0   3rd Qu.: 78.00   3rd Qu.:  2.0000  \n Max.   :126.0   Max.   :121.00   Max.   : 26.0000  \n\ndf_stats(~ PULSE1 + PULSE2 + PULSEDIFF, data = dat1) |> \n  mutate(across(.cols = -c(response, n, missing), num, digits = 2)) |>\n  gt()\n\n\n\n\n\n  \n  \n    \n      response\n      min\n      Q1\n      median\n      Q3\n      max\n      mean\n      sd\n      n\n      missing\n    \n  \n  \n    PULSE1\n38.00\n62.00\n69.00\n77.00\n126.00\n70.30\n11.61\n3314\n0\n    PULSE2\n37.00\n63.00\n70.00\n78.00\n121.00\n70.96\n11.57\n3314\n0\n    PULSEDIFF\n-22.00\n-1.00\n1.00\n2.00\n26.00\n0.65\n3.43\n3314\n0\n  \n  \n  \n\n\n\n\n\n3.5.1 Distribution of Paired Differences\n\np1 <- ggplot(dat1, aes(sample = PULSEDIFF)) +\n  geom_qq(col = \"dodgerblue\") + geom_qq_line(col = \"navy\") + \n  theme(aspect.ratio = 1) + \n  labs(title = \"Normal Q-Q plot\")\n\np2 <- ggplot(dat1, aes(x = PULSEDIFF)) +\n  geom_histogram(aes(y = after_stat(density)), \n                 bins = 25, fill = \"dodgerblue\", col = \"white\") +\n  stat_function(fun = dnorm, \n                args = list(mean = mean(dat1$PULSEDIFF), \n                            sd = sd(dat1$PULSEDIFF)),\n                col = \"navy\", linewidth = 1.5) +\n  labs(title = \"Histogram and Normal Density\")\n\np3 <- ggplot(dat1, aes(x = PULSEDIFF, y = \"\")) +\n  geom_boxplot(fill = \"dodgerblue\", outlier.color = \"dodgerblue\") + \n  stat_summary(fun = \"mean\", geom = \"point\", \n               shape = 23, size = 3, fill = \"white\") +\n  labs(title = \"Boxplot with mean\", y = \"\")\n\np1 + (p2 / p3 + plot_layout(heights = c(4,1))) +\n  plot_annotation(title = \"Pulse 2 - Pulse 1 difference in nh432\",\n                  subtitle = glue(nrow(dat1), \" NHANES Participants ages 30-59 in nh432 data\"))\n\n\n\n\nThese data appear to come from a distribution that is essentially symmetric, but extremely heavy-tailed, with many outlier candidates on both the low and high end of the distribution. It seems unwise to assume a Normal distribution for these differences in pulse rate.\n\n\n3.5.2 Using t.test to obtain a 90% CI for the mean pulse difference\nNote that I use 90% as my confidence level here, mostly to make sure that we don’t always simply default to 95% without engaging our brains.\n\ntt1 <- t.test(dat1$PULSEDIFF)\n\ntt1\n\n\n    One Sample t-test\n\ndata:  dat1$PULSEDIFF\nt = 10.98, df = 3313, p-value < 2.2e-16\nalternative hypothesis: true mean is not equal to 0\n95 percent confidence interval:\n 0.5366358 0.7699424\nsample estimates:\nmean of x \n0.6532891 \n\ntidy(tt1, conf.int = TRUE, conf = 0.90) |>\n  select(estimate, low.90 = conf.low, hi.90 = conf.high, \n         \"Paired t\" = statistic, p.value, method) |>\n  mutate(across(.cols = -method, num, digits = 3)) |>\n  gt() |>\n  tab_header(title = \"Pulse 2 - Pulse 1 Difference Estimate with 90% T-based CI\",\n             subtitle = glue(nrow(dat1), \" NHANES Participants ages 30-59 in nh432 data\"))\n\n\n\n\n\n  \n    \n      Pulse 2 - Pulse 1 Difference Estimate with 90% T-based CI\n    \n    \n      3314 NHANES Participants ages 30-59 in nh432 data\n    \n  \n  \n    \n      estimate\n      low.90\n      hi.90\n      Paired t\n      p.value\n      method\n    \n  \n  \n    0.653\n0.537\n0.770\n10.980\n0.000\nOne Sample t-test\n  \n  \n  \n\n\n\n\n\n\n3.5.3 Using linear regression to obtain a 90% CI for the mean pulse difference\nA linear regression model predicting the paired differences with an intercept alone produces the same result as the paired t test.\n\nlm1 <- lm(PULSEDIFF ~ 1, data = dat1)\n\nsummary(lm1)\n\n\nCall:\nlm(formula = PULSEDIFF ~ 1, data = dat1)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-22.6533  -1.6533   0.3467   1.3467  25.3467 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)   0.6533     0.0595   10.98   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.425 on 3313 degrees of freedom\n\ntidy(lm1, conf.int = TRUE, conf = 0.90) |>\n  mutate(method = c(\"Linear Model\")) |>\n  select(estimate, low.90 = conf.low, hi.90 = conf.high, \n         \"Paired t\" = statistic, p.value, method) |>\n  mutate(across(.cols = -method, num, digits = 3)) |>\n  gt() |>\n  tab_header(title = \"Pulse 2 - Pulse 1 Difference Estimate with 90% T-based CI\",\n             subtitle = glue(nrow(dat1), \" NHANES Participants ages 30-59 in nh432 data\"))\n\n\n\n\n\n  \n    \n      Pulse 2 - Pulse 1 Difference Estimate with 90% T-based CI\n    \n    \n      3314 NHANES Participants ages 30-59 in nh432 data\n    \n  \n  \n    \n      estimate\n      low.90\n      hi.90\n      Paired t\n      p.value\n      method\n    \n  \n  \n    0.653\n0.555\n0.751\n10.980\n0.000\nLinear Model\n  \n  \n  \n\n\n\n\n\n\n3.5.4 Using the bootstrap to obtain a 90% CI for the mean pulse difference\nThis is a better choice than the t test if the distribution of the paired differences veer far away from a Normal distribution, but you are still interested in making inferences about the population mean. This is a different approach to obtaining a bootstrap than I have used in the past, but I prefer it because it works well with the tidy() function in the broom package.\n\nset.seed(4321)\nbs1 <- boot.t.test(dat1$PULSEDIFF, conf.level = 0.90, \n                           boot = TRUE, R = 999)\n\nbs1\n\n\n    Bootstrap One Sample t-test\n\ndata:  dat1$PULSEDIFF\nbootstrap p-value < 2.2e-16 \nbootstrap mean of x (SE) = 0.6554904 (0.05954107) \n90 percent bootstrap percentile confidence interval:\n 0.5554617 0.7526554\n\nResults without bootstrap:\nt = 10.98, df = 3313, p-value < 2.2e-16\nalternative hypothesis: true mean is not equal to 0\n90 percent confidence interval:\n 0.5553989 0.7511792\nsample estimates:\nmean of x \n0.6532891 \n\ntidy(bs1, conf.int = TRUE, conf = 0.90) |>\n  select(estimate, low.90 = conf.low, hi.90 = conf.high, \n         \"Bootstrap t\" = statistic, p.value, method) |>\n  mutate(across(.cols = -method, num, digits = 3)) |>\n  gt() |>\n  tab_header(title = \"Pulse 2 - Pulse 1 Difference Estimate with 90% Bootstrap CI\",\n             subtitle = glue(nrow(dat1), \" NHANES Participants ages 30-59 in nh432 data\"))\n\n\n\n\n\n  \n    \n      Pulse 2 - Pulse 1 Difference Estimate with 90% Bootstrap CI\n    \n    \n      3314 NHANES Participants ages 30-59 in nh432 data\n    \n  \n  \n    \n      estimate\n      low.90\n      hi.90\n      Bootstrap t\n      p.value\n      method\n    \n  \n  \n    0.653\n0.555\n0.751\n10.980\n0.000\nBootstrap One Sample t-test\n  \n  \n  \n\n\n\n\nGiven our large sample size, it is perhaps not overly surprising that even a small difference in mean pulse rate (0.653 beats per minute) turns out to have a 90% confidence interval well above the value (0) that would occur if there were no difference at all between the groups.\n\n\n3.5.5 Wilcoxon signed rank approach to comparing pulse rates\nWe can obtain a 90% confidence interval for the pseudo-median of our paired differences in pulse rate with the Wilcoxon signed rank approach.\n\nwt1 <- wilcox.test(dat1$PULSEDIFF, conf.int = TRUE, conf.level = 0.90)\n\nwt1\n\n\n    Wilcoxon signed rank test with continuity correction\n\ndata:  dat1$PULSEDIFF\nV = 2449203, p-value < 2.2e-16\nalternative hypothesis: true location is not equal to 0\n90 percent confidence interval:\n 0.5000466 0.9999290\nsample estimates:\n(pseudo)median \n     0.9999809 \n\n\nBut this is of limited value, even though it doesn’t assume Normality of the distribution of paired differences, because the summary statistic is a pseudo-median4, which isn’t straightforward to interpret, unless the true distribution of the paired differences is symmetric, in which case the pseudo-median and the median have the same value.\nLet’s consider another example using two paired samples to compare means, this time with a somewhat smaller sample size."
  },
  {
    "objectID": "03-431review1.html#comparing-weight-to-estwt",
    "href": "03-431review1.html#comparing-weight-to-estwt",
    "title": "3  431 Review: Comparing Means",
    "section": "3.6 Comparing WEIGHT to ESTWT",
    "text": "3.6 Comparing WEIGHT to ESTWT\nWe have two assessments of each participant’s weight in nh432: their WEIGHT (as measured using a scale, in kilograms) and their ESTWT (self-reported weight via questionnaire, in pounds.) First, let’s create a data set containing those values, and converting pounds to kilograms for the ESTWT results so that we can compare the two assessments fairly. To shrink the sample size a bit, let’s only look at people whose age is 43, and who describe their overall health as either Good or Fair.\n\ndat2 <- nh432 |> select(SEQN, AGE, SROH, WEIGHT, ESTWT) |>\n  filter(AGE == 43, SROH %in% c(\"Good\", \"Fair\")) |>\n  drop_na() |>\n  mutate(ESTWTKG = ESTWT*0.45359,\n         WTDIFF = WEIGHT - ESTWTKG)\n\nglimpse(dat2)\n\nRows: 70\nColumns: 7\n$ SEQN    <chr> \"109342\", \"109602\", \"109805\", \"110286\", \"110645\", \"111149\", \"1…\n$ AGE     <dbl> 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43…\n$ SROH    <fct> Good, Good, Good, Fair, Good, Good, Good, Good, Good, Fair, Go…\n$ WEIGHT  <dbl> 92.1, 76.5, 133.0, 86.8, 119.3, 74.1, 75.8, 106.8, 102.1, 77.0…\n$ ESTWT   <dbl> 200, 167, 260, 198, 230, 145, 167, 240, 223, 172, 150, 265, 22…\n$ ESTWTKG <dbl> 90.71800, 75.74953, 117.93340, 89.81082, 104.32570, 65.77055, …\n$ WTDIFF  <dbl> 1.38200, 0.75047, 15.06660, -3.01082, 14.97430, 8.32945, 0.050…\n\ndf_stats(~ WEIGHT + ESTWTKG + WTDIFF, data = dat2) |> \n  mutate(across(.cols = -c(response, n, missing), num, digits = 3)) |>\n  gt()\n\n\n\n\n\n  \n  \n    \n      response\n      min\n      Q1\n      median\n      Q3\n      max\n      mean\n      sd\n      n\n      missing\n    \n  \n  \n    WEIGHT\n36.900\n74.525\n89.400\n106.175\n204.600\n93.040\n29.211\n70\n0\n    ESTWTKG\n45.359\n74.842\n89.130\n103.759\n204.115\n92.532\n27.869\n70\n0\n    WTDIFF\n-9.871\n-2.256\n-0.028\n1.923\n15.067\n0.508\n4.671\n70\n0\n  \n  \n  \n\n\n\n\n\n3.6.1 Plotting The Paired Difference in Weight\n\np1 <- ggplot(dat2, aes(sample = WTDIFF)) +\n  geom_qq(col = \"seagreen\") + geom_qq_line(col = \"deeppink\") + \n  theme(aspect.ratio = 1) + \n  labs(title = \"Normal Q-Q plot\")\n\np2 <- ggplot(dat2, aes(x = WTDIFF)) +\n  geom_histogram(aes(y = after_stat(density)), \n                   bins = 15, fill = \"seagreen\", col = \"white\") +\n  stat_function(fun = dnorm, \n                args = list(mean = mean(dat2$WTDIFF), \n                            sd = sd(dat2$WTDIFF)),\n                col = \"deeppink\", linewidth = 1.5) +\n  labs(title = \"Histogram and Normal Density\")\n\np3 <- ggplot(dat2, aes(x = WTDIFF, y = \"\")) +\n  geom_boxplot(fill = \"seagreen\", outlier.color = \"seagreen\") + \n  stat_summary(fun = \"mean\", geom = \"point\", \n               shape = 23, size = 3, fill = \"white\") +\n  labs(title = \"Boxplot with mean\", y = \"\")\n\np1 + (p2 / p3 + plot_layout(heights = c(4,1))) +\n  plot_annotation(title = \"Measured - Self-reported Weight (in kilograms)\",\n                  subtitle = glue(nrow(dat2), \" participants in Good or Fair Health aged 43 in nh432\"))\n\n\n\n\nAs we saw with the differences in pulse rate, the differences in weight for this sample appear to come from a distribution that might be symmetric, but that still has several outlier candidates, especially on the high end of the distribution. We may want to consider whether the assumption of a t-based confidence interval is reasonable here, and whether we might be better off using a bootstrap approach.\n\n\n3.6.2 t.test 90% CI for the mean weight difference\n\ntt2 <- t.test(dat2$WTDIFF)\n\ntidy(tt2, conf.int = TRUE, conf = 0.90) |>\n  select(estimate, low.90 = conf.low, hi.90 = conf.high, \n         \"Paired t\" = statistic, p.value, method) |>\n  mutate(across(.cols = -method, num, digits = 3)) |>\n  gt() |>\n  tab_header(title = \"Measured - Self-reported Weight with 90% T-based CI\",\n             subtitle = glue(nrow(dat2), \" NHANES Participants aged 43 in Good or Fair Health in nh432 data\"))\n\n\n\n\n\n  \n    \n      Measured - Self-reported Weight with 90% T-based CI\n    \n    \n      70 NHANES Participants aged 43 in Good or Fair Health in nh432 data\n    \n  \n  \n    \n      estimate\n      low.90\n      hi.90\n      Paired t\n      p.value\n      method\n    \n  \n  \n    0.508\n-0.606\n1.621\n0.909\n0.366\nOne Sample t-test\n  \n  \n  \n\n\n\n\n\n\n3.6.3 Linear Regression: 90% CI for mean weight difference\n\nlm2 <- lm(WTDIFF ~ 1, data = dat2)\n\ntidy(lm2, conf.int = TRUE, conf = 0.90) |>\n  mutate(method = c(\"Linear Model\")) |>\n  select(estimate, low.90 = conf.low, hi.90 = conf.high, \n         \"Paired t\" = statistic, p.value, method) |>\n  mutate(across(.cols = -method, num, digits = 3)) |>\n  gt() |>\n  tab_header(title = \"Measured - Self-reported Weight with 90% T-based CI\",\n             subtitle = glue(nrow(dat2), \" NHANES Participants aged 43 in Good or Fair Health in nh432 data\"))\n\n\n\n\n\n  \n    \n      Measured - Self-reported Weight with 90% T-based CI\n    \n    \n      70 NHANES Participants aged 43 in Good or Fair Health in nh432 data\n    \n  \n  \n    \n      estimate\n      low.90\n      hi.90\n      Paired t\n      p.value\n      method\n    \n  \n  \n    0.508\n-0.423\n1.438\n0.909\n0.366\nLinear Model\n  \n  \n  \n\n\n\n\n\n\n3.6.4 Bootstrap 90% CI for the mean weight difference\n\nset.seed(4322)\nbs2 <- boot.t.test(dat2$WTDIFF, conf.level = 0.90, \n                           boot = TRUE, R = 999)\n\ntidy(bs2, conf.int = TRUE, conf = 0.90) |>\n  select(estimate, low.90 = conf.low, hi.90 = conf.high, \n         \"Bootstrap t\" = statistic, p.value, method) |>\n  mutate(across(.cols = -method, num, digits = 3)) |>\n  gt() |>\n  tab_header(title = \"Measured - Self-reported Weight\",\n             subtitle = \"with 90% Bootstrap CI\") |>\n  tab_footnote(footnote = glue(nrow(dat1), \" NHANES Participants age 43 in Good or Fair Health in nh432 data\"))\n\n\n\n\n\n  \n    \n      Measured - Self-reported Weight\n    \n    \n      with 90% Bootstrap CI\n    \n  \n  \n    \n      estimate\n      low.90\n      hi.90\n      Bootstrap t\n      p.value\n      method\n    \n  \n  \n    0.508\n-0.423\n1.438\n0.909\n0.366\nBootstrap One Sample t-test\n  \n  \n  \n    \n       3314 NHANES Participants age 43 in Good or Fair Health in nh432 data\n    \n  \n\n\n\n\nIn light of the clear issue with outliers in the plots of the weight differences, I think I would choose the bootstrap confidence interval, which clearly includes both negative and positive values as plausible estimates of the population mean difference.\n\n\n3.6.5 Wilcoxon signed rank approach to comparing weight estimates\nWe can obtain a 90% confidence interval for the pseudo-median of our paired differences in weight with the Wilcoxon signed rank approach.\n\nwt2 <- wilcox.test(dat2$WTDIFF, conf.int = TRUE, conf.level = 0.90)\n\nwt2\n\n\n    Wilcoxon signed rank test with continuity correction\n\ndata:  dat2$WTDIFF\nV = 1262, p-value = 0.9115\nalternative hypothesis: true location is not equal to 0\n90 percent confidence interval:\n -0.6347756  0.7941699\nsample estimates:\n(pseudo)median \n    0.06155979 \n\n\nBut this is of limited value, even though it doesn’t assume Normality of the distribution of paired differences, because the summary statistic is a pseudo-median5, which isn’t straightforward to interpret, unless the true distribution of the paired differences is symmetric, in which case the pseudo-median and the median have the same value."
  },
  {
    "objectID": "03-431review1.html#comparing-two-means-using-independent-samples",
    "href": "03-431review1.html#comparing-two-means-using-independent-samples",
    "title": "3  431 Review: Comparing Means",
    "section": "3.7 Comparing Two Means using Independent Samples",
    "text": "3.7 Comparing Two Means using Independent Samples\nNow, we’ll demonstrate some approaches to comparing two means coming from independent samples. This will include:\n\na pooled t test (t test assuming equal population variances), which we can obtain from a linear model, or from t.test()\na Welch t test (t test not assuming equal population variances), from t.test()\n\nEach of these t tests requires the distribution of each of our two independent samples to be well-approximated by a Normal model. As an alternative without that requirement, we’ll focus on a bootstrap comparison (not assuming equal variances or Normality) from boot.t.test() (again from the MKinfer package.) Once more, it is also possible to generate rank-based inference, such as using the Wilcoxon-Mann-Whitney rank sum approach, but again this does not provide us with estimates of either the difference in population means or medians, which limits its utility."
  },
  {
    "objectID": "03-431review1.html#comparing-white-blood-cell-count-by-hospitalization-status",
    "href": "03-431review1.html#comparing-white-blood-cell-count-by-hospitalization-status",
    "title": "3  431 Review: Comparing Means",
    "section": "3.8 Comparing White Blood Cell Count by Hospitalization Status",
    "text": "3.8 Comparing White Blood Cell Count by Hospitalization Status\nNow, we’ll use independent samples to compare subjects who were hospitalized in the past year to those who were not, in terms of their white blood cell count. The normal range of WBCs in the blood is 4.5 to 11 on the scale (1000 cells per microliter) our data is available.\n\n3.8.1 Exploring the Data\n\ndat3 <- nh432 |>\n  select(SEQN, HOSPITAL, WBC) |>\n  drop_na()\n\nggplot(dat3, aes(x = factor(HOSPITAL), y = WBC)) +\n  geom_violin(aes(fill = factor(HOSPITAL))) +\n  geom_boxplot(width = 0.3, notch = TRUE) +\n  stat_summary(aes(fill = factor(HOSPITAL)), fun = \"mean\", geom = \"point\", \n               shape = 23, size = 3) +\n  guides(fill = \"none\", col = \"none\") +\n  scale_fill_viridis_d(option = \"cividis\", alpha = 0.3) +\n  coord_flip() +\n  labs(x = \"Hospitalized in Past Year? (0 - No, 1 = Yes)\",\n       y = \"White blood cell count (1000 cells / uL)\",\n       title = \"White Blood Cell Count by Hospitalization Status\",\n       subtitle = glue(nrow(dat3), \" NHANES participants in nh432\"))\n\n\n\n\nEach of these distributions shows some signs of right skew, or at least more than a few outlier candidates on the upper end of the white blood cell count’s distribution, according to the boxplot. A pair of Normal Q-Q plots should help clarify issues for us.\n\nggplot(dat3, aes(sample = WBC)) +\n  geom_qq(aes(col = factor(HOSPITAL))) + geom_qq_line(col = \"red\") + \n  facet_wrap(~ HOSPITAL, labeller = \"label_both\") +\n  guides(col = \"none\") +\n  scale_color_viridis_d(option = \"cividis\", end = 0.5) +\n  labs(x = \"Expectation Under Standard Normal Distribution\",\n       y = \"Observed White Blood Cell Count (1000 cells/uL)\",\n       title = \"Normal Q-Q plots of White Blood Cell Count\",\n       subtitle = \"By Hospitalization Status in the Past Year\",\n       caption = glue(nrow(dat3), \" NHANES participants in nh432\"))\n\n\n\n\nIt seems unreasonable to assume that each of these samples comes from a distribution that is well-approximated by the Normal. There’s just too much skew here. Here are some key numerical summaries of the data in each sample.\n\nfavstats(WBC ~ HOSPITAL, data = dat3) |>\n  mutate(across(.cols = c(mean, sd), num, digits = 3)) |>\n  gt()\n\n\n\n\n\n  \n  \n    \n      HOSPITAL\n      min\n      Q1\n      median\n      Q3\n      max\n      mean\n      sd\n      n\n      missing\n    \n  \n  \n    0\n2.3\n5.7\n7.0\n8.4\n22.8\n7.246\n2.170\n3430\n0\n    1\n2.5\n5.6\n6.9\n8.7\n17.1\n7.332\n2.392\n325\n0\n  \n  \n  \n\n\n\n\n\n\n3.8.2 Pooled t test (assumes equal variances) via linear model\nThe pooled t test for comparison of two population means using independent samples assumes:\n\nthat the WBC (outcome) in each of the two HOSPITAL (exposure) groups follows a Normal distribution, and\nthat the population variances are equal in the two groups\n\nThe “equal population variances” assumption can be relaxed and a pooled t test used if we have a balanced design, with the same number of subjects in each exposure group.\nIn our setting, we shouldn’t be particularly comfortable with the assumption of Normality, as mentioned above. Were we able to get past that, though, we can see that the two distributions have fairly similar sample variances (remember this is just the square of the standard deviation.) The sample sizes are wildly different, with many more non-hospitalized subjects than hospitalized ones.\nFor completeness, though, we’ll start by running the pooled t test.\n\nlm3 <- lm(WBC ~ HOSPITAL, data = dat3)\n\nsummary(lm3)\n\n\nCall:\nlm(formula = WBC ~ HOSPITAL, data = dat3)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-4.9463 -1.5463 -0.3463  1.1537 15.5537 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  7.24633    0.03740 193.760   <2e-16 ***\nHOSPITAL     0.08567    0.12712   0.674      0.5    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.19 on 3753 degrees of freedom\nMultiple R-squared:  0.000121,  Adjusted R-squared:  -0.0001454 \nF-statistic: 0.4542 on 1 and 3753 DF,  p-value: 0.5004\n\nconfint(lm3, level = 0.90)\n\n                   5 %      95 %\n(Intercept)  7.1847964 7.3078567\nHOSPITAL    -0.1234734 0.2948204\n\ntidy(lm3, conf.int = TRUE, conf.level = 0.90) |>\n  filter(term == \"HOSPITAL\") |>\n  mutate(method = \"Pooled t\") |>\n  select(estimate, low.90 = conf.low, hi.90 = conf.high, \n         \"Pooled t\" = statistic, p.value, method) |>\n  mutate(across(.cols = -method, num, digits = 3)) |>\n  gt() |>\n  tab_header(title = \"White Blood Cell Count (Hospitalized - Non-Hospitalized)\",\n             subtitle = \"with 90% Pooled t-based CI via Linear Model\") |>\n  tab_footnote(footnote = glue(nrow(dat3), \" NHANES Participants ages 30-59 in nh432 data\"))\n\n\n\n\n\n  \n    \n      White Blood Cell Count (Hospitalized - Non-Hospitalized)\n    \n    \n      with 90% Pooled t-based CI via Linear Model\n    \n  \n  \n    \n      estimate\n      low.90\n      hi.90\n      Pooled t\n      p.value\n      method\n    \n  \n  \n    0.086\n-0.123\n0.295\n0.674\n0.500\nPooled t\n  \n  \n  \n    \n       3755 NHANES Participants ages 30-59 in nh432 data\n    \n  \n\n\n\n\n\n\n3.8.3 Pooled t test (assumes equal variances) via t.test\nNote that this approach estimates the difference with Not Hospitalized - Hospitalized, as opposed to the approach used in the linear model. Be careful to check the sample estimates provided in your output against the original summary of the sample data to avoid making a mistake.\n\ntt3p <- t.test(WBC ~ HOSPITAL, data = dat3, var.equal = TRUE)\n\ntt3p\n\n\n    Two Sample t-test\n\ndata:  WBC by HOSPITAL\nt = -0.67395, df = 3753, p-value = 0.5004\nalternative hypothesis: true difference in means between group 0 and group 1 is not equal to 0\n95 percent confidence interval:\n -0.3349062  0.1635593\nsample estimates:\nmean in group 0 mean in group 1 \n       7.246327        7.332000 \n\ntidy(tt3p, conf.int = TRUE, conf.level = 0.90) |>\n  select(estimate, low.90 = conf.low, hi.90 = conf.high, \n         \"Pooled t\" = statistic, p.value, method) |>\n  mutate(across(.cols = -method, num, digits = 3)) |>\n  gt() |>\n  tab_header(title = \"White Blood Cell Count (Non-Hospitalized minus Hospitalized)\",\n             subtitle = \"with 90% Pooled t-based Confidence Interval\") |>\n  tab_footnote(footnote = glue(nrow(dat3), \" NHANES Participants ages 30-59 in nh432 data\"))\n\n\n\n\n\n  \n    \n      White Blood Cell Count (Non-Hospitalized minus Hospitalized)\n    \n    \n      with 90% Pooled t-based Confidence Interval\n    \n  \n  \n    \n      estimate\n      low.90\n      hi.90\n      Pooled t\n      p.value\n      method\n    \n  \n  \n    -0.086\n-0.335\n0.164\n-0.674\n0.500\nTwo Sample t-test\n  \n  \n  \n    \n       3755 NHANES Participants ages 30-59 in nh432 data\n    \n  \n\n\n\n\nAgain, note that the t.test() approach estimates Non-Hospitalized - Hospitalized (so that the sample mean is negative.)\n\n\n3.8.4 Welch t test (doesn’t assume equal variance) via t.test\nThe Welch t test (which is actually the default t.test in R) assumes that the two groups each follow a Normal distribution, but does not require that those distributions have the same population variance.\n\ntt3w <- t.test(WBC ~ HOSPITAL, data = dat3)\n\ntt3w\n\n\n    Welch Two Sample t-test\n\ndata:  WBC by HOSPITAL\nt = -0.6218, df = 376.28, p-value = 0.5345\nalternative hypothesis: true difference in means between group 0 and group 1 is not equal to 0\n95 percent confidence interval:\n -0.3565964  0.1852494\nsample estimates:\nmean in group 0 mean in group 1 \n       7.246327        7.332000 \n\ntidy(tt3w, conf.int = TRUE, conf.level = 0.90) |>\n  select(estimate, low.90 = conf.low, hi.90 = conf.high, \n         \"Welch t\" = statistic, p.value, method) |>\n  mutate(across(.cols = -method, num, digits = 3)) |>\n  gt() |>\n  tab_header(title = \"White Blood Cell Count (Non-Hospitalized minus Hospitalized)\",\n             subtitle = \"with 90% Welch t-based Confidence Interval\") |>\n  tab_footnote(footnote = glue(nrow(dat3), \" NHANES Participants ages 30-59 in nh432 data\"))\n\n\n\n\n\n  \n    \n      White Blood Cell Count (Non-Hospitalized minus Hospitalized)\n    \n    \n      with 90% Welch t-based Confidence Interval\n    \n  \n  \n    \n      estimate\n      low.90\n      hi.90\n      Welch t\n      p.value\n      method\n    \n  \n  \n    -0.086\n-0.357\n0.185\n-0.622\n0.534\nWelch Two Sample t-test\n  \n  \n  \n    \n       3755 NHANES Participants ages 30-59 in nh432 data\n    \n  \n\n\n\n\n\n\n3.8.5 Bootstrap comparison of WBC by HOSPITAL\nThe bootstrap approach is appealing in part because it neither assumes Normality or equal population variances.\n\nset.seed(4323)\nbs3 <- boot.t.test(WBC ~ HOSPITAL, data = dat3, \n                   R = 999, conf.level = 0.90)\n\nbs3\n\n\n    Bootstrap Welch Two Sample t-test\n\ndata:  WBC by HOSPITAL\nbootstrap p-value = 0.5325 \nbootstrap difference of means (SE) = -0.08734076 (0.1376246) \n90 percent bootstrap percentile confidence interval:\n -0.3137606  0.1414317\n\nResults without bootstrap:\nt = -0.6218, df = 376.28, p-value = 0.5345\nalternative hypothesis: true difference in means is not equal to 0\n90 percent confidence interval:\n -0.3128672  0.1415202\nsample estimates:\nmean in group 0 mean in group 1 \n       7.246327        7.332000 \n\ntidy(bs3, conf.int = TRUE, conf.level = 0.90) |>\n  mutate(diff = estimate1 - estimate2) |>\n  select(est1 = estimate1, est2 = estimate2, diff, \n         low.90 = conf.low, hi.90 = conf.high, \n         p.value, method) |>\n  mutate(across(.cols = -method, num, digits = 3)) |>\n  gt() |>\n  tab_header(title = \"White Blood Cell Count (Non-Hospitalized minus Hospitalized)\",\n             subtitle = \"with 90% Bootstrap Confidence Interval\") |>\n  tab_footnote(footnote = glue(nrow(dat3), \" NHANES Participants ages 30-59 in nh432 data\"))\n\n\n\n\n\n  \n    \n      White Blood Cell Count (Non-Hospitalized minus Hospitalized)\n    \n    \n      with 90% Bootstrap Confidence Interval\n    \n  \n  \n    \n      est1\n      est2\n      diff\n      low.90\n      hi.90\n      p.value\n      method\n    \n  \n  \n    7.246\n7.332\n-0.086\n-0.313\n0.142\n0.534\nBootstrap Welch Two Sample t-test\n  \n  \n  \n    \n       3755 NHANES Participants ages 30-59 in nh432 data\n    \n  \n\n\n\n\nIn any case, though, we come to the same basic conclusion - both positive and negative differences in WBC count are plausible.\nGiven the huge imbalance between the two groups in terms of sample size, and the apparent skew in the distribution of each sample, I would probably be most comfortable with the bootstrap approach here than the t-based intervals.\n\n\n3.8.6 Transforming the WBC Counts\nSince the White Blood Cell counts are far from Normally distributed, and in fact appear to be substantially skewed (asymmetric) we might want to consider a transformation of the data. The Box-Cox approach can be used to suggest potential transformations even in a simple case like this. We can use the boxCox() function from the car package, for example.\n\nm3 <- lm(WBC ~ HOSPITAL, data = dat3)\n\nboxCox(m3)\n\n\n\n\nThe estimated power (\\(\\lambda\\)) shown in the plot is close to 0. The ladder of power transformations looks like this:\n\n\n\n$\nTransformation\nFormula\n\n\n\n\n-2\ninverse square\n\\(1/y^2\\)\n\n\n-1\ninverse\n\\(1/y\\)\n\n\n-0.5\ninverse square root\n\\(1/\\sqrt{y}\\)\n\n\n0\nlogarithm\n\\(log y\\)\n\n\n0.5\nsquare root\n\\(\\sqrt{y}\\)\n\n\n1\nno transformation\ny\n\n\n2\nsquare\n\\(y^2\\)\n\n\n3\ncube\n\\(y^3\\)\n\n\n\nSo in this case, the Box-Cox approach is suggesting we try the logarithm (we use the natural logarithm, with base e, here) of WBC.\nLet’s redraw our Normal Q-Q plots with this transformation applied.\n\nggplot(dat3, aes(sample = log(WBC))) +\n  geom_qq(aes(col = factor(HOSPITAL))) + geom_qq_line(col = \"red\") + \n  facet_wrap(~ HOSPITAL, labeller = \"label_both\") +\n  guides(col = \"none\") +\n  scale_color_viridis_d(option = \"cividis\", end = 0.5) +\n  labs(x = \"Expectation Under Standard Normal Distribution\",\n       y = \"Log of White Blood Cell Count (1000 cells/uL)\",\n       title = \"Normal Q-Q plots of log(WBC)\",\n       subtitle = \"By Hospitalization Status in the Past Year\",\n       caption = glue(nrow(dat3), \" NHANES participants in nh432\"))\n\n\n\n\nThe assumption of Normality now looks much more plausible for each of our samples. So we might try building a 90% confidence interval for the mean of log(WBC), as follows:\n\nfavstats(log(WBC) ~ HOSPITAL, data = dat3) |>\n  mutate(across(.cols = -c(HOSPITAL, n, missing), num, digits = 3)) |>\n  gt() |>\n  tab_header(title = \"log(WBC) by Hospitalization Status\",\n             subtitle = \"NHANES participants in nh432\")\n\n\n\n\n\n  \n    \n      log(WBC) by Hospitalization Status\n    \n    \n      NHANES participants in nh432\n    \n  \n  \n    \n      HOSPITAL\n      min\n      Q1\n      median\n      Q3\n      max\n      mean\n      sd\n      n\n      missing\n    \n  \n  \n    0\n0.833\n1.740\n1.946\n2.128\n3.127\n1.938\n0.293\n3430\n0\n    1\n0.916\n1.723\n1.932\n2.163\n2.839\n1.941\n0.322\n325\n0\n  \n  \n  \n\n\n\n\nWe see that there’s essentially no difference at all in the means of the log(WBC) values across the two levels of hospitalization status.\n\ntt3log <- t.test(log(WBC) ~ HOSPITAL, data = dat3, var.equal = TRUE)\n\ntidy(tt3log, conf.int = TRUE, conf.level = 0.90) |>\n  select(estimate1, estimate2, estimate, conf.low, conf.high, p.value)\n\n# A tibble: 1 × 6\n  estimate1 estimate2 estimate conf.low conf.high p.value\n      <dbl>     <dbl>    <dbl>    <dbl>     <dbl>   <dbl>\n1      1.94      1.94 -0.00320  -0.0369    0.0305   0.852\n\n\nLet’s consider a second example for comparing means from independent samples."
  },
  {
    "objectID": "03-431review1.html#comparing-waist-circumference-by-sleep-trouble",
    "href": "03-431review1.html#comparing-waist-circumference-by-sleep-trouble",
    "title": "3  431 Review: Comparing Means",
    "section": "3.9 Comparing Waist Circumference by Sleep Trouble",
    "text": "3.9 Comparing Waist Circumference by Sleep Trouble\nNow, we’ll restrict ourselves to NHANES participants who rated their overall health as “Fair”, and we’ll compare the mean waist circumference (WAIST, in cm) of people in that group who responded Yes (vs. No) to the question of whether they had told a doctor that they had trouble sleeping (gathered in the SLPTROUB variable.)\n\n3.9.1 Summarizing the Data\n\ndat4 <- nh432 |>\n  select(SEQN, SROH, SLPTROUB, WAIST) |>\n  filter(SROH == \"Fair\") |>\n  drop_na()\n\nggplot(dat4, aes(x = factor(SLPTROUB), y = WAIST)) +\n  geom_violin(aes(fill = factor(SLPTROUB))) +\n  geom_boxplot(width = 0.3, notch = TRUE) +\n  stat_summary(fill = \"red\", fun = \"mean\", geom = \"point\", \n               shape = 23, size = 3) +\n  guides(fill = \"none\", col = \"none\") +\n  scale_fill_brewer(palette = \"Accent\") +\n  labs(x = \"Reported Sleep Trouble to a Doctor? (0 - No, 1 = Yes)\",\n       y = \"Waist circumference (cm)\",\n       title = \"Waist Circumference by Sleep Trouble\",\n       subtitle = glue(nrow(dat4), \" NHANES participants in Fair health in nh432\"))\n\n\n\n\n\nggplot(dat4, aes(sample = WAIST)) +\n  geom_qq(aes(col = factor(SLPTROUB))) + geom_qq_line(col = \"red\") + \n  facet_wrap(~ SLPTROUB, labeller = \"label_both\") +\n  guides(col = \"none\") +\n  scale_color_brewer(palette = \"Accent\") +\n  labs(x = \"Expectation Under Standard Normal Distribution\",\n       y = \"Observed Waist Circumference (in cm)\",\n       title = \"Normal Q-Q plots of Waist Circumference\",\n       subtitle = \"By Reported Sleep Trouble\",\n       caption = glue(nrow(dat4), \" NHANES participants in Fair health in nh432\"))\n\n\n\n\nHere’s a situation where we might be willing to consider a t test, since a Normal distribution is a much better fit for the data in each of our two samples. Let’s look at some brief numerical summaries, too.\n\nfavstats(WAIST ~ SLPTROUB, data = dat4) |>\n  mutate(across(.cols = c(mean, sd), num, digits = 2)) |>\n  gt()\n\n\n\n\n\n  \n  \n    \n      SLPTROUB\n      min\n      Q1\n      median\n      Q3\n      max\n      mean\n      sd\n      n\n      missing\n    \n  \n  \n    0\n65.8\n91.7\n102.1\n117.5\n178\n104.71\n18.35\n425\n0\n    1\n69.6\n97.4\n109.4\n124.0\n166\n110.69\n18.86\n309\n0\n  \n  \n  \n\n\n\n\n\n\n3.9.2 Pooled t test (assumes equal variances) via linear model\nHere’s the pooled t test via linear model.\n\nlm4 <- lm(WAIST ~ SLPTROUB, data = dat4)\n\nsummary(lm4)\n\n\nCall:\nlm(formula = WAIST ~ SLPTROUB, data = dat4)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-41.093 -13.293  -2.293  12.790  73.290 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 104.7099     0.9006  116.27  < 2e-16 ***\nSLPTROUB      5.9830     1.3881    4.31 1.85e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 18.57 on 732 degrees of freedom\nMultiple R-squared:  0.02475,   Adjusted R-squared:  0.02342 \nF-statistic: 18.58 on 1 and 732 DF,  p-value: 1.853e-05\n\nconfint(lm4, level = 0.90)\n\n                   5 %       95 %\n(Intercept) 103.226621 106.193144\nSLPTROUB      3.696944   8.269052\n\ntidy(lm4, conf.int = TRUE, conf.level = 0.90) |>\n  filter(term == \"SLPTROUB\") |>\n  mutate(method = \"Pooled t\") |>\n  select(estimate, low.90 = conf.low, hi.90 = conf.high, \n         \"Pooled t\" = statistic, p.value, method) |>\n  mutate(across(.cols = -method, num, digits = 2)) |>\n  gt() |>\n  tab_header(title = \"Waist Circumference by Sleep Trouble\",\n             subtitle = \"with 90% Pooled t-based CI via Linear Model\") |>\n  tab_footnote(footnote = glue(nrow(dat4), \" NHANES Participants in Fair Health in nh432 data\"))\n\n\n\n\n\n  \n    \n      Waist Circumference by Sleep Trouble\n    \n    \n      with 90% Pooled t-based CI via Linear Model\n    \n  \n  \n    \n      estimate\n      low.90\n      hi.90\n      Pooled t\n      p.value\n      method\n    \n  \n  \n    5.98\n3.70\n8.27\n4.31\n0.00\nPooled t\n  \n  \n  \n    \n       734 NHANES Participants in Fair Health in nh432 data\n    \n  \n\n\n\n\n\n\n3.9.3 Pooled t test (assumes equal variances) via t.test\n\ntt4p <- t.test(WAIST ~ SLPTROUB, data = dat4, var.equal = TRUE)\n\ntt4p\n\n\n    Two Sample t-test\n\ndata:  WAIST by SLPTROUB\nt = -4.3103, df = 732, p-value = 1.853e-05\nalternative hypothesis: true difference in means between group 0 and group 1 is not equal to 0\n95 percent confidence interval:\n -8.708058 -3.257938\nsample estimates:\nmean in group 0 mean in group 1 \n       104.7099        110.6929 \n\ntidy(tt4p, conf.int = TRUE, conf.level = 0.90) |>\n  select(estimate, low.90 = conf.low, hi.90 = conf.high, \n         \"Pooled t\" = statistic, p.value, method) |>\n  mutate(across(.cols = -method, num, digits = 2)) |>\n  gt() |>\n  tab_header(title = \"Waist Circumference by Sleep Trouble\",\n             subtitle = \"with 90% Pooled t-based Confidence Interval\") |>\n  tab_footnote(footnote = glue(nrow(dat4), \" NHANES Participants in Fair Health in nh432 data\"))\n\n\n\n\n\n  \n    \n      Waist Circumference by Sleep Trouble\n    \n    \n      with 90% Pooled t-based Confidence Interval\n    \n  \n  \n    \n      estimate\n      low.90\n      hi.90\n      Pooled t\n      p.value\n      method\n    \n  \n  \n    -5.98\n-8.71\n-3.26\n-4.31\n0.00\nTwo Sample t-test\n  \n  \n  \n    \n       734 NHANES Participants in Fair Health in nh432 data\n    \n  \n\n\n\n\n\n\n3.9.4 Welch t test (doesn’t assume equal variance) via t.test\n\ntt4w <- t.test(WAIST ~ SLPTROUB, data = dat4)\n\ntt4w\n\n\n    Welch Two Sample t-test\n\ndata:  WAIST by SLPTROUB\nt = -4.2919, df = 653.24, p-value = 2.04e-05\nalternative hypothesis: true difference in means between group 0 and group 1 is not equal to 0\n95 percent confidence interval:\n -8.720327 -3.245669\nsample estimates:\nmean in group 0 mean in group 1 \n       104.7099        110.6929 \n\ntidy(tt4w, conf.int = TRUE, conf.level = 0.90) |>\n  select(estimate, low.90 = conf.low, hi.90 = conf.high, \n         \"Welch t\" = statistic, p.value, method) |>\n  mutate(across(.cols = -method, num, digits = 2)) |>\n  gt() |>\n  tab_header(title = \"Waist Circumference by Sleep Trouble\",\n             subtitle = \"with 90% Welch t-based Confidence Interval\") |>\n  tab_footnote(footnote = glue(nrow(dat4), \" NHANES Participants in Fair Health in nh432 data\"))\n\n\n\n\n\n  \n    \n      Waist Circumference by Sleep Trouble\n    \n    \n      with 90% Welch t-based Confidence Interval\n    \n  \n  \n    \n      estimate\n      low.90\n      hi.90\n      Welch t\n      p.value\n      method\n    \n  \n  \n    -5.98\n-8.72\n-3.25\n-4.29\n0.00\nWelch Two Sample t-test\n  \n  \n  \n    \n       734 NHANES Participants in Fair Health in nh432 data\n    \n  \n\n\n\n\n\n\n3.9.5 Bootstrap comparison of WAIST by SLPTROUB\n\nset.seed(4324)\nbs4 <- boot.t.test(WAIST ~ SLPTROUB, data = dat4, \n                   R = 999, conf.level = 0.90)\n\nbs4\n\n\n    Bootstrap Welch Two Sample t-test\n\ndata:  WAIST by SLPTROUB\nbootstrap p-value < 2.2e-16 \nbootstrap difference of means (SE) = -5.991814 (1.389937) \n90 percent bootstrap percentile confidence interval:\n -8.207833 -3.747965\n\nResults without bootstrap:\nt = -4.2919, df = 653.24, p-value = 2.04e-05\nalternative hypothesis: true difference in means is not equal to 0\n90 percent confidence interval:\n -8.279237 -3.686759\nsample estimates:\nmean in group 0 mean in group 1 \n       104.7099        110.6929 \n\ntidy(bs4, conf.int = TRUE, conf.level = 0.90) |>\n  mutate(diff = estimate1 - estimate2) |>\n  select(est1 = estimate1, est2 = estimate2, diff, \n         low.90 = conf.low, hi.90 = conf.high, \n         p.value, method) |>\n  mutate(across(.cols = -method, num, digits = 2)) |>\n  gt() |>\n  tab_header(title = \"Waist Circumference by Sleep Trouble\",\n             subtitle = \"with 90% Bootstrap Confidence Interval\") |>\n  tab_footnote(footnote = glue(nrow(dat4), \" NHANES Participants in Fair Health in nh432 data\"))\n\n\n\n\n\n  \n    \n      Waist Circumference by Sleep Trouble\n    \n    \n      with 90% Bootstrap Confidence Interval\n    \n  \n  \n    \n      est1\n      est2\n      diff\n      low.90\n      hi.90\n      p.value\n      method\n    \n  \n  \n    104.71\n110.69\n-5.98\n-8.28\n-3.69\n0.00\nBootstrap Welch Two Sample t-test\n  \n  \n  \n    \n       734 NHANES Participants in Fair Health in nh432 data\n    \n  \n\n\n\n\n\n\n3.9.6 Wilcoxon-Mann-Whitney Rank Sum Approach\nThe Wilcoxon-Mann-Whitney rank sum approach also allows us (like the bootstrap) to avoid the assumptions of Normality and equal population variances, but at the cost of no longer yielding direct inference about the population mean.\n\nwt4 <- wilcox.test(WAIST ~ SLPTROUB, data = dat4, \n            conf.int = TRUE, conf.level = 0.90, paired = FALSE)\nwt4\n\n\n    Wilcoxon rank sum test with continuity correction\n\ndata:  WAIST by SLPTROUB\nW = 53322, p-value = 1.355e-05\nalternative hypothesis: true location shift is not equal to 0\n90 percent confidence interval:\n -8.599997 -3.900026\nsample estimates:\ndifference in location \n             -6.299963 \n\n\nNote that the estimated “difference in location” here is not the difference in the medians across the two groups, but instead the median of the difference between a sample from the SLPTROUB = Yes group and a sample from the SLPTROUB = No group.\nJust to prove my point, here are the sample median WAIST results in the two SLPTROUB groups. You can see that the difference between these medians does not match the “difference in location” estimate from the Wilcoxon-Mann-Whitney rank sum output.\n\ndat4 |> group_by(SLPTROUB) |> summarise(median(WAIST))\n\n# A tibble: 2 × 2\n  SLPTROUB `median(WAIST)`\n     <dbl>           <dbl>\n1        0            102.\n2        1            109.\n\n\nIn conclusion, the confidence intervals (from any of these approaches) suggest that plausible means of waist circumference are around 3-8 centimeters larger in the “told Dr. about sleep problems” group, which I suppose isn’t especially surprising, at least in terms of its direction."
  },
  {
    "objectID": "03-431review1.html#comparing-3-means-using-independent-samples-systolic-bp-by-weight-goal",
    "href": "03-431review1.html#comparing-3-means-using-independent-samples-systolic-bp-by-weight-goal",
    "title": "3  431 Review: Comparing Means",
    "section": "3.10 Comparing 3 Means using Independent Samples: Systolic BP by Weight Goal",
    "text": "3.10 Comparing 3 Means using Independent Samples: Systolic BP by Weight Goal\nWe’ll compare systolic blood pressure means across the three samples defined by WTGOAL (goal is to weigh more, less or stay about the same), restricting to our participants of Hispanic or Latinx ethnicity in nh432.\n\ndat5 <- nh432 |>\n  select(SEQN, RACEETH, SBP, WTGOAL) |>\n  filter(RACEETH == \"Hispanic\") |>\n  drop_na()\n\n\n3.10.1 Summarizing SBP by WTGOAL\n\nggplot(dat5, aes(x = SBP, y = WTGOAL)) + \n  geom_violin(aes(fill = WTGOAL)) +\n  geom_boxplot(width = 0.3, notch = TRUE) + \n  guides(fill = \"none\") + \n  labs(title = \"Comparing Mean Systolic BP by Weight Goal\",\n       subtitle = glue(\"among \", nrow(dat5), \" Hispanic participants in nh432\"),\n    x = \"Systolic Blood Pressure (mm Hg)\", y = \"Weight Goal\")\n\n\n\n\n\nfavstats(SBP ~ WTGOAL, data = dat5) |> \n  as_tibble() |>\n  mutate(across(.cols = c(\"mean\", \"sd\"), num, digits = 2)) |>\n  gt()\n\n\n\n\n\n  \n  \n    \n      WTGOAL\n      min\n      Q1\n      median\n      Q3\n      max\n      mean\n      sd\n      n\n      missing\n    \n  \n  \n    More\n84\n104.5\n114.0\n123\n150\n114.08\n14.76\n36\n0\n    Same\n87\n107.0\n116.5\n128\n200\n119.26\n16.49\n188\n0\n    Less\n80\n109.0\n119.0\n131\n199\n120.70\n17.17\n564\n0\n  \n  \n  \n\n\n\n\nThe analysis of variance is our primary tool for comparing more than two means (this is the extension of the pooled t test, with similar assumptions.) So the assumptions we might want to think about here are:\n\nSBP in each Weight Goal group is assumed to follow a Normal distribution\nSBP in each Weight Goal group is assumed to have the same population variance\n\nThe ANOVA, however, is far more robust to minor violations of these assumptions than is the pooled t test. So we might go ahead and fit the ANOVA model anyway, despite the apparent right skew in the “Less” group.\n\n\n3.10.2 Fitting an ANOVA Model\n\nm5 <- lm(SBP ~ WTGOAL, data = dat5)\n\nanova(m5)\n\nAnalysis of Variance Table\n\nResponse: SBP\n           Df Sum Sq Mean Sq F value  Pr(>F)  \nWTGOAL      2   1639  819.45  2.8656 0.05754 .\nResiduals 785 224477  285.96                  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nA small p value (remember we are using 90% confidence in our 432 work) like this isn’t really very important - usually it simply steers us towards trying to identify confidence intervals for differences between pairs of SBP means defined by WTGOAL.\n\n3.10.2.1 ANOVA without assuming Equal Variances?\nR will also fit an ANOVA-style model and produce a p value without the assumption of equal population SBP variance across the three groups of WTGOAL.\n\noneway.test(SBP ~ WTGOAL, data = dat5)\n\n\n    One-way analysis of means (not assuming equal variances)\n\ndata:  SBP and WTGOAL\nF = 3.5255, num df = 2.000, denom df = 93.753, p-value = 0.0334\n\n\nI don’t use this approach much, as ANOVA is pretty robust to the assumption of equal variance. The huge differences in sample size in this study (many more participants are in the Less group than the More, for instance) are most of the cause of the difference we see here.\n\n\n3.10.2.2 Testing for Equal Population Variance?\nSome people like to perform tests for equal population variance to help choose between ANOVA and the oneway.test() approach, but I do not. If I’m happy with the assumption of Normality, I virtually always just use ANOVA. There are many such tests of “equal variance”, including:\n\nBartlett’s test\nLevene’s test (which in R comes from the car package)\nFligner-Killeen test\n\nBartlett’s test is the least reliable of these when the data in at least one sample appear to be poorly described by the Normal distribution. Either Levene or Fligner-Killeen is a better choice in that setting, but again, I don’t use any of these in my work.\n\nbartlett.test(SBP ~ WTGOAL, data = dat5)\n\n\n    Bartlett test of homogeneity of variances\n\ndata:  SBP by WTGOAL\nBartlett's K-squared = 1.6722, df = 2, p-value = 0.4334\n\n\n\nleveneTest(SBP ~ WTGOAL, data = dat5)\n\nLevene's Test for Homogeneity of Variance (center = median)\n       Df F value Pr(>F)\ngroup   2  0.6557 0.5193\n      785               \n\n\n\nfligner.test(SBP ~ WTGOAL, data = dat5)\n\n\n    Fligner-Killeen test of homogeneity of variances\n\ndata:  SBP by WTGOAL\nFligner-Killeen:med chi-squared = 1.26, df = 2, p-value = 0.5326\n\n\n\n\n3.10.2.3 Is there a bootstrap one-way ANOVA approach?\nIf all you are looking for is a p value for the ANOVA model, then yes, there is a bootstrap approach available to perform one-way ANOVA testing. But I don’t actually use it, again usually preferring the usual ANOVA if the data seem reasonably likely to have been drawn from a Normal distribution, and the Kruskal-Wallis rank-based test otherwise. If you are willing to install the lmboot package, and use its ANOVA.boot() function, you can do so, like this.\n\nbs5 <- ANOVA.boot(SBP ~ WTGOAL, B = 1000, seed = 4325, data = dat5)\nbs5$`p-value`\n\n[1] 0.052\n\n\nIn this case, it doesn’t seem that we have a wildly different result than we got from the original ANOVA. That is often the case, and I have never actually used ANOVA.boot() in practical work.\n\n\n\n3.10.3 Tukey HSD Pairwise Comparisons\nWhen pairwise comparisons are pre-planned, especially when the design is close to balanced, my favorite choice for generating adjusted inferences about the means is Tukey’s Honestly Significant Differences (HSD) approach.\nHere, we generate confidence intervals for the pairwise differences in the SBP means by WTGOAL group with a 90% family-wise confidence level.\n\nth5 <- TukeyHSD(aov(SBP ~ WTGOAL, data = dat5), \n                ordered = TRUE, conf.level = 0.90)\n\ntidy(th5) |> \n  select(-c(term, null.value)) |>\n  mutate(across(.cols = -contrast, num, digits = 3)) |>\n  arrange(desc(estimate)) |>\n  gt() |>\n  tab_header(title = \"Comparing Mean Systolic BP across pairs of WTGOAL groups\",\n             subtitle = \"90% Tukey HSD Confidence Intervals\") |>\n  tab_footnote(footnote = glue(nrow(dat5), \" Hispanic participants in nh432\"))\n\n\n\n\n\n  \n    \n      Comparing Mean Systolic BP across pairs of WTGOAL groups\n    \n    \n      90% Tukey HSD Confidence Intervals\n    \n  \n  \n    \n      contrast\n      estimate\n      conf.low\n      conf.high\n      adj.p.value\n    \n  \n  \n    Less-More\n6.617\n0.642\n12.592\n0.060\n    Same-More\n5.172\n-1.151\n11.495\n0.213\n    Less-Same\n1.445\n-1.482\n4.372\n0.568\n  \n  \n  \n    \n       788 Hispanic participants in nh432\n    \n  \n\n\n\ntidy(th5) |>\n  mutate(contrast = fct_reorder(contrast, estimate, .desc = TRUE)) %>%\n  ggplot(., aes(x = contrast, y = estimate, ymin = conf.low, ymax = conf.high)) +\n  geom_crossbar() +\n  geom_hline(yintercept = 0, col = \"red\", linetype = \"dashed\") +\n  geom_label(aes(label = round_half_up(estimate, 2))) +\n  labs(title = \"Comparing Mean Systolic BP across pairs of WTGOAL groups\",\n       subtitle = \"90% Tukey HSD Confidence intervals\",\n       caption = glue(nrow(dat5), \" Hispanic participants in nh432\"),\n       x = \"Pairwise Difference between WTGOAL groups\",\n       y = \"Difference in Systolic Blood Pressure (mm Hg)\")\n\n\n\n\nThe main problems here are that:\n\nthe sample sizes in the various levels of WTGOAL are very different from one another, and\nthe SBP data are not especially well-described by a Normal distribution, at least in the “Less” group.\n\n\n\n3.10.4 Holm pairwise comparisons of means\nAnother approach to developing pairwise inferences would be to use either Bonferroni or (my preference) Holm-adjusted p values for the relevant t tests. First, we’ll run the appropriate Holm comparison of means assuming equal population variances of SBP across all three WTGOAL groups.\n\nht5 <- pairwise.t.test(dat5$SBP, dat5$WTGOAL, pool.sd = TRUE, \n                       p.adjust.method = \"holm\")\ntidy(ht5) |>\n  gt()\n\n\n\n\n\n  \n  \n    \n      group1\n      group2\n      p.value\n    \n  \n  \n    Same\nMore\n0.18625400\n    Less\nMore\n0.06929224\n    Less\nSame\n0.31056211\n  \n  \n  \n\n\n\n\nThe results are merely p-values, and not confidence intervals. There’s nothing being estimated here of interest. We can also perform these Holm comparisons without assuming equal population variances, as shown below.\n\nht5un <- pairwise.t.test(dat5$SBP, dat5$WTGOAL, pool.sd = FALSE, \n                       p.adjust.method = \"holm\")\ntidy(ht5un) |>\n  gt()\n\n\n\n\n\n  \n  \n    \n      group1\n      group2\n      p.value\n    \n  \n  \n    Same\nMore\n0.12866665\n    Less\nMore\n0.04046259\n    Less\nSame\n0.30396220\n  \n  \n  \n\n\n\n\nAgain, the problem with this approach is that it’s only producing a p value, which tempts us into talking about useless things like “statistical significance.” This is part of the reason I prefer Tukey HSD approaches when appropriate."
  },
  {
    "objectID": "03-431review1.html#comparing-4-means-using-independent-samples-weight-by-food-security",
    "href": "03-431review1.html#comparing-4-means-using-independent-samples-weight-by-food-security",
    "title": "3  431 Review: Comparing Means",
    "section": "3.11 Comparing 4 Means using Independent Samples: Weight by Food Security",
    "text": "3.11 Comparing 4 Means using Independent Samples: Weight by Food Security\n\ndat6 <- nh432 |>\n  select(SEQN, WEIGHT, FOODSEC) |>\n  drop_na()\n\n\n3.11.1 Summarizing the Data\n\nggplot(dat6, aes(x = FOODSEC, y = WEIGHT)) + \n  geom_violin(aes(fill = FOODSEC)) +\n  geom_boxplot(width = 0.3, notch = TRUE) + \n  guides(fill = \"none\") + \n  scale_fill_viridis_d(option = \"rocket\") +\n  labs(title = \"Comparing Mean Weight by Food Security\",\n       subtitle = glue(\"among \", nrow(dat6), \" participants in nh432\"),\n    x = \"Food Security Category\", y = \"Weight (kg)\")\n\n\n\n\n\nfavstats(WEIGHT ~ FOODSEC, data = dat6) |> \n  as_tibble() |>\n  mutate(across(.cols = -c(\"FOODSEC\", \"n\", \"missing\"), num, digits = 1)) |>\n  gt()\n\n\n\n\n\n  \n  \n    \n      FOODSEC\n      min\n      Q1\n      median\n      Q3\n      max\n      mean\n      sd\n      n\n      missing\n    \n  \n  \n    Full\n36.9\n68.6\n81.0\n97.7\n210.8\n85.2\n24.5\n2233\n0\n    Marginal\n39.9\n69.2\n83.0\n100.2\n242.6\n87.0\n25.0\n560\n0\n    Low\n40.9\n70.4\n83.6\n102.3\n201.0\n88.2\n24.6\n501\n0\n    Very Low\n46.1\n73.4\n85.8\n101.3\n254.3\n90.1\n24.4\n379\n0\n  \n  \n  \n\n\n\n\n\n\n3.11.2 Fitting the ANOVA model\n\nm6 <- lm(WEIGHT ~ FOODSEC, data = dat6)\n\nanova(m6)\n\nAnalysis of Variance Table\n\nResponse: WEIGHT\n            Df  Sum Sq Mean Sq F value    Pr(>F)    \nFOODSEC      3   10021  3340.3   5.525 0.0008786 ***\nResiduals 3669 2218162   604.6                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nDoes the bootstrap ANOVA give a meaningfully different result? No.\n\nbs6 <- ANOVA.boot(WEIGHT ~ FOODSEC, B = 5000, seed = 4326, data = dat6)\nbs6$`p-value`\n\n[1] 8e-04\n\n\n\n\n3.11.3 Tukey HSD Pairwise Comparisons\n\nth6 <- TukeyHSD(aov(WEIGHT ~ FOODSEC, data = dat6), \n                ordered = TRUE, conf.level = 0.90)\n\ntidy(th6) |> \n  select(-c(term, null.value)) |>\n  mutate(across(.cols = -contrast, num, digits = 3)) |>\n  arrange(desc(estimate)) |>\n  gt() |>\n  tab_header(title = \"Comparing Mean Weight across pairs of Food Security groups\",\n             subtitle = \"90% Tukey HSD Confidence Intervals\") |>\n  tab_footnote(footnote = glue(nrow(dat6), \" participants in nh432\"))\n\n\n\n\n\n  \n    \n      Comparing Mean Weight across pairs of Food Security groups\n    \n    \n      90% Tukey HSD Confidence Intervals\n    \n  \n  \n    \n      contrast\n      estimate\n      conf.low\n      conf.high\n      adj.p.value\n    \n  \n  \n    Very Low-Full\n4.877\n1.746\n8.008\n0.002\n    Very Low-Marginal\n3.103\n-0.646\n6.851\n0.229\n    Low-Full\n2.930\n0.144\n5.717\n0.075\n    Very Low-Low\n1.946\n-1.891\n5.783\n0.650\n    Marginal-Full\n1.774\n-0.889\n4.438\n0.422\n    Low-Marginal\n1.156\n-2.310\n4.622\n0.870\n  \n  \n  \n    \n       3673 participants in nh432\n    \n  \n\n\n\ntidy(th6) |> \n  mutate(contrast = fct_reorder(contrast, estimate, .desc = TRUE)) %>%\n  ggplot(., aes(x = contrast, y = estimate, ymin = conf.low, ymax = conf.high)) +\n  geom_pointrange() +\n  geom_hline(yintercept = 0, col = \"blue\", linetype = \"dashed\") +\n  geom_label(aes(label = round_half_up(estimate, 2))) +\n  labs(title = \"Comparing Mean Weight across pairs of Food Security groups\",\n       subtitle = \"90% Tukey HSD Confidence intervals\",\n       caption = glue(nrow(dat5), \" non-Hispanic Black participants in nh432\"),\n       x = \"Pairwise Difference between FOODSEC groups\",\n       y = \"Difference in Weight (kg)\")\n\n\n\n\n\n\n3.11.4 Kruskal-Wallis Test\nWhen the assumption of Normality is really unreasonable, many people (including me) will instead use a rank-based method, called the Kruskal-Wallis test to compare the locations of WEIGHT across levels of FOODSEC.\n\nkruskal.test(WEIGHT ~ FOODSEC, data = dat6)\n\n\n    Kruskal-Wallis rank sum test\n\ndata:  WEIGHT by FOODSEC\nKruskal-Wallis chi-squared = 21.102, df = 3, p-value = 0.0001003\n\n\n\n\n3.11.5 Dunn Test for Pairwise Comparisons after Kruskal-Wallis Test\nShould you develop a Kruskal-Wallis test result which implies that running a set of pairwise comparisons is important, I would suggest the use of the Dunn test, available in the dunn_test() function from the rstatix package.\n\ndunn_test(data = dat6, WEIGHT ~ FOODSEC, \n                   p.adjust.method = \"holm\", detailed = TRUE) |>\n  select(group1, group2, p.adj, n1, n2, estimate1, estimate2, estimate) |>\n  mutate(across(.cols = -c(group1, group2, n1, n2), num, digits = 3)) |>\n  gt() |>\n  tab_header(title = \"Dunn Tests comparing WEIGHT by FOODSEC\",\n             subtitle = \"Pairwise Comparisons after Kruskal-Wallis test\") |>\n  tab_footnote(footnote = glue(nrow(dat6), \" participants in nh432\"))\n\n\n\n\n\n  \n    \n      Dunn Tests comparing WEIGHT by FOODSEC\n    \n    \n      Pairwise Comparisons after Kruskal-Wallis test\n    \n  \n  \n    \n      group1\n      group2\n      p.adj\n      n1\n      n2\n      estimate1\n      estimate2\n      estimate\n    \n  \n  \n    Full\nMarginal\n0.269\n2233\n560\n1780.595\n1865.621\n85.026\n    Full\nLow\n0.049\n2233\n501\n1780.595\n1916.147\n135.551\n    Full\nVery Low\n0.000\n2233\n379\n1780.595\n2022.412\n241.816\n    Marginal\nLow\n0.438\n560\n501\n1865.621\n1916.147\n50.525\n    Marginal\nVery Low\n0.105\n560\n379\n1865.621\n2022.412\n156.790\n    Low\nVery Low\n0.282\n501\n379\n1916.147\n2022.412\n106.265\n  \n  \n  \n    \n       3673 participants in nh432\n    \n  \n\n\n\n\nAgain, a problem with this approach is that all it provides is a set of adjusted p values for these comparisons, but if we’re not willing to assume even very approximate Normality (and thus use an ANOVA approach) this is what we’ll have to cope with."
  },
  {
    "objectID": "04-431review2.html#r-setup",
    "href": "04-431review2.html#r-setup",
    "title": "4  431 Review: Comparing Rates",
    "section": "4.1 R Setup",
    "text": "4.1 R Setup\n\nknitr::opts_chunk$set(comment = NA)\n\nlibrary(janitor) \nlibrary(broom)\nlibrary(DescTools)\nlibrary(Epi)\nlibrary(gt)\nlibrary(Hmisc)\nlibrary(vcd)\n\nlibrary(tidyverse) \n\ntheme_set(theme_bw())\n\n\n4.1.1 Data Load\n\nnh432 <- read_rds(\"data/nh432.Rds\")"
  },
  {
    "objectID": "04-431review2.html#x2-contingency-table-dr_lose-and-nowlose",
    "href": "04-431review2.html#x2-contingency-table-dr_lose-and-nowlose",
    "title": "4  431 Review: Comparing Rates",
    "section": "4.2 2x2 Contingency Table: DR_LOSE and NOWLOSE",
    "text": "4.2 2x2 Contingency Table: DR_LOSE and NOWLOSE\nLet’s compare the probability that NOWLOSE is 1 (The subject is currently working on losing or controlling their body weight) between NHANES participants who have (vs. who have not) been told by a doctor to lose or control their weight in the past 12 months (DR_LOSE). Each of these (DR_LOSE and NOWLOSE) is stored in R as a numeric variable with non-missing values equal to 0 or 1.\n\ntemp <- nh432 |> \n  select(SEQN, DR_LOSE, NOW_LOSE) |> \n  drop_na()\n\nAs with any categorical variable, we start by counting, and the natural way to display the counts of these two variables (DR_LOSE and NOW_LOSE) is in a table, rather than a graph, I think.\n\ntemp |> \n  tabyl(DR_LOSE, NOW_LOSE) |> \n  adorn_totals(where = c(\"row\", \"col\")) |> \n  adorn_title() \n\n         NOW_LOSE           \n DR_LOSE        0    1 Total\n       0     1198 1541  2739\n       1      246  943  1189\n   Total     1444 2484  3928\n\n\nNow that we have a 2x2 table, we could consider obtaining some more detailed summary statistics, with a tool like the twoby2() function in the Epi package. There is a problem with this, though.\n\ntwoby2(temp$DR_LOSE, temp$NOW_LOSE)\n\n2 by 2 table analysis: \n------------------------------------------------------ \nOutcome   : 0 \nComparing : 0 vs. 1 \n\n     0    1    P(0) 95% conf. interval\n0 1198 1541  0.4374    0.4189   0.4560\n1  246  943  0.2069    0.1848   0.2309\n\n                                   95% conf. interval\n             Relative Risk: 2.1140    1.8766   2.3815\n         Sample Odds Ratio: 2.9801    2.5412   3.4949\nConditional MLE Odds Ratio: 2.9793    2.5350   3.5096\n    Probability difference: 0.2305    0.2002   0.2594\n\n             Exact P-value: 0.0000 \n        Asymptotic P-value: 0.0000 \n------------------------------------------------------\n\n\nThe code runs fine, but the table isn’t really in a useful format. This table shows the probability that NOWLOSE = 0 (“No”) comparing DR_LOSE = 0 (“No”) to DR_LOSE = 1 (“Yes”), and that’s just confusing.\nIt would be much better if we did two things:\n\nused factors with meaningful labels to represent the 1/0 variables for this table\nset up the table in standard epidemiological format, and then made a better choice as to what combination should be in the top left of the 2x2 table.\n\nSo let’s do that.\n\n4.2.1 Standard Epidemiological Format\nStandard Epidemiological Format for a 2x2 table places the exposure in the rows, and the outcome in the columns, with the top left representing the combination of interest when we obtain things like an odds ratio or probability difference. Typically this means we want to put the “Yes” and “Yes” combination in the top left.\nFirst, let’s create factor versions (with more meaningful labels than 1 and 0) out of the two variables of interest: DR_LOSE and NOW_LOSE.\n\ndat1 <- nh432 |> \n  select(SEQN, DR_LOSE, NOW_LOSE) |> \n  drop_na() |>\n  mutate(DR_LOSE_f = fct_recode(factor(DR_LOSE), \"Dr_said_Lose_Wt\" = \"1\", No = \"0\"),\n         DR_LOSE_f = fct_relevel(DR_LOSE_f, \"Dr_said_Lose_Wt\", \"No\"),\n         NOW_LOSE_f = fct_recode(factor(NOW_LOSE), \"Now_losing_Wt\" = \"1\", No = \"0\"),\n         NOW_LOSE_f = fct_relevel(NOW_LOSE_f, \"Now_losing_Wt\", \"No\"))\n\nNote that after recoding the levels to more meaningful labels, we also re-leveled the factors so that the “Yes” result comes first rather than last.\nThis produces the following table, which is now in standard epidemiological format, where we are using the DR_LOSE_f information to predict NOW_LOSE_f.\n\ndat1 |> \n  tabyl(DR_LOSE_f, NOW_LOSE_f) |> \n  adorn_totals(where = c(\"row\", \"col\")) |> \n  adorn_title() \n\n                    NOW_LOSE_f           \n       DR_LOSE_f Now_losing_Wt   No Total\n Dr_said_Lose_Wt           943  246  1189\n              No          1541 1198  2739\n           Total          2484 1444  3928\n\n\nWe could, I suppose, make the table even prettier.\n\ntab1 <- dat1 |> \n  tabyl(DR_LOSE_f, NOW_LOSE_f) |> \n  adorn_totals(where = c(\"row\", \"col\")) \n\ngt(tab1, rowname_col = \"DR_LOSE_f\") |>\n  tab_header(title = \"DR_LOSE vs. NOW_LOSE\",\n             subtitle = \"Standard Epidemiological Format\") |>\n  tab_stubhead(label = \"Dr said Lose Weight?\") |>\n  tab_spanner(label = \"Currently Losing Weight?\", \n              columns = c(Now_losing_Wt, No))\n\n\n\n\n\n  \n    \n      DR_LOSE vs. NOW_LOSE\n    \n    \n      Standard Epidemiological Format\n    \n  \n  \n    \n      Dr said Lose Weight?\n      \n        Currently Losing Weight?\n      \n      Total\n    \n    \n      Now_losing_Wt\n      No\n    \n  \n  \n    Dr_said_Lose_Wt\n943\n246\n1189\n    No\n1541\n1198\n2739\n    Total\n2484\n1444\n3928\n  \n  \n  \n\n\n\n\n\n\n4.2.2 Obtaining Key Summaries with twoby2()\nAnd, finally, we can obtain necessary summaries (including estimates and confidence intervals) using the twoby2() function.\n\ntwoby2(dat1$DR_LOSE_f, dat1$NOW_LOSE_f, conf.level = 0.90)\n\n2 by 2 table analysis: \n------------------------------------------------------ \nOutcome   : Now_losing_Wt \nComparing : Dr_said_Lose_Wt vs. No \n\n                Now_losing_Wt   No    P(Now_losing_Wt) 90% conf. interval\nDr_said_Lose_Wt           943  246              0.7931    0.7731   0.8118\nNo                       1541 1198              0.5626    0.5470   0.5781\n\n                                   90% conf. interval\n             Relative Risk: 1.4097    1.3586   1.4627\n         Sample Odds Ratio: 2.9801    2.6071   3.4065\nConditional MLE Odds Ratio: 2.9793    2.5998   3.4195\n    Probability difference: 0.2305    0.2052   0.2548\n\n             Exact P-value: 0.0000 \n        Asymptotic P-value: 0.0000 \n------------------------------------------------------\n\n\nSome brief descriptions of these results:\n\nThe probability that a participant is now losing weight (NOW_LOSE is 1) is estimated to be 0.79 (with 90% CI 0.77, 0.81) if the participant has been told to lose weight by a doctor in the past 12 months (DR_LOSE = 1), but only 0.56 (with 90% CI 0.55, 0.58) if the participant has not been told this.\nThe relative risk of a participant now losing weight is estimated to be \\(\\frac{0.7931}{0.5626}\\) = 1.41 (with 90% CI 1.36, 1.46) for a participant who has been told to lose weight vs. a participant who has not.\nThe odds of a participant now losing weight are \\(\\frac{0.7931(1-0.5626)}{0.5626(1-0.7931)}\\) = 2.98 times as high for a participant who has been told to lose weight than for one who has not, with 90% CI (2.61, 3.41).\nThe difference in probability is estimated to be 0.7931 - 0.5626 = 0.2305 (90% CI: 0.21, 0.25), indicating again that the true probability of now losing weight is higher in participants who have been told to lose weight than in those who have not.\n\nThe “exact” p-value listed comes from the Fisher exact test, while the “asymptotic” p-value comes from a Pearson \\(\\chi^2\\) (chi-squared) test. I would focus on the meaningful estimates (those with confidence intervals) in making comparisons, rather than on trying to determine “statistical significance” with the p-values."
  },
  {
    "objectID": "04-431review2.html#x2-table-sedate-category-and-now_exer",
    "href": "04-431review2.html#x2-table-sedate-category-and-now_exer",
    "title": "4  431 Review: Comparing Rates",
    "section": "4.3 2x2 Table: SEDATE category and NOW_EXER",
    "text": "4.3 2x2 Table: SEDATE category and NOW_EXER\nLet’s now look at another example, where we compare the probability that a participant is “now exercising” (NOW_EXER = 1) on the basis of their level of sedentary activity in a typical day (collected in the SEDATE variable, in minutes.)\n\ndat2 <- nh432 |> \n  select(SEQN, SEDATE, NOW_EXER) |> \n  drop_na()\n\nsummary(dat2 |> select(-SEQN))\n\n     SEDATE          NOW_EXER     \n Min.   :   2.0   Min.   :0.0000  \n 1st Qu.: 180.0   1st Qu.:0.0000  \n Median : 300.0   Median :1.0000  \n Mean   : 332.8   Mean   :0.6019  \n 3rd Qu.: 480.0   3rd Qu.:1.0000  \n Max.   :1320.0   Max.   :1.0000  \n\n\nAs you can see above, the information in SEDATE is quantitative, and suppose we want to compare a High SEDATE group vs. a Low SEDATE group.\n\n4.3.1 Creating a Low and High Group on SEDATE\nWe can use the cut2() function from the Hmisc package to partition the data by the SEDATE variable into three groups of equal sample size. At the same time, we’ll make NOW_EXER into a more useful (for tabulation) factor with more meaningful level descriptions.\n\ndat2 <- dat2 |>\n  mutate(SED_f = cut2(SEDATE, g = 3),\n         NOW_EXER_f = fct_recode(factor(NOW_EXER), \"Now_exercising\" = \"1\", No = \"0\"),\n         NOW_EXER_f = fct_relevel(NOW_EXER_f, \"Now_exercising\", \"No\"))\n\nAs you can see, we now have three groups defined by their SEDATE values, of roughly equal sample sizes.\n\ndat2 |> tabyl(SED_f)\n\n      SED_f    n   percent\n [  2, 200) 1323 0.3387097\n [200, 420) 1301 0.3330773\n [420,1320] 1282 0.3282130\n\n\nThe group labeled [2, 200) contains the 1323 subjects who had SEDATE values ranging from 2 up to (but not including) 200 minutes, for example.\n\nggplot(dat2, aes(x = SEDATE)) +\n  geom_histogram(aes(fill = SED_f), col = \"black\", bins = 25) +\n  scale_fill_manual(values = c(\"seagreen\", \"white\", \"seagreen\")) +\n  labs(title = \"Comparing Low SEDATE to High SEDATE\",\n       subtitle = \"Identification of Groups\")\n\n\n\n\nNow, we want to compare the Lowest SEDATE group (SED_F = [2, 200)) to the Highest SEDATE group (SED_F = [420, 1320]). To do that, we’ll drop the middle group, and then look at the cross-tabulation of our two remaining SEDATE groups with our outcome: NOW_EXER (in factor form.)\n\ndat2 <- dat2 |>\n  filter(SED_f != \"[200, 420)\") |>\n  mutate(SED_f = fct_drop(SED_f))\n\ndat2 |> tabyl(SED_f, NOW_EXER_f)\n\n      SED_f Now_exercising  No\n [  2, 200)            776 547\n [420,1320]            789 493\n\n\n\n\n4.3.2 Two-by-Two Table Summaries\nLet’s look at the analytic results for this table.\n\ntwoby2(dat2$SED_f, dat2$NOW_EXER_f) \n\n2 by 2 table analysis: \n------------------------------------------------------ \nOutcome   : Now_exercising \nComparing : [  2, 200) vs. [420,1320] \n\n           Now_exercising  No    P(Now_exercising) 95% conf. interval\n[  2, 200)            776 547               0.5865    0.5598   0.6128\n[420,1320]            789 493               0.6154    0.5885   0.6417\n\n                                    95% conf. interval\n             Relative Risk:  0.9530    0.8952   1.0146\n         Sample Odds Ratio:  0.8864    0.7577   1.0371\nConditional MLE Odds Ratio:  0.8865    0.7553   1.0403\n    Probability difference: -0.0289   -0.0664   0.0087\n\n             Exact P-value: 0.1388 \n        Asymptotic P-value: 0.1322 \n------------------------------------------------------\n\n\nUh, oh. There’s a bit of a problem here now. We have the right rows and the right columns, but they’re not in the best possible order, since the estimated probability of Now Exercising for the group on top (SED = [2, 200)) is smaller than it is for the people in the high group in terms of sedentary activity As a result of this problem with ordering, our relative risk and odds ratio estimates are less than 1, and our probability difference is negative.\n\n\n4.3.3 Flipping Levels\nSince which exposure goes at the top is an arbitrary decision, let’s switch the factor levels in SED_f, so that the people with high sedentary activity and who are now exercising are shown in the top left cell of the table. This should flip the point estimates of the relative risk and odds ratio above 1, and the estimated probability difference to a positive number. Note the use of the fct_rev() function from the forcats package to accomplish this.\n\ndat2 <- dat2 |>\n  mutate(SED_f = fct_rev(SED_f))\n\ndat2 |> tabyl(SED_f, NOW_EXER_f) |> \n  adorn_totals(where = c(\"row\", \"col\")) |> \n  adorn_title() \n\n                NOW_EXER_f           \n      SED_f Now_exercising   No Total\n [420,1320]            789  493  1282\n [  2, 200)            776  547  1323\n      Total           1565 1040  2605\n\ntwoby2(dat2$SED_f, dat2$NOW_EXER_f, conf.level = 0.90)\n\n2 by 2 table analysis: \n------------------------------------------------------ \nOutcome   : Now_exercising \nComparing : [420,1320] vs. [  2, 200) \n\n           Now_exercising  No    P(Now_exercising) 90% conf. interval\n[420,1320]            789 493               0.6154    0.5929   0.6375\n[  2, 200)            776 547               0.5865    0.5641   0.6086\n\n                                   90% conf. interval\n             Relative Risk: 1.0493    0.9956   1.1059\n         Sample Odds Ratio: 1.1281    0.9889   1.2869\nConditional MLE Odds Ratio: 1.1281    0.9858   1.2910\n    Probability difference: 0.0289   -0.0027   0.0604\n\n             Exact P-value: 0.1388 \n        Asymptotic P-value: 0.1322 \n------------------------------------------------------\n\n\nWe conclude now that the participants who were in the high SEDATION group (as compared to those in the low SEDATION group) had:\n\na relative risk of 1.05 (90% CI: 0.995, 1.106) for Now exercising,\na sample odds ratio of 1.13 (90% CI: 0.989, 1.287) for Now exercising,\nand probability for Now exercising that was 0.029 higher (-0.003, 0.060) than for those in the low SEDATION group."
  },
  {
    "objectID": "04-431review2.html#a-larger-5x3-2-way-table-dietqual-and-wtgoal-in-lighter-men",
    "href": "04-431review2.html#a-larger-5x3-2-way-table-dietqual-and-wtgoal-in-lighter-men",
    "title": "4  431 Review: Comparing Rates",
    "section": "4.4 A Larger (5x3) 2-Way Table: DIETQUAL and WTGOAL in Lighter Men",
    "text": "4.4 A Larger (5x3) 2-Way Table: DIETQUAL and WTGOAL in Lighter Men\nHere, we’ll look at Male participants who weighed less than 100 kg (approximately 220 pounds) and ask whether their DIETQUAL (diet quality: self-rated as Excellent to poor in 5 categories) response is associated with their response to WTGOAL (would you like to weigh more, about the same, or less than you do now: 3 categories.)\nThe resulting two-way contingency table includes 5 rows and 3 columns. We are interested in evaluating the relationship between the rows and the columns. It’s called a two-way table because there are two categorical variables (DIETQUAL and WTGOAL) under study.\nIf the rows and columns were found to be independent of one another, this would mean that the probabilities of falling in each column do not change, regardless of what row of the table we look at.\nIf the rows and columns are associated, then the probabilities of falling in each column do depend on which row we’re looking at.\n\ndat3 <- nh432 |> \n  select(SEQN, DIETQUAL, WTGOAL, WEIGHT, SEX) |>\n  filter(WEIGHT < 100 & SEX == \"Male\") |>\n  drop_na()\n\ndat3 |> \n  tabyl(DIETQUAL, WTGOAL)\n\n  DIETQUAL More Same Less\n Excellent   16   53   32\n Very Good   37  153  117\n      Good   68  179  238\n      Fair   44  111  144\n      Poor   15   18   43\n\n\nIf we want a graphical representation of a two-way table, the most common choice is probably a mosaic plot.\n\nvcd::mosaic(~ DIETQUAL + WTGOAL, data = dat3,\n            highlighting = \"WTGOAL\")\n\n\n\n\nLarger observed frequencies in the contingency table show up with larger tile areas in the in the mosaic plot. So, for instance, we see the larger proportion of “less” WTGOAL in the “Poor” DIETQUAL category, as compared to most of the other DIETQUAL categories.\n\n4.4.1 What would independence look like?\nA mosaic plot displaying perfect independence (using simulated data) might look something like this:\n\nvar1 <- c(rep(\"A\", 48), rep(\"B\", 54), rep(\"C\", 60), rep(\"D\", 24) )\nvar2 <- c( rep(c(\"G1\", \"G1\", \"G2\", \"G2\", \"G2\", \"G3\"), 31) )\ntemp_tab <- tibble(var1, var2); rm(var1, var2)\nvcd::mosaic(~ var1 + var2, data = temp_tab, highlighting = \"var1\")\n\n\n\n\nHere’s the table for our simulated data, where independence holds perfectly.\n\nxtabs(~ var1 + var2, data = temp_tab)\n\n    var2\nvar1 G1 G2 G3\n   A 16 24  8\n   B 18 27  9\n   C 20 30 10\n   D  8 12  4\n\n\nNote that in these simulated data, we have the same fraction of people in each of the four var1 categories (A, B, C, and D) regardless of which of the three var2 categories (G1, G2 and G3) we are in, and vice versa. That’s what it means for rows and columns to be independent.\n\n\n4.4.2 Back to the DIETQUAL and WTGOAL table\nNow, returning to our problem, to obtain detailed results from the Pearson \\(\\chi^2\\) test, I use the xtabs() function and then the chisq.test() function, like this:\n\nchi3 <- chisq.test(xtabs(~ DIETQUAL + WTGOAL, data = dat3))\n\nchi3\n\n\n    Pearson's Chi-squared test\n\ndata:  xtabs(~DIETQUAL + WTGOAL, data = dat3)\nX-squared = 32.603, df = 8, p-value = 7.261e-05\n\n\nThe null hypothesis being tested here is that DIETQUAL and WTGOAL are independent of each other. A small p value like this is indicative of an association between the two variables.\nThe chi3 object we have created also contains:\n\nthe observed frequencies in each cell, as well as\nthe expected frequencies under the hypothesis of independence of the rows and the columns1, and\nthe Pearson residuals \\((\\mbox{observed - expected})/\\sqrt{\\mbox{expected}}\\) for each cell, among other things.\n\n\nchi3$observed\n\n           WTGOAL\nDIETQUAL    More Same Less\n  Excellent   16   53   32\n  Very Good   37  153  117\n  Good        68  179  238\n  Fair        44  111  144\n  Poor        15   18   43\n\nchi3$expected\n\n           WTGOAL\nDIETQUAL        More      Same      Less\n  Excellent 14.33754  40.94164  45.72082\n  Very Good 43.58044 124.44637 138.97319\n  Good      68.84858 196.60095 219.55047\n  Fair      42.44479 121.20347 135.35174\n  Poor      10.78864  30.80757  34.40379\n\nchi3$residuals # Pearson residuals\n\n           WTGOAL\nDIETQUAL          More       Same       Less\n  Excellent  0.4390501  1.8845411 -2.0291917\n  Very Good -0.9968028  2.5595886 -1.8639211\n  Good      -0.1022694 -1.2552875  1.2451396\n  Fair       0.2387127 -0.9268093  0.7433564\n  Poor       1.2821492 -2.3074805  1.4655618\n\n\nAn association plot presents a graphical description of the Pearson residuals, with the area of each box shown proportional to the difference between the observed and expected frequencies.\n\nIf the observed frequency of a cell is greater than the expectation under the hypothesis of independence, then the box rises above the baseline.\n\nAn example here is the (DIETQUAL = Very Good, WTGOAL = Same) which had an observed frequency of 153 but an expected frequency of 124.4, yielding the largest positive Pearson residual at 2.56.\n\nBoxes shown below the baseline indicate that the observed frequency was less than the expectation under the independence hypothesis.\n\nThe largest negative Pearson residual is the (DIETQUAL = Poor, WTGOAL = Same) cell, where we observed 18 observations but the independence model would predict 30.8, yielding a Pearson residual of -2.31.\n\n\n\nvcd::assoc(~ DIETQUAL + WTGOAL, data = dat3)\n\n\n\n\nSome people also like to calculate a correlation between categorical variables. If each of your categorical variables is ordinal (as in this case) then Kendall’s tau (version b) is probably the best choice. As with a Pearson correlation for quantities, the value for this measure ranges from -1 to 1, with -1 indicating a strong negative correlation, and +1 a strong positive correlation, with 0 indicating no correlation.\nTo use this approach, though, we first have to be willing to treat our multi-categorical variables as if they were numeric, which may or may not be reasonable.\n\ndat3 <- dat3 |>\n  mutate(DIETQUAL_num = as.numeric(DIETQUAL))\n\ndat3 |> tabyl(DIETQUAL_num, DIETQUAL)\n\n DIETQUAL_num Excellent Very Good Good Fair Poor\n            1       101         0    0    0    0\n            2         0       307    0    0    0\n            3         0         0  485    0    0\n            4         0         0    0  299    0\n            5         0         0    0    0   76\n\n\n\ndat3 <- dat3 |>\n  mutate(WTGOAL_num = as.numeric(WTGOAL))\n\ndat3 |> tabyl(WTGOAL_num, WTGOAL)\n\n WTGOAL_num More Same Less\n          1  180    0    0\n          2    0  514    0\n          3    0    0  574\n\n\n\ncor(dat3$DIETQUAL_num, dat3$WTGOAL_num, method = \"kendall\")\n\n[1] 0.07193663\n\n\nIf you want to obtain a confidence interval for this correlation coefficient, then you would need to use the KendallTauB() function from the DescTools package.\n\nKendallTauB(dat3$DIETQUAL_num, dat3$WTGOAL_num, conf.level = 0.90)\n\n     tau_b     lwr.ci     upr.ci \n0.07193663 0.03147130 0.11240196 \n\n\nAgain, it’s just a number, and not especially valuable."
  },
  {
    "objectID": "04-431review2.html#phq9-category-and-raceethnicity",
    "href": "04-431review2.html#phq9-category-and-raceethnicity",
    "title": "4  431 Review: Comparing Rates",
    "section": "4.5 PHQ9 Category and Race/Ethnicity",
    "text": "4.5 PHQ9 Category and Race/Ethnicity\nLet’s look next at the association of race-ethnicity (RACEETH, which has 5 levels) and the depression category (minimal, mild, moderate, moderately severe, or severe) available in PHQ9_CAT, which we derived from the PHQ-9 depression screener score. We’ll restrict this small analysis to NHANES participants who did not receive care from a mental health provider (so MENTALH is 0) in the last 12 months.\n\ntemp <- nh432 |> \n  select(SEQN, RACEETH, PHQ9_CAT, MENTALH) |>\n  filter(MENTALH == 0) |>\n  drop_na()\n\nSo here’s our first attempt at a 5x5 table describing this association.\n\ntemp |> \n  tabyl(RACEETH, PHQ9_CAT)\n\n     RACEETH minimal mild moderate moderately severe severe\n Non-H White     770  151       41                20      8\n Non-H Black     668  143       34                13      1\n    Hispanic     581  133       36                12      5\n Non-H Asian     428   49       11                 1      1\n  Other Race     105   34       12                 5      1\n\n\nWe note some very small observed frequencies, especially in the bottom right of the table. Should we try to run a Pearson \\(\\chi^2\\) test on these results, we will generate a warning that the Chi-square approximation may be incorrect.\n\nxtabs( ~ RACEETH + PHQ9_CAT, data = temp ) |>\n  chisq.test()\n\nWarning in stats::chisq.test(x, y, ...): Chi-squared approximation may be\nincorrect\n\n\n\n    Pearson's Chi-squared test\n\ndata:  xtabs(~RACEETH + PHQ9_CAT, data = temp)\nX-squared = 49.288, df = 16, p-value = 2.974e-05\n\n\n\n4.5.1 The Cochran conditions\nR sets off this warning when the “Cochran conditions” are not met. The Cochran conditions require that we have:\n\nno cells with 0 counts\nat least 80% of the cells in our table with counts of 5 or higher\nexpected counts in each cell of the table should be 5 or more\n\nIn our table, we have four cells with observed counts below 5 (all have count 1) and two more with observed counts of exactly 5. If we look at the expected frequencies under the hypothesis of independence, what do we see?\n\ntemp_chi <- xtabs( ~ RACEETH + PHQ9_CAT, data = temp ) |>\n  chisq.test()\n\nWarning in stats::chisq.test(x, y, ...): Chi-squared approximation may be\nincorrect\n\ntemp_chi$expected\n\n             PHQ9_CAT\nRACEETH        minimal      mild  moderate moderately severe    severe\n  Non-H White 774.2813 154.73491 40.655838         15.473491 4.8544284\n  Non-H Black 671.8259 134.25988 35.276126         13.425988 4.2120748\n  Hispanic    599.8725 119.88048 31.498008         11.988048 3.7609562\n  Non-H Asian 383.2302  76.58596 20.122587          7.658596 2.4026969\n  Other Race  122.7901  24.53877  6.447441          2.453877 0.7698437\n\n\nEvery cell in the “severe” category has an expected frequency below 5, and we also have some generally small counts, in the Non-Hispanic Asian and Other Race categories, as well as the “moderately severe” category.\n\n\n4.5.2 Collapsing Categories\nSo what might we do about this?\nLet us consider two approaches that we’ll use simultaneously:\n\ndrop two of the RACEETH groups, and just use the top 3 (Non-H White, Non-H Black and Hispanic) using filter()\ncollapse together the two right-most levels of PHQ9_CAT (moderately severe and severe) into a new level which I’ll call “More Severe”, using fct_lump_n()\n\n\ndat5 <- nh432 |> \n  select(SEQN, RACEETH, PHQ9_CAT, MENTALH) |>\n  filter(MENTALH == 0) |>\n  filter(RACEETH %in% c(\"Non-H White\", \"Non-H Black\", \"Hispanic\")) |>\n  drop_na() |>\n  mutate(RACEETH = fct_drop(RACEETH),\n         PHQ9_CAT = fct_lump_n(PHQ9_CAT, 3, \n                               other_level = \"More Severe\"))\n\ndat5 |> \n  tabyl(RACEETH, PHQ9_CAT)\n\n     RACEETH minimal mild moderate More Severe\n Non-H White     770  151       41          28\n Non-H Black     668  143       34          14\n    Hispanic     581  133       36          17\n\n\nNow, we have at least 14 participants in every cell of the table.\n\n\n4.5.3 Pearson \\(\\chi^2\\) Analysis\nNow, let’s consider what the Pearson \\(\\chi^2\\) test suggests.\n\ntab5 <- xtabs(~ RACEETH + PHQ9_CAT, data = dat5)\n\ntab5\n\n             PHQ9_CAT\nRACEETH       minimal mild moderate More Severe\n  Non-H White     770  151       41          28\n  Non-H Black     668  143       34          14\n  Hispanic        581  133       36          17\n\nchisq.test(tab5)\n\n\n    Pearson's Chi-squared test\n\ndata:  tab5\nX-squared = 5.0157, df = 6, p-value = 0.5418\n\n\nNow we have no warning, and notice also how large a change this has meant in terms of the p-value, as compared to our original \\(\\chi^2\\) result.\n\n\n4.5.4 Mosaic Plot\nHere’s a mosaic plot2 of the table.\n\nvcd::mosaic(tab5, highlighting = \"PHQ9_CAT\")\n\n\n\n\n\n\n4.5.5 Examining the Fit\nWe’ll finish up with a look at the expected frequencies, and a table and association plot of the Pearson residuals.\n\nchisq.test(tab5)$observed\n\n             PHQ9_CAT\nRACEETH       minimal mild moderate More Severe\n  Non-H White     770  151       41          28\n  Non-H Black     668  143       34          14\n  Hispanic        581  133       36          17\n\nchisq.test(tab5)$expected\n\n             PHQ9_CAT\nRACEETH        minimal     mild moderate More Severe\n  Non-H White 764.0711 161.5940 42.00688    22.32798\n  Non-H Black 662.9667 140.2114 36.44839    19.37347\n  Hispanic    591.9622 125.1946 32.54472    17.29855\n\nchisq.test(tab5)$residuals\n\n             PHQ9_CAT\nRACEETH           minimal        mild    moderate More Severe\n  Non-H White  0.21449006 -0.83339100 -0.15535235  1.20036381\n  Non-H Black  0.19548040  0.23550271 -0.40554793 -1.22081874\n  Hispanic    -0.45055624  0.69759600  0.60567876 -0.07178083\n\nassoc(tab5)"
  },
  {
    "objectID": "05-431review3.html#r-setup",
    "href": "05-431review3.html#r-setup",
    "title": "5  431 Review: Fitting Linear Models",
    "section": "5.1 R Setup",
    "text": "5.1 R Setup\n\nknitr::opts_chunk$set(comment = NA)\n\nlibrary(janitor) \nlibrary(broom)\nlibrary(car)\nlibrary(equatiomatic)\nlibrary(GGally)\nlibrary(gt)\nlibrary(Hmisc)\nlibrary(patchwork)\n\nlibrary(tidyverse) \n\ntheme_set(theme_bw())\n\n\n5.1.1 Data Load\n\nnh432 <- read_rds(\"data/nh432.Rds\")"
  },
  {
    "objectID": "05-431review3.html#modeling-weekend-sleep-hours",
    "href": "05-431review3.html#modeling-weekend-sleep-hours",
    "title": "5  431 Review: Fitting Linear Models",
    "section": "5.2 Modeling Weekend Sleep Hours",
    "text": "5.2 Modeling Weekend Sleep Hours\nIn this example, we’ll try to build an effective model to predict our outcome: average weekend hours of sleep (SLPWKEND) on the basis of four predictors:\n\naverage weekday hours of sleep (SLPWKDAY)\nsystolic blood pressure (SBP)\nPHQ-9 depression screener score (PHQ9), and\nwhether or not the participant has mentioned trouble sleeping to a physician (SLPTROUB)\n\nWe’ll compare a model using all four of these predictors to a model using just the two directly related to sleep (SLPWKDAY and SLPTROUB), and we’ll restrict our analysis to those participants whose self-reported overall health (SROH) was “Good”.\n\ndat1 <- nh432 |>\n  select(SEQN, SLPWKEND, SLPWKDAY, SBP, PHQ9, SLPTROUB, SROH) |>\n  filter(SROH == \"Good\") |>\n  drop_na()\n\ndat1\n\n# A tibble: 1,293 × 7\n   SEQN   SLPWKEND SLPWKDAY   SBP  PHQ9 SLPTROUB SROH \n   <chr>     <dbl>    <dbl> <dbl> <int>    <dbl> <fct>\n 1 109273      8        6.5   110    15        1 Good \n 2 109293      6.5      7.5   130     3        0 Good \n 3 109295      7        7     161     0        1 Good \n 4 109305      6.5      6     125     0        0 Good \n 5 109307     11        7.5   114     0        0 Good \n 6 109315      5        5     123     1        0 Good \n 7 109336      8        4     148     1        1 Good \n 8 109342      8        6.5   107    16        1 Good \n 9 109365      9.5      9.5   133     7        0 Good \n10 109378      9        9     133     0        0 Good \n# … with 1,283 more rows\n\n\n\n5.2.1 Should we transform our outcome?\nWe can develop a Box-Cox plot to help us choose between potential transformations of our outcome, so as to improve the adherence to regression assumptions. To do so, we first fit our larger model.\n\nm1 <- lm(SLPWKEND ~ SLPWKDAY + SBP + PHQ9 + SLPTROUB, data = dat1)\n\nboxCox(m1)\n\n\n\n\nThe Box-Cox suggested set of transformations centers near \\(\\lambda = 1\\). As we saw back in Chapter 3, The ladder of power transformations looks like this:\n\n\n\n\\(\\lambda\\)\nTransformation\nFormula\n\n\n\n\n-2\ninverse square\n\\(1/y^2\\)\n\n\n-1\ninverse\n\\(1/y\\)\n\n\n-0.5\ninverse square root\n\\(1/\\sqrt{y}\\)\n\n\n0\nlogarithm\n\\(log y\\)\n\n\n0.5\nsquare root\n\\(\\sqrt{y}\\)\n\n\n1\nno transformation\ny\n\n\n2\nsquare\n\\(y^2\\)\n\n\n3\ncube\n\\(y^3\\)\n\n\n\nSo, in this case, the Box-Cox approach (again, with \\(\\lambda\\) near 1) suggests that we leave the existing SLPWKEND outcome alone.\n\n\n5.2.2 Scatterplot Matrix\n\nggpairs(dat1, columns = c(3:6, 2), switch = \"both\",\n        lower=list(combo=wrap(\"facethist\", bins=25)))\n\n\n\n\n\nThe reason I included column 2 (our outcome: SLPWKEND) last in this plot is so that the bottom row would include each of our predictors plotted on the X (horizontal) axis against the outcome on the Y (vertical) axis, next to a density plot of the outcome.\nI also switched the locations of the facet labels on both the x and y axis from their defaults, so that the labels are to the left and below the plots, since I find that a bit easier to work with.\nThe lower business is to avoid getting a warning about binwidths.\nThe binary variable (SLPTROUB) is included here as a 1-0 numeric variable, rather than a factor, which is why the scatterplot matrix looks as it does, rather than creating a series of boxplots (as we’ll see when we work with a factor later.)\n\n\n\n5.2.3 Collinearity?\nIn any multiple regression setting, two or more of the predictors might be highly correlated with one another, and this is referred to as multicollinearity or just collinearity. If we have a serious problem with collinearity, this can cause several problems, including difficulty fitting and interpreting the resulting model.\nIs collinearity a serious concern in our situation? Looking at the scatterplot matrix, we see that the largest observed correlation between two predictors is between PHQ9 and SLPTROUB. Does that rise to the level of a problem?\nI usually use the vif() function from the car package to help make this decision. The variance inflation factor (or VIF) measures how much the variance of a regression coefficient is inflated due to collinearity in the model. The smallest possible VIF value is 1, and VIFs near 1, as we’ll see here, indicate no problems with collinearity worth worrying about.\n\nvif(m1)\n\nSLPWKDAY      SBP     PHQ9 SLPTROUB \n1.002807 1.006778 1.102341 1.098187 \n\n\nShould we see a VIF (or generalized VIF, which is produced by the vif() function when we have factor variables in the model) above, say, 5, that would be an indication that the model would be improved by not including the variable that exhibits collinearity. Here, we have no such issues, and will proceed to fit the model including all of these predictors.\n\n\n5.2.4 Fitting and Displaying Model m1\nHere are the coefficients obtained from fitting the model m1.\n\nm1 <- lm(SLPWKEND ~ SLPWKDAY + SBP + PHQ9 + SLPTROUB, data = dat1)\n\nm1\n\n\nCall:\nlm(formula = SLPWKEND ~ SLPWKDAY + SBP + PHQ9 + SLPTROUB, data = dat1)\n\nCoefficients:\n(Intercept)     SLPWKDAY          SBP         PHQ9     SLPTROUB  \n    5.26266      0.52661     -0.00560     -0.02945     -0.20813  \n\n\nWe can use the extract_eq() function from the equatiomatic package to display the model attractively.\n\nextract_eq(m1, use_coefs = TRUE, coef_digits = 3,\n           wrap = TRUE, operator_location = \"start\")\n\n\\[\n\\begin{aligned}\n\\operatorname{\\widehat{SLPWKEND}} &= 5.263 + 0.527(\\operatorname{SLPWKDAY}) - 0.006(\\operatorname{SBP}) - 0.029(\\operatorname{PHQ9})\\\\\n&\\quad - 0.208(\\operatorname{SLPTROUB})\n\\end{aligned}\n\\]\n\n\nIf Harry and Sally have the same values of SLPWKDAY, SBP and SLPTROUB, but Harry’s PHQ9 is one point higher than Sally’s, then model m1 predicts that Harry will sleep 0.029 hours less than Sally on the weekends.\nA summary of the regression model m1 provides lots of useful information about the parameters (including their standard errors) and the quality of fit (at least as measured by \\(R^2\\) and adjusted \\(R^2\\).)\n\nsummary(m1)\n\n\nCall:\nlm(formula = SLPWKEND ~ SLPWKDAY + SBP + PHQ9 + SLPTROUB, data = dat1)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-5.1173 -0.9609 -0.1005  0.9248  6.3659 \n\nCoefficients:\n             Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  5.262657   0.388427  13.549  < 2e-16 ***\nSLPWKDAY     0.526612   0.028950  18.190  < 2e-16 ***\nSBP         -0.005600   0.002515  -2.227  0.02613 *  \nPHQ9        -0.029450   0.010955  -2.688  0.00727 ** \nSLPTROUB    -0.208129   0.098758  -2.107  0.03527 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.554 on 1288 degrees of freedom\nMultiple R-squared:  0.2171,    Adjusted R-squared:  0.2147 \nF-statistic:  89.3 on 4 and 1288 DF,  p-value: < 2.2e-16\n\n\n\n\n5.2.5 Using broom functions on Model m1\nIf we want to actually use the information in the model summary elsewhere, we use the tidy() and glance() functions from the broom package to help us.\n\ntidy(m1, conf.int = TRUE, conf.level = 0.90)\n\n# A tibble: 5 × 7\n  term        estimate std.error statistic  p.value conf.low conf.high\n  <chr>          <dbl>     <dbl>     <dbl>    <dbl>    <dbl>     <dbl>\n1 (Intercept)  5.26      0.388       13.5  3.41e-39  4.62      5.90   \n2 SLPWKDAY     0.527     0.0290      18.2  5.52e-66  0.479     0.574  \n3 SBP         -0.00560   0.00251     -2.23 2.61e- 2 -0.00974  -0.00146\n4 PHQ9        -0.0294    0.0110      -2.69 7.27e- 3 -0.0475   -0.0114 \n5 SLPTROUB    -0.208     0.0988      -2.11 3.53e- 2 -0.371    -0.0456 \n\n\nHere’s a cleaner presentation of the tidy() output.\n\ntidy(m1, conf.int = TRUE, conf.level = 0.90) |> \n  mutate(across(.cols = -c(term), num, digits = 3)) |>\n  gt()\n\n\n\n\n\n  \n  \n    \n      term\n      estimate\n      std.error\n      statistic\n      p.value\n      conf.low\n      conf.high\n    \n  \n  \n    (Intercept)\n5.263\n0.388\n13.549\n0.000\n4.623\n5.902\n    SLPWKDAY\n0.527\n0.029\n18.190\n0.000\n0.479\n0.574\n    SBP\n-0.006\n0.003\n-2.227\n0.026\n-0.010\n-0.001\n    PHQ9\n-0.029\n0.011\n-2.688\n0.007\n-0.047\n-0.011\n    SLPTROUB\n-0.208\n0.099\n-2.107\n0.035\n-0.371\n-0.046\n  \n  \n  \n\n\n\n\nNote that none of the 90% confidence intervals here cross zero. This just means that we have a pretty good handle on the direction of effects - for example, our estimate for the slope of SLPWKDAY is positive, suggesting that people who sleep more during the week also sleep more on the weekend, after accounting for SBP, PHQ9 and SLPTROUB.\n\nglance(m1)\n\n# A tibble: 1 × 12\n  r.squared adj.r.squa…¹ sigma stati…²  p.value    df logLik   AIC   BIC devia…³\n      <dbl>        <dbl> <dbl>   <dbl>    <dbl> <dbl>  <dbl> <dbl> <dbl>   <dbl>\n1     0.217        0.215  1.55    89.3 4.93e-67     4 -2402. 4817. 4848.   3111.\n# … with 2 more variables: df.residual <int>, nobs <int>, and abbreviated\n#   variable names ¹​adj.r.squared, ²​statistic, ³​deviance\n\n\nModel m1 shows an \\(R^2\\) value of 0.217, which means that 21.7% of the variation in our outcome SLPWKEND is accounted for by the model using SLPWKDAY, SBP, PHQ9 and SLPTROUBLE.\nThe adjusted \\(R^2\\) value isn’t a percentage or proportion of anything, but it is a handy index when comparing two models fit to the same outcome for the same observations. It penalizes the raw \\(R^2\\) value for models that require more coefficients to be fit. If the raw \\(R^2\\) is much larger than the adjusted \\(R^2\\) value, this is also an indication that the model may be “overfit” - capitalizing on noise in the data more than we’d like, so that the amount of signal in the predictors may be overstated by raw \\(R^2\\).\nHere’s a cleaner presentation of some of the more important elements in the glance() output:\n\nglance(m1) |> \n  select(r2 = r.squared, adjr2 = adj.r.squared, \n         AIC, BIC, sigma, nobs, df, df.res = df.residual) |>\n  gt()\n\n\n\n\n\n  \n  \n    \n      r2\n      adjr2\n      AIC\n      BIC\n      sigma\n      nobs\n      df\n      df.res\n    \n  \n  \n    0.217108\n0.2146766\n4816.624\n4847.613\n1.55416\n1293\n4\n1288\n  \n  \n  \n\n\n\n\n\nAIC and BIC are measures used to compare models for the same outcome using the same data, so we’ll see those again when we fit a second model to these data. In those comparisons, smaller values of AIC and BIC indicate better fitting models.\nnobs is the number of observations used to actually fit our model m1,\ndf indicates the number of degrees of freedom used by the model, and represents the number of estimated coefficients fit, while\ndf.res = nobs - df - 1 = residual degrees of freedom.\n\n\n\n5.2.6 Residual Plots for Model m1\nThe key assumptions for a linear regression model include:\n\nLinearity of the association under study\nNormality of the residuals\nConstant Variance (Homoscedasticity)\nIndependence (not an issue with cross-sectional data like this)\n\nA residual for a point in a regression model is just the observed value of our outcome (here, SLPWKEND) minus the value predicted by the model based on the predictor values (also called the fitted value.)\nThe four key plots that R will generate for you to help assess these results are shown below for model m1.\n\n## I used \n## #| fig.height: 8 \n## at the top of this code chunk\n## to make the plots tall enough to see well\n\npar(mfrow = c(2,2)); plot(m1); par(mfrow = c(1,1))\n\n\n\n\n\n5.2.6.1 Residuals vs. Fitted values\nThe top left plot (Residuals vs. Fitted Values) helps us to assess the linearity and constant variance assumptions.\n\nWe want to see a “fuzzy football” shape.\nA clear curve is indicative of a problem with linearity, and suggests that perhaps a transformation of the outcome (or perhaps one or more predictors) may be in order\nA fan shape, with much more variation at one end of the fitted values (left or right) than the other indicates a problem with the constant variance assumption, and again a transformation may be needed.\n\nThe diagonal lines we see in the Residuals vs. Fitted plot are the result of the fact that both the outcome (SLPWKEND) and a key predictor (SLPWKDAYS) aren’t really continuous in the data, as most of the responses to those questions were either integers, or used 0.5 as the fraction. So those two variables are more discrete than we might have expected.\n\n\n5.2.6.2 Normal Q-Q plot of standardized residuals\nThe top right plot (Normal Q-Q) is a Normal Q-Q plot of the standardized regression residuals for our model m1. Substantial issues with skew (a curve in the plot) or a major problem with outliers (as indicated by a reverse S-shape) indicate potential concerns with the Normality assumption. Since the y-axis here shows standardized residuals, we can also assess whether what we’re seeing is especially surprising relative to our expectations for any standardized values (for example, we should see values above +3 or below -3 approximately 3 times in 1000).\n\nRemember that this plot represents nobs = 1293 residuals, so a few values near 3 in absolute value aren’t surprising.\nWe’re looking for big deviations from Normality here.\nThe plot() function in R will always identify three of the cases, by default, in these four plots.\n\nSuppose we wanted to look at the data for case 210, identified by these plots as a potential outlier, or at least a poorly fit point.\n\ndat1_aug <- augment(m1, data = dat1)\n\ndat1_aug |> slice(210) |>\n  select(SEQN, SLPWKEND, .fitted, .resid, .std.resid, everything())\n\n# A tibble: 1 × 13\n  SEQN  SLPWK…¹ .fitted .resid .std.…² SLPWK…³   SBP  PHQ9 SLPTR…⁴ SROH     .hat\n  <chr>   <dbl>   <dbl>  <dbl>   <dbl>   <dbl> <dbl> <int>   <dbl> <fct>   <dbl>\n1 1116…      13    6.63   6.37    4.11       4   126     1       0 Good  0.00504\n# … with 2 more variables: .sigma <dbl>, .cooksd <dbl>, and abbreviated\n#   variable names ¹​SLPWKEND, ²​.std.resid, ³​SLPWKDAY, ⁴​SLPTROUB\n\n\nWe see that row 210 contains SEQN 111688, whose\n\nobserved SLPWKEND is 13\nfitted SLPWKEND is 6.63\nyielding a residual of 6.37,\nor a standardized residual of 4.11\n\nWe can use the outlierTest() function in the car package to help assess whether this value is unusual enough to merit more careful consideration. This function actually works with the studentized residual, which is similar to the standardized residual we saw above. Here, this point (SEQN 111688) is fit poorly enough to be flagged by the Bonferroni outlier test as a mean-shift outlier.\n\noutlierTest(m1)\n\n    rstudent unadjusted p-value Bonferroni p\n210 4.131972         3.8289e-05     0.049508\n\n\nHaving seen that, though, I’m going to essentially ignore it for the moment, and press on to the rest of our residual analysis.\n\n\n5.2.6.3 Scale-Location plot\nThe bottom left plot in our set of four residual plots is the Scale-Location plot, which presents the square root of the standardized residuals against the fitted values. This plot provides another check on the “equal variance” assumption - if the plot shows a clear trend either up or down as we move left to right, then that indicates an issue with constant variance. While a loess smooth is provided (red curve) to help guide our thinking, it’s important not to get too excited about small changes or changes associated with small numbers of observations.\nYou’ll also note the presence of curves (in particular, little “V” shapes) formed by the points of the plot. Again, this is caused by the discrete nature of the outcome (and one of the key predictors) and wouldn’t be evident if our outcome was more continuous.\nDespite the drop in the red loess smooth as fitted values move from 5 to about 8, I don’t see much of a pattern here to indicate trouble with non-constant variance.\n\n\n5.2.6.4 Residuals vs. Leverage plot\nThe bottom-left plot is a plot of residuals vs. leverage, with influence contours.\nHighly leveraged points have unusual combinations of predictor values.\nHighly influential points have a big impact on the model, in that the model’s coefficients or quality of fit would change markedly were those points to be removed from the model. To measure influence, we combine leverage and residuals together, with a measure like Cook’s distance.\n\nTo look for points with substantial leverage on the model by virtue of having unusual values of the predictors - look for points whose leverage is at least 3 times as large as the average leverage value.\nThe average leverage is always k/n, where k is the number of coefficients fit by the model (including the slopes and intercept), and n is the number of observations in the model.\nTo obtain the leverage values, the augment() function stores them in .hat.\nTo look for points with substantial influence on the model, that is, removing them from the model would change it substantially, consider the Cook’s distance, plotted in contours here.\nAny Cook’s distance point > 1 will likely have a substantial impact on the model.\nAny points with Cook’s distance > 0.5, if any, will be indicated in the bottom-right (Residuals vs. Leverage) plot, and are worthy of investigation.\nIn model m1, we have no points with values of Cook’s distance > 0.5. To obtain the Cook’s distance values for each point, use the augment() function, which stores them in .cooksd.\n\nHere, for example, we identify the points with largest leverage and with largest Cook’s distance, across the points used to fit m1.\n\ndat1_aug <- augment(m1, data = dat1)\n\ndat1_aug |> slice_max(.hat) |>\n  select(SEQN, .hat, .resid, .fitted, .cooksd, everything())\n\n# A tibble: 1 × 13\n  SEQN     .hat .resid .fitted .cooksd SLPWK…¹ SLPWK…²   SBP  PHQ9 SLPTR…³ SROH \n  <chr>   <dbl>  <dbl>   <dbl>   <dbl>   <dbl>   <dbl> <dbl> <int>   <dbl> <fct>\n1 123474 0.0268  -1.03    8.03 0.00250       7       8   126    25       0 Good \n# … with 2 more variables: .sigma <dbl>, .std.resid <dbl>, and abbreviated\n#   variable names ¹​SLPWKEND, ²​SLPWKDAY, ³​SLPTROUB\n\ndat1_aug |> slice_max(.cooksd) |>\n  select(SEQN, .hat, .resid, .fitted, .cooksd, everything())\n\n# A tibble: 1 × 13\n  SEQN     .hat .resid .fitted .cooksd SLPWK…¹ SLPWK…²   SBP  PHQ9 SLPTR…³ SROH \n  <chr>   <dbl>  <dbl>   <dbl>   <dbl>   <dbl>   <dbl> <dbl> <int>   <dbl> <fct>\n1 122894 0.0113   5.38    5.62  0.0277      11       2   114     2       0 Good \n# … with 2 more variables: .sigma <dbl>, .std.resid <dbl>, and abbreviated\n#   variable names ¹​SLPWKEND, ²​SLPWKDAY, ³​SLPTROUB\n\n\nIt turns out that SEQN 123474 has the largest value of leverage (.hat) and SEQN 122894 has the largest value of influence (.cooksd) in our model. We will worry about .cooksd values above 0.5, but the largest value in this model is much smaller than that, so I think we’re OK for now.\n\n\n\n5.2.7 Fitting and Displaying Model m2\nWe will now move on to compare the results of this model (m1) to a smaller model.\nOur second model, m2 is a subset of m1, including only the two predictors directly related to sleep, SLPWKDAY and SLPTROUB.\n\nm2 <- lm(SLPWKEND ~ SLPWKDAY + SLPTROUB, data = dat1)\n\nm2\n\n\nCall:\nlm(formula = SLPWKEND ~ SLPWKDAY + SLPTROUB, data = dat1)\n\nCoefficients:\n(Intercept)     SLPWKDAY     SLPTROUB  \n     4.4813       0.5301      -0.2862  \n\n\n\nextract_eq(m2, use_coefs = TRUE, coef_digits = 3,\n           wrap = TRUE, operator_location = \"start\")\n\n\\[\n\\begin{aligned}\n\\operatorname{\\widehat{SLPWKEND}} &= 4.481 + 0.53(\\operatorname{SLPWKDAY}) - 0.286(\\operatorname{SLPTROUB})\n\\end{aligned}\n\\]\n\n\nNote that the slopes of both SLPWKDAY and SLPTROUB have changed from model m1 (although not very much), and that the intercept has changed more substantially.\n\n\n5.2.8 Using broom functions on m2\n\ntidy(m2, conf.int = TRUE, conf.level = 0.90) |> \n  mutate(across(.cols = -c(term), num, digits = 3)) |>\n  gt()\n\n\n\n\n\n  \n  \n    \n      term\n      estimate\n      std.error\n      statistic\n      p.value\n      conf.low\n      conf.high\n    \n  \n  \n    (Intercept)\n4.481\n0.219\n20.475\n0.000\n4.121\n4.842\n    SLPWKDAY\n0.530\n0.029\n18.268\n0.000\n0.482\n0.578\n    SLPTROUB\n-0.286\n0.095\n-3.025\n0.003\n-0.442\n-0.130\n  \n  \n  \n\n\n\n\n\nglance(m2) |> \n  select(r2 = r.squared, adjr2 = adj.r.squared, \n         AIC, BIC, sigma, nobs, df, df.res = df.residual) |>\n  gt()\n\n\n\n\n\n  \n  \n    \n      r2\n      adjr2\n      AIC\n      BIC\n      sigma\n      nobs\n      df\n      df.res\n    \n  \n  \n    0.2101295\n0.2089049\n4824.099\n4844.758\n1.559861\n1293\n2\n1290\n  \n  \n  \n\n\n\n\nSince we want to compare the fit of m1 to that of m2, we probably want to do so in a single table, like this:\n\ntemp1 <- glance(m1) |> \n  select(r2 = r.squared, adjr2 = adj.r.squared, \n         AIC, BIC, sigma, nobs, df, df.res = df.residual) |>\n  mutate(model = \"m1\") |>\n  relocate(model)\n\ntemp2 <- glance(m2) |> \n  select(r2 = r.squared, adjr2 = adj.r.squared, \n         AIC, BIC, sigma, nobs, df, df.res = df.residual) |>\n  mutate(model = \"m2\") |>\n  relocate(model)\n\nbind_rows(temp1, temp2) |>\n  gt()\n\n\n\n\n\n  \n  \n    \n      model\n      r2\n      adjr2\n      AIC\n      BIC\n      sigma\n      nobs\n      df\n      df.res\n    \n  \n  \n    m1\n0.2171080\n0.2146766\n4816.624\n4847.613\n1.554160\n1293\n4\n1288\n    m2\n0.2101295\n0.2089049\n4824.099\n4844.758\n1.559861\n1293\n2\n1290\n  \n  \n  \n\n\n\n\nEach model uses the same number of observations to predict the same outcome (SLPWKEND). So we can compare them directly. As compared to model m2, model m1 has:\n\nthe larger \\(R^2\\) (as it must, since model m2 includes a subset of the predictors in model m1),\nthe larger adjusted \\(R^2\\),\nthe smaller AIC (Akaike Information Criterion: smaller values are better),\nthe larger BIC (Bayes Information Criterion: again, smaller values are better),\n\nand the smaller residual standard error (\\(\\sigma\\)) (smaller values are better.)\n\nThe key realizations for these data are that the AIC, adjusted \\(R^2\\) and \\(\\sigma\\) results favor model m1 while the BIC favors model m2.\n\n\n5.2.9 Residual Plots for Model m2\n\n## I used #| fig.height: 8 in this code chunk\n## to make the plots tall enough to see well\n\npar(mfrow = c(2,2)); plot(m2); par(mfrow = c(1,1))\n\n\n\n\nThe residual plots here show (even more starkly than in model m1) the discrete nature of our outcome and the two variables we’re using to predict it. I see no especially serious problems with the assumptions of linearity or constant variance here, and while there are still some fairly poorly fit values, there are no highly influential points, so I’ll accept these residual plots as indicative of a fairly reasonable model on the whole.\n\n\n5.2.10 Conclusions\nThree of our four in-sample measures of fit quality (AIC, \\(\\sigma\\) and adjusted \\(R^2\\)) favor the larger model m1 over m2, but there’s not a lot to choose from here. Neither model showed important problems with regression assumptions, so I would probably wind up choosing m1 based on the analyses we’ve done in this Chapter.\nHowever, a more appropriate strategy for prediction assessment would be to partition the data into separate samples for model training (the development or building sample) and model testing. We adopt such a model validation strategy in our next little study."
  },
  {
    "objectID": "05-431review3.html#modeling-high-sensitivity-c-reactive-protein",
    "href": "05-431review3.html#modeling-high-sensitivity-c-reactive-protein",
    "title": "5  431 Review: Fitting Linear Models",
    "section": "5.3 Modeling High-Sensitivity C-Reactive Protein",
    "text": "5.3 Modeling High-Sensitivity C-Reactive Protein\nIn this, our second linear modeling example, we will try to predict High-Sensitivity C-Reactive Protein levels (HSCRP) on the basis of these three predictor variables:\n\nthe participant’s mean pulse rate, specifically the mean of the two gathered pulse rates, PULSE1 and PULSE2\nthe participant’s self-reported overall health (SROH, which is an ordinal factor with levels Excellent, Very Good, Good, Fair and Poor)\nHOSPITAL, a 1-0 binary variable indicating whether or not the participant was hospitalized in the past year.\n\nIn this case, we’ll use all NHANES participants with complete data on the relevant variables to fit the three-predictor model, and then a second model using mean pulse rate alone.\n\ndat2 <- nh432 |>\n  select(SEQN, HSCRP, PULSE1, PULSE2, SROH, HOSPITAL) |>\n  drop_na() |>\n  mutate(MEANPULSE = 0.5*(PULSE1 + PULSE2))\n\nglimpse(dat2)\n\nRows: 3,117\nColumns: 7\n$ SEQN      <chr> \"109271\", \"109273\", \"109291\", \"109292\", \"109293\", \"109295\", …\n$ HSCRP     <dbl> 28.68, 0.98, 5.31, 3.08, 15.10, 6.28, 0.56, 1.45, 0.32, 0.86…\n$ PULSE1    <dbl> 73, 71, 77, 93, 62, 93, 74, 59, 66, 83, 64, 55, 54, 63, 68, …\n$ PULSE2    <dbl> 71, 70, 76, 91, 64, 93, 74, 58, 64, 87, 68, 55, 54, 63, 70, …\n$ SROH      <fct> Fair, Good, Fair, Very Good, Good, Good, Very Good, Excellen…\n$ HOSPITAL  <dbl> 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ MEANPULSE <dbl> 72.0, 70.5, 76.5, 92.0, 63.0, 93.0, 74.0, 58.5, 65.0, 85.0, …\n\n\n\n5.3.1 Partitioning the Data\nBefore partitioning, it’s always a good idea to be sure that the number of rows in the tibble matches the number of distinct (unique) values in the identifier column.\n\nidentical(nrow(dat2), n_distinct(dat2 |> select(SEQN)))\n\n[1] TRUE\n\n\nOK. Now, be sure to set a seed so that we can replicate the selection. We’ll put 70% of the data in our training sample, setting aside the remaining 30% for the test sample.\n\nset.seed(432005)\n\ndat2_train <- slice_sample(dat2, prop = 0.70)\n\ndat2_test <- anti_join(dat2, dat2_train, by = \"SEQN\")\n\nc(nrow(dat2), nrow(dat2_train), nrow(dat2_test))\n\n[1] 3117 2181  936\n\n\nIn what follows, we’ll work with the dat2_train sample, and set aside the dat2_test sample for a while.\n\n\n5.3.2 Transforming the Outcome?\nLet’s use the Box-Cox approach to help us think about which potential transformations of our outcome might be helpful, within our training sample.\n\nm_temp <- lm(HSCRP ~ MEANPULSE + SROH + HOSPITAL, data = dat2_train)\n\nboxCox(m_temp)\n\n\n\n\nThe estimated \\(\\lambda\\) value is very close to 0, which according to the ladder of power transformations, suggests we take the logarithm of our outcome, so as to improve the residual plots for the model. This will also, as it turns out, lead to a much less right-skewed outcome variable.\n\np1 <- ggplot(dat2_train, aes(sample = HSCRP)) +\n  geom_qq() + geom_qq_line(col = \"red\") +\n  labs(title = \"Normal Q-Q: untransformed HSCRP\")\n\np2 <- ggplot(dat2_train, aes(sample = log(HSCRP))) +\n  geom_qq() + geom_qq_line(col = \"red\") +\n  labs(title = \"Normal Q-Q: Natural Log of HSCRP\")\n\np1 + p2\n\n\n\n\nClearly, one benefit of the transformation is some improvement in the Normality of our outcome’s distribution.\n\n\n5.3.3 Scatterplot Matrix and Collinearity\nTo build the relevant scatterplot matrix with our transformed outcome, I’ll create a variable containing the result of the transformation within our training sample.\n\ndat2_train <- dat2_train |>\n  mutate(logHSCRP = log(HSCRP))\n\nnames(dat2_train)\n\n[1] \"SEQN\"      \"HSCRP\"     \"PULSE1\"    \"PULSE2\"    \"SROH\"      \"HOSPITAL\" \n[7] \"MEANPULSE\" \"logHSCRP\" \n\nggpairs(dat2_train, columns = c(7,5,6,8), switch = \"both\",\n        lower=list(combo=wrap(\"facethist\", bins=25)))\n\n\n\n\nAs a collinearity check, we’ll run vif() from the car package here.\n\nm3 <- lm(log(HSCRP) ~ MEANPULSE + SROH + HOSPITAL,\n         data = dat2_train)\n\nvif(m3)\n\n              GVIF Df GVIF^(1/(2*Df))\nMEANPULSE 1.036382  1        1.018028\nSROH      1.063545  4        1.007731\nHOSPITAL  1.027860  1        1.013834\n\n\nAgain, no signs of meaningful collinearity. Note the presentation of the factor variable SROH in the scatterplot matrix, and in the generalized VIF output.\n\n\n5.3.4 Fit Model m3\n\nm3 <- lm(log(HSCRP) ~ MEANPULSE + SROH + HOSPITAL,\n         data = dat2_train)\n\n\nextract_eq(m3, use_coefs = TRUE, coef_digits = 3,\n           terms_per_line = 3, wrap = TRUE, \n           operator_location = \"start\")\n\n\\[\n\\begin{aligned}\n\\operatorname{\\widehat{log(HSCRP)}} &= -1.108 + 0.02(\\operatorname{MEANPULSE}) + 0.237(\\operatorname{SROH}_{\\operatorname{Very\\ Good}})\\\\\n&\\quad + 0.532(\\operatorname{SROH}_{\\operatorname{Good}}) + 0.641(\\operatorname{SROH}_{\\operatorname{Fair}}) + 0.86(\\operatorname{SROH}_{\\operatorname{Poor}})\\\\\n&\\quad + 0.052(\\operatorname{HOSPITAL})\n\\end{aligned}\n\\]\n\n\n\ntidy(m3, conf.int = TRUE, conf.level = 0.90) |>\n  mutate(across(.cols = -c(term), num, digits = 3)) |>\n  gt()\n\n\n\n\n\n  \n  \n    \n      term\n      estimate\n      std.error\n      statistic\n      p.value\n      conf.low\n      conf.high\n    \n  \n  \n    (Intercept)\n-1.108\n0.160\n-6.914\n0.000\n-1.372\n-0.845\n    MEANPULSE\n0.020\n0.002\n9.359\n0.000\n0.017\n0.024\n    SROHVery Good\n0.237\n0.081\n2.939\n0.003\n0.104\n0.370\n    SROHGood\n0.532\n0.078\n6.857\n0.000\n0.404\n0.660\n    SROHFair\n0.641\n0.087\n7.353\n0.000\n0.497\n0.784\n    SROHPoor\n0.860\n0.143\n6.020\n0.000\n0.625\n1.095\n    HOSPITAL\n0.052\n0.088\n0.594\n0.552\n-0.092\n0.197\n  \n  \n  \n\n\n\n\n\nIf Harry and Sally have the same values of HOSPITAL and MEANPULSE, but Harry’s SROH is “Very Good” while Sally’s is “Excellent”, then model m3 predicts that Harry will have a log(HSCRP) that is 0.237 (90% CI: 0.104, 0.370) larger than Sally’s log(HSCRP).\nOn the other hand, if Harry and Gary have the same values of HOSPITAL and MEANPULSE, but Harry’s SROH remains “Very Good” while Gary’s is only “Good”, then model m3 predicts that Gary will have a log(HSCRP) that is (0.532 - 0.237 = 0.295) larger than Harry’s log(HSCRP).\n\n\n\n5.3.5 Residual Plots for m3\n\n## don't forget to use #| fig.height: 8\n## to make the residual plots taller\n\npar(mfrow = c(2,2)); plot(m3); par(mfrow = c(1,1))\n\n\n\n\nI see no serious concerns with regression assumptions here. The residuals vs. fitted plot shows no signs of meaningful non-linearity or heteroscedasticity. The standardized residuals in the Normal Q-Q plot follow the reference line closely. There is no clear trend in the scale-location plot, and the residuals vs. leverage plot reveals no particularly influential points.\n\n\n5.3.6 Fit Model m4\nLet’s now fit the simple regression model, m4, with only MEANPULSE as a predictor of the log of HSCRP.\n\nm4 <- lm(log(HSCRP) ~ MEANPULSE,\n         data = dat2_train)\n\n\nextract_eq(m4, use_coefs = TRUE, coef_digits = 3,\n           wrap = TRUE, operator_location = \"start\")\n\n\\[\n\\begin{aligned}\n\\operatorname{\\widehat{log(HSCRP)}} &= -0.929 + 0.023(\\operatorname{MEANPULSE})\n\\end{aligned}\n\\]\n\n\nNow, let’s look at the tidied coefficients.\n\ntidy(m4, conf.int = TRUE, conf.level = 0.90) |>\n  mutate(across(.cols = -c(term), num, digits = 3)) |>\n  gt()\n\n\n\n\n\n  \n  \n    \n      term\n      estimate\n      std.error\n      statistic\n      p.value\n      conf.low\n      conf.high\n    \n  \n  \n    (Intercept)\n-0.929\n0.154\n-6.048\n0.000\n-1.181\n-0.676\n    MEANPULSE\n0.023\n0.002\n10.934\n0.000\n0.020\n0.027\n  \n  \n  \n\n\n\n\n\nIf Harry’s mean pulse rate is one beat per minute higher than Sally’s, then model m4 predicts that the logarithm of Harry’s HSCRP will be 0.023 higher than Sally’s, with 90% CI (0.020, 0.027).\nNote that if Harry’s mean pulse rate is ten beats per minute higher than Sally’s, then model m4 predicts that the logarithm of Harry’s HSCRP will be 0.23 higher than Sally’s, with 90% CI (0.20, 0.27).\n\n\n\n5.3.7 Residual Plots for m4\n\npar(mfrow = c(2,2)); plot(m4); par(mfrow = c(1,1))\n\n\n\n\n\n\n5.3.8 In-Sample Fit Quality Comparison (m3 vs. m4)\n\ng3 <- glance(m3) |> \n  select(r2 = r.squared, adjr2 = adj.r.squared, \n         AIC, BIC, sigma, nobs, df, df.res = df.residual) |>\n  mutate(model = \"m3\") |>\n  relocate(model)\n\ng4 <- glance(m4) |> \n  select(r2 = r.squared, adjr2 = adj.r.squared, \n         AIC, BIC, sigma, nobs, df, df.res = df.residual) |>\n  mutate(model = \"m4\") |>\n  relocate(model)\n\nbind_rows(g3, g4) |> gt()\n\n\n\n\n\n  \n  \n    \n      model\n      r2\n      adjr2\n      AIC\n      BIC\n      sigma\n      nobs\n      df\n      df.res\n    \n  \n  \n    m3\n0.09070792\n0.08819837\n6636.024\n6681.525\n1.105532\n2181\n6\n2174\n    m4\n0.05200815\n0.05157309\n6716.928\n6733.990\n1.127517\n2181\n1\n2179\n  \n  \n  \n\n\n\n\nThe larger model (model m3) has better results than model m4 in the sense that it produces a larger adjusted \\(R^2\\), and smaller values for AIC, BIC and \\(\\sigma\\). Based on this comparison within the training sample, we clearly prefer m3, since each model shows reasonable adherence to the assumptions of a linear regression model.\n\n\n5.3.9 Testing the models in new data\nAt last we return to the dat2_test sample which was not used in fitting models m3 and m4 to investigate which of these models has better predictive results in new data. When doing this sort of testing, I recommend a look at the following 4 summaries, each of which is based on the fitted (predicted) and observed values of our outcome in our new data, using the models we want to compare:\n\nsquared correlation of predicted with observed values (validated \\(R^2\\); higher values are better)\nmean absolute prediction error (MAPE; smaller values indicate smaller errors, hence better prediction)\nsquare root of the mean squared prediction error (RMSPE; again, smaller values indicate better predictions)\nmaximum (in absolute value) prediction error (Max Error)\n\nTo obtain observed, predicted, and error (observed - predicted) values for each new data point when we apply model m3, we first use the augment() function from the broom package to obtain our .fitted values.\n\nm3_test_aug <- augment(m3, newdata = dat2_test)\nhead(m3_test_aug)\n\n# A tibble: 6 × 9\n  SEQN   HSCRP PULSE1 PULSE2 SROH      HOSPITAL MEANPULSE .fitted .resid\n  <chr>  <dbl>  <dbl>  <dbl> <fct>        <dbl>     <dbl>   <dbl>  <dbl>\n1 109273  0.98     71     70 Good             0      70.5   0.839 -0.859\n2 109293 15.1      62     64 Good             0      63     0.688  2.03 \n3 109312  0.86     83     87 Very Good        0      85     0.835 -0.985\n4 109332  2.29     63     63 Excellent        0      63     0.156  0.673\n5 109340  4.64     78     78 Fair             0      78     1.10   0.437\n6 109342  5.51     72     70 Good             0      71     0.849  0.858\n\n\nRemember, however, that our models m3 and m4 do not predict HSCRP, but rather the logarithm of HSCRP, so we need to exponentiate the .fitted values to get what we want.\n\nm3_test_aug <- augment(m3, newdata = dat2_test) |>\n  mutate(fits = exp(.fitted),\n         resid = HSCRP - fits) |>\n  select(SEQN, HSCRP, fits, resid, everything())\n\nhead(m3_test_aug)\n\n# A tibble: 6 × 11\n  SEQN   HSCRP  fits resid PULSE1 PULSE2 SROH     HOSPI…¹ MEANP…² .fitted .resid\n  <chr>  <dbl> <dbl> <dbl>  <dbl>  <dbl> <fct>      <dbl>   <dbl>   <dbl>  <dbl>\n1 109273  0.98  2.31 -1.33     71     70 Good           0    70.5   0.839 -0.859\n2 109293 15.1   1.99 13.1      62     64 Good           0    63     0.688  2.03 \n3 109312  0.86  2.30 -1.44     83     87 Very Go…       0    85     0.835 -0.985\n4 109332  2.29  1.17  1.12     63     63 Excelle…       0    63     0.156  0.673\n5 109340  4.64  3.00  1.64     78     78 Fair           0    78     1.10   0.437\n6 109342  5.51  2.34  3.17     72     70 Good           0    71     0.849  0.858\n# … with abbreviated variable names ¹​HOSPITAL, ²​MEANPULSE\n\n\nNow, we can obtain our summaries, as follows.\n\nm3_test_results <- m3_test_aug |>\n  summarize(validated_R_sq = cor(HSCRP, fits)^2,\n            MAPE = mean(abs(resid)),\n            RMSPE = sqrt(mean(resid^2)),\n            max_Error = max(abs(resid)))\n\nm3_test_results\n\n# A tibble: 1 × 4\n  validated_R_sq  MAPE RMSPE max_Error\n           <dbl> <dbl> <dbl>     <dbl>\n1          0.114  3.45  10.7      177.\n\n\nFor model m4, we have:\n\nm4_test_aug <- augment(m4, newdata = dat2_test) |>\n  mutate(fits = exp(.fitted),\n         resid = HSCRP - fits) |>\n  select(SEQN, HSCRP, fits, resid, everything())\n\nm4_test_results <- m4_test_aug |>\n  summarize(validated_R_sq = cor(HSCRP, fits)^2,\n            MAPE = mean(abs(resid)),\n            RMSPE = sqrt(mean(resid^2)),\n            max_Error = max(abs(resid)))\n\nm4_test_results\n\n# A tibble: 1 × 4\n  validated_R_sq  MAPE RMSPE max_Error\n           <dbl> <dbl> <dbl>     <dbl>\n1          0.102  3.50  10.8      177.\n\n\nAnd we can put the two sets of results together into a nice table.\n\nbind_rows(m3_test_results, m4_test_results) |>\n  mutate(model = c(\"m3\", \"m4\")) |>\n  relocate(model) |>\n  gt()\n\n\n\n\n\n  \n  \n    \n      model\n      validated_R_sq\n      MAPE\n      RMSPE\n      max_Error\n    \n  \n  \n    m3\n0.1135858\n3.451295\n10.72894\n176.9855\n    m4\n0.1015280\n3.496673\n10.79613\n177.0716\n  \n  \n  \n\n\n\n\nBased on these out-of-sample validation results, it seems that Model m3 has the better results across each of these four summaries than Model 4 does.\n\n\n5.3.10 Conclusions\nWe fit two models to predict HSCRP, a larger model (m3) containing three predictors (MEANPULSE, SROH and HOSPITAL), and a smaller model (m4) containing only the MEANPULSE as a predictor.\n\nBoth models (after transforming to log(HSCRP) for our outcome) seem to generally meet the assumptions of linear regression\nModel m3 had a raw \\(R^2\\) value of 0.091, so it accounted for about 9.1% of the variation in log(HSCRP) within our training sample. Model m4 accounted for 5.2%.\nIn our in-sample checks, Model m3 had better results in terms of adjusted \\(R^2\\), AIC, BIC and \\(\\sigma\\).\nIn a validation (test) sample, our Model m3 also showed superior predictive performance, including better results in terms of MAPE, RMSPE and maximum absolute error, as well as a validated \\(R^2\\) of 11.4%, higher than model m4’s result of 10.2%.\n\nOverall, model m3 seems like the meaningfully better choice."
  },
  {
    "objectID": "06-smart.html#r-setup-used-here",
    "href": "06-smart.html#r-setup-used-here",
    "title": "6  BRFSS SMART Data",
    "section": "6.1 R Setup Used Here",
    "text": "6.1 R Setup Used Here\n\nknitr::opts_chunk$set(comment = NA)\n\nlibrary(janitor) \nlibrary(Hmisc)\nlibrary(patchwork)\nlibrary(tidyverse) \n\ntheme_set(theme_bw())"
  },
  {
    "objectID": "06-smart.html#key-resources",
    "href": "06-smart.html#key-resources",
    "title": "6  BRFSS SMART Data",
    "section": "6.2 Key resources",
    "text": "6.2 Key resources\n\nthe “raw” data, in the form of the 2017 SMART BRFSS MMSA Data, found in a zipped SAS Transport Format file. The data were released in October 2018.\nthe MMSA Variable Layout which simply lists the variables included in the data file\nthe Calculated Variables PDF which describes the risk factors by data variable names - there is also an online summary matrix of these calculated variables.\nthe lengthy 2017 Survey Questions PDF which lists all questions asked as part of the BRFSS in 2017\nthe enormous Codebook for the 2017 BRFSS Survey PDF which identifies the variables by name for us.\n\nAlso, for each subject, we are also provided with a sampling weight, in _MMSAWT, which will help us incorporate the sampling design later. These weights are at the MMSA level, and are used for generating MMSA-level estimates for variables in the data set. Details on the weighting methodology are available at this PDF."
  },
  {
    "objectID": "06-smart.html#ingesting-the-raw-data",
    "href": "06-smart.html#ingesting-the-raw-data",
    "title": "6  BRFSS SMART Data",
    "section": "6.3 Ingesting the Raw Data",
    "text": "6.3 Ingesting the Raw Data\nTo create the data files we’ll use, I used the read_xpt function from the haven package to bring in the SAS XPT data file that is provided by CDC. The codes I used (but won’t use in these Notes) were:\n\nsmart_raw <- read_xpt(\"MMSA2017/MMSA2017.xpt\")\n\nThis gives the nationwide data, which has 230,875 rows and 177 columns.\nBut for the purposes of putting these Notes online, I needed to crank down the sample size enormously. To that end, I created a new data file, which I developed by\n\nimporting the MMSA2017.xpt file as above\nfiltering away all observations except those from MMSAs which include Ohio in their name, and\nsaving the result, which now has 7,412 rows and 177 columns.\n\nThe code (again, not run here) that I used to filter to the OH-based MMSAs was:\n\nsmart_ohio_raw <- smart_raw |> \n    filter(str_detect(MMSANAME, \"OH\"))\n\nwrite_csv(smart_ohio_raw, \"data/smart_ohio_raw.csv\")\n\nSo, for purposes of these notes, our complete data set is actually coming from smart_ohio_raw.csv and consists only of the 7,412 observations associated with the six MMSAs that include Ohio in their names."
  },
  {
    "objectID": "06-smart.html#ingesting-from-our-csv-file",
    "href": "06-smart.html#ingesting-from-our-csv-file",
    "title": "6  BRFSS SMART Data",
    "section": "6.4 Ingesting from our CSV file",
    "text": "6.4 Ingesting from our CSV file\nNote that the smart_ohio_raw.csv and other data files we’re developing in this Chapter are available on our 432-Data website\n\nsmart_ohio_raw <- read_csv(\"data/smart_ohio_raw.csv\", show_col_types = FALSE)\n\ndim(smart_ohio_raw)\n\n[1] 7412  177"
  },
  {
    "objectID": "06-smart.html#what-does-the-raw-data-look-like",
    "href": "06-smart.html#what-does-the-raw-data-look-like",
    "title": "6  BRFSS SMART Data",
    "section": "6.5 What does the raw data look like?",
    "text": "6.5 What does the raw data look like?\nHere is a list of all variable names included in this file. We’re not going to use all of those variables, but this will give you a sense of what is available.\n\nnames(smart_ohio_raw)\n\n  [1] \"DISPCODE\" \"STATERE1\" \"SAFETIME\" \"HHADULT\"  \"GENHLTH\"  \"PHYSHLTH\"\n  [7] \"MENTHLTH\" \"POORHLTH\" \"HLTHPLN1\" \"PERSDOC2\" \"MEDCOST\"  \"CHECKUP1\"\n [13] \"BPHIGH4\"  \"BPMEDS\"   \"CHOLCHK1\" \"TOLDHI2\"  \"CHOLMED1\" \"CVDINFR4\"\n [19] \"CVDCRHD4\" \"CVDSTRK3\" \"ASTHMA3\"  \"ASTHNOW\"  \"CHCSCNCR\" \"CHCOCNCR\"\n [25] \"CHCCOPD1\" \"HAVARTH3\" \"ADDEPEV2\" \"CHCKIDNY\" \"DIABETE3\" \"DIABAGE2\"\n [31] \"LMTJOIN3\" \"ARTHDIS2\" \"ARTHSOCL\" \"JOINPAI1\" \"SEX\"      \"MARITAL\" \n [37] \"EDUCA\"    \"RENTHOM1\" \"NUMHHOL2\" \"NUMPHON2\" \"CPDEMO1A\" \"VETERAN3\"\n [43] \"EMPLOY1\"  \"CHILDREN\" \"INCOME2\"  \"INTERNET\" \"WEIGHT2\"  \"HEIGHT3\" \n [49] \"PREGNANT\" \"DEAF\"     \"BLIND\"    \"DECIDE\"   \"DIFFWALK\" \"DIFFDRES\"\n [55] \"DIFFALON\" \"SMOKE100\" \"SMOKDAY2\" \"STOPSMK2\" \"LASTSMK2\" \"USENOW3\" \n [61] \"ECIGARET\" \"ECIGNOW\"  \"ALCDAY5\"  \"AVEDRNK2\" \"DRNK3GE5\" \"MAXDRNKS\"\n [67] \"FRUIT2\"   \"FRUITJU2\" \"FVGREEN1\" \"FRENCHF1\" \"POTATOE1\" \"VEGETAB2\"\n [73] \"EXERANY2\" \"EXRACT11\" \"EXEROFT1\" \"EXERHMM1\" \"EXRACT21\" \"EXEROFT2\"\n [79] \"EXERHMM2\" \"STRENGTH\" \"SEATBELT\" \"FLUSHOT6\" \"FLSHTMY2\" \"PNEUVAC3\"\n [85] \"SHINGLE2\" \"HIVTST6\"  \"HIVTSTD3\" \"HIVRISK5\" \"CASTHDX2\" \"CASTHNO2\"\n [91] \"CALLBCKZ\" \"WDUSENOW\" \"WDINFTRK\" \"WDHOWOFT\" \"WDSHARE\"  \"NAMTRIBE\"\n [97] \"NAMOTHR\"  \"_URBNRRL\" \"_STSTR\"   \"_IMPSEX\"  \"_RFHLTH\"  \"_PHYS14D\"\n[103] \"_MENT14D\" \"_HCVU651\" \"_RFHYPE5\" \"_CHOLCH1\" \"_RFCHOL1\" \"_MICHD\"  \n[109] \"_LTASTH1\" \"_CASTHM1\" \"_ASTHMS1\" \"_DRDXAR1\" \"_LMTACT1\" \"_LMTWRK1\"\n[115] \"_LMTSCL1\" \"_PRACE1\"  \"_MRACE1\"  \"_HISPANC\" \"_RACE\"    \"_RACEG21\"\n[121] \"_RACEGR3\" \"_AGEG5YR\" \"_AGE65YR\" \"_AGE80\"   \"_AGE_G\"   \"WTKG3\"   \n[127] \"_BMI5\"    \"_BMI5CAT\" \"_RFBMI5\"  \"_EDUCAG\"  \"_INCOMG\"  \"_SMOKER3\"\n[133] \"_RFSMOK3\" \"_ECIGSTS\" \"_CURECIG\" \"DRNKANY5\" \"_RFBING5\" \"_DRNKWEK\"\n[139] \"_RFDRHV5\" \"FTJUDA2_\" \"FRUTDA2_\" \"GRENDA1_\" \"FRNCHDA_\" \"POTADA1_\"\n[145] \"VEGEDA2_\" \"_MISFRT1\" \"_MISVEG1\" \"_FRTRES1\" \"_VEGRES1\" \"_FRUTSU1\"\n[151] \"_VEGESU1\" \"_FRTLT1A\" \"_VEGLT1A\" \"_FRT16A\"  \"_VEG23A\"  \"_FRUITE1\"\n[157] \"_VEGETE1\" \"_TOTINDA\" \"_MINAC11\" \"_MINAC21\" \"_PACAT1\"  \"_PAINDX1\"\n[163] \"_PA150R2\" \"_PA300R2\" \"_PA30021\" \"_PASTRNG\" \"_PAREC1\"  \"_PASTAE1\"\n[169] \"_RFSEAT2\" \"_RFSEAT3\" \"_FLSHOT6\" \"_PNEUMO2\" \"_AIDTST3\" \"_MMSA\"   \n[175] \"_MMSAWT\"  \"SEQNO\"    \"MMSANAME\""
  },
  {
    "objectID": "06-smart.html#cleaning-the-brfss-data",
    "href": "06-smart.html#cleaning-the-brfss-data",
    "title": "6  BRFSS SMART Data",
    "section": "6.6 Cleaning the BRFSS Data",
    "text": "6.6 Cleaning the BRFSS Data\n\n6.6.1 Identifying Information\nThe identifying variables for each subject are gathered in SEQNO, which I’ll leave alone.\n\nEach statistical (geographic) area is identified by a _MMSA variable, which I’ll rename mmsa_code, and by an MMSANAME which I’ll rename as mmsa_name\nFor each subject, we are also provided with a sampling weight, in _MMSAWT, which will help us incorporate the sampling design later in the semester. We’ll rename this as mmsa_wt. Details on the weighting methodology are available at https://www.cdc.gov/brfss/annual_data/2017/pdf/2017_SMART_BRFSS_MMSA_Methodology-508.pdf\n\n\nsmart_ohio_raw <- smart_ohio_raw |>\n    mutate(mmsa_code = `_MMSA`,\n           mmsa_name = `MMSANAME`,\n           mmsa_wt = `_MMSAWT`)\n\nsmart_ohio_raw |> count(mmsa_code, mmsa_name)\n\n# A tibble: 6 × 3\n  mmsa_code mmsa_name                                                       n\n      <dbl> <chr>                                                       <int>\n1     17140 Cincinnati, OH-KY-IN, Metropolitan Statistical Area          1737\n2     17460 Cleveland-Elyria, OH, Metropolitan Statistical Area          1133\n3     18140 Columbus, OH, Metropolitan Statistical Area                  2033\n4     19380 Dayton, OH, Metropolitan Statistical Area                     587\n5     26580 Huntington-Ashland, WV-KY-OH, Metropolitan Statistical Area  1156\n6     45780 Toledo, OH, Metropolitan Statistical Area                     766\n\n\nThose names are very long. I’ll build some shorter ones, by dropping everything after the comma.\n\nsmart_ohio_raw <- smart_ohio_raw |>\n    mutate(mmsa = str_replace_all(string = mmsa_name, pattern=\"\\\\,.*$\",replacement=\" \"))\n\nsmart_ohio_raw |> count(mmsa, mmsa_name)\n\n# A tibble: 6 × 3\n  mmsa                  mmsa_name                                              n\n  <chr>                 <chr>                                              <int>\n1 \"Cincinnati \"         Cincinnati, OH-KY-IN, Metropolitan Statistical Ar…  1737\n2 \"Cleveland-Elyria \"   Cleveland-Elyria, OH, Metropolitan Statistical Ar…  1133\n3 \"Columbus \"           Columbus, OH, Metropolitan Statistical Area         2033\n4 \"Dayton \"             Dayton, OH, Metropolitan Statistical Area            587\n5 \"Huntington-Ashland \" Huntington-Ashland, WV-KY-OH, Metropolitan Statis…  1156\n6 \"Toledo \"             Toledo, OH, Metropolitan Statistical Area            766\n\n\nAnd here are the sampling weights for the subjects in the Cleveland-Elyria MSA.\n\nsmart_ohio_raw |> \n    filter(mmsa_code == 17460) %>%\n    ggplot(., aes(x = mmsa_wt)) +\n    geom_histogram(bins = 30, fill = \"blue\", col = \"white\")\n\n\n\n\n\n\n6.6.2 Survey Method\n\n6.6.2.1 DISPCODE and its cleanup to completed\nDISPCODE which is 1100 if the subject completed the interview, and 1200 if they partially completed the interview. We’ll create a variable called completed that indicates (1 = complete, 0 = not) whether the subject completed the interview.\n\nsmart_ohio_raw <- smart_ohio_raw |>\n    mutate(completed = 12 - (DISPCODE/100))\n\nsmart_ohio_raw |> count(DISPCODE, completed)\n\n# A tibble: 2 × 3\n  DISPCODE completed     n\n     <dbl>     <dbl> <int>\n1     1100         1  6277\n2     1200         0  1135\n\n\n\n\n6.6.2.2 STATERE1 and SAFETIME and their reduction to landline\nBRFSSS is conducted by telephone. The next two variables help us understand whether the subject was contacted via land line or via cellular phone.\n\nSTATERE1 is 1 if the subject is a resident of the state (only asked of people in the land line version of the survey).\nSAFETIME is 1 if this is a safe time to talk (only asked of people in the cell phone version of the survey).\nWe’ll use STATERE1 and SAFETIME to create an indicator variable landline that specifies how the respondent was surveyed (1 = land line, 0 = cell phone), as follows…\n\n\nsmart_ohio_raw <- smart_ohio_raw |> \n    mutate(landline = replace_na(STATERE1, 0))\n\nsmart_ohio_raw |> count(STATERE1, SAFETIME, landline)\n\n# A tibble: 2 × 4\n  STATERE1 SAFETIME landline     n\n     <dbl>    <dbl>    <dbl> <int>\n1        1       NA        1  3649\n2       NA        1        0  3763\n\n\n\n\n6.6.2.3 HHADULT and its cleanup to hhadults\n\nHHADULT is the response to “How many members of your household, including yourself, are 18 years of age or older?”\n\nThe permitted responses range from 1-76, with special values 77 for Don’t Know/Not Sure and 99 for refused, with BLANK for missing or not asked.\nSo we should change all numerical values above 76 to NA for our analyses (the blanks are already regarded as NAs by R in the ingestion process.)\n\n\n\nsmart_ohio_raw |> tabyl(HHADULT)\n\n HHADULT    n      percent valid_percent\n       1  274 0.0369670804   0.236206897\n       2  603 0.0813545602   0.519827586\n       3  170 0.0229357798   0.146551724\n       4   73 0.0098488937   0.062931034\n       5   28 0.0037776579   0.024137931\n       6    4 0.0005396654   0.003448276\n       7    3 0.0004047491   0.002586207\n       8    1 0.0001349164   0.000862069\n      10    1 0.0001349164   0.000862069\n      11    1 0.0001349164   0.000862069\n      99    2 0.0002698327   0.001724138\n      NA 6252 0.8434970318            NA\n\n\n\nsmart_ohio_raw <- smart_ohio_raw |> \n    mutate(hhadults = HHADULT,\n           hhadults = replace(hhadults, hhadults > 76, NA))\n\nsmart_ohio_raw |> count(HHADULT, hhadults) |> tail()\n\n# A tibble: 6 × 3\n  HHADULT hhadults     n\n    <dbl>    <dbl> <int>\n1       7        7     3\n2       8        8     1\n3      10       10     1\n4      11       11     1\n5      99       NA     2\n6      NA       NA  6252\n\n\n\n\n\n6.6.3 Health Status (1 item)\nThe next variable describes relate to the subject’s health status.\n\n6.6.3.1 GENHLTH and its cleanup to genhealth\n\nGENHLTH, the General Health variable, which is the response to “Would you say that in general your health is …”\n\n1 = Excellent\n2 = Very good\n3 = Good\n4 = Fair\n5 = Poor\n7 = Don’t know/Not sure\n9 = Refused\nBLANK = Not asked or missing\n\n\nTo clean up the GENHLTH data into a new variable called genhealth we’ll need to - convince R that the 7 and 9 values are in fact best interpreted as NA, - and perhaps change the variable to a factor and incorporate the names into the levels.\n\nsmart_ohio_raw <- smart_ohio_raw |> \n    mutate(genhealth = fct_recode(factor(GENHLTH), \n                                \"1_Excellent\" = \"1\",\n                                \"2_VeryGood\" = \"2\",\n                                \"3_Good\" = \"3\",\n                                \"4_Fair\" = \"4\", \n                                \"5_Poor\" = \"5\",\n                                NULL = \"7\",\n                                NULL = \"9\"))\n\nsmart_ohio_raw |> count(GENHLTH, genhealth)\n\n# A tibble: 7 × 3\n  GENHLTH genhealth       n\n    <dbl> <fct>       <int>\n1       1 1_Excellent  1057\n2       2 2_VeryGood   2406\n3       3 3_Good       2367\n4       4 4_Fair       1139\n5       5 5_Poor        428\n6       7 <NA>           10\n7       9 <NA>            5\n\n\n\n\n\n6.6.4 Healthy Days - Health-Related Quality of Life (3 items)\nThe next three variables describe the subject’s health-related quality of life.\n\n6.6.4.1 PHYSHLTH and its cleanup to physhealth\nPHYSHLTH`, the Number of Days Physical Health Not Good variable, which is the response to “Now thinking about your physical health, which includes physical illness and injury, for how many days during the past 30 days was your physical health not good?”\n\nValues of 1-30 are numeric and reasonable.\nA value of 88 indicates “none” and should be recoded to 0.\n77 is the code for Don’t know/Not sure\n99 is the code for Refused\nBLANK indicates Not asked or missing, and R recognizes this as NA properly.\n\nTo clean up PHYSHLTH to a new variable called physhealth, we’ll need: - to convince R that the 77 and 99 values are in fact best interpreted as NA, and - to convince R that the 88 should be interpreted as 0.\n\nsmart_ohio_raw <- smart_ohio_raw |> \n    mutate(physhealth = PHYSHLTH,\n           physhealth = replace(physhealth, physhealth %in% c(77, 99), NA),\n           physhealth = replace(physhealth, physhealth == 88, 0))\n\nsmart_ohio_raw |> count(PHYSHLTH, physhealth) |> tail()\n\n# A tibble: 6 × 3\n  PHYSHLTH physhealth     n\n     <dbl>      <dbl> <int>\n1       28         28    12\n2       29         29    14\n3       30         30   677\n4       77         NA   123\n5       88          0  4380\n6       99         NA    15\n\n\nNote that we present the tail of the counts in this case so we can see what happens to the key values (77, 88, 99) of our original variable PHYSHLTH.\n\n\n6.6.4.2 MENTHLTH and its cleanup to menthealth\nMENTHLTH`, the Number of Days Mental Health Not Good variable, which is the response to “Now thinking about your mental health, which includes stress, depression, and problems with emotions, for how many days during the past 30 days was your mental health not good?”\n\nThis is coded just like the PHYSHLTH variable, so we need to do the same cleaning we did there.\n\nTo clean up MENTHLTH to a new variable called menthealth, we’ll need: - to convince R that the 77 and 99 values are in fact best interpreted as NA, and - to convince R that the 88 should be interpreted as 0.\n\nsmart_ohio_raw <- smart_ohio_raw |> \n    mutate(menthealth = MENTHLTH,\n           menthealth = replace(menthealth, menthealth %in% c(77, 99), NA),\n           menthealth = replace(menthealth, menthealth == 88, 0))\n\nsmart_ohio_raw |> count(MENTHLTH, menthealth) |> tail()\n\n# A tibble: 6 × 3\n  MENTHLTH menthealth     n\n     <dbl>      <dbl> <int>\n1       28         28     7\n2       29         29    10\n3       30         30   475\n4       77         NA    86\n5       88          0  4823\n6       99         NA    28\n\n\n\n\n6.6.4.3 POORHLTH and its cleanup to poorhealth\nPOORHLTH, the Poor Physical or Mental Health variable, which is the response to “During the past 30 days, for about how many days did poor physical or mental health keep you from doing your usual activities, such as self-care, work, or recreation?”\n\nAgain, we recode just like the PHYSHLTH variable.\n\n\nsmart_ohio_raw <- smart_ohio_raw |> \n    mutate(poorhealth = POORHLTH,\n           poorhealth = replace(poorhealth, poorhealth %in% c(77, 99), NA),\n           poorhealth = replace(poorhealth, poorhealth == 88, 0))\n\nsmart_ohio_raw |> count(POORHLTH, poorhealth) |> tail()\n\n# A tibble: 6 × 3\n  POORHLTH poorhealth     n\n     <dbl>      <dbl> <int>\n1       29         29     4\n2       30         30   382\n3       77         NA    64\n4       88          0  2194\n5       99         NA    11\n6       NA         NA  3337\n\n\nThere’s a lot more missingness in the poorhealth counts than in the other health-related quality of life measures. There’s also a strong mode at 0, and a smaller mode at 30 in each variable.\n\np1 <- ggplot(smart_ohio_raw, aes(x = physhealth)) +\n    geom_histogram(binwidth = 1, fill = \"orange\") + \n    labs(title = paste0(\"Bad Physical Health Days (\",\n                        sum(is.na(smart_ohio_raw$physhealth)),\n                        \" NA)\"))\n\np2 <- ggplot(smart_ohio_raw, aes(x = menthealth)) +\n    geom_histogram(binwidth = 1, fill = \"blue\") + \n    labs(title = paste0(\"Bad Mental Health Days (\",\n                        sum(is.na(smart_ohio_raw$menthealth)), \n                        \" NA)\"))\n\np3 <- ggplot(smart_ohio_raw, aes(x = poorhealth)) +\n    geom_histogram(binwidth = 1, fill = \"red\") + \n    labs(title = paste0(\"Unable to Do Usual Activities Days (\",\n                        sum(is.na(smart_ohio_raw$poorhealth)), \n                        \" NA)\"))\n\n(p1 + p2) / p3 +\n    plot_annotation(title = \"Health Related Quality of Life Measures in BRFSS/SMART (Ohio MMSAs)\")\n\n\n\n\n\n\n\n6.6.5 Health Care Access (4 items)\nThe next four variables relate to the subject’s health care access.\n\n6.6.5.1 HLTHPLN1 and its cleanup to healthplan\nHLTHPLN1, the Have any health care coverage variable, is the response to “Do you have any kind of health care coverage, including health insurance, prepaid plans such as HMOs, or government plans such as Medicare, or Indian Health Service?”\n\n1 = Yes\n2 = No\n7 = Don’t know/Not sure\n9 = Refused\n\nTo clean up the HLTHPLN1 data into a new variable called healthplan we’ll\n- convince R that the 7 and 9 values are in fact best interpreted as NA, - and turn it into an indicator variable, e.g., we will leave the variable as numeric, but change the values to 1 = Yes and 0 = No.\n\nsmart_ohio_raw <- smart_ohio_raw |> \n    mutate(healthplan = HLTHPLN1,\n           healthplan = replace(healthplan, healthplan %in% c(7, 9), NA),\n           healthplan = replace(healthplan, healthplan == 2, 0))\n\nsmart_ohio_raw |> count(HLTHPLN1, healthplan)\n\n# A tibble: 4 × 3\n  HLTHPLN1 healthplan     n\n     <dbl>      <dbl> <int>\n1        1          1  6994\n2        2          0   398\n3        7         NA    10\n4        9         NA    10\n\n\n\n\n6.6.5.2 PERSDOC2 and its cleanup to hasdoc and to numdocs2\nPERSDOC2, the Multiple Health Care Professionals variable, is the response to “Do you have one person you think of as your personal doctor or health care provider?” where if the response is “No”, the survey then asks “Is there more than one or is there no person who you think of as your personal doctor or health care provider?”\n\n1 = Yes, only one\n2 = More than one\n3 = No\n7 = Don’t know/Not sure\n9 = Refused\nBLANK = Not asked or missing\n\nTo clean up the PERSDOC2 data into a new variable called hasdoc we’ll\n- convince R that the 7 and 9 values are in fact best interpreted as NA, - and turn it into an indicator variable, e.g., we will leave the variable as numeric, but change the values to 1 = Yes and 0 = No, so that the original 1 and 2 become 1, and the original 3 becomes 0.\n\nsmart_ohio_raw <- smart_ohio_raw |> \n    mutate(hasdoc = PERSDOC2,\n           hasdoc = replace(hasdoc, hasdoc %in% c(7, 9), NA),\n           hasdoc = replace(hasdoc, hasdoc %in% c(1, 2), 1),\n           hasdoc = replace(hasdoc, hasdoc == 3, 0))\n\nsmart_ohio_raw |> count(PERSDOC2, hasdoc)\n\n# A tibble: 5 × 3\n  PERSDOC2 hasdoc     n\n     <dbl>  <dbl> <int>\n1        1      1  5784\n2        2      1   623\n3        3      0   990\n4        7     NA    14\n5        9     NA     1\n\n\n\n\n6.6.5.3 MEDCOST and its cleanup to costprob\nMEDCOST, the Could Not See Doctor Because of Cost variable, is the response to “Was there a time in the past 12 months when you needed to see a doctor but could not because of cost?”\n\n1 = Yes\n2 = No\n7 = Don’t know/Not sure\n9 = Refused\nBLANK = Not asked or missing\n\nThis is just like HLTHPLAN.\n\nsmart_ohio_raw <- smart_ohio_raw |> \n    mutate(costprob = MEDCOST,\n           costprob = replace(costprob, costprob %in% c(7, 9), NA),\n           costprob = replace(costprob, costprob == 2, 0))\n\nsmart_ohio_raw |> count(MEDCOST, costprob)\n\n# A tibble: 4 × 3\n  MEDCOST costprob     n\n    <dbl>    <dbl> <int>\n1       1        1   714\n2       2        0  6680\n3       7       NA    14\n4       9       NA     4\n\n\n\n\n6.6.5.4 CHECKUP1 and its cleanup to t_checkup\nCHECKUP1, the Length of time since last routine checkup variable, is the response to “About how long has it been since you last visited a doctor for a routine checkup? [A routine checkup is a general physical exam, not an exam for a specific injury, illness, or condition.]”\n\n1 = Within past year (anytime less than 12 months ago)\n2 = Within past 2 years (1 year but less than 2 years ago)\n3 = Within past 5 years (2 years but less than 5 years ago)\n4 = 5 or more years ago\n7 = Don’t know/Not sure\n8 = Never\n9 = Refused\nBLANK = Not asked or missing\n\nTo clean up the CHECKUP1 data into a new variable called t_checkup we’ll - convince R that the 7 and 9 values are in fact best interpreted as NA, - relabel options 1, 2, 3, 4 and 8 while turning the variable into a factor.\n\nsmart_ohio_raw <- smart_ohio_raw |> \n    mutate(t_checkup = fct_recode(factor(CHECKUP1),\n                                 \"1_In-past-year\" = \"1\",\n                                 \"2_1-to-2-years\" = \"2\",\n                                 \"3_2-to-5-years\" = \"3\",\n                                 \"4_5_plus_years\" = \"4\",\n                                 \"8_Never\" = \"8\",\n                                 NULL = \"7\",\n                                 NULL = \"9\"))\n\nsmart_ohio_raw |> count(CHECKUP1, t_checkup)\n\n# A tibble: 7 × 3\n  CHECKUP1 t_checkup          n\n     <dbl> <fct>          <int>\n1        1 1_In-past-year  5803\n2        2 2_1-to-2-years   714\n3        3 3_2-to-5-years   413\n4        4 4_5_plus_years   376\n5        7 <NA>              68\n6        8 8_Never           32\n7        9 <NA>               6\n\n\n\n\n\n6.6.6 Blood Pressure (2 measures)\n\n6.6.6.1 BPHIGH4 and its cleanup to bp_high\nBPHIGH4 is asking about awareness of a hypertension diagnosis. It’s the response to the question: “Have you EVER been told by a doctor, nurse or other health professional that you have high blood pressure?” In addition, if the answer was “Yes” and the respondent is female, they were then asked “Was this only when you were pregnant?”\nThe available codes are:\n\n1 = Yes\n2 = Yes, but female told only during pregnancy\n3 = No\n4 = Told borderline high or pre-hypertensive\n7 = Don’t know/Not sure\n9 = Refused\nBLANK = Not asked or missing\n\nTo clean up the BPHIGH4 data into a new variable called bp_high we’ll - convince R that the 7 and 9 values are in fact best interpreted as NA, - relabel (and re-order) options 1, 2, 3, 4 while turning the variable into a factor.\n\nsmart_ohio_raw <- smart_ohio_raw |> \n    mutate(bp_high = fct_recode(factor(BPHIGH4),\n                                 \"0_No\" = \"3\",\n                                 \"1_Yes\" = \"1\",\n                                 \"2_Only_while_pregnant\" = \"2\",\n                                 \"4_Borderline\" = \"4\",\n                                 NULL = \"7\",\n                                 NULL = \"9\"),\n           bp_high = fct_relevel(bp_high,\n                                 \"0_No\", \"1_Yes\", \n                                 \"2_Only_while_pregnant\", \n                                 \"4_Borderline\"))\n\nsmart_ohio_raw |> count(BPHIGH4, bp_high)\n\n# A tibble: 6 × 3\n  BPHIGH4 bp_high                   n\n    <dbl> <fct>                 <int>\n1       1 1_Yes                  3161\n2       2 2_Only_while_pregnant    67\n3       3 0_No                   4114\n4       4 4_Borderline             49\n5       7 <NA>                     19\n6       9 <NA>                      2\n\n\n\n\n6.6.6.2 BPMEDS and its cleanup to bp_meds\nBPMEDS is the response to the question “Are you currently taking medicine for your high blood pressure?”\n\n1 = Yes\n2 = No\n7 = Don’t know/Not sure\n9 = Refused\nBLANK = Not asked or missing\n\nTo clean up the BPMEDS data into a new variable called bp_meds we’ll treat it just as we did with HLTHPLN1 and - convince R that the 7 and 9 values are in fact best interpreted as NA, - and turn it into an indicator variable, e.g., we will leave the variable as numeric, but change the values to 1 = Yes and 0 = No.\n\nsmart_ohio_raw <- smart_ohio_raw |> \n    mutate(bp_meds = BPMEDS,\n           bp_meds = replace(bp_meds, bp_meds %in% c(7, 9), NA),\n           bp_meds = replace(bp_meds, bp_meds == 2, 0))\n\nsmart_ohio_raw |> count(BPMEDS, bp_meds)\n\n# A tibble: 5 × 3\n  BPMEDS bp_meds     n\n   <dbl>   <dbl> <int>\n1      1       1  2675\n2      2       0   481\n3      7      NA     4\n4      9      NA     1\n5     NA      NA  4251\n\n\nWhat is the relationship between our two blood pressure variables? Only the people with bp_meds = “1_Yes” were asked the bp_meds question.\n\nsmart_ohio_raw |> tabyl(bp_high, bp_meds)\n\n               bp_high   0    1  NA_\n                  0_No   0    0 4114\n                 1_Yes 481 2675    5\n 2_Only_while_pregnant   0    0   67\n          4_Borderline   0    0   49\n                  <NA>   0    0   21\n\n\n\n\n\n6.6.7 Cholesterol (3 items)\n\n6.6.7.1 CHOLCHK1 and its cleanup to t_chol\nCHOLCHK1, the Length of time since cholesterol was checked, is the response to “Blood cholesterol is a fatty substance found in the blood. About how long has it been since you last had your blood cholesterol checked?”\n\n1 = Never\n2 = Within past year (anytime less than 12 months ago)\n3 = Within past 2 years (1 year but less than 2 years ago)\n4 = Within past 5 years (2 years but less than 5 years ago)\n5 = 5 or more years ago\n7 = Don’t know/Not sure\n9 = Refused\nBLANK = Not asked or missing\n\nTo clean up the CHOLCHK1 data into a new variable called t_chol we’ll - convince R that the 7 and 9 values are in fact best interpreted as NA, - relabel options 1, 2, 3, 4 and 8 while turning the variable into a factor.\n\nsmart_ohio_raw <- smart_ohio_raw |> \n    mutate(t_chol = fct_recode(factor(CHOLCHK1),\n                                 \"1_Never\" = \"1\",\n                                 \"2_In-past-year\" = \"2\",\n                                 \"3_1-to-2-years\" = \"3\",\n                                 \"4_2-to-5-years\" = \"4\",\n                                 \"5_5_plus_years\" = \"5\",\n                                 NULL = \"7\",\n                                 NULL = \"9\"))\n\nsmart_ohio_raw |> count(CHOLCHK1, t_chol)\n\n# A tibble: 8 × 3\n  CHOLCHK1 t_chol             n\n     <dbl> <fct>          <int>\n1        1 1_Never          424\n2        2 2_In-past-year  5483\n3        3 3_1-to-2-years   559\n4        4 4_2-to-5-years   289\n5        5 5_5_plus_years   272\n6        7 <NA>             376\n7        9 <NA>               8\n8       NA <NA>               1\n\n\nThe next two measures are not gathered from the people who answered “Never” to this question.\n\n\n6.6.7.2 TOLDHI2 and its cleanup to chol_high\nTOLDHI2 is asking about awareness of a diagnosis of high cholesterol. It’s the response to the question: “Have you EVER been told by a doctor, nurse or other health professional that your blood cholesterol is high?”\nThe available codes are:\n\n1 = Yes\n2 = No\n7 = Don’t know/Not sure\n9 = Refused\nBLANK = Not asked or missing\n\nTo clean up the TOLDHI2 data into a new variable called chol_high we’ll treat it like BPMEDS and HLTHPLN1 - convince R that the 7 and 9 values are in fact best interpreted as NA, - and turn it into an indicator variable, e.g., we will leave the variable as numeric, but change the values to 1 = Yes and 0 = No.\n\nsmart_ohio_raw <- smart_ohio_raw |> \n    mutate(chol_high = TOLDHI2,\n           chol_high = replace(chol_high, chol_high %in% c(7, 9), NA),\n           chol_high = replace(chol_high, chol_high == 2, 0))\n\nsmart_ohio_raw |> count(TOLDHI2, chol_high)\n\n# A tibble: 5 × 3\n  TOLDHI2 chol_high     n\n    <dbl>     <dbl> <int>\n1       1         1  2612\n2       2         0  4286\n3       7        NA    70\n4       9        NA     4\n5      NA        NA   440\n\n\n\n\n6.6.7.3 CHOLMED1 and its cleanup to chol_meds\nCHOLMED1 is the response to the question “Are you currently taking medicine prescribed by a doctor or other health professional for your blood cholesterol?”\n\n1 = Yes\n2 = No\n7 = Don’t know/Not sure\n9 = Refused\nBLANK = Not asked or missing\n\nTo clean up the CHOLMED1 data into a new variable called chol_meds we’ll treat it just as we did with HLTHPLN1 and - convince R that the 7 and 9 values are in fact best interpreted as NA, - and turn it into an indicator variable, e.g., we will leave the variable as numeric, but change the values to 1 = Yes and 0 = No.\n\nsmart_ohio_raw <- smart_ohio_raw |> \n    mutate(chol_meds = CHOLMED1,\n           chol_meds = replace(chol_meds, chol_meds %in% c(7, 9), NA),\n           chol_meds = replace(chol_meds, chol_meds == 2, 0))\n\nsmart_ohio_raw |> count(CHOLMED1, chol_meds)\n\n# A tibble: 4 × 3\n  CHOLMED1 chol_meds     n\n     <dbl>     <dbl> <int>\n1        1         1  1781\n2        2         0   826\n3        7        NA     5\n4       NA        NA  4800\n\n\n\n\n\n6.6.8 Chronic Health Conditions (14 items)\n\n6.6.8.1 Self-reported diagnosis history (11 items)\nThe next few variables describe whether or not the subject meets a particular standard, and are all coded in the raw data the same way:\n\n1 = Yes\n2 = No\n7 = Don’t know/Not sure\n9 = Refused\nBLANK = Not asked or missing\n\nand we’ll recode them all to 1 = Yes, 0 = No, otherwise NA, as we’ve done previously.\nThe questions are all started with “Has a doctor, nurse, or other health professional ever told you that you had any of the following? For each, tell me Yes, No, or you’re Not sure.”\n\n\n\n\n\n\n\n\nOriginal\nRevised\nDetails\n\n\n\n\nCVDINFR4\nhx_mi\n(Ever told) you had a heart attack, also called a myocardial infarction?\n\n\nCVDCRHD4\nhx_chd\n(Ever told) you had angina or coronary heart disease?\n\n\nCVDSTRK3\nhx_stroke\n(Ever told) you had a stroke?\n\n\nASTHMA3\nhx_asthma\n(Ever told) you had asthma?\n\n\nASTHNOW\nnow_asthma\nDo you still have asthma? (only asked of those with Yes in ASTHMA3)\n\n\nCHCSCNCR\nhx_skinc\n(Ever told) you had skin cancer?\n\n\nCHCOCNCR\nhx_otherc\n(Ever told) you had any other types of cancer?\n\n\nCHCCOPD1\nhx_copd\n(Ever told) you have Chronic Obstructive Pulmonary Disease or COPD, emphysema or chronic bronchitis?\n\n\nHAVARTH3\nhx_arthr\n(Ever told) you have some form of arthritis, rheumatoid arthritis, gout, lupus, or fibromyalgia? (Arthritis diagnoses include: rheumatism, polymyalgia rheumatica; osteoarthritis (not osteporosis); tendonitis, bursitis, bunion, tennis elbow; carpal tunnel syndrome, tarsal tunnel syndrome; joint infection, etc.)\n\n\nADDEPEV2\nhx_depress\n(Ever told) you that you have a depressive disorder, including depression, major depression, dysthymia, or minor depression?\n\n\nCHCKIDNY\nhx_kidney\n(Ever told) you have kidney disease? Do NOT include kidney stones, bladder infection or incontinence.\n\n\n\n\nsmart_ohio_raw <- smart_ohio_raw |> \n    mutate(hx_mi = CVDINFR4,\n           hx_mi = replace(hx_mi, hx_mi %in% c(7, 9), NA),\n           hx_mi = replace(hx_mi, hx_mi == 2, 0),\n           hx_chd = CVDCRHD4,\n           hx_chd = replace(hx_chd, hx_chd %in% c(7, 9), NA),\n           hx_chd = replace(hx_chd, hx_chd == 2, 0),\n           hx_stroke = CVDSTRK3,\n           hx_stroke = replace(hx_stroke, hx_stroke %in% c(7, 9), NA),\n           hx_stroke = replace(hx_stroke, hx_stroke == 2, 0),\n           hx_asthma = ASTHMA3,\n           hx_asthma = replace(hx_asthma, hx_asthma %in% c(7, 9), NA),\n           hx_asthma = replace(hx_asthma, hx_asthma == 2, 0),\n           now_asthma = ASTHNOW,\n           now_asthma = replace(now_asthma, now_asthma %in% c(7, 9), NA),\n           now_asthma = replace(now_asthma, now_asthma == 2, 0),\n           hx_skinc = CHCSCNCR,\n           hx_skinc = replace(hx_skinc, hx_skinc %in% c(7, 9), NA),\n           hx_skinc = replace(hx_skinc, hx_skinc == 2, 0),\n           hx_otherc = CHCOCNCR,\n           hx_otherc = replace(hx_otherc, hx_otherc %in% c(7, 9), NA),\n           hx_otherc = replace(hx_otherc, hx_otherc == 2, 0),\n           hx_copd = CHCCOPD1,\n           hx_copd = replace(hx_copd, hx_copd %in% c(7, 9), NA),\n           hx_copd = replace(hx_copd, hx_copd == 2, 0),\n           hx_arthr = HAVARTH3,\n           hx_arthr = replace(hx_arthr, hx_arthr %in% c(7, 9), NA),\n           hx_arthr = replace(hx_arthr, hx_arthr == 2, 0),\n           hx_depress = ADDEPEV2,\n           hx_depress = replace(hx_depress, hx_depress %in% c(7, 9), NA),\n           hx_depress = replace(hx_depress, hx_depress == 2, 0),\n           hx_kidney = CHCKIDNY,\n           hx_kidney = replace(hx_kidney, hx_kidney %in% c(7, 9), NA),\n           hx_kidney = replace(hx_kidney, hx_kidney == 2, 0))\n\nWe definitely should have written a function to do that, of course.\n\n\n6.6.8.2 _ASTHMS1 and its cleanup to asthma\n_ASTHMS1 categorizes subjects by asthma status as:\n\n1 = Current\n2 = Former\n3 = Never\n9 = Don’t Know / Not Sure / Refused / Missing\n\nWe’ll turn this into a factor with appropriate levels and NA information.\n\nsmart_ohio_raw <- smart_ohio_raw |> \n    mutate(asthma = fct_recode(\n        factor(`_ASTHMS1`),\n        \"Current\" = \"1\",\n        \"Former\" = \"2\",\n        \"Never\" = \"3\",\n        NULL = \"9\"))\n\nsmart_ohio_raw |> count(`_ASTHMS1`, asthma)\n\n# A tibble: 4 × 3\n  `_ASTHMS1` asthma      n\n       <dbl> <fct>   <int>\n1          1 Current   734\n2          2 Former    248\n3          3 Never    6376\n4          9 <NA>       54\n\n\n\n\n6.6.8.3 DIABETE3 and its cleanup to hx_diabetes and dm_status\nDIABETE3, the (Ever told) you have diabetes variable, is the response to “(Ever told) you have diabetes (If Yes and respondent is female, ask Was this only when you were pregnant?. If Respondent says pre-diabetes or borderline diabetes, use response code 4.)”\n\n1 = Yes\n2 = Yes, but female told only during pregnancy\n3 = No\n4 = No, pre-diabetes or borderline diabetes\n7 = Don’t know/Not sure\n9 = Refused\nBLANK = Not asked or missing\n\nI’ll create one variable called hx_diabetes which is 1 if DIABETE3 = 1, and 0 otherwise, with appropriate NAs, like our other variables. Then I’ll create dm_status to include all of this information in a factor, but again recode the missing values properly.\n\nsmart_ohio_raw <- smart_ohio_raw |> \n    mutate(hx_diabetes = DIABETE3,\n           hx_diabetes = replace(hx_diabetes, hx_diabetes %in% c(7, 9), NA),\n           hx_diabetes = replace(hx_diabetes, hx_diabetes %in% 2:4, 0),\n           dm_status = fct_recode(factor(DIABETE3),\n                                  \"Diabetes\" = \"1\",\n                                  \"Pregnancy-Induced\" = \"2\",\n                                  \"No-Diabetes\" = \"3\",\n                                  \"Pre-Diabetes\" = \"4\",\n                                  NULL = \"7\",\n                                  NULL = \"9\"),\n           dm_status = fct_relevel(dm_status,\n                                   \"No-Diabetes\",\n                                   \"Pre-Diabetes\",\n                                   \"Pregnancy-Induced\",\n                                   \"Diabetes\"))\n\nsmart_ohio_raw |> count(DIABETE3, hx_diabetes, dm_status)\n\n# A tibble: 6 × 4\n  DIABETE3 hx_diabetes dm_status             n\n     <dbl>       <dbl> <fct>             <int>\n1        1           1 Diabetes           1098\n2        2           0 Pregnancy-Induced    67\n3        3           0 No-Diabetes        6100\n4        4           0 Pre-Diabetes        133\n5        7          NA <NA>                 12\n6        9          NA <NA>                  2\n\n\n\n\n6.6.8.4 DIABAGE2 and its cleanup to dm_age\nDIABAGE2, the Age When Told Diabetic variable, is the response to “How old were you when you were told you have diabetes?” It is asked only of people with DIABETE3 = 1 (Yes).\n\nThe response is 1-97, with special values 98 for Don’t Know/Not Sure and 99 for refused, with BLANK for missing or not asked. People 97 years of age and above were listed as 97.\n\n\nsmart_ohio_raw <- smart_ohio_raw |> \n    mutate(dm_age = DIABAGE2,\n           dm_age = replace(dm_age, dm_age > 97, NA))\n\nsmart_ohio_raw |> count(DIABAGE2, dm_age) |> tail()\n\n# A tibble: 6 × 3\n  DIABAGE2 dm_age     n\n     <dbl>  <dbl> <int>\n1       84     84     1\n2       85     85     2\n3       90     90     1\n4       98     NA    61\n5       99     NA     4\n6       NA     NA  6314\n\n\n\n\n\n6.6.9 Arthritis Burden (4 items)\nThe first two measures are only asked of people with hx_arthr = 1, and are coded as:\n\n1 = Yes\n2 = No\n7 = Don’t know/Not sure\n9 = Refused\nBLANK = Not asked or missing\n\nand we’ll recode them to 1 = Yes, 0 = No, otherwise NA, as we’ve done previously.\n\n6.6.9.1 LMTJOIN3 (Limited because of joint symptoms), and its cleanup to arth_lims\nThis is the response to “Are you now limited in any way in any of your usual activities because of arthritis or joint symptoms?”\n\nsmart_ohio_raw <- smart_ohio_raw |> \n    mutate(arth_lims = LMTJOIN3,\n           arth_lims = replace(arth_lims, arth_lims %in% c(7, 9), NA),\n           arth_lims = replace(arth_lims, arth_lims == 2, 0))\n\nsmart_ohio_raw |> count(hx_arthr, LMTJOIN3, arth_lims)\n\n# A tibble: 6 × 4\n  hx_arthr LMTJOIN3 arth_lims     n\n     <dbl>    <dbl>     <dbl> <int>\n1        0       NA        NA  4587\n2        1        1         1  1378\n3        1        2         0  1388\n4        1        7        NA    17\n5        1        9        NA     2\n6       NA       NA        NA    40\n\n\n\n\n6.6.9.2 ARTHDIS2 (Does Arthritis Affect Whether You Work), and its cleanup to arth_work\nThis is the response to “Do arthritis or joint symptoms now affect whether you work, the type of work you do or the amount of work you do?”\n\nsmart_ohio_raw <- smart_ohio_raw |> \n    mutate(arth_work = ARTHDIS2,\n           arth_work = replace(arth_work, arth_work %in% c(7, 9), NA),\n           arth_work = replace(arth_work, arth_work == 2, 0))\n\nsmart_ohio_raw |> count(ARTHDIS2, arth_work)\n\n# A tibble: 5 × 3\n  ARTHDIS2 arth_work     n\n     <dbl>     <dbl> <int>\n1        1         1   925\n2        2         0  1808\n3        7        NA    42\n4        9        NA    10\n5       NA        NA  4627\n\n\n\n\n6.6.9.3 ARTHSOCL (Social Activities Limited Because of Joint Symptoms) and its cleanup to arth_soc\nThis is the response to “During the past 30 days, to what extent has your arthritis or joint symptoms interfered with your normal social activities, such as going shopping, to the movies, or to religious or social gatherings?”\nThe responses are:\n\n1 = A lot\n2 = A little\n3 = Not at all\n7 = Don’t know/Not sure\n9 = Refused\nBLANK = Not asked or missing\n\n\nsmart_ohio_raw <- smart_ohio_raw |> \n    mutate(arth_soc = fct_recode(factor(ARTHSOCL),\n                                  \"A lot\" = \"1\",\n                                  \"A little\" = \"2\",\n                                  \"Not at all\" = \"3\",\n                                  NULL = \"7\",\n                                  NULL = \"9\"))\n\nsmart_ohio_raw |> count(ARTHSOCL, arth_soc)\n\n# A tibble: 6 × 3\n  ARTHSOCL arth_soc       n\n     <dbl> <fct>      <int>\n1        1 A lot        606\n2        2 A little     734\n3        3 Not at all  1427\n4        7 <NA>          15\n5        9 <NA>           3\n6       NA <NA>        4627\n\n\n\n\n6.6.9.4 JOINPAI1 (How Bad Was Joint Pain - scale of 0-10) and its cleanup to joint_pain\nThis is the response to the following question: “Please think about the past 30 days, keeping in mind all of your joint pain or aching and whether or not you have taken medication. On a scale of 0 to 10 where 0 is no pain or aching and 10 is pain or aching as bad as it can be, DURING THE PAST 30 DAYS, how bad was your joint pain ON AVERAGE?”\nThe available values are 0-10, plus codes 77 (Don’t Know / Not Sure), 99 (Refused) and BLANK.\nTo clean up JOINPAI1 to a new variable called joint_pain, we’ll need to convince R that the 77 and 99 values are, like BLANK, in fact best interpreted as NA.\n\nsmart_ohio_raw <- smart_ohio_raw |> \n    mutate(joint_pain = JOINPAI1,\n           joint_pain = replace(joint_pain, joint_pain %in% c(77, 99), NA))\n\nsmart_ohio_raw |> count(JOINPAI1, joint_pain) |> tail()\n\n# A tibble: 6 × 3\n  JOINPAI1 joint_pain     n\n     <dbl>      <dbl> <int>\n1        8          8   277\n2        9          9    72\n3       10         10   158\n4       77         NA    28\n5       99         NA     5\n6       NA         NA  4627\n\n\n\n\n\n6.6.10 Demographics (25 items)\n\n6.6.10.1 _AGEG5YR, which we’ll edit into agegroup\nThe _AGEG5YR variable is a calculated variable (by CDC) obtained from the subject’s age. Since the age data are not available, we instead get these groupings, which we’ll rearrange into the agegroup factor.\n\n\n\n_AGEG5YR\nAge range\nagegroup\n\n\n\n\n1\n18 <= AGE <= 24\n18-24\n\n\n2\n25 <= AGE <= 29\n25-29\n\n\n3\n30 <= AGE <= 34\n30-34\n\n\n4\n35 <= AGE <= 39\n35-39\n\n\n5\n40 <= AGE <= 44\n40-44\n\n\n6\n45 <= AGE <= 49\n45-49\n\n\n7\n50 <= AGE <= 54\n50-54\n\n\n8\n55 <= AGE <= 59\n55-59\n\n\n9\n60 <= AGE <= 64\n60-64\n\n\n10\n65 <= AGE <= 69\n65-69\n\n\n11\n70 <= AGE <= 74\n70-74\n\n\n12\n75 <= AGE <= 79\n75-79\n\n\n13\nAGE >= 80\n80plus\n\n\n14\nDon’t Know, Refused or Missing\nNA\n\n\n\n\nsmart_ohio_raw <- smart_ohio_raw |> \n    mutate(agegroup = fct_recode(factor(`_AGEG5YR`),\n                                \"18-24\" = \"1\",\n                                \"25-29\" = \"2\",\n                                \"30-34\" = \"3\",\n                                \"35-39\" = \"4\",\n                                \"40-44\" = \"5\",\n                                \"45-49\" = \"6\",\n                                \"50-54\" = \"7\",\n                                \"55-59\" = \"8\",\n                                \"60-64\" = \"9\",\n                                \"65-69\" = \"10\",\n                                \"70-74\" = \"11\",\n                                \"75-79\" = \"12\",\n                                \"80-96\" = \"13\",\n                                NULL = \"14\"))\n\nsmart_ohio_raw |> count(`_AGEG5YR`, agegroup)\n\n# A tibble: 14 × 3\n   `_AGEG5YR` agegroup     n\n        <dbl> <fct>    <int>\n 1          1 18-24      448\n 2          2 25-29      327\n 3          3 30-34      375\n 4          4 35-39      446\n 5          5 40-44      426\n 6          6 45-49      509\n 7          7 50-54      604\n 8          8 55-59      786\n 9          9 60-64      837\n10         10 65-69      810\n11         11 70-74      685\n12         12 75-79      499\n13         13 80-96      592\n14         14 <NA>        68\n\n\n\n\n6.6.10.2 _MRACE1 recoded to race\nWe’ll create three variables describing race/ethnicity. The first comes from the _MRACE1 variable categorized by CDC, and the available responses are:\n\n1 = White only\n2 = Black or African-American only\n3 = American Indian or Alaskan Native only\n4 = Asian only\n5 = Native Hawaiian or Pacific Islander only\n6 = Other race only\n7 = Multiracial\n77 = Don’t know / Not Sure\n99 = Refused\nBLANK = Missing\n\nWe’ll create a factor out of this information, with appropriate level names.\n\nsmart_ohio_raw <- smart_ohio_raw |> \n    mutate(race = fct_recode(factor(`_MRACE1`),\n                                  \"White\" = \"1\",\n                                  \"Black or African A\" = \"2\",\n                                  \"Amer Indian or Alaskan\" = \"3\",\n                                  \"Asian\" = \"4\", \n                                  \"Hawaiian or Pac Island\" = \"5\",\n                                  \"Other Race\" = \"6\",\n                                  \"Multiracial\" = \"7\",\n                                  NULL = \"77\",\n                                  NULL = \"99\"))\n\nsmart_ohio_raw |> count(`_MRACE1`, race)\n\n# A tibble: 9 × 3\n  `_MRACE1` race                       n\n      <dbl> <fct>                  <int>\n1         1 White                   6177\n2         2 Black or African A       739\n3         3 Amer Indian or Alaskan    66\n4         4 Asian                    115\n5         5 Hawaiian or Pac Island     5\n6         6 Other Race                43\n7         7 Multiracial              153\n8        77 <NA>                      14\n9        99 <NA>                     100\n\n\n\n\n6.6.10.3 _HISPANC recoded to hispanic\nThe _HISPANC variable specifies whether or not the respondent is of Hispanic or Latinx origin. The available responses are:\n\n1 = Hispanic, Latinx or Spanish origin\n2 = Not of Hispanic, Latinx or Spanish origin\n9 = Don’t Know, Refused, or Missing\n\nWe’ll turn the 9s into NA, and create an indicator variable (1 = Hispanic or Latinx, 0 = not)\n\nsmart_ohio_raw <- smart_ohio_raw |> \n    mutate(hispanic = 2 - `_HISPANC`,\n           hispanic = replace(hispanic, hispanic < 0, NA))\n\nsmart_ohio_raw |> count(`_HISPANC`, hispanic)\n\n# A tibble: 3 × 3\n  `_HISPANC` hispanic     n\n       <dbl>    <dbl> <int>\n1          1        1   146\n2          2        0  7217\n3          9       NA    49\n\n\n\n\n6.6.10.4 _RACEGR3 recoded to race_eth\nThe _RACEGR3 variable is a five-level combination of race and ethnicity. The responses are:\n\n1 = White non-Hispanic\n2 = Black non-Hispanic\n3 = Other race non-Hispanic\n4 = Multiracial non-Hispanic\n5 = Hispanic\n9 = Don’t Know / Not Sure / Refused\n\nWe’ll create a factor out of this information, with appropriate level names.\n\nsmart_ohio_raw <- smart_ohio_raw |> \n    mutate(race_eth = fct_recode(\n        factor(`_RACEGR3`),\n        \"White non-Hispanic\" = \"1\",\n        \"Black non-Hispanic\" = \"2\",\n        \"Other race non-Hispanic\" = \"3\",\n        \"Multiracial non-Hispanic\" = \"4\", \n        \"Hispanic\" = \"5\",\n        NULL = \"9\"))\n\nsmart_ohio_raw |> count(`_RACEGR3`, race_eth)\n\n# A tibble: 6 × 3\n  `_RACEGR3` race_eth                     n\n       <dbl> <fct>                    <int>\n1          1 White non-Hispanic        6086\n2          2 Black non-Hispanic         725\n3          3 Other race non-Hispanic    193\n4          4 Multiracial non-Hispanic   143\n5          5 Hispanic                   146\n6          9 <NA>                       119\n\n\n\n\n6.6.10.5 SEX recoded to female\nThe available levels of SEX are:\n\n1 = Male\n2 = Female\n9 = Refused\n\nWe’ll recode that to female = 1 for Female, 0 Male, otherwise NA. Note the trick here is to subtract one from the coded SEX to get the desired female, but this requires that we move 8 to NA, rather than 9.\n\nsmart_ohio_raw <- smart_ohio_raw |> \n    mutate(female = SEX - 1,\n           female = replace(female, female == 8, NA))\n\nsmart_ohio_raw |> count(SEX, female)\n\n# A tibble: 2 × 3\n    SEX female     n\n  <dbl>  <dbl> <int>\n1     1      0  3136\n2     2      1  4276\n\n\n\n\n6.6.10.6 MARITAL status, revised to marital\nThe available levels of MARITAL are:\n\n1 = Married\n2 = Divorced\n3 = Widowed\n4 = Separated\n5 = Never married\n6 = A member of an unmarried couple\n9 = Refused\nBLANK = Not asked or missing\n\nWe’ll just turn this into a factor, and move 9 to NA.\n\nsmart_ohio_raw <- smart_ohio_raw |> \n    mutate(marital = fct_recode(factor(MARITAL),\n                                \"Married\" = \"1\",\n                                \"Divorced\" = \"2\",\n                                \"Widowed\" = \"3\",\n                                \"Separated\" = \"4\",\n                                \"Never_Married\" = \"5\",\n                                \"Unmarried_Couple\" = \"6\",\n                                NULL = \"9\"))\n\nsmart_ohio_raw |> count(MARITAL, marital)\n\n# A tibble: 7 × 3\n  MARITAL marital              n\n    <dbl> <fct>            <int>\n1       1 Married           3668\n2       2 Divorced          1110\n3       3 Widowed            978\n4       4 Separated          142\n5       5 Never_Married     1248\n6       6 Unmarried_Couple   208\n7       9 <NA>                58\n\n\n\n\n6.6.10.7 EDUCA recoded to educgroup\nThe available levels of EDUCA (Education Level) are responses to: “What is the highest grade or year of school you completed?”\n\n1 = Never attended school or only kindergarten\n2 = Grades 1 through 8 (Elementary)\n3 = Grades 9 through 11 (Some high school)\n4 = Grade 12 or GED (High school graduate)\n5 = College 1 year to 3 years (Some college or technical school)\n6 = College 4 years or more (College graduate)\n9 = Refused\nBLANK = Not asked or missing\n\nWe’ll just turn this into a factor, and move 9 to NA.\n\nsmart_ohio_raw <- smart_ohio_raw |> \n    mutate(educgroup = fct_recode(factor(EDUCA),\n                                \"Kindergarten\" = \"1\",\n                                \"Elementary\" = \"2\",\n                                \"Some_HS\" = \"3\",\n                                \"HS_Grad\" = \"4\",\n                                \"Some_College\" = \"5\",\n                                \"College_Grad\" = \"6\",\n                                NULL = \"9\"))\n\nsmart_ohio_raw |> count(EDUCA, educgroup)\n\n# A tibble: 7 × 3\n  EDUCA educgroup        n\n  <dbl> <fct>        <int>\n1     1 Kindergarten     3\n2     2 Elementary     117\n3     3 Some_HS        332\n4     4 HS_Grad       2209\n5     5 Some_College  2079\n6     6 College_Grad  2646\n7     9 <NA>            26\n\n\n\n\n6.6.10.8 RENTHOM1 recoded to home_own\nThe available levels of RENTHOM1 (Own or Rent Home) are responses to: “Do you own or rent your home? (Home is defined as the place where you live most of the time/the majority of the year.)”\n\n1 = Own\n2 = Rent\n3 = Other Arrangement\n7 = Don’t know/Not Sure\n9 = Refused\nBLANK = Not asked or missing\n\nWe’ll recode as home_own = 1 if they own their home, and 0 otherwise, and dealing with missingness properly.\n\nsmart_ohio_raw <- smart_ohio_raw |> \n    mutate(home_own = RENTHOM1,\n           home_own = replace(home_own, home_own %in% c(7,9), NA),\n           home_own = replace(home_own, home_own %in% c(2,3), 0))\n\nsmart_ohio_raw |> count(RENTHOM1, home_own)\n\n# A tibble: 5 × 3\n  RENTHOM1 home_own     n\n     <dbl>    <dbl> <int>\n1        1        1  5216\n2        2        0  1793\n3        3        0   348\n4        7       NA    28\n5        9       NA    27\n\n\n\n\n6.6.10.9 CPDEMO1A and its cleanup to cell_own\nCPDEMO1A is the response to “Including phones for business and personal use, do you have a cell phone for personal use?”\nAvailable responses are:\n\n1 = Yes\n2 = No\n7 = Don’t know/Not sure\n9 = Refused\nBLANK = Not asked or missing\n\nand we’ll recode them to 1 = Yes, 0 = No, otherwise NA, as we’ve done previously.\n\nsmart_ohio_raw <- smart_ohio_raw |> \n    mutate(cell_own = 2 - CPDEMO1A,\n           cell_own = replace(cell_own, cell_own < 0, NA))\n\nsmart_ohio_raw |> count(CPDEMO1A, cell_own)\n\n# A tibble: 5 × 3\n  CPDEMO1A cell_own     n\n     <dbl>    <dbl> <int>\n1        1        1  2930\n2        2        0   698\n3        7       NA     2\n4        9       NA    19\n5       NA       NA  3763\n\n\n\n\n6.6.10.10 VETERAN3 and its cleanup to veteran\nVETERAN3, the Are You A Veteran variable, is the response to “Have you ever served on active duty in the United States Armed Forces, either in the regular military or in a National Guard or military reserve unit? (Active duty does not include training for the Reserves or National Guard, but DOES include activation, for example, for the Persian Gulf War.)”\n\n1 = Yes\n2 = No\n7 = Don’t know/Not sure\n9 = Refused\nBLANK = Not asked or missing\n\n\nsmart_ohio_raw <- smart_ohio_raw |> \n    mutate(veteran = VETERAN3,\n           veteran = replace(veteran, veteran %in% c(7, 9), NA),\n           veteran = replace(veteran, veteran == 2, 0))\n\nsmart_ohio_raw |> count(VETERAN3, veteran)\n\n# A tibble: 3 × 3\n  VETERAN3 veteran     n\n     <dbl>   <dbl> <int>\n1        1       1   927\n2        2       0  6479\n3        9      NA     6\n\n\n\n\n6.6.10.11 EMPLOY1 and its cleanup to employment\nEMPLOY1, the Employment Status variable, is the response to “Are you currently … ?”\n\n1 = Employed for wages\n2 = Self-employed\n3 = Out of work for 1 year or more\n4 = Out of work for less than 1 year\n5 = A homemaker\n6 = A student\n7 = Retired\n8 = Unable to work\n9 = Refused\nBLANK = Not asked or missing\n\nWe’ll just turn this into a factor, and move 9 to NA.\n\nsmart_ohio_raw <- smart_ohio_raw |> \n    mutate(employment = fct_recode(factor(EMPLOY1),\n                                \"Employed_for_wages\" = \"1\",\n                                \"Self-employed\" = \"2\",\n                                \"Outofwork_1yearormore\" = \"3\",\n                                \"Outofwork_lt1year\" = \"4\",\n                                \"Homemaker\" = \"5\",\n                                \"Student\" = \"6\",\n                                \"Retired\" = \"7\",\n                                \"Unable_to_work\" = \"8\",\n                                NULL = \"9\"))\n\nsmart_ohio_raw |> count(EMPLOY1, employment)\n\n# A tibble: 9 × 3\n  EMPLOY1 employment                n\n    <dbl> <fct>                 <int>\n1       1 Employed_for_wages     3119\n2       2 Self-employed           466\n3       3 Outofwork_1yearormore   254\n4       4 Outofwork_lt1year       134\n5       5 Homemaker               411\n6       6 Student                 190\n7       7 Retired                2202\n8       8 Unable_to_work          603\n9       9 <NA>                     33\n\n\n\n\n6.6.10.12 CHILDREN and its cleanup to kids\nCHILDREN, the Number of Children in Household variable, is the response to “How many children less than 18 years of age live in your household?”\n\n1-87 = legitimate responses\n88 = None\n99 = Refused\nBLANK = Not asked or missing\n\n\nsmart_ohio_raw <- smart_ohio_raw |> \n    mutate(kids = CHILDREN,\n           kids = replace(kids, kids == 99, NA),\n           kids = replace(kids, kids == 88, 0))\n\nsmart_ohio_raw |> count(CHILDREN, kids) |> tail()\n\n# A tibble: 6 × 3\n  CHILDREN  kids     n\n     <dbl> <dbl> <int>\n1        6     6     7\n2        7     7     5\n3        8     8     2\n4       12    12     1\n5       88     0  5449\n6       99    NA    43\n\n\n\n\n6.6.10.13 INCOME2 to incomegroup\nThe available levels of INCOME2 (Income Level) are responses to: “Is your annual household income from all sources …”\n\n1 = Less than $10,000\n2 = $10,000 to less than $15,000\n3 = $15,000 to less than $20,000\n4 = $20,000 to less than $25,000\n5 = $25,000 to less than $35,000\n6 = $35,000 to less than $50,000\n7 = $50,000 to less than $75,000\n8 = $75,000 or more\n77 = Don’t know/Not sure\n99 = Refused\nBLANK = Not asked or missing\n\nWe’ll just turn this into a factor, and move 77 and 99 to NA.\n\nsmart_ohio_raw <- smart_ohio_raw |> \n    mutate(incomegroup = fct_recode(factor(`INCOME2`),\n                                \"0-9K\" = \"1\",\n                                \"10-14K\" = \"2\",\n                                \"15-19K\" = \"3\",\n                                \"20-24K\" = \"4\",\n                                \"25-34K\" = \"5\",\n                                \"35-49K\" = \"6\",\n                                \"50-74K\" = \"7\",\n                                \"75K+\" = \"8\",\n                                NULL = \"77\",\n                                NULL = \"99\"))\n\nsmart_ohio_raw |> count(`INCOME2`, incomegroup)\n\n# A tibble: 11 × 3\n   INCOME2 incomegroup     n\n     <dbl> <fct>       <int>\n 1       1 0-9K          285\n 2       2 10-14K        306\n 3       3 15-19K        477\n 4       4 20-24K        589\n 5       5 25-34K        685\n 6       6 35-49K        922\n 7       7 50-74K        928\n 8       8 75K+         1910\n 9      77 <NA>          610\n10      99 <NA>          678\n11      NA <NA>           22\n\n\n\n\n6.6.10.14 INTERNET and its cleanup to internet30\nINTERNET, the Internet use in the past 30 days variable, is the response to “Have you used the internet in the past 30 days?”\n\n1 = Yes\n2 = No\n7 = Don’t know/Not sure\n9 = Refused\nBLANK = Not asked or missing\n\n\nsmart_ohio_raw <- smart_ohio_raw |> \n    mutate(internet30 = INTERNET,\n           internet30 = replace(internet30, internet30 %in% c(7, 9), NA),\n           internet30 = replace(internet30, internet30 == 2, 0))\n\nsmart_ohio_raw |> count(INTERNET, internet30)\n\n# A tibble: 5 × 3\n  INTERNET internet30     n\n     <dbl>      <dbl> <int>\n1        1          1  6020\n2        2          0  1335\n3        7         NA    10\n4        9         NA    10\n5       NA         NA    37\n\n\n\n\n6.6.10.15 WTKG3 is weight_kg\nWTKG3 is computed by CDC, as the respondent’s weight in kilograms with two implied decimal places. We calculate the actual weight in kg, with the following:\n\nsmart_ohio_raw <- smart_ohio_raw |> \n    mutate(weight_kg = WTKG3/100)\n\nsmart_ohio_raw |> count(WTKG3, weight_kg) |> tail()\n\n# A tibble: 6 × 3\n  WTKG3 weight_kg     n\n  <dbl>     <dbl> <int>\n1 19051      191.     1\n2 19278      193.     1\n3 19504      195.     1\n4 20412      204.     2\n5 20865      209.     1\n6    NA       NA    462\n\n\n\n\n6.6.10.16 HEIGHT3 is replaced with height_m\nHEIGHT3 is strangely gathered to allow people to specify their height in either feet and inches or in meters and centimeters.\n\n200-711 indicates height in feet (first digit) and inches (second two digits)\n9000 - 9998 indicates height in meters (second digit) and centimeters (last two digits)\n7777 = Don’t know/Not sure\n9999 = Refused\n\nNote that there is one impossible value of 575 in the data set. We’ll make that an NA, and we’ll also make NA any heights below 3 feet, or above 2.24 meters. Specifically, we calculate the actual height in meters, with the following:\n\nsmart_ohio_raw <- smart_ohio_raw |>\n    mutate(height_m = case_when(\n        HEIGHT3 >= 300 & HEIGHT3 <= 511 ~ round((12*floor(HEIGHT3/100) + (HEIGHT3 - 100*floor(HEIGHT3/100)))*0.0254,2),\n        HEIGHT3 >= 600 & HEIGHT3 <= 711 ~ round((12*floor(HEIGHT3/100) + (HEIGHT3 - 100*floor(HEIGHT3/100)))*0.0254,2),\n        HEIGHT3 >= 9000 & HEIGHT3 <= 9224 ~ ((HEIGHT3 - 9000)/100)))\n\nsmart_ohio_raw |> count(HEIGHT3, height_m) |> tail()\n\n# A tibble: 6 × 3\n  HEIGHT3 height_m     n\n    <dbl>    <dbl> <int>\n1     607     2.01     2\n2     608     2.03     6\n3     609     2.06     1\n4    7777    NA       27\n5    9999    NA       86\n6      NA    NA       67\n\n\n\n\n6.6.10.17 bmi is calculated from height_m and weight_kg\nWe’ll calculate body-mass index from height and weight.\n\nsmart_ohio_raw <- smart_ohio_raw |> \n    mutate(bmi = round(weight_kg/(height_m)^2,2))\n\nsmart_ohio_raw |> count(height_m, weight_kg, bmi)# |> tail()\n\n# A tibble: 1,806 × 4\n   height_m weight_kg   bmi     n\n      <dbl>     <dbl> <dbl> <int>\n 1     1.35      39.0  21.4     1\n 2     1.35      52.2  28.6     1\n 3     1.4       89.8  45.8     1\n 4     1.42      31.8  15.8     1\n 5     1.42      45.4  22.5     1\n 6     1.42      55.8  27.7     1\n 7     1.42      58.5  29.0     1\n 8     1.42      59.9  29.7     1\n 9     1.42      60.8  30.1     1\n10     1.42      71.2  35.3     1\n# … with 1,796 more rows\n\n\n\n\n6.6.10.18 bmigroup is calculated from bmi\nWe’ll then divide the respondents into adult BMI categories, in the usual way.\n\nBMI < 18.5 indicates underweight\nBMI from 18.5 up to 25 indicates normal weight\nBMI from 25 up to 30 indicates overweight\nBMI of 30 and higher indicates obesity\n\n\nsmart_ohio_raw <- smart_ohio_raw |>\n    mutate(bmigroup = factor(cut2(as.numeric(bmi), \n                           cuts = c(18.5, 25.0, 30.0))))\n\nsmart_ohio_raw |> count(bmigroup)\n\n# A tibble: 5 × 2\n  bmigroup        n\n  <fct>       <int>\n1 [13.3,18.5)   119\n2 [18.5,25.0)  2017\n3 [25.0,30.0)  2445\n4 [30.0,75.5]  2338\n5 <NA>          493\n\n\n\n\n6.6.10.19 PREGNANT and its cleanup to pregnant\nPREGNANT, the Pregnancy Status variable, is the response to “To your knowledge, are you now pregnant?”\n\n1 = Yes\n2 = No\n7 = Don’t know/Not sure\n9 = Refused\nBLANK = Not asked or missing (includes SEX = male)\n\n\nsmart_ohio_raw <- smart_ohio_raw |> \n    mutate(pregnant = PREGNANT,\n           pregnant = replace(pregnant, pregnant %in% c(7, 9), NA),\n           pregnant = replace(pregnant, pregnant == 2, 0))\n\nsmart_ohio_raw |> count(PREGNANT, pregnant)\n\n# A tibble: 5 × 3\n  PREGNANT pregnant     n\n     <dbl>    <dbl> <int>\n1        1        1    41\n2        2        0  1329\n3        7       NA     3\n4        9       NA     3\n5       NA       NA  6036\n\n\n\n\n6.6.10.20 DEAF and its cleanup to deaf\nDEAF, the Are you deaf or do you have serious difficulty hearing variable, is the response to “Are you deaf or do you have serious difficulty hearing?”\n\n1 = Yes\n2 = No\n7 = Don’t know/Not sure\n9 = Refused\nBLANK = Not asked or missing\n\n\nsmart_ohio_raw <- smart_ohio_raw |> \n    mutate(deaf = DEAF,\n           deaf = replace(deaf, deaf %in% c(7, 9), NA),\n           deaf = replace(deaf, deaf == 2, 0))\n\nsmart_ohio_raw |> count(DEAF, deaf)\n\n# A tibble: 5 × 3\n   DEAF  deaf     n\n  <dbl> <dbl> <int>\n1     1     1   708\n2     2     0  6551\n3     7    NA    15\n4     9    NA     4\n5    NA    NA   134\n\n\n\n\n6.6.10.21 BLIND and its cleanup to blind\nBLIND, the Blind or Difficulty seeing variable, is the response to “Are you blind or do you have serious difficulty seeing, even when wearing glasses?”\n\n1 = Yes\n2 = No\n7 = Don’t know/Not sure\n9 = Refused\nBLANK = Not asked or missing\n\n\nsmart_ohio_raw <- smart_ohio_raw |> \n    mutate(blind = BLIND,\n           blind = replace(blind, blind %in% c(7, 9), NA),\n           blind = replace(blind, blind == 2, 0))\n\nsmart_ohio_raw |> count(BLIND, blind)\n\n# A tibble: 5 × 3\n  BLIND blind     n\n  <dbl> <dbl> <int>\n1     1     1   415\n2     2     0  6834\n3     7    NA    14\n4     9    NA     1\n5    NA    NA   148\n\n\n\n\n6.6.10.22 DECIDE and its cleanup to decide\nDECIDE, the Difficulty Concentrating or Remembering variable, is the response to “Because of a physical, mental, or emotional condition, do you have serious difficulty concentrating, remembering, or making decisions?”\n\n1 = Yes\n2 = No\n7 = Don’t know/Not sure\n9 = Refused\nBLANK = Not asked or missing\n\n\nsmart_ohio_raw <- smart_ohio_raw |> \n    mutate(decide = DECIDE,\n           decide = replace(decide, decide %in% c(7, 9), NA),\n           decide = replace(decide, decide == 2, 0))\n\nsmart_ohio_raw |> count(DECIDE, decide)\n\n# A tibble: 5 × 3\n  DECIDE decide     n\n   <dbl>  <dbl> <int>\n1      1      1   870\n2      2      0  6348\n3      7     NA    30\n4      9     NA     2\n5     NA     NA   162\n\n\n\n\n6.6.10.23 DIFFWALK and its cleanup to diffwalk\nDIFFWALK, the Difficulty Walking or Climbing Stairs variable, is the response to “Do you have serious difficulty walking or climbing stairs?”\n\n1 = Yes\n2 = No\n7 = Don’t know/Not sure\n9 = Refused\nBLANK = Not asked or missing\n\n\nsmart_ohio_raw <- smart_ohio_raw |> \n    mutate(diffwalk = DIFFWALK,\n           diffwalk = replace(diffwalk, diffwalk %in% c(7, 9), NA),\n           diffwalk = replace(diffwalk, diffwalk == 2, 0))\n\nsmart_ohio_raw |> count(DIFFWALK, diffwalk)\n\n# A tibble: 5 × 3\n  DIFFWALK diffwalk     n\n     <dbl>    <dbl> <int>\n1        1        1  1482\n2        2        0  5738\n3        7       NA    19\n4        9       NA     2\n5       NA       NA   171\n\n\n\n\n6.6.10.24 DIFFDRES and its cleanup to diffdress\nDIFFDRES, the Difficulty Dressing or Bathing variable, is the response to “Do you have difficulty dressing or bathing?”\n\n1 = Yes\n2 = No\n7 = Don’t know/Not sure\n9 = Refused\nBLANK = Not asked or missing\n\n\nsmart_ohio_raw <- smart_ohio_raw |> \n    mutate(diffdress = DIFFDRES,\n           diffdress = replace(diffdress, diffdress %in% c(7, 9), NA),\n           diffdress = replace(diffdress, diffdress == 2, 0))\n\nsmart_ohio_raw |> count(DIFFDRES, diffdress)\n\n# A tibble: 5 × 3\n  DIFFDRES diffdress     n\n     <dbl>     <dbl> <int>\n1        1         1   352\n2        2         0  6868\n3        7        NA    12\n4        9        NA     1\n5       NA        NA   179\n\n\n\n\n6.6.10.25 DIFFALON and its cleanup to diffalone\nDIFFALON, the Difficulty Doing Errands Alone variable, is the response to “Because of a physical, mental, or emotional condition, do you have difficulty doing errands alone such as visiting a doctor’s office or shopping?”\n\n1 = Yes\n2 = No\n7 = Don’t know/Not sure\n9 = Refused\nBLANK = Not asked or missing\n\n\nsmart_ohio_raw <- smart_ohio_raw |> \n    mutate(diffalone = DIFFALON,\n           diffalone = replace(diffalone, diffalone %in% c(7, 9), NA),\n           diffalone = replace(diffalone, diffalone == 2, 0))\n\nsmart_ohio_raw |> count(DIFFALON, diffalone)\n\n# A tibble: 5 × 3\n  DIFFALON diffalone     n\n     <dbl>     <dbl> <int>\n1        1         1   636\n2        2         0  6560\n3        7        NA    15\n4        9        NA     4\n5       NA        NA   197\n\n\n\n\n\n6.6.11 Tobacco Use (2 items)\n\n6.6.11.1 SMOKE100 and its cleanup to smoke100\nSMOKE100, the Smoked at Least 100 Cigarettes variable, is the response to “Have you smoked at least 100 cigarettes in your entire life? [Note: 5 packs = 100 cigarettes]”\n\n1 = Yes\n2 = No\n7 = Don’t know/Not sure\n9 = Refused\n\n\nsmart_ohio_raw <- smart_ohio_raw |> \n    mutate(smoke100 = SMOKE100,\n           smoke100 = replace(smoke100, smoke100 %in% c(7, 9), NA),\n           smoke100 = replace(smoke100, smoke100 == 2, 0))\n\nsmart_ohio_raw |> count(SMOKE100, smoke100)\n\n# A tibble: 5 × 3\n  SMOKE100 smoke100     n\n     <dbl>    <dbl> <int>\n1        1        1  3294\n2        2        0  3881\n3        7       NA    31\n4        9       NA     4\n5       NA       NA   202\n\n\n\n\n6.6.11.2 _SMOKER3 and its cleanup to smoker\n_SMOKER3, is a calculated variable which categorizes subjects by their smoking status:\n\n1 = Current smoker who smokes daily\n2 = Current smoker but not every day\n3 = Former smoker\n4 = Never smoked\n9 = Don’t Know / Refused / Missing\n\nWe’ll reclassify this as a factor with appropriate labels and NAs.\n\nsmart_ohio_raw <- smart_ohio_raw |> \n    mutate(smoker = fct_recode(factor(`_SMOKER3`),\n                                \"Current_daily\" = \"1\",\n                                \"Current_not_daily\" = \"2\",\n                                \"Former\" = \"3\",\n                                \"Never\" = \"4\",\n                                NULL = \"9\"))\n\nsmart_ohio_raw |> count(`_SMOKER3`, smoker)\n\n# A tibble: 5 × 3\n  `_SMOKER3` smoker                n\n       <dbl> <fct>             <int>\n1          1 Current_daily       990\n2          2 Current_not_daily   300\n3          3 Former             1999\n4          4 Never              3881\n5          9 <NA>                242\n\n\n\n\n\n6.6.12 E-Cigarettes (2 items)\n\n6.6.12.1 ECIGARET and its cleanup to ecig_ever\nECIGARET, the Ever used an e-cigarette variable, is the response to “Have you ever used an e-cigarette or other electronic vaping product, even just one time, in your entire life?”\n\n1 = Yes\n2 = No\n7 = Don’t know/Not sure\n9 = Refused\n\n\nsmart_ohio_raw <- smart_ohio_raw |> \n    mutate(ecig_ever = ECIGARET,\n           ecig_ever = replace(ecig_ever, ecig_ever %in% c(7, 9), NA),\n           ecig_ever = replace(ecig_ever, ecig_ever == 2, 0))\n\nsmart_ohio_raw |> count(ECIGARET, ecig_ever)\n\n# A tibble: 5 × 3\n  ECIGARET ecig_ever     n\n     <dbl>     <dbl> <int>\n1        1         1  1354\n2        2         0  5799\n3        7        NA     9\n4        9        NA     3\n5       NA        NA   247\n\n\n\n\n6.6.12.2 _ECIGSTS and its cleanup to ecigs\n_ECIGSTS, is a calculated variable which categorizes subjects by their smoking status:\n\n1 = Current and uses daily\n2 = Current user but not every day\n3 = Former user\n4 = Never used e-cigarettes\n9 = Don’t Know / Refused / Missing\n\nWe’ll reclassify this as a factor with appropriate labels and NAs.\n\nsmart_ohio_raw <- smart_ohio_raw |> \n    mutate(ecigs = fct_recode(factor(`_ECIGSTS`),\n                                \"Current_daily\" = \"1\",\n                                \"Current_not_daily\" = \"2\",\n                                \"Former\" = \"3\",\n                                \"Never\" = \"4\",\n                                NULL = \"9\"))\n\nsmart_ohio_raw |> count(`_ECIGSTS`, ecigs)\n\n# A tibble: 5 × 3\n  `_ECIGSTS` ecigs                 n\n       <dbl> <fct>             <int>\n1          1 Current_daily       102\n2          2 Current_not_daily   165\n3          3 Former             1085\n4          4 Never              5799\n5          9 <NA>                261\n\n\n\n\n\n6.6.13 Alcohol Consumption (6 items)\n\n6.6.13.1 ALCDAY5 and its cleanup to alcdays\nALCDAY5, the Days in past 30 had alcoholic beverage variable, is the response to “During the past 30 days, how many days per week or per month did you have at least one drink of any alcoholic beverage such as beer, wine, a malt beverage or liquor?”\n\n101-107 = # of days per week (101 = 1 day per week, 107 = 7 days per week)\n201-230 = # of days in past 30 days (201 = 1 day in last 30, 230 = 30 days in last 30)\n777 = Don’t know/Not sure\n888 = No drinks in past 30 days\n999 = Refused\nBLANK = Not asked or Missing\n\nWe’re going to convert this to a single numeric value. Answers in days per week (in the past 7 days) will be converted (after rounding) to days in the past 30. This is a little bit of a mess, really, but we can do it.\n\nsmart_ohio_raw <- smart_ohio_raw |> \n    mutate(alcdays = as.numeric(ALCDAY5)) |>\n    mutate(alcdays = replace(alcdays, alcdays == 888, 0),\n           alcdays = replace(alcdays, alcdays %in% c(777, 999), NA)) |>\n    mutate(alcdays = case_when(ALCDAY5 > 199 & ALCDAY5 < 231 ~ ALCDAY5 - 200,\n                               ALCDAY5 > 100 & ALCDAY5 < 108 ~ round((ALCDAY5 - 100)*30/7,0),\n                               TRUE ~ alcdays))\n\nsmart_ohio_raw |> count(ALCDAY5, alcdays)\n\n# A tibble: 39 × 3\n   ALCDAY5 alcdays     n\n     <dbl>   <dbl> <int>\n 1     101       4   263\n 2     102       9   197\n 3     103      13   142\n 4     104      17    76\n 5     105      21    53\n 6     106      26    18\n 7     107      30   114\n 8     201       1   621\n 9     202       2   448\n10     203       3   233\n# … with 29 more rows\n\n\n\n\n6.6.13.2 AVEDRNK2 and its cleanup to avgdrinks\nAVEDRNK2, the Avg alcoholic drinks per day in past 30 variable, is the response to “One drink is equivalent to a 12-ounce beer, a 5-ounce glass of wine, or a drink with one shot of liquor. During the past 30 days, on the days when you drank, about how many drinks did you drink on the average? (A 40 ounce beer would count as 3 drinks, or a cocktail drink with 2 shots would count as 2 drinks.)”\n\n1-76 = # of drinks per day\n77 = Don’t know/Not sure\n99 = Refused\nBLANK = Not asked or Missing (always happens when ALCDAY5 = 777, 888 or 999)\n\n\nsmart_ohio_raw <- smart_ohio_raw |> \n    mutate(avgdrinks = AVEDRNK2,\n           avgdrinks = replace(avgdrinks, avgdrinks > 76, NA))\n\nsmart_ohio_raw |> count(AVEDRNK2, avgdrinks) |> tail()\n\n# A tibble: 6 × 3\n  AVEDRNK2 avgdrinks     n\n     <dbl>     <dbl> <int>\n1       42        42     1\n2       60        60     2\n3       76        76     1\n4       77        NA    46\n5       99        NA     5\n6       NA        NA  3876\n\n\n\n\n6.6.13.3 MAXDRNKS and its cleanup to maxdrinks\nMAXDRINKS, the most drinks on a single occasion in the past 30 days variable, is the response to “During the past 30 days, what is the largest number of drinks you had on any occasion?”\n\n1-76 = # of drinks\n77 = Don’t know/Not sure\n99 = Refused\nBLANK = Not asked or Missing (always happens when ALCDAY5 = 777, 888 or 999)\n\n\nsmart_ohio_raw <- smart_ohio_raw |> \n    mutate(maxdrinks = MAXDRNKS,\n           maxdrinks = replace(maxdrinks, maxdrinks > 76, NA))\n\nsmart_ohio_raw |> count(MAXDRNKS, maxdrinks) |> tail()\n\n# A tibble: 6 × 3\n  MAXDRNKS maxdrinks     n\n     <dbl>     <dbl> <int>\n1       42        42     1\n2       48        48     1\n3       76        76     2\n4       77        NA    94\n5       99        NA    11\n6       NA        NA  3899\n\n\n\n\n6.6.13.4 _RFBING5 and its cleanup to binge\n_RFBING5 identifies binge drinkers (males having five or more drinks on one occasion, females having four or more drinks on one occasion in the past 30 days)\nThe values are\n\n1 = No\n2 = Yes\n9 = Don’t Know / Refused / Missing\n\nPeople who reported no alcdays are reported here as “No”, so we’ll adjust this into an indicator variable, and create the necessary NAs.\n\nsmart_ohio_raw <- smart_ohio_raw |> \n    mutate(binge = `_RFBING5` - 1,\n           binge = replace(binge, binge > 1, NA))\n\nsmart_ohio_raw |> count(`_RFBING5`, binge)\n\n# A tibble: 3 × 3\n  `_RFBING5` binge     n\n       <dbl> <dbl> <int>\n1          1     0  6035\n2          2     1  1000\n3          9    NA   377\n\n\n\n\n6.6.13.5 _DRNKWEK and its cleanup to drinks_wk\n_DRNKWEK provides the computed number of alcoholic drinks per week, with two implied decimal places. The code 99900 is used for “Don’t know / Not sure / Refused / Missing” so we’ll fix that, and also divide by 100 to get an average with a decimal point.\nNote: We’re also going to treat all results of 100 or more drinks per week as incorrect, and thus indicate them as missing data here.\n\nsmart_ohio_raw <- smart_ohio_raw |> \n    mutate(drinks_wk = `_DRNKWEK` / 100,\n           drinks_wk = replace(drinks_wk, drinks_wk > 99, NA))\n\nsmart_ohio_raw |> count(`_DRNKWEK`, drinks_wk) |> tail(12)\n\n# A tibble: 12 × 3\n   `_DRNKWEK` drinks_wk     n\n        <dbl>     <dbl> <int>\n 1       9333      93.3     2\n 2      10000      NA       1\n 3      10500      NA       2\n 4      11667      NA       1\n 5      14000      NA       2\n 6      16800      NA       2\n 7      17500      NA       1\n 8      18200      NA       1\n 9      28000      NA       1\n10      29400      NA       1\n11      53200      NA       1\n12      99900      NA     379\n\n\n\n\n6.6.13.6 _RFDRHV5 and its cleanup to drink_heavy\n_RFDRHV5 identifies heavy drinkers (males having 14 or more drinks per week, females having 7 or more drinks per week)\nThe values are\n\n1 = No\n2 = Yes\n9 = Don’t Know / Refused / Missing\n\nPeople who reported no alcdays are reported here as “No”, so we’ll adjust this into an indicator variable, and create the necessary NAs.\n\nsmart_ohio_raw <- smart_ohio_raw |> \n    mutate(drink_heavy = `_RFDRHV5` - 1,\n           drink_heavy = replace(drink_heavy, drink_heavy > 1, NA))\n\nsmart_ohio_raw |> count(`_RFDRHV5`, drink_heavy)\n\n# A tibble: 3 × 3\n  `_RFDRHV5` drink_heavy     n\n       <dbl>       <dbl> <int>\n1          1           0  6607\n2          2           1   426\n3          9          NA   379\n\n\n\n\n\n6.6.14 Fruits and Vegetables (8 items)\n\n6.6.14.1 _FRUTSU1 and its cleanup to fruit_day\n_FRUTSU1 provides the computed number of fruit servings consumed per day, with two implied decimal places. We’ll divide by 100 to insert the decimal point.\nNote: We’re also going to treat all results exceeding 16 servings per day as implausible, and thus indicate them as missing data here, following some CDC procedures.\n\nsmart_ohio_raw <- smart_ohio_raw |> \n    mutate(fruit_day = `_FRUTSU1` / 100,\n           fruit_day = replace(fruit_day, fruit_day > 16, NA))\n\nsmart_ohio_raw |> count(`_FRUTSU1`, fruit_day) |> tail()\n\n# A tibble: 6 × 3\n  `_FRUTSU1` fruit_day     n\n       <dbl>     <dbl> <int>\n1        913      9.13     1\n2       1000     10        4\n3       1400     14        1\n4       3000     NA        1\n5       7600     NA        1\n6         NA     NA      555\n\n\n\n\n6.6.14.2 _VEGESU1 and its cleanup to veg_day\n_VEGESU1 provides the computed number of vegetable servings consumed per day, with two implied decimal places. We’ll divide by 100 to insert the decimal point.\nNote: We’re also going to treat all results exceeding 23 servings per day as implausible, and thus indicate them as missing data here, following some CDC procedures.\n\nsmart_ohio_raw <- smart_ohio_raw |> \n    mutate(veg_day = `_VEGESU1` / 100,\n           veg_day = replace(veg_day, veg_day > 23, NA))\n\nsmart_ohio_raw |> count(`_VEGESU1`, veg_day) |> tail()\n\n# A tibble: 6 × 3\n  `_VEGESU1` veg_day     n\n       <dbl>   <dbl> <int>\n1       1414    14.1     1\n2       1603    16.0     1\n3       1891    18.9     1\n4       2167    21.7     1\n5       3150    NA       1\n6         NA    NA     666\n\n\n\n\n6.6.14.3 FTJUDA2_ and its cleanup to eat_juice\nFTJUDA2_ provides the servings of fruit juice consumed per day, with two implied decimal places. We’ll divide by 100 to insert the decimal point.\nNote: We’re also going to treat all results exceeding 16 servings per day as implausible, and thus indicate them as missing data here.\n\nsmart_ohio_raw <- smart_ohio_raw |> \n    mutate(eat_juice = `FTJUDA2_` / 100,\n           eat_juice = replace(eat_juice, eat_juice > 16, NA))\n\nsmart_ohio_raw |> count(`FTJUDA2_`, eat_juice) |> tail()\n\n# A tibble: 6 × 3\n  FTJUDA2_ eat_juice     n\n     <dbl>     <dbl> <int>\n1      500         5     6\n2      600         6     1\n3      700         7     1\n4     1200        12     1\n5     7500        NA     1\n6       NA        NA   469\n\n\n\n\n6.6.14.4 FRUTDA2_ and its cleanup to eat_fruit\nFRUTDA2_ provides the servings of fruit consumed per day, with two implied decimal places. We’ll divide by 100 to insert the decimal point.\nNote: We’re also going to treat all results exceeding 16 servings per day as implausible, and thus indicate them as missing data here.\n\nsmart_ohio_raw <- smart_ohio_raw |> \n    mutate(eat_fruit = `FRUTDA2_` / 100,\n           eat_fruit = replace(eat_fruit, eat_fruit > 16, NA))\n\nsmart_ohio_raw |> count(`FRUTDA2_`, eat_fruit) |> tail()\n\n# A tibble: 6 × 3\n  FRUTDA2_ eat_fruit     n\n     <dbl>     <dbl> <int>\n1      700         7     5\n2      800         8     3\n3      900         9     1\n4     1000        10     1\n5     3000        NA     1\n6       NA        NA   456\n\n\n\n\n6.6.14.5 GRENDA1_ and its cleanup to eat_greenveg\nGRENDA1_ provides the servings of dark green vegetables consumed per day, with two implied decimal places. We’ll divide by 100 to insert the decimal point.\nNote: We’re also going to treat all results exceeding 16 servings per day as implausible, and thus indicate them as missing data here.\n\nsmart_ohio_raw <- smart_ohio_raw |> \n    mutate(eat_greenveg = `GRENDA1_` / 100,\n           eat_greenveg = replace(eat_greenveg, eat_greenveg > 16, NA))\n\nsmart_ohio_raw |> count(`GRENDA1_`, eat_greenveg) |> tail()\n\n# A tibble: 6 × 3\n  GRENDA1_ eat_greenveg     n\n     <dbl>        <dbl> <int>\n1      700         7        4\n2      786         7.86     1\n3      800         8        2\n4     2000        NA        1\n5     3000        NA        1\n6       NA        NA      447\n\n\n\n\n6.6.14.6 FRNCHDA_ and its cleanup to eat_fries\nFRNCHDA_ provides the servings of french fries consumed per day, with two implied decimal places. We’ll divide by 100 to insert the decimal point.\nNote: We’re also going to treat all results exceeding 16 servings per day as implausible, and thus indicate them as missing data here.\n\nsmart_ohio_raw <- smart_ohio_raw |> \n    mutate(eat_fries = `FRNCHDA_` / 100,\n           eat_fries = replace(eat_fries, eat_fries > 16, NA))\n\nsmart_ohio_raw |> count(`FRNCHDA_`, eat_fries) |> tail()\n\n# A tibble: 6 × 3\n  FRNCHDA_ eat_fries     n\n     <dbl>     <dbl> <int>\n1      300      3        9\n2      314      3.14     1\n3      400      4        3\n4      500      5        1\n5      700      7        1\n6       NA     NA      453\n\n\n\n\n6.6.14.7 POTADA1_ and its cleanup to eat_potato\nPOTADA1_ provides the servings of potatoes consumed per day, with two implied decimal places. We’ll divide by 100 to insert the decimal point.\nNote: We’re also going to treat all results exceeding 16 servings per day as implausible, and thus indicate them as missing data here.\n\nsmart_ohio_raw <- smart_ohio_raw |> \n    mutate(eat_potato = `POTADA1_` / 100,\n           eat_potato = replace(eat_potato, eat_potato > 16, NA))\n\nsmart_ohio_raw |> count(`POTADA1_`, eat_potato) |> tail()\n\n# A tibble: 6 × 3\n  POTADA1_ eat_potato     n\n     <dbl>      <dbl> <int>\n1      314       3.14     1\n2      329       3.29     1\n3      400       4        3\n4      471       4.71     1\n5      700       7        1\n6       NA      NA      501\n\n\n\n\n6.6.14.8 VEGEDA2_ and its cleanup to eat_otherveg\nVEGEDA2_ provides the servings of other vegetables consumed per day, with two implied decimal places. We’ll divide by 100 to insert the decimal point.\nNote: We’re also going to treat all results exceeding 16 servings per day as implausible, and thus indicate them as missing data here.\n\nsmart_ohio_raw <- smart_ohio_raw |> \n    mutate(eat_otherveg = `VEGEDA2_` / 100,\n           eat_otherveg = replace(eat_otherveg, eat_otherveg > 16, NA))\n\nsmart_ohio_raw |> count(`VEGEDA2_`, eat_otherveg) |> tail()\n\n# A tibble: 6 × 3\n  VEGEDA2_ eat_otherveg     n\n     <dbl>        <dbl> <int>\n1      600            6     3\n2      700            7    11\n3      800            8     1\n4     1000           10     2\n5     1100           11     1\n6       NA           NA   509\n\n\n\n\n\n6.6.15 Exercise and Physical Activity (8 items)\n\n6.6.15.1 _TOTINDA and its cleanup to exerany\n_TOTINDA, the Exercise in Past 30 Days variable, is the response to “During the past month, other than your regular job, did you participate in any physical activities or exercises such as running, calisthenics, golf, gardening, or walking for exercise?”\n\n1 = Yes\n2 = No\n7 = Don’t know/Not sure\n9 = Refused\nBLANK = Not asked or missing\n\nThis is just like HLTHPLAN.\n\nsmart_ohio_raw <- smart_ohio_raw |> \n    mutate(exerany = `_TOTINDA`,\n           exerany = replace(exerany, exerany %in% c(7, 9), NA),\n           exerany = replace(exerany, exerany == 2, 0))\n\nsmart_ohio_raw |> count(`_TOTINDA`, exerany)\n\n# A tibble: 3 × 3\n  `_TOTINDA` exerany     n\n       <dbl>   <dbl> <int>\n1          1       1  4828\n2          2       0  2137\n3          9      NA   447\n\n\n\n\n6.6.15.2 _PACAT1 and its cleanup to activity\n_PACAT1 contains physical activity categories, estimated from responses to the BRFSS. The categories are:\n\n1 = Highly Active\n2 = Active\n3 = Insufficiently Active\n4 = Inactive\n9 = Don’t Know / Not Sure / Refused / Missing\n\nSo we’ll create a factor.\n\nsmart_ohio_raw <- smart_ohio_raw |>\n    mutate(activity = factor(`_PACAT1`),\n           activity = fct_recode(activity,\n                               \"Highly_Active\" = \"1\",\n                               \"Active\" = \"2\",\n                               \"Insufficiently_Active\" = \"3\",\n                               \"Inactive\" = \"4\",\n                               NULL = \"9\"))\n\nsmart_ohio_raw |> count(`_PACAT1`, activity)\n\n# A tibble: 5 × 3\n  `_PACAT1` activity                  n\n      <dbl> <fct>                 <int>\n1         1 Highly_Active          2053\n2         2 Active                 1132\n3         3 Insufficiently_Active  1293\n4         4 Inactive               2211\n5         9 <NA>                    723\n\n\n\n\n6.6.15.3 _PAINDX1 and its cleanup to rec_aerobic\n_PAINDX1 indicates whether the respondent’s stated levels of physical activity meet recommendations for aerobic activity. The responses are:\n\n1 = Yes\n2 = No\n9 = Don’t know/Not sure/Refused/Missing\n\n\nsmart_ohio_raw <- smart_ohio_raw |> \n    mutate(rec_aerobic = 2 - `_PAINDX1`,\n           rec_aerobic = replace(rec_aerobic, rec_aerobic < 0, NA))\n\nsmart_ohio_raw |> count(`_PAINDX1`, rec_aerobic)\n\n# A tibble: 3 × 3\n  `_PAINDX1` rec_aerobic     n\n       <dbl>       <dbl> <int>\n1          1           1  3228\n2          2           0  3504\n3          9          NA   680\n\n\n\n\n6.6.15.4 _PASTRNG and its cleanup to rec_strength\n_PASTRNG indicates whether the respondent’s stated levels of physical activity meet recommendations for strength-building activity. The responses are:\n\n1 = Yes\n2 = No\n9 = Don’t know/Not sure/Refused/Missing\n\n\nsmart_ohio_raw <- smart_ohio_raw |> \n    mutate(rec_strength = 2 - `_PASTRNG`,\n           rec_strength = replace(rec_strength, rec_strength < 0, NA))\n\nsmart_ohio_raw |> count(`_PASTRNG`, rec_strength)\n\n# A tibble: 3 × 3\n  `_PASTRNG` rec_strength     n\n       <dbl>        <dbl> <int>\n1          1            1  1852\n2          2            0  5004\n3          9           NA   556\n\n\n\n\n6.6.15.5 EXRACT11 and its cleanup to exer1_type\nRespondents are asked “What type of physical activity or exercise did you spend the most time doing during the past month?” and these responses are gathered into a set of 76 named categories, including an “other” category. Codes 77 (Don’t Know / Not Sure) and 99 (Refused) are dropped into NA in my code below, and Code 98 (“Other type of activity”) remains. Then I went through the tedious work of converting the factor levels from numbers to names, following the value labels provided by BRFSS.\n\nsmart_ohio_raw <- smart_ohio_raw |>\n    mutate(exer1_type = factor(EXRACT11),\n           exer1_type = fct_recode(\n               exer1_type,\n               \"Active Gaming Devices\" = \"1\",\n               \"Aerobics video or class\" = \"2\",\n               \"Backpacking\" = \"3\",\n               \"Badminton\" = \"4\",\n               \"Basketball\" = \"5\",\n               \"Bicycling machine\" = \"6\",\n               \"Bicycling\" = \"7\",\n               \"Boating\" = \"8\",\n               \"Bowling\" = \"9\",\n               \"Boxing\" = \"10\",\n               \"Calisthenics\" = \"11\",\n               \"Canoeing\" = \"12\",\n               \"Carpentry\" = \"13\",\n               \"Dancing\" = \"14\",\n               \"Elliptical machine\" = \"15\",\n               \"Fishing\" = \"16\",\n               \"Frisbee\" = \"17\",\n               \"Gardening\" = \"18\",\n               \"Golf with cart\" = \"19\",\n               \"Golf without cart\" = \"20\",\n               \"Handball\" = \"21\",\n               \"Hiking\" = \"22\",\n               \"Hockey\" = \"23\",\n               \"Horseback riding\" = \"24\",\n               \"Hunting large game\" = \"25\",\n               \"Hunting small game\" = \"26\",\n               \"Inline skating\" = \"27\",\n               \"Jogging\" = \"28\",\n               \"Lacrosse\" = \"29\",\n               \"Mountain climbing\" = \"30\",\n               \"Mowing lawn\" = \"31\",\n               \"Paddleball\" = \"32\",\n               \"Painting house\" = \"33\",\n               \"Pilates\" = \"34\",\n               \"Racquetball\" = \"35\",\n               \"Raking lawn\" = \"36\",\n               \"Running\" = \"37\",\n               \"Rock climbing\" = \"38\",\n               \"Rope skipping\" = \"39\",\n               \"Rowing machine\" = \"40\",\n               \"Rugby\" = \"41\",\n               \"Scuba diving\" = \"42\",\n               \"Skateboarding\" = \"43\",\n               \"Skating\" = \"44\",\n               \"Sledding\" = \"45\",\n               \"Snorkeling\" = \"46\",\n               \"Snow blowing\" = \"47\",\n               \"Snow shoveling\" = \"48\",\n               \"Snow skiing\" = \"49\",\n               \"Snowshoeing\" = \"50\",\n               \"Soccer\" = \"51\",\n               \"Softball/Baseball\" = \"52\",\n               \"Squash\" = \"53\",\n               \"Stair Climbing\" = \"54\",\n               \"Stream fishing\" = \"55\",\n               \"Surfing\" = \"56\",\n               \"Swimming\" = \"57\",\n               \"Swimming in laps\" = \"58\",\n               \"Table tennis\" = \"59\",\n               \"Tai Chi\" = \"60\",\n               \"Tennis\" = \"61\",\n               \"Touch football\" = \"62\",\n               \"Volleyball\" = \"63\",\n               \"Walking\" = \"64\",\n               \"Waterskiing\" = \"66\",\n               \"Weight lifting\" = \"67\",\n               \"Wrestling\" = \"68\",\n               \"Yoga\" = \"69\",\n               \"Child Care\" = \"71\",\n               \"Farm Work\" = \"72\",\n               \"Household Activities\" = \"73\",\n               \"Martial Arts\" = \"74\",\n               \"Upper Body Cycle\" = \"75\",\n               \"Yard Work\" = \"76\",\n               \"Other Activities\" = \"98\",\n               NULL = \"77\", \n               NULL = \"99\")\n    )\n\nWarning: Unknown levels in `f`: 3, 17, 21, 32, 36, 41, 42, 45, 47, 53, 55, 56,\n59\n\n\nThe warning generated here is caused by the fact that some of the available types of exercise were not mentioned by people in our sample. Looking at the last few results, we can see how many people fell into several categories.\n\nsmart_ohio_raw |> count(EXRACT11, exer1_type) |> tail()\n\n# A tibble: 6 × 3\n  EXRACT11 exer1_type           n\n     <dbl> <fct>            <int>\n1       75 Upper Body Cycle     6\n2       76 Yard Work           78\n3       77 <NA>                10\n4       98 Other Activities   276\n5       99 <NA>                 4\n6       NA <NA>              2588\n\n\nThe most common activities are:\n\nsmart_ohio_raw |> count(exer1_type, sort = TRUE) |> head(10)\n\n# A tibble: 10 × 2\n   exer1_type                  n\n   <fct>                   <int>\n 1 Walking                  2605\n 2 <NA>                     2602\n 3 Running                   324\n 4 Other Activities          276\n 5 Gardening                 242\n 6 Weight lifting            189\n 7 Aerobics video or class   103\n 8 Bicycling machine         103\n 9 Bicycling                  96\n10 Golf with cart             90\n\n\n\n\n6.6.15.6 EXRACT21 and its cleanup to exer2_type\nAs a follow-up, respondents are asked “What other type of physical activity gave you the next most exercise during the past month?” and these responses are also gathered into the same set of 76 named categories, including an “other” category, but now also adding a “No Other Activity” category (code 88). Codes 77 (Don’t Know / Not Sure) and 99 (Refused) are dropped into NA in my code below, and Code 98 (“Other type of activity”) remains. Then I went through the tedious work of converting the factor levels from numbers to names, following the value labels provided by BRFSS. I’m sure there’s a better way to do this.\n\nsmart_ohio_raw <- smart_ohio_raw |>\n    mutate(exer2_type = factor(EXRACT21),\n           exer2_type = fct_recode(\n               exer2_type,\n               \"Active Gaming Devices\" = \"1\",\n               \"Aerobics video or class\" = \"2\",\n               \"Backpacking\" = \"3\",\n               \"Badminton\" = \"4\",\n               \"Basketball\" = \"5\",\n               \"Bicycling machine\" = \"6\",\n               \"Bicycling\" = \"7\",\n               \"Boating\" = \"8\",\n               \"Bowling\" = \"9\",\n               \"Boxing\" = \"10\",\n               \"Calisthenics\" = \"11\",\n               \"Canoeing\" = \"12\",\n               \"Carpentry\" = \"13\",\n               \"Dancing\" = \"14\",\n               \"Elliptical machine\" = \"15\",\n               \"Fishing\" = \"16\",\n               \"Frisbee\" = \"17\",\n               \"Gardening\" = \"18\",\n               \"Golf with cart\" = \"19\",\n               \"Golf without cart\" = \"20\",\n               \"Handball\" = \"21\",\n               \"Hiking\" = \"22\",\n               \"Hockey\" = \"23\",\n               \"Horseback riding\" = \"24\",\n               \"Hunting large game\" = \"25\",\n               \"Hunting small game\" = \"26\",\n               \"Inline skating\" = \"27\",\n               \"Jogging\" = \"28\",\n               \"Lacrosse\" = \"29\",\n               \"Mountain climbing\" = \"30\",\n               \"Mowing lawn\" = \"31\",\n               \"Paddleball\" = \"32\",\n               \"Painting house\" = \"33\",\n               \"Pilates\" = \"34\",\n               \"Racquetball\" = \"35\",\n               \"Raking lawn\" = \"36\",\n               \"Running\" = \"37\",\n               \"Rock climbing\" = \"38\",\n               \"Rope skipping\" = \"39\",\n               \"Rowing machine\" = \"40\",\n               \"Rugby\" = \"41\",\n               \"Scuba diving\" = \"42\",\n               \"Skateboarding\" = \"43\",\n               \"Skating\" = \"44\",\n               \"Sledding\" = \"45\",\n               \"Snorkeling\" = \"46\",\n               \"Snow blowing\" = \"47\",\n               \"Snow shoveling\" = \"48\",\n               \"Snow skiing\" = \"49\",\n               \"Snowshoeing\" = \"50\",\n               \"Soccer\" = \"51\",\n               \"Softball/Baseball\" = \"52\",\n               \"Squash\" = \"53\",\n               \"Stair Climbing\" = \"54\",\n               \"Stream fishing\" = \"55\",\n               \"Surfing\" = \"56\",\n               \"Swimming\" = \"57\",\n               \"Swimming in laps\" = \"58\",\n               \"Table tennis\" = \"59\",\n               \"Tai Chi\" = \"60\",\n               \"Tennis\" = \"61\",\n               \"Touch football\" = \"62\",\n               \"Volleyball\" = \"63\",\n               \"Walking\" = \"64\",\n               \"Waterskiing\" = \"66\",\n               \"Weight lifting\" = \"67\",\n               \"Wrestling\" = \"68\",\n               \"Yoga\" = \"69\",\n               \"Child Care\" = \"71\",\n               \"Farm Work\" = \"72\",\n               \"Household Activities\" = \"73\",\n               \"Martial Arts\" = \"74\",\n               \"Upper Body Cycle\" = \"75\",\n               \"Yard Work\" = \"76\",\n               \"No Other Activity\" = \"88\",\n               \"Other Activities\" = \"98\",\n               NULL = \"77\", \n               NULL = \"99\")\n    )\n\nWarning: Unknown levels in `f`: 3, 21, 30, 39, 41, 46, 50, 62\n\nsmart_ohio_raw |> count(EXRACT21, exer2_type) |> tail()\n\n# A tibble: 6 × 3\n  EXRACT21 exer2_type            n\n     <dbl> <fct>             <int>\n1       76 Yard Work           153\n2       77 <NA>                 26\n3       88 No Other Activity  1854\n4       98 Other Activities    246\n5       99 <NA>                 19\n6       NA <NA>               2627\n\n\nThe most common activity types in this group are:\n\nsmart_ohio_raw |> count(exer2_type, sort = TRUE) |> head(10)\n\n# A tibble: 10 × 2\n   exer2_type               n\n   <fct>                <int>\n 1 <NA>                  2672\n 2 No Other Activity     1854\n 3 Walking                629\n 4 Weight lifting         272\n 5 Other Activities       246\n 6 Gardening              202\n 7 Household Activities   169\n 8 Yard Work              153\n 9 Running                148\n10 Bicycling              118\n\n\n\n\n6.6.15.7 _MINAC11 and its cleanup to exer1_min\n_MINAC11 is minutes of physical activity per week for the first activity (listed as exer1_type above.) Since there are only about 10,080 minutes in a typical week, we’ll treat as implausible any values larger than 4200 minutes (which would indicate 70 hours per week.)\n\nsmart_ohio_raw <- smart_ohio_raw |> \n    mutate(exer1_min = `_MINAC11`,\n           exer1_min = replace(exer1_min, exer1_min > 4200, NA))\n\nsmart_ohio_raw |> count(`_MINAC11`, exer1_min) |> tail()\n\n# A tibble: 6 × 3\n  `_MINAC11` exer1_min     n\n       <dbl>     <dbl> <int>\n1       3780      3780     8\n2       3959      3959     1\n3       3960      3960     1\n4       4193      4193     6\n5      27000        NA     1\n6         NA        NA  2760\n\n\n\n\n6.6.15.8 _MINAC21 and its cleanup to exer2_min\n_MINAC21 is minutes of physical activity per week for the second activity (listed as exer2_type above.) Again, we’ll treat as implausible any values larger than 4200 minutes (which would indicate 70 hours per week.)\n\nsmart_ohio_raw <- smart_ohio_raw |> \n    mutate(exer2_min = `_MINAC21`,\n           exer2_min = replace(exer2_min, exer2_min > 4200, NA))\n\nsmart_ohio_raw |> count(`_MINAC21`, exer2_min) |> tail()\n\n# A tibble: 6 × 3\n  `_MINAC21` exer2_min     n\n       <dbl>     <dbl> <int>\n1       3360      3360     3\n2       3780      3780     7\n3       4193      4193     3\n4       6120        NA     1\n5       8400        NA     1\n6         NA        NA  2770\n\n\n\n\n\n6.6.16 Seatbelt Use (1 item)\n\n6.6.16.1 SEATBELT and its cleanup to seatbelt\nThis question asks “How often do you use seat belts when you drive or ride in a car?” Possible responses are:\n\n1 = Always\n2 = Nearly always\n3 = Sometimes\n4 = Seldom\n5 = Never\n7 = Don’t know / Not sure\n8 = Never drive or ride in a car\n9 = Refused\n\nWe’ll treat codes 7, 8 and 9 as NA, and turn this into a factor.\n\nsmart_ohio_raw <- smart_ohio_raw |> \n    mutate(seatbelt = fct_recode(factor(SEATBELT),\n                                  \"Always\" = \"1\",\n                                  \"Nearly_always\" = \"2\",\n                                  \"Sometimes\" = \"3\",\n                                  \"Seldom\" = \"4\", \n                                  \"Never\" = \"5\",\n                                  NULL = \"7\",\n                                  NULL = \"8\",\n                                  NULL = \"9\"))\n\nsmart_ohio_raw |> count(SEATBELT, seatbelt)\n\n# A tibble: 9 × 3\n  SEATBELT seatbelt          n\n     <dbl> <fct>         <int>\n1        1 Always         6047\n2        2 Nearly_always   409\n3        3 Sometimes       191\n4        4 Seldom           81\n5        5 Never           148\n6        7 <NA>              7\n7        8 <NA>             21\n8        9 <NA>              2\n9       NA <NA>            506\n\n\n\n\n\n6.6.17 Immunization (3 items)\n\n6.6.17.1 FLUSHOT6 and its cleanup to vax_flu\nFLUSHOT6 gives the response to “During the past 12 months, have you had either a flu shot or a flu vaccine that was sprayed in your nose?” The responses are:\n\n1 = Yes\n2 = No\n7 = Don’t know/Not sure\n9 = Refused\n\n\nsmart_ohio_raw <- smart_ohio_raw |> \n    mutate(vax_flu = 2 - FLUSHOT6,\n           vax_flu = replace(vax_flu, vax_flu < 0, NA))\n\nsmart_ohio_raw |> count(FLUSHOT6, vax_flu)\n\n# A tibble: 5 × 3\n  FLUSHOT6 vax_flu     n\n     <dbl>   <dbl> <int>\n1        1       1  3453\n2        2       0  3410\n3        7      NA    26\n4        9      NA     3\n5       NA      NA   520\n\n\n\n\n6.6.17.2 PNEUVAC3 and its cleanup to vax_pneumo\nPNEUVAC3 gives the response to “A pneumonia shot or pneumococcal vaccine is usually given only once or twice in a person’s lifetime and is different from the flu shot. Have you ever had a pneumonia shot?” The responses are:\n\n1 = Yes\n2 = No\n7 = Don’t know/Not sure\n9 = Refused\n\n\nsmart_ohio_raw <- smart_ohio_raw |> \n    mutate(vax_pneumo = 2 - PNEUVAC3,\n           vax_pneumo = replace(vax_pneumo, vax_pneumo < 0, NA))\n\nsmart_ohio_raw |> count(PNEUVAC3, vax_pneumo)\n\n# A tibble: 5 × 3\n  PNEUVAC3 vax_pneumo     n\n     <dbl>      <dbl> <int>\n1        1          1  3112\n2        2          0  3262\n3        7         NA   509\n4        9         NA     3\n5       NA         NA   526\n\n\n\n\n6.6.17.3 SHINGLE2 and its cleanup to vax_shingles\nSHINGLE2 gives the response to “Have you ever had the shingles or zoster vaccine?” The responses are:\n\n1 = Yes\n2 = No\n7 = Don’t know/Not sure\n9 = Refused\n\n\nsmart_ohio_raw <- smart_ohio_raw |> \n    mutate(vax_shingles = 2 - SHINGLE2,\n           vax_shingles = replace(vax_shingles, vax_shingles < 0, NA))\n\nsmart_ohio_raw |> count(SHINGLE2, vax_shingles)\n\n# A tibble: 4 × 3\n  SHINGLE2 vax_shingles     n\n     <dbl>        <dbl> <int>\n1        1            1  1503\n2        2            0  2979\n3        7           NA    78\n4       NA           NA  2852\n\n\n\n\n\n6.6.18 HIV/AIDS (2 items)\n\n6.6.18.1 HIVTST6 and its cleanup to hiv_test\nHIVTST6 gives the response to “Have you ever been tested for HIV? Do not count tests you may have had as part of a blood donation. Include testing fluid from your mouth.” The responses are:\n\n1 = Yes\n2 = No\n7 = Don’t know/Not sure\n9 = Refused\n\n\nsmart_ohio_raw <- smart_ohio_raw |> \n    mutate(hiv_test = 2 - HIVTST6,\n           hiv_test = replace(hiv_test, hiv_test < 0, NA))\n\nsmart_ohio_raw |> count(HIVTST6, hiv_test)\n\n# A tibble: 5 × 3\n  HIVTST6 hiv_test     n\n    <dbl>    <dbl> <int>\n1       1        1  2017\n2       2        0  4565\n3       7       NA   260\n4       9       NA    14\n5      NA       NA   556\n\n\n\n\n6.6.18.2 HIVRISK5 and its cleanup to hiv_risk\nHIVRISK5 gives the response to “I am going to read you a list. When I am done, please tell me if any of the situations apply to you. You do not need to tell me which one. You have injected any drug other than those prescribed for you in the past year. You have been treated for a sexually transmitted disease or STD in the past year. You have given or received money or drugs in exchange for sex in the past year.” The responses are:\n\n1 = Yes\n2 = No\n7 = Don’t know/Not sure\n9 = Refused\n\n\nsmart_ohio_raw <- smart_ohio_raw |> \n    mutate(hiv_risk = 2 - HIVRISK5,\n           hiv_risk = replace(hiv_risk, hiv_risk < 0, NA))\n\nsmart_ohio_raw |> count(HIVRISK5, hiv_risk)\n\n# A tibble: 5 × 3\n  HIVRISK5 hiv_risk     n\n     <dbl>    <dbl> <int>\n1        1        1   277\n2        2        0  6537\n3        7       NA     2\n4        9       NA    17\n5       NA       NA   579"
  },
  {
    "objectID": "06-smart.html#imputing-age-and-income-as-quantitative-from-thin-air",
    "href": "06-smart.html#imputing-age-and-income-as-quantitative-from-thin-air",
    "title": "6  BRFSS SMART Data",
    "section": "6.7 Imputing Age and Income as Quantitative from Thin Air",
    "text": "6.7 Imputing Age and Income as Quantitative from Thin Air\nThis section is purely for teaching purposes. I would never use the variables created in this section for research work.\n\n6.7.1 age_imp: Imputing Age Data\nI want a quantitative age variable, so I’m going to create an imputed age_imp value for each subject based on their agegroup. For each age group, I will assume that each of the ages represented by a value in that age group will be equally likely, and will draw from the relevant uniform distribution to impute age.\n\nset.seed(2020432002)\n\nsmart_ohio_raw <- smart_ohio_raw |>\n    mutate(age_low = as.numeric(str_sub(as.character(agegroup), 1, 2))) |>\n    mutate(age_high = as.numeric(str_sub(as.character(agegroup), 4, 5))) |>\n    rowwise() |>\n    mutate(age_imp = ifelse(!is.na(agegroup), \n                            round(runif(1, min = age_low, max = age_high),0),\n                            NA)) \n\nsmart_ohio_raw |> count(agegroup, age_imp) #|> tail()\n\n# A tibble: 80 × 3\n# Rowwise: \n   agegroup age_imp     n\n   <fct>      <dbl> <int>\n 1 18-24         18    46\n 2 18-24         19    75\n 3 18-24         20    76\n 4 18-24         21    82\n 5 18-24         22    80\n 6 18-24         23    54\n 7 18-24         24    35\n 8 25-29         25    42\n 9 25-29         26    93\n10 25-29         27    77\n# … with 70 more rows\n\n\nHere is a histogram of the age_imp variable.\n\nggplot(smart_ohio_raw, aes(x = age_imp)) +\n    geom_histogram(fill = \"navy\", col = \"white\",\n                   binwidth = 1) +\n    scale_x_continuous(breaks = c(18, 25, 35, 45, 55, 65, 75, 85, 96)) +\n    labs(x = \"Imputed Age in Years\",\n         title = paste0(\"Imputed Income: \", \n                        sum(is.na(smart_ohio_raw$age_imp)), \n                        \" respondents have missing age group\"))\n\n\n\n\n\n\n6.7.2 inc_imp: Imputing Income Data\nI want a quantitative income variable, so I’m going to create an imputed inc_imp value for each subject based on their incomegroup. For most income groups, I will assume that each of the incomes represented by a value in that income group will be equally likely, and will draw from the relevant uniform distribution to impute income. The exception is the highest income group, where I will impute a value drawn from a distribution that places all values at $75,000 or more, but has a substantial right skew and long tail.\n\nset.seed(2020432001)\n\nsmart_ohio_raw <- smart_ohio_raw |>\n    mutate(inc_imp = case_when(\n        incomegroup == \"0-9K\" ~ round(runif(1, min = 100, max = 9999)),\n        incomegroup == \"10-14K\" ~ round(runif(1, min = 10000, max = 14999)),\n        incomegroup == \"15-19K\" ~ round(runif(1, min = 15000, max = 19999)),\n        incomegroup == \"20-24K\" ~ round(runif(1, min = 20000, max = 24999)),\n        incomegroup == \"25-34K\" ~ round(runif(1, min = 25000, max = 34999)),\n        incomegroup == \"35-49K\" ~ round(runif(1, min = 35000, max = 49999)),\n        incomegroup == \"50-74K\" ~ round(runif(1, min = 50000, max = 74999)),\n        incomegroup == \"75K+\" ~ round((rnorm(n = 1, mean = 0, sd = 300)^2) + 74999)))\n\nsmart_ohio_raw |> count(incomegroup, inc_imp) |> tail()\n\n# A tibble: 6 × 3\n# Rowwise: \n  incomegroup inc_imp     n\n  <fct>         <dbl> <int>\n1 75K+         774009     1\n2 75K+         798174     1\n3 75K+         806161     1\n4 75K+         847758     1\n5 75K+        1085111     1\n6 <NA>             NA  1310\n\n\nHere are density plots of the inc_imp variable. The top picture shows the results on a linear scale, and the bottom shows them on a log (base 10) scale.\n\np1 <- ggplot(smart_ohio_raw, aes(x = inc_imp/1000)) +\n    geom_density(fill = \"darkgreen\", col = \"white\") +\n    labs(x = \"Imputed Income in Thousands of Dollars\",\n         title = \"Imputed Income on the Linear scale\") + \n    scale_x_continuous(breaks = c(25, 75, 250, 1000))\n\np2 <- ggplot(smart_ohio_raw, aes(x = inc_imp/1000)) +\n    geom_density(fill = \"darkgreen\", col = \"white\") +\n    labs(x = \"Imputed Income in Thousands of Dollars\",\n         title = \"Imputed Income on the Log (base 10) scale\") + \n    scale_x_log10(breaks = c(0.1, 1, 5, 25, 75, 250, 1000))\n\np1 / p2 + \n    plot_annotation(title = \n                        paste0(\"Imputed Income: \", sum(is.na(smart_ohio_raw$inc_imp)), \" respondents have missing income group\"))"
  },
  {
    "objectID": "06-smart.html#clean-data-in-the-state-of-ohio",
    "href": "06-smart.html#clean-data-in-the-state-of-ohio",
    "title": "6  BRFSS SMART Data",
    "section": "6.8 Clean Data in the State of Ohio",
    "text": "6.8 Clean Data in the State of Ohio\nThere are six MMSAs associated with the state of Ohio. We’re going to create a smart_ohio that includes each of them. First, I’ll ungroup the data that I created earlier, so I get a clean tibble.\n\nsmart_ohio_raw <- smart_ohio_raw |> ungroup()\n\nNext, I’ll select the variables I want to retain (they are the ones I created, plus SEQNO.)\n\nsmart_ohio <- smart_ohio_raw |>\n    select(SEQNO, mmsa, mmsa_code, mmsa_name, mmsa_wt, completed,\n           landline, hhadults, \n           genhealth, physhealth, menthealth, poorhealth, \n           agegroup, age_imp, race, hispanic, race_eth, \n           female, marital, kids, educgroup, home_own, \n           veteran, employment, incomegroup, inc_imp,\n           cell_own, internet30, \n           weight_kg, height_m, bmi, bmigroup, \n           pregnant, deaf, blind, decide, \n           diffwalk, diffdress, diffalone, \n           smoke100, smoker, ecig_ever, ecigs, \n           healthplan, hasdoc, costprob, t_checkup, \n           bp_high, bp_meds, \n           t_chol, chol_high, chol_meds,\n           asthma, hx_asthma, now_asthma, \n           hx_mi, hx_chd, hx_stroke, hx_skinc, hx_otherc, \n           hx_copd, hx_depress, hx_kidney, \n           hx_diabetes, dm_status, dm_age, \n           hx_arthr, arth_lims, arth_work, arth_soc, \n           joint_pain, alcdays, avgdrinks, maxdrinks, \n           binge, drinks_wk, drink_heavy, \n           fruit_day, veg_day, eat_juice, eat_fruit, \n           eat_greenveg, eat_fries, eat_potato, \n           eat_otherveg, exerany, activity, rec_aerobic, \n           rec_strength, exer1_type, exer2_type, \n           exer1_min, exer2_min, seatbelt,\n           vax_flu, vax_pneumo, vax_shingles, \n           hiv_test, hiv_risk)\n\nsaveRDS(smart_ohio, \"data/smart_ohio.Rds\")\n\nwrite_csv(smart_ohio, \"data/smart_ohio.csv\")\n\nThe smart_ohio file should contain 99 variables, describing 7412 respondents."
  },
  {
    "objectID": "06-smart.html#clean-cleveland-elyria-data",
    "href": "06-smart.html#clean-cleveland-elyria-data",
    "title": "6  BRFSS SMART Data",
    "section": "6.9 Clean Cleveland-Elyria Data",
    "text": "6.9 Clean Cleveland-Elyria Data\n\n6.9.1 Cleveland - Elyria Data\nThe mmsa_name variable is probably the simplest way for us to filter our data down to the MMSA we are interested in. Here, I’m using the str_detect function to identify the values of mmsa_name that contain the text “Cleveland”.\n\nsmart_cle <- smart_ohio |> \n  filter(str_detect(mmsa_name, 'Cleveland')) \n\nsaveRDS(smart_cle, \"data/smart_cle.Rds\")\n\nIn the Cleveland-Elyria MSA, we have 1133 observations on the same 99 variables.\nWe’ll build a variety of smaller subsets from these data, eventually."
  },
  {
    "objectID": "07-singleimputation.html#r-setup-used-here",
    "href": "07-singleimputation.html#r-setup-used-here",
    "title": "7  Dealing with Missingness: Single Imputation",
    "section": "7.1 R Setup Used Here",
    "text": "7.1 R Setup Used Here\n\nknitr::opts_chunk$set(comment = NA)\n\nlibrary(janitor) \nlibrary(ggridges)\nlibrary(knitr)\nlibrary(naniar)\nlibrary(simputation)\nlibrary(Hmisc)\nlibrary(mosaic)\nlibrary(patchwork)\nlibrary(visdat)\nlibrary(tidyverse) \n\ntheme_set(theme_bw())\n\n\n7.1.1 Data Load\n\nsmart_cle <- readRDS(\"data/smart_cle.Rds\")"
  },
  {
    "objectID": "07-singleimputation.html#selecting-some-variables-from-the-smart_cle-data",
    "href": "07-singleimputation.html#selecting-some-variables-from-the-smart_cle-data",
    "title": "7  Dealing with Missingness: Single Imputation",
    "section": "7.2 Selecting Some Variables from the smart_cle data",
    "text": "7.2 Selecting Some Variables from the smart_cle data\n\nsmart_cle1 <- smart_cle |> \n  select(SEQNO, physhealth, genhealth, bmi, \n         age_imp, female, race_eth, internet30, \n         smoke100, activity, drinks_wk, veg_day)\n\nThe smart_cle.Rds data file available on the Data and Code page of our website describes information on 99 variables for 1133 respondents to the BRFSS 2017, who live in the Cleveland-Elyria, OH, Metropolitan Statistical Area. The variables in the smart_cle1.csv file are listed below, along with the items that generate these responses.\n\n\n\n\n\n\n\nVariable\nDescription\n\n\n\n\nSEQNO\nrespondent identification number (all begin with 2016)\n\n\nphyshealth\nNow thinking about your physical health, which includes physical illness and injury, for how many days during the past 30 days was your physical health not good?\n\n\ngenhealth\nWould you say that in general, your health is … (five categories: Excellent, Very Good, Good, Fair or Poor)\n\n\nbmi\nBody mass index, in kg/m2\n\n\nage_imp\nAge, imputed, in years\n\n\nfemale\nSex, 1 = female, 0 = male\n\n\nrace_eth\nRace and Ethnicity, in five categories\n\n\ninternet30\nHave you used the internet in the past 30 days? (1 = yes, 0 = no)\n\n\nsmoke100\nHave you smoked at least 100 cigarettes in your life? (1 = yes, 0 = no)\n\n\nactivity\nPhysical activity (Highly Active, Active, Insufficiently Active, Inactive)\n\n\ndrinks_wk\nOn average, how many drinks of alcohol do you consume in a week?\n\n\nveg_day\nHow many servings of vegetables do you consume per day, on average?\n\n\n\n\nstr(smart_cle1)\n\ntibble [1,133 × 12] (S3: tbl_df/tbl/data.frame)\n $ SEQNO     : num [1:1133] 2.02e+09 2.02e+09 2.02e+09 2.02e+09 2.02e+09 ...\n $ physhealth: num [1:1133] 4 0 0 0 0 2 2 0 0 0 ...\n $ genhealth : Factor w/ 5 levels \"1_Excellent\",..: 1 1 3 3 3 2 3 2 4 1 ...\n $ bmi       : num [1:1133] NA 23.1 26.9 26.5 24.2 ...\n $ age_imp   : num [1:1133] 51 28 37 36 88 43 23 34 58 54 ...\n $ female    : num [1:1133] 1 1 1 1 0 0 0 0 0 1 ...\n $ race_eth  : Factor w/ 5 levels \"White non-Hispanic\",..: 1 1 3 1 1 1 1 3 2 1 ...\n $ internet30: num [1:1133] 1 1 0 1 1 1 1 1 1 1 ...\n $ smoke100  : num [1:1133] 1 0 0 1 1 1 0 0 0 1 ...\n $ activity  : Factor w/ 4 levels \"Highly_Active\",..: 4 4 3 1 1 NA 1 1 1 1 ...\n $ drinks_wk : num [1:1133] 0.7 0 0 4.67 0.93 0 2 0 0 0.47 ...\n $ veg_day   : num [1:1133] NA 3 4.06 2.07 1.31 NA 1.57 0.83 0.49 1.72 ..."
  },
  {
    "objectID": "07-singleimputation.html#smart_cle1-seeing-our-missing-data",
    "href": "07-singleimputation.html#smart_cle1-seeing-our-missing-data",
    "title": "7  Dealing with Missingness: Single Imputation",
    "section": "7.3 smart_cle1: Seeing our Missing Data",
    "text": "7.3 smart_cle1: Seeing our Missing Data\nThe naniar package provides several useful functions for summarizing missingness in our data set. Like all tidy data sets, our smart_cle1 tibble contains rows which describe observations, sometimes called cases, and also contains columns which describe variables.\nOverall, there are 1133 cases, and 1133 observations in our smart_cle1 tibble.\n\nWe can obtain a count of the number of missing cells in the entire tibble.\n\n\nsmart_cle1 |> n_miss()\n\n[1] 479\n\n\n\nWe can use the miss_var_summary function to get a sorted table of each variable by number missing.\n\n\nmiss_var_summary(smart_cle1) |> kable()\n\n\n\n\nvariable\nn_miss\npct_miss\n\n\n\n\nactivity\n109\n9.6204766\n\n\nveg_day\n101\n8.9143866\n\n\nbmi\n91\n8.0317741\n\n\ndrinks_wk\n66\n5.8252427\n\n\nsmoke100\n40\n3.5304501\n\n\nrace_eth\n26\n2.2947926\n\n\nphyshealth\n24\n2.1182701\n\n\nage_imp\n11\n0.9708738\n\n\ninternet30\n7\n0.6178288\n\n\ngenhealth\n4\n0.3530450\n\n\nSEQNO\n0\n0.0000000\n\n\nfemale\n0\n0.0000000\n\n\n\n\n\n\nOr we can use the miss_var_table function to tabulate the number of variables that have each observed level of missingness.\n\n\nmiss_var_table(smart_cle1) \n\n# A tibble: 11 × 3\n   n_miss_in_var n_vars pct_vars\n           <int>  <int>    <dbl>\n 1             0      2    16.7 \n 2             4      1     8.33\n 3             7      1     8.33\n 4            11      1     8.33\n 5            24      1     8.33\n 6            26      1     8.33\n 7            40      1     8.33\n 8            66      1     8.33\n 9            91      1     8.33\n10           101      1     8.33\n11           109      1     8.33\n\n\n\nOr we can get a count for a specific variable, like activity:\n\n\nsmart_cle1 |> select(activity) |> n_miss()\n\n[1] 109\n\n\n\nWe can also use prop_miss_case or pct_miss_case to specify the proportion (or percentage) of missing observations across an entire data set, or within a specific variable.\n\n\nprop_miss_case(smart_cle1)\n\n[1] 0.2127096\n\n\n\nsmart_cle1 |> select(activity) |> pct_miss_case()\n\n[1] 9.620477\n\n\n\nWe can also use prop_miss_var or pct_miss_var to specify the proportion (or percentage) of variables with missing observations across an entire data set.\n\n\nprop_miss_var(smart_cle1)\n\n[1] 0.8333333\n\npct_miss_var(smart_cle1)\n\n[1] 83.33333\n\n\n\nWe use miss_case_table to identify the number of missing values for each of the cases (rows) in our tibble.\n\n\nmiss_case_table(smart_cle1)\n\n# A tibble: 7 × 3\n  n_miss_in_case n_cases pct_cases\n           <int>   <int>     <dbl>\n1              0     892    78.7  \n2              1     129    11.4  \n3              2      51     4.50 \n4              3      22     1.94 \n5              4      21     1.85 \n6              5      10     0.883\n7              6       8     0.706\n\n\n\nUse miss_case_summary to specify individual observations and count their missing values.\n\n\nmiss_case_summary(smart_cle1)\n\n# A tibble: 1,133 × 3\n    case n_miss pct_miss\n   <int>  <int>    <dbl>\n 1    17      6     50  \n 2    42      6     50  \n 3   254      6     50  \n 4   425      6     50  \n 5   521      6     50  \n 6   729      6     50  \n 7   757      6     50  \n 8  1051      6     50  \n 9    89      5     41.7\n10    94      5     41.7\n# … with 1,123 more rows\n\n\nThe case numbers identified here are row numbers. Extract the data for case 17, for instance, with the slice function.\n\nsmart_cle1 |> slice(17)\n\n# A tibble: 1 × 12\n      SEQNO physh…¹ genhe…²   bmi age_imp female race_…³ inter…⁴ smoke…⁵ activ…⁶\n      <dbl>   <dbl> <fct>   <dbl>   <dbl>  <dbl> <fct>     <dbl>   <dbl> <fct>  \n1    2.02e9       0 1_Exce…    NA      50      0 White …      NA      NA <NA>   \n# … with 2 more variables: drinks_wk <dbl>, veg_day <dbl>, and abbreviated\n#   variable names ¹​physhealth, ²​genhealth, ³​race_eth, ⁴​internet30, ⁵​smoke100,\n#   ⁶​activity\n\n\n\n7.3.1 Plotting Missingness\nThe gg_miss_var function plots the number of missing observations in each variable in our data set.\n\ngg_miss_var(smart_cle1)\n\nWarning: The `guide` argument in `scale_*()` cannot be `FALSE`. This was deprecated in\nggplot2 3.3.4.\nℹ Please use \"none\" instead.\nℹ The deprecated feature was likely used in the naniar package.\n  Please report the issue at <https://github.com/njtierney/naniar/issues>.\n\n\n\n\n\nSo the most commonly missing variable is activity.\nTo get a general sense of the missingness in our data, we might use either the vis_dat or the vis_miss function from the visdat package.\n\nvis_miss(smart_cle1)\n\nWarning: `gather_()` was deprecated in tidyr 1.2.0.\nℹ Please use `gather()` instead.\nℹ The deprecated feature was likely used in the visdat package.\n  Please report the issue at <https://github.com/ropensci/visdat/issues>.\n\n\n\n\n\n\nvis_dat(smart_cle1)"
  },
  {
    "objectID": "07-singleimputation.html#missing-data-mechanisms",
    "href": "07-singleimputation.html#missing-data-mechanisms",
    "title": "7  Dealing with Missingness: Single Imputation",
    "section": "7.4 Missing-data mechanisms",
    "text": "7.4 Missing-data mechanisms\nMy source for this description of mechanisms is Chapter 25 of @GelmanHill2007, and that chapter is available at this link.\n\nMCAR = Missingness completely at random. A variable is missing completely at random if the probability of missingness is the same for all units, for example, if for each subject, we decide whether to collect the diabetes status by rolling a die and refusing to answer if a “6” shows up. If data are missing completely at random, then throwing out cases with missing data does not bias your inferences.\nMissingness that depends only on observed predictors. A more general assumption, called missing at random or MAR, is that the probability a variable is missing depends only on available information. Here, we would have to be willing to assume that the probability of nonresponse to diabetes depends only on the other, fully recorded variables in the data. It is often reasonable to model this process as a logistic regression, where the outcome variable equals 1 for observed cases and 0 for missing. When an outcome variable is missing at random, it is acceptable to exclude the missing cases (that is, to treat them as NA), as long as the regression controls for all the variables that affect the probability of missingness.\nMissingness that depends on unobserved predictors. Missingness is no longer “at random” if it depends on information that has not been recorded and this information also predicts the missing values. If a particular treatment causes discomfort, a patient is more likely to drop out of the study. This missingness is not at random (unless “discomfort” is measured and observed for all patients). If missingness is not at random, it must be explicitly modeled, or else you must accept some bias in your inferences.\nMissingness that depends on the missing value itself. Finally, a particularly difficult situation arises when the probability of missingness depends on the (potentially missing) variable itself. For example, suppose that people with higher earnings are less likely to reveal them.\n\nEssentially, situations 3 and 4 are referred to collectively as non-random missingness, and cause more trouble for us than 1 and 2."
  },
  {
    "objectID": "07-singleimputation.html#options-for-dealing-with-missingness",
    "href": "07-singleimputation.html#options-for-dealing-with-missingness",
    "title": "7  Dealing with Missingness: Single Imputation",
    "section": "7.5 Options for Dealing with Missingness",
    "text": "7.5 Options for Dealing with Missingness\nThere are several available methods for dealing with missing data that are MCAR or MAR, but they basically boil down to:\n\nComplete Case (or Available Case) analyses\nSingle Imputation\nMultiple Imputation"
  },
  {
    "objectID": "07-singleimputation.html#complete-case-and-available-case-analyses",
    "href": "07-singleimputation.html#complete-case-and-available-case-analyses",
    "title": "7  Dealing with Missingness: Single Imputation",
    "section": "7.6 Complete Case (and Available Case) analyses",
    "text": "7.6 Complete Case (and Available Case) analyses\nIn Complete Case analyses, rows containing NA values are omitted from the data before analyses commence. This is the default approach for many statistical software packages, and may introduce unpredictable bias and fail to include some useful, often hard-won information.\n\nA complete case analysis can be appropriate when the number of missing observations is not large, and the missing pattern is either MCAR (missing completely at random) or MAR (missing at random.)\nTwo problems arise with complete-case analysis:\n\nIf the units with missing values differ systematically from the completely observed cases, this could bias the complete-case analysis.\nIf many variables are included in a model, there may be very few complete cases, so that most of the data would be discarded for the sake of a straightforward analysis.\n\nA related approach is available-case analysis where different aspects of a problem are studied with different subsets of the data, perhaps identified on the basis of what is missing in them."
  },
  {
    "objectID": "07-singleimputation.html#single-imputation",
    "href": "07-singleimputation.html#single-imputation",
    "title": "7  Dealing with Missingness: Single Imputation",
    "section": "7.7 Single Imputation",
    "text": "7.7 Single Imputation\nIn single imputation analyses, NA values are estimated/replaced one time with one particular data value for the purpose of obtaining more complete samples, at the expense of creating some potential bias in the eventual conclusions or obtaining slightly less accurate estimates than would be available if there were no missing values in the data.\n\nA single imputation can be just a replacement with the mean or median (for a quantity) or the mode (for a categorical variable.) However, such an approach, though easy to understand, underestimates variance and ignores the relationship of missing values to other variables.\nSingle imputation can also be done using a variety of models to try to capture information about the NA values that are available in other variables within the data set.\nThe simputation package can help us execute single imputations using a wide variety of techniques, within the pipe approach used by the tidyverse. Another approach I have used in the past is the mice package, which can also perform single imputations."
  },
  {
    "objectID": "07-singleimputation.html#multiple-imputation",
    "href": "07-singleimputation.html#multiple-imputation",
    "title": "7  Dealing with Missingness: Single Imputation",
    "section": "7.8 Multiple Imputation",
    "text": "7.8 Multiple Imputation\nMultiple imputation, where NA values are repeatedly estimated/replaced with multiple data values, for the purpose of obtaining mode complete samples and capturing details of the variation inherent in the fact that the data have missingness, so as to obtain more accurate estimates than are possible with single imputation.\n\nWe’ll postpone the discussion of multiple imputation for a while."
  },
  {
    "objectID": "07-singleimputation.html#approach-1-building-a-complete-case-analysis-smart_cle1_cc",
    "href": "07-singleimputation.html#approach-1-building-a-complete-case-analysis-smart_cle1_cc",
    "title": "7  Dealing with Missingness: Single Imputation",
    "section": "7.9 Approach 1: Building a Complete Case Analysis: smart_cle1_cc",
    "text": "7.9 Approach 1: Building a Complete Case Analysis: smart_cle1_cc\nIn the 431 course, we usually dealt with missing data by restricting our analyses to respondents with complete data on all variables. Let’s start by doing that here. We’ll create a new tibble called smart_cle1_cc which includes all respondents with complete data on all of these variables.\n\nsmart_cle1_cc <- smart_cle1 |> \n  drop_na()\n\ndim(smart_cle1_cc)\n\n[1] 892  12\n\n\nOur smart_cle1_cc tibble now has many fewer observations than its predecessors, but all of the variables in this complete cases tibble have no missing observations.\n\n\n\nData Set\nRows\nColumns\nMissingness?\n\n\n\n\nsmart_cle\n1133\n99\nQuite a bit.\n\n\nsmart_cle1\n1133\n12\nQuite a bit.\n\n\nsmart_cle1_cc\n892\n12\nNone."
  },
  {
    "objectID": "07-singleimputation.html#approach-2-single-imputation-to-create-smart_cle1_sh",
    "href": "07-singleimputation.html#approach-2-single-imputation-to-create-smart_cle1_sh",
    "title": "7  Dealing with Missingness: Single Imputation",
    "section": "7.10 Approach 2: Single Imputation to create smart_cle1_sh",
    "text": "7.10 Approach 2: Single Imputation to create smart_cle1_sh\nNext, we’ll create a data set which has all of the rows in the original smart_cle1 tibble, but deals with missingness by imputing (estimating / filling in) new values for each of the missing values. To do this, we’ll make heavy use of the simputation package in R.\nThe simputation package is designed for single imputation work. Note that we’ll eventually adopt a multiple imputation strategy in some of our modeling work, and we’ll use some specialized tools to facilitate that later.\nTo begin, we’ll create a “shadow” in our tibble to track what we’ll need to impute.\n\nsmart_cle1_sh <- bind_shadow(smart_cle1)\n\nnames(smart_cle1_sh)\n\n [1] \"SEQNO\"         \"physhealth\"    \"genhealth\"     \"bmi\"          \n [5] \"age_imp\"       \"female\"        \"race_eth\"      \"internet30\"   \n [9] \"smoke100\"      \"activity\"      \"drinks_wk\"     \"veg_day\"      \n[13] \"SEQNO_NA\"      \"physhealth_NA\" \"genhealth_NA\"  \"bmi_NA\"       \n[17] \"age_imp_NA\"    \"female_NA\"     \"race_eth_NA\"   \"internet30_NA\"\n[21] \"smoke100_NA\"   \"activity_NA\"   \"drinks_wk_NA\"  \"veg_day_NA\"   \n\n\nNote that the bind_shadow() function doubles the number of variables in our tibble, specifically by creating a new variable for each that takes the value !NA or NA. For example, consider\n\nsmart_cle1_sh |> count(activity, activity_NA)\n\n# A tibble: 5 × 3\n  activity              activity_NA     n\n  <fct>                 <fct>       <int>\n1 Highly_Active         !NA           338\n2 Active                !NA           173\n3 Insufficiently_Active !NA           201\n4 Inactive              !NA           312\n5 <NA>                  NA            109\n\n\nThe activity_NA variable takes the value !NA (meaning not missing) when the value of the activity variable is known, and takes the value NA for observations where the activity variable is missing. This background tracking will be helpful to us when we try to assess the impact of imputation on some of our summaries.\n\n7.10.1 What Type of Missingness Do We Have?\nThere are three types of missingness that we might assume in any given setting: missing completely at random (MCAR), missing at random (MAR) and missing not at random (MNAR). Together, MCAR and MAR are sometimes called ignorable non-response, which essentially means that imputation provides a way to useful estimates. MNAR or missing NOT at random is sometimes called non-ignorable missingness, implying that even high-quality imputation may not be sufficient to provide useful information to us.\nMissing Completely at Random means that the missing data points are a random subset of the data. Essentially, there is nothing that makes some data more likely to be missing than others. If the data truly match the standard for MCAR, then a complete-case analysis will be about as good as an analysis after single or multiple imputation.\nMissing at Random means that there is a systematic relationship between the observed data and the missingness mechanism. Another way to say this is that the missing value is not related to the reason why it is missing, but is related to the other variables collected in the study. The implication is that the missingness can be accounted for by studying the variables with complete information. Imputation strategies can be very helpful here, incorporating what we know (or think we know) about the relationships between the results that are missing and the results that we see.\n\nWikipedia provides a nice example. If men are less likely to fill in a depression survey, but this has nothing to do with their level of depression after accounting for the fact that they are male, then the missingess can be assumed MAR.\nDetermining whether missingness is MAR or MNAR can be tricky. We’ll spend more time discussing this later.\n\nMissing NOT at Random means that the missing value is related to the reason why it is missing.\n\nContinuing the Wikipedia example, if men failed to fill in a depression survey because of their level of depression, then this would be MNAR.\nSingle imputation is most helpful in the MAR situation, although it is also appropriate when we assume MCAR.\nMultiple imputation will, similarly, be more helpful in MCAR and MAR situations than when data are missing NOT at random.\n\nIt’s worth noting that many people are unwilling to impute values for outcomes or key predictors in a modeling setting, but are happy to impute for less important covariates. For now, we’ll assume MCAR or MAR for all of the missingness in our smart_cle1 data, which will allow us to adopt a single imputation strategy.\n\n\n7.10.2 Single imputation into smart_cle1_sh\nWhich variables in smart_cle1_sh contain missing data?\n\nmiss_var_summary(smart_cle1_sh)\n\n# A tibble: 24 × 3\n   variable   n_miss pct_miss\n   <chr>       <int>    <dbl>\n 1 activity      109    9.62 \n 2 veg_day       101    8.91 \n 3 bmi            91    8.03 \n 4 drinks_wk      66    5.83 \n 5 smoke100       40    3.53 \n 6 race_eth       26    2.29 \n 7 physhealth     24    2.12 \n 8 age_imp        11    0.971\n 9 internet30      7    0.618\n10 genhealth       4    0.353\n# … with 14 more rows\n\n\nWe will impute these variables using several different strategies, all supported nicely by the simputation package.\nThese include imputation methods based solely on the distribution of the complete cases of the variable being imputed.\n\nimpute_median: impute the median value of all non-missing observations into the missing values for the variable\nimpute_rhd: random “hot deck” imputation involves drawing at random from the complete cases for that variable\n\nAlso available are imputation strategies that impute predicted values from models using other variables in the data set besides the one being imputed.\n\nimpute_pmm: imputation using predictive mean matching\nimpute_rlm: imputation using robust linear models\nimpute_cart: imputation using classification and regression trees\nimpute_knn: imputation using k-nearest neighbors methods\n\n\n\n7.10.3 Imputing Binary Categorical Variables\nHere, we’ll arbitrarily impute our 1/0 variables as follows:\n\nFor internet30 we’ll use the impute_rhd approach to draw a random observation from the existing set of 1s and 0s in the complete internet30 data.\nFor smoke100 we’ll use a method called predictive mean matching (impute_pmm) which takes the result from a model based on the (imputed) internet30 value and whether or not the subject is female, and converts it to the nearest value in the observed smoke100 data. This is a good approach for imputing discrete variables.\n\nThese are completely arbitrary choices, for demonstration purposes.\n\nset.seed(2020001)\n\nsmart_cle1_sh <- smart_cle1_sh |> data.frame() |>\n    impute_rhd(internet30 ~ 1) |>\n    impute_pmm(smoke100 ~ internet30 + female) |>\n  as_tibble()\n\nsmart_cle1_sh |> count(smoke100, smoke100_NA)\n\n# A tibble: 4 × 3\n  smoke100 smoke100_NA     n\n     <dbl> <fct>       <int>\n1        0 !NA           579\n2        0 NA             21\n3        1 !NA           514\n4        1 NA             19\n\nsmart_cle1_sh |> count(internet30, internet30_NA)\n\n# A tibble: 4 × 3\n  internet30 internet30_NA     n\n       <dbl> <fct>         <int>\n1          0 !NA             207\n2          0 NA                1\n3          1 !NA             919\n4          1 NA                6\n\n\nOther approaches that may be used with 1/0 variables include impute_knn and impute_pmm.\n\n\n7.10.4 Imputing Quantitative Variables\nWe’ll demonstrate a different approach for imputing each of the quantitative variables with missing observations. Again, we’re making purely arbitrary decisions here about what to include in each imputation. In practical work, we’d want to be a bit more thoughtful about this.\nNote that I’m choosing to use impute_pmm with the physhealth and age_imp variables. This is (in part) because I want my imputations to be integers, as the other observations are for those variables. impute_rhd would also accomplish this.\n\nset.seed(2020001)\nsmart_cle1_sh <- smart_cle1_sh |> data.frame() |>\n    impute_rhd(veg_day ~ 1) |>\n    impute_median(drinks_wk ~ 1) |>\n    impute_pmm(physhealth ~ drinks_wk + female + smoke100) |>\n    impute_pmm(age_imp ~ drinks_wk + physhealth) |>\n    impute_rlm(bmi ~ physhealth + smoke100) |>\n  as_tibble()\n\n\n\n7.10.5 Imputation Results\nLet’s plot a few of these results, so we can see what imputation has done to the distribution of these quantities.\n\nveg_day\n\n\nggplot(smart_cle1_sh, aes(x = veg_day_NA, y = veg_day)) +\n  geom_count() + \n  labs(title = \"Imputation Results for veg_day\")\n\n\n\n\n\nfavstats(veg_day ~ veg_day_NA, data = smart_cle1_sh)\n\n  veg_day_NA  min     Q1 median   Q3  max     mean       sd    n missing\n1        !NA 0.00 1.2675   1.72 2.42 7.49 1.912548 1.038403 1032       0\n2         NA 0.26 1.3400   1.86 2.72 5.97 2.085050 1.062316  101       0\n\n\n\ndrinks_wk for which we imputed the median value…\n\n\nggplot(smart_cle1_sh, aes(x = drinks_wk_NA, y = drinks_wk)) +\n  geom_count() + \n  labs(title = \"Imputation Results for drinks_wk\")\n\n\n\n\n\nsmart_cle1_sh |> filter(drinks_wk_NA == \"NA\") |>\n  tabyl(drinks_wk)\n\n drinks_wk  n percent\n      0.23 66       1\n\n\n\nphyshealth, a count between 0 and 30…\n\n\nggplot(smart_cle1_sh, \n       aes(x = physhealth, y = physhealth_NA)) +\n  geom_density_ridges() +\n  labs(title = \"Imputation Results for physhealth\")\n\nPicking joint bandwidth of 0.426\n\n\n\n\n\n\nsmart_cle1_sh |> filter(physhealth_NA == \"NA\") |>\n  tabyl(physhealth)\n\n physhealth  n    percent\n          3  1 0.04166667\n          4  2 0.08333333\n          5 13 0.54166667\n          6  8 0.33333333\n\n\n\nage_imp, in (integer) years\n\n\nggplot(smart_cle1_sh, \n       aes(x = age_imp, color = age_imp_NA)) +\n  geom_freqpoly(binwidth = 2) +\n  labs(title = \"Imputation Results for age_imp\")\n\n\n\n\n\nsmart_cle1_sh |> filter(age_imp_NA == \"NA\") |>\n  tabyl(age_imp)\n\n age_imp n    percent\n      48 1 0.09090909\n      57 7 0.63636364\n      58 1 0.09090909\n      61 1 0.09090909\n      63 1 0.09090909\n\n\n\nbmi or body mass index\n\n\nggplot(smart_cle1_sh, aes(x = bmi, fill = bmi_NA)) +\n  geom_histogram(bins = 30) + \n  labs(title = \"Histogram of BMI and imputed BMI\")\n\n\n\n\n\nfavstats(bmi ~ bmi_NA, data = smart_cle1_sh)\n\n  bmi_NA     min      Q1   median       Q3      max     mean        sd    n\n1    !NA 13.3000 24.1100 27.30000 31.68000 70.56000 28.40947 6.6289286 1042\n2     NA 27.0693 27.0693 27.50229 27.66574 30.75898 27.66057 0.8964101   91\n  missing\n1       0\n2       0\n\n\n\n\n7.10.6 Imputing Multi-Categorical Variables\nThe three multi-categorical variables we have left to impute are activity, race_eth and genhealth, and each is presented as a factor in R, rather than as a character variable.\nWe’ll arbitrarily decide to impute\n\nactivity and genhealth with a classification tree using physhealth, bmi and smoke100,\nand then impute race_eth with a random draw from the distribution of complete cases.\n\n\nset.seed(2020001)\nsmart_cle1_sh <- smart_cle1_sh |>\n  data.frame() |>\n    impute_cart(activity + genhealth ~ physhealth + bmi + smoke100) |>\n    impute_rhd(race_eth ~ 1) |>\n  as_tibble()\n\nLet’s check our results.\n\nsmart_cle1_sh |> count(activity_NA, activity)\n\n# A tibble: 6 × 3\n  activity_NA activity                  n\n  <fct>       <fct>                 <int>\n1 !NA         Highly_Active           338\n2 !NA         Active                  173\n3 !NA         Insufficiently_Active   201\n4 !NA         Inactive                312\n5 NA          Highly_Active            90\n6 NA          Inactive                 19\n\n\n\nsmart_cle1_sh |> count(race_eth_NA, race_eth)\n\n# A tibble: 9 × 3\n  race_eth_NA race_eth                     n\n  <fct>       <fct>                    <int>\n1 !NA         White non-Hispanic         805\n2 !NA         Black non-Hispanic         222\n3 !NA         Other race non-Hispanic     24\n4 !NA         Multiracial non-Hispanic    22\n5 !NA         Hispanic                    34\n6 NA          White non-Hispanic          19\n7 NA          Black non-Hispanic           4\n8 NA          Multiracial non-Hispanic     2\n9 NA          Hispanic                     1\n\n\n\nsmart_cle1_sh |> count(genhealth_NA, genhealth)\n\n# A tibble: 7 × 3\n  genhealth_NA genhealth       n\n  <fct>        <fct>       <int>\n1 !NA          1_Excellent   164\n2 !NA          2_VeryGood    383\n3 !NA          3_Good        364\n4 !NA          4_Fair        158\n5 !NA          5_Poor         60\n6 NA           2_VeryGood      3\n7 NA           3_Good          1\n\n\nAnd now, we should have no missing values in the data, at all.\n\nmiss_case_table(smart_cle1_sh)\n\n# A tibble: 1 × 3\n  n_miss_in_case n_cases pct_cases\n           <int>   <int>     <dbl>\n1              0    1133       100\n\n\n\n\n7.10.7 Saving the new tibbles\n\nsaveRDS(smart_cle1_cc, (\"data/smart_cle1_cc.Rds\"))\nsaveRDS(smart_cle1_sh, (\"data/smart_cle1_sh.Rds\"))"
  },
  {
    "objectID": "08-datasummaries.html#r-setup-used-here",
    "href": "08-datasummaries.html#r-setup-used-here",
    "title": "8  Summarizing smart_cle1",
    "section": "8.1 R Setup Used Here",
    "text": "8.1 R Setup Used Here\n\nknitr::opts_chunk$set(comment = NA)\n\nlibrary(janitor) \nlibrary(broom)\nlibrary(Hmisc)\nlibrary(mosaic)\nlibrary(knitr)\nlibrary(tidyverse) \n\ntheme_set(theme_bw())\n\n\n8.1.1 Data Load\n\nsmart_cle1_sh <- read_rds(\"data/smart_cle1_sh.Rds\")\nsmart_cle1_cc <- read_rds(\"data/smart_cle1_cc.Rds\")"
  },
  {
    "objectID": "08-datasummaries.html#whats-in-these-data",
    "href": "08-datasummaries.html#whats-in-these-data",
    "title": "8  Summarizing smart_cle1",
    "section": "8.2 What’s in these data?",
    "text": "8.2 What’s in these data?\nThose files (_sh contains single imputations, and a shadow set of variables which have _NA at the end of their names, while _cc contains only the complete cases) describe information on the following variables from the BRFSS 2017, who live in the Cleveland-Elyria, OH, Metropolitan Statistical Area.\n\n\n\n\n\n\n\nVariable\nDescription\n\n\n\n\nSEQNO\nrespondent identification number (all begin with 2016)\n\n\nphyshealth\nNow thinking about your physical health, which includes physical illness and injury, for how many days during the past 30 days was your physical health not good?\n\n\ngenhealth\nWould you say that in general, your health is … (five categories: Excellent, Very Good, Good, Fair or Poor)\n\n\nbmi\nBody mass index, in kg/m2\n\n\nage_imp\nAge, imputed, in years\n\n\nfemale\nSex, 1 = female, 0 = male\n\n\nrace_eth\nRace and Ethnicity, in five categories\n\n\ninternet30\nHave you used the internet in the past 30 days? (1 = yes, 0 = no)\n\n\nsmoke100\nHave you smoked at least 100 cigarettes in your life? (1 = yes, 0 = no)\n\n\nactivity\nPhysical activity (Highly Active, Active, Insufficiently Active, Inactive)\n\n\ndrinks_wk\nOn average, how many drinks of alcohol do you consume in a week?\n\n\nveg_day\nHow many servings of vegetables do you consume per day, on average?"
  },
  {
    "objectID": "08-datasummaries.html#general-approaches-to-obtaining-numeric-summaries",
    "href": "08-datasummaries.html#general-approaches-to-obtaining-numeric-summaries",
    "title": "8  Summarizing smart_cle1",
    "section": "8.3 General Approaches to Obtaining Numeric Summaries",
    "text": "8.3 General Approaches to Obtaining Numeric Summaries\n\n8.3.1 summary for a data frame\nOf course, we can use the usual summary to get some basic information about the data.\n\nsummary(smart_cle1_cc)\n\n     SEQNO             physhealth           genhealth        bmi       \n Min.   :2.017e+09   Min.   : 0.000   1_Excellent:134   Min.   :13.30  \n 1st Qu.:2.017e+09   1st Qu.: 0.000   2_VeryGood :314   1st Qu.:24.23  \n Median :2.017e+09   Median : 0.000   3_Good     :284   Median :27.48  \n Mean   :2.017e+09   Mean   : 4.361   4_Fair     :114   Mean   :28.51  \n 3rd Qu.:2.017e+09   3rd Qu.: 3.000   5_Poor     : 46   3rd Qu.:31.82  \n Max.   :2.017e+09   Max.   :30.000                     Max.   :63.00  \n    age_imp          female                           race_eth  \n Min.   :18.00   Min.   :0.0000   White non-Hispanic      :661  \n 1st Qu.:43.00   1st Qu.:0.0000   Black non-Hispanic      :168  \n Median :58.00   Median :1.0000   Other race non-Hispanic : 20  \n Mean   :56.46   Mean   :0.5807   Multiracial non-Hispanic: 17  \n 3rd Qu.:69.00   3rd Qu.:1.0000   Hispanic                : 26  \n Max.   :95.00   Max.   :1.0000                                 \n   internet30       smoke100                       activity     drinks_wk     \n Min.   :0.000   Min.   :0.0000   Highly_Active        :308   Min.   : 0.000  \n 1st Qu.:1.000   1st Qu.:0.0000   Active               :155   1st Qu.: 0.000  \n Median :1.000   Median :0.0000   Insufficiently_Active:167   Median : 0.470  \n Mean   :0.833   Mean   :0.4787   Inactive             :262   Mean   : 2.727  \n 3rd Qu.:1.000   3rd Qu.:1.0000                               3rd Qu.: 2.850  \n Max.   :1.000   Max.   :1.0000                               Max.   :56.000  \n    veg_day     \n Min.   :0.000  \n 1st Qu.:1.280  \n Median :1.730  \n Mean   :1.919  \n 3rd Qu.:2.422  \n Max.   :7.300  \n\n\n\n\n8.3.2 The inspect function from the mosaic package\n\ninspect(smart_cle1_cc)\n\n\ncategorical variables:  \n       name  class levels   n missing\n1 genhealth factor      5 892       0\n2  race_eth factor      5 892       0\n3  activity factor      4 892       0\n                                   distribution\n1 2_VeryGood (35.2%), 3_Good (31.8%) ...       \n2 White non-Hispanic (74.1%) ...               \n3 Highly_Active (34.5%) ...                    \n\nquantitative variables:  \n        name   class       min         Q1       median           Q3\n1      SEQNO numeric 2.017e+09 2.0170e+09 2.017001e+09 2.017001e+09\n2 physhealth numeric 0.000e+00 0.0000e+00 0.000000e+00 3.000000e+00\n3        bmi numeric 1.330e+01 2.4235e+01 2.747500e+01 3.181500e+01\n4    age_imp numeric 1.800e+01 4.3000e+01 5.800000e+01 6.900000e+01\n5     female numeric 0.000e+00 0.0000e+00 1.000000e+00 1.000000e+00\n6 internet30 numeric 0.000e+00 1.0000e+00 1.000000e+00 1.000000e+00\n7   smoke100 numeric 0.000e+00 0.0000e+00 0.000000e+00 1.000000e+00\n8  drinks_wk numeric 0.000e+00 0.0000e+00 4.700000e-01 2.850000e+00\n9    veg_day numeric 0.000e+00 1.2800e+00 1.730000e+00 2.422500e+00\n           max         mean          sd   n missing\n1 2017001133.0 2.017001e+09 326.8928344 892       0\n2         30.0 4.360987e+00   8.8153373 892       0\n3         63.0 2.850905e+01   6.5057975 892       0\n4         95.0 5.645516e+01  18.0333027 892       0\n5          1.0 5.807175e-01   0.4937185 892       0\n6          1.0 8.329596e-01   0.3732212 892       0\n7          1.0 4.786996e-01   0.4998263 892       0\n8         56.0 2.726525e+00   5.7172011 892       0\n9          7.3 1.918643e+00   1.0262415 892       0\n\n\n\n\n8.3.3 The describe function in Hmisc\nThis provides some useful additional summaries, including a list of the lowest and highest values (which is very helpful when checking data.)\n\nsmart_cle1_cc |>\n  select(bmi, genhealth, female) |>\n  describe()\n\nselect(smart_cle1_cc, bmi, genhealth, female) \n\n 3  Variables      892  Observations\n--------------------------------------------------------------------------------\nbmi \n       n  missing distinct     Info     Mean      Gmd      .05      .10 \n     892        0      481        1    28.51    6.952    20.09    21.40 \n     .25      .50      .75      .90      .95 \n   24.23    27.48    31.81    36.78    41.09 \n\nlowest : 13.30 13.64 15.71 15.75 17.07, highest: 52.74 56.31 57.12 58.98 63.00\n--------------------------------------------------------------------------------\ngenhealth \n       n  missing distinct \n     892        0        5 \n\nlowest : 1_Excellent 2_VeryGood  3_Good      4_Fair      5_Poor     \nhighest: 1_Excellent 2_VeryGood  3_Good      4_Fair      5_Poor     \n                                                                      \nValue      1_Excellent  2_VeryGood      3_Good      4_Fair      5_Poor\nFrequency          134         314         284         114          46\nProportion       0.150       0.352       0.318       0.128       0.052\n--------------------------------------------------------------------------------\nfemale \n       n  missing distinct     Info      Sum     Mean      Gmd \n     892        0        2     0.73      518   0.5807   0.4875 \n\n--------------------------------------------------------------------------------\n\n\n\nThe Info measure is used for quantitative and binary variables. It is a relative information measure that increases towards 1 for variables with no ties, and is smaller for variables with many ties.\nThe Gmd is the Gini mean difference. It is a measure of spread (or dispersion), where larger values indicate greater spread in the distribution, like the standard deviation or the interquartile range. It is defined as the mean absolute difference between any pairs of observations.\n\nSee the Help file for describe in the Hmisc package for more details on these measures, and on the settings for describe."
  },
  {
    "objectID": "08-datasummaries.html#counting-as-exploratory-data-analysis",
    "href": "08-datasummaries.html#counting-as-exploratory-data-analysis",
    "title": "8  Summarizing smart_cle1",
    "section": "8.4 Counting as exploratory data analysis",
    "text": "8.4 Counting as exploratory data analysis\nCounting and/or tabulating things can be amazingly useful. Suppose we want to understand the genhealth values, after our single imputation.\n\nsmart_cle1_sh |> count(genhealth) |>\n  mutate(percent = 100*n / sum(n))\n\n# A tibble: 5 × 3\n  genhealth       n percent\n  <fct>       <int>   <dbl>\n1 1_Excellent   164   14.5 \n2 2_VeryGood    386   34.1 \n3 3_Good        365   32.2 \n4 4_Fair        158   13.9 \n5 5_Poor         60    5.30\n\n\nWe might use tabyl to do this job…\n\nsmart_cle1_sh |> \n  tabyl(genhealth) |>\n  adorn_pct_formatting(digits = 1) |>\n  kable()\n\n\n\n\ngenhealth\nn\npercent\n\n\n\n\n1_Excellent\n164\n14.5%\n\n\n2_VeryGood\n386\n34.1%\n\n\n3_Good\n365\n32.2%\n\n\n4_Fair\n158\n13.9%\n\n\n5_Poor\n60\n5.3%\n\n\n\n\n\n\n8.4.1 Did genhealth vary by smoking status?\n\nsmart_cle1_sh |> \n  count(genhealth, smoke100) |> \n  mutate(percent = 100*n / sum(n))\n\n# A tibble: 10 × 4\n   genhealth   smoke100     n percent\n   <fct>          <dbl> <int>   <dbl>\n 1 1_Excellent        0   105    9.27\n 2 1_Excellent        1    59    5.21\n 3 2_VeryGood         0   220   19.4 \n 4 2_VeryGood         1   166   14.7 \n 5 3_Good             0   184   16.2 \n 6 3_Good             1   181   16.0 \n 7 4_Fair             0    67    5.91\n 8 4_Fair             1    91    8.03\n 9 5_Poor             0    24    2.12\n10 5_Poor             1    36    3.18\n\n\nSuppose we want to find the percentage within each smoking status group. Here’s one approach…\n\nsmart_cle1_sh |>\n    count(smoke100, genhealth) |>\n    group_by(smoke100) |>\n    mutate(prob = 100*n / sum(n)) \n\n# A tibble: 10 × 4\n# Groups:   smoke100 [2]\n   smoke100 genhealth       n  prob\n      <dbl> <fct>       <int> <dbl>\n 1        0 1_Excellent   105 17.5 \n 2        0 2_VeryGood    220 36.7 \n 3        0 3_Good        184 30.7 \n 4        0 4_Fair         67 11.2 \n 5        0 5_Poor         24  4   \n 6        1 1_Excellent    59 11.1 \n 7        1 2_VeryGood    166 31.1 \n 8        1 3_Good        181 34.0 \n 9        1 4_Fair         91 17.1 \n10        1 5_Poor         36  6.75\n\n\nAnd here’s another …\n\nsmart_cle1_sh |>\n  tabyl(smoke100, genhealth) |>\n  adorn_totals(where = c(\"row\", \"col\")) |> \n  adorn_percentages(denominator = \"row\") |>\n  adorn_pct_formatting(digits = 1) |>\n  adorn_ns(position = \"front\")\n\n smoke100 1_Excellent  2_VeryGood      3_Good      4_Fair    5_Poor\n        0 105 (17.5%) 220 (36.7%) 184 (30.7%)  67 (11.2%) 24 (4.0%)\n        1  59 (11.1%) 166 (31.1%) 181 (34.0%)  91 (17.1%) 36 (6.8%)\n    Total 164 (14.5%) 386 (34.1%) 365 (32.2%) 158 (13.9%) 60 (5.3%)\n         Total\n  600 (100.0%)\n  533 (100.0%)\n 1133 (100.0%)\n\n\n\n\n8.4.2 What’s the distribution of physhealth?\nWe can count quantitative variables with discrete sets of possible values, like physhealth, which is captured as an integer (that must fall between 0 and 30.)\n\nsmart_cle1_sh |> count(physhealth)\n\n# A tibble: 21 × 2\n   physhealth     n\n        <dbl> <int>\n 1          0   690\n 2          1    49\n 3          2    61\n 4          3    39\n 5          4    17\n 6          5    43\n 7          6    13\n 8          7    18\n 9          8     5\n10         10    32\n# … with 11 more rows\n\n\nOf course, a natural summary of a quantitative variable like this would be graphical.\n\nggplot(smart_cle1_sh, aes(physhealth)) +\n    geom_histogram(binwidth = 1, \n                   fill = \"dodgerblue\", col = \"white\") +\n    labs(title = \"Days with Poor Physical Health in the Past 30\",\n         subtitle = \"Most subjects are pretty healthy in this regard, but there are some 30s\")\n\n\n\n\n\n\n8.4.3 What’s the distribution of bmi?\nbmi is the body-mass index, an indicator of size (thickness, really.)\n\nggplot(smart_cle1_sh, aes(bmi)) +\n    geom_histogram(bins = 30, \n                   fill = \"firebrick\", col = \"white\") + \n    labs(title = paste0(\"Body-Mass Index for \", \n                        nrow(smart_cle1_sh), \n                        \" BRFSS respondents\"))\n\n\n\n\n\n\n8.4.4 How many of the respondents have a BMI below 30?\n\nsmart_cle1_sh |> count(bmi < 30) |> \n  mutate(proportion = n / sum(n))\n\n# A tibble: 2 × 3\n  `bmi < 30`     n proportion\n  <lgl>      <int>      <dbl>\n1 FALSE        330      0.291\n2 TRUE         803      0.709\n\n\n\n\n8.4.5 How many of the respondents with a BMI < 30 are highly active?\n\nsmart_cle1_sh |> \n  filter(bmi < 30) |> \n  tabyl(activity) |>\n  adorn_pct_formatting()\n\n              activity   n percent\n         Highly_Active 343   42.7%\n                Active 133   16.6%\n Insufficiently_Active 129   16.1%\n              Inactive 198   24.7%\n\n\n\n\n8.4.6 Is obesity associated with smoking history?\n\nsmart_cle1_sh |> count(smoke100, bmi < 30) |>\n    group_by(smoke100) |>\n    mutate(percent = 100*n/sum(n))\n\n# A tibble: 4 × 4\n# Groups:   smoke100 [2]\n  smoke100 `bmi < 30`     n percent\n     <dbl> <lgl>      <int>   <dbl>\n1        0 FALSE        163    27.2\n2        0 TRUE         437    72.8\n3        1 FALSE        167    31.3\n4        1 TRUE         366    68.7\n\n\n\n\n8.4.7 Comparing drinks_wk summaries by obesity status\nCan we compare the drinks_wk means, medians and 75th percentiles for respondents whose BMI is below 30 to the respondents whose BMI is not?\n\nsmart_cle1_sh |>\n    group_by(bmi < 30) |>\n    summarize(mean(drinks_wk), median(drinks_wk), \n              q75 = quantile(drinks_wk, 0.75))\n\n# A tibble: 2 × 4\n  `bmi < 30` `mean(drinks_wk)` `median(drinks_wk)`   q75\n  <lgl>                  <dbl>               <dbl> <dbl>\n1 FALSE                   1.67                0.23  1.17\n2 TRUE                    2.80                0.23  2.8"
  },
  {
    "objectID": "08-datasummaries.html#can-bmi-predict-physhealth",
    "href": "08-datasummaries.html#can-bmi-predict-physhealth",
    "title": "8  Summarizing smart_cle1",
    "section": "8.5 Can bmi predict physhealth?",
    "text": "8.5 Can bmi predict physhealth?\nWe’ll start with an effort to predict physhealth using bmi. A natural graph would be a scatterplot.\n\nggplot(data = smart_cle1_sh, aes(x = bmi, y = physhealth)) +\n    geom_point()\n\n\n\n\nA good question to ask ourselves here might be: “In what BMI range can we make a reasonable prediction of physhealth?”\nNow, we might take the plot above and add a simple linear model …\n\nggplot(data = smart_cle1_sh, aes(x = bmi, y = physhealth)) +\n    geom_point() +\n    geom_smooth(method = \"lm\", se = FALSE, col = \"red\")\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\nwhich shows the same least squares regression model that we can fit with the lm command.\n\n8.5.1 Fitting a Simple Regression Model\n\nmodel_A <- lm(physhealth ~ bmi, data = smart_cle1_sh)\n\nmodel_A\n\n\nCall:\nlm(formula = physhealth ~ bmi, data = smart_cle1_sh)\n\nCoefficients:\n(Intercept)          bmi  \n    -2.8121       0.2643  \n\nsummary(model_A)\n\n\nCall:\nlm(formula = physhealth ~ bmi, data = smart_cle1_sh)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-10.5258  -4.5943  -3.5608  -0.5106  29.2965 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) -2.81208    1.21672  -2.311    0.021 *  \nbmi          0.26433    0.04188   6.312 3.95e-10 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 8.968 on 1131 degrees of freedom\nMultiple R-squared:  0.03403,   Adjusted R-squared:  0.03317 \nF-statistic: 39.84 on 1 and 1131 DF,  p-value: 3.95e-10\n\nconfint(model_A, level = 0.95)\n\n                 2.5 %     97.5 %\n(Intercept) -5.1993624 -0.4247909\nbmi          0.1821599  0.3464915\n\n\nThe model coefficients can be obtained by printing the model object, and the summary function provides several useful descriptions of the model’s residuals, its statistical significance, and quality of fit.\n\n\n8.5.2 Model Summary for a Simple (One-Predictor) Regression\nThe fitted model predicts physhealth using a prediction equation we can read off from the model coefficient estimates. Specifically, we have:\n\ncoef(model_A)\n\n(Intercept)         bmi \n -2.8120766   0.2643257 \n\n\nso the equation is physhealth = -2.82 + 0.265 bmi.\nEach of the 1133 respondents included in the smart_cle1_sh data makes a contribution to this model.\n\n8.5.2.1 Residuals\nSuppose Harry is one of the people in that group, and Harry’s data is bmi = 20, and physhealth = 3.\n\nHarry’s observed value of physhealth is just the value we have in the data for them, in this case, observed physhealth = 3 for Harry.\nHarry’s fitted or predicted physhealth value is the result of calculating -2.82 + 0.265 bmi for Harry. So, if Harry’s BMI was 20, then Harry’s predicted physhealth value is -2.82 + 0.265 (20) = 2.48.\nThe residual for Harry is then his observed outcome minus his fitted outcome, so Harry has a residual of 3 - 2.48 = 0.52.\nGraphically, a residual represents vertical distance between the observed point and the fitted regression line.\nPoints above the regression line will have positive residuals, and points below the regression line will have negative residuals. Points on the line have zero residuals.\n\nThe residuals are summarized at the top of the summary output for linear model.\n\nsummary(model_A)\n\n\nCall:\nlm(formula = physhealth ~ bmi, data = smart_cle1_sh)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-10.5258  -4.5943  -3.5608  -0.5106  29.2965 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) -2.81208    1.21672  -2.311    0.021 *  \nbmi          0.26433    0.04188   6.312 3.95e-10 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 8.968 on 1131 degrees of freedom\nMultiple R-squared:  0.03403,   Adjusted R-squared:  0.03317 \nF-statistic: 39.84 on 1 and 1131 DF,  p-value: 3.95e-10\n\n\n\nThe mean residual will always be zero in an ordinary least squares model, but a five number summary of the residuals is provided by the summary, as is an estimated standard deviation of the residuals (called here the Residual standard error.)\nIn the smart_cle1_sh data, the minimum residual was -10.53, so for one subject, the observed value was 10.53 days smaller than the predicted value. This means that the prediction was 10.53 days too large for that subject.\nSimilarly, the maximum residual was 29.30 days, so for one subject the prediction was 29.30 days too small. Not a strong performance.\nIn a least squares model, the residuals are assumed to follow a Normal distribution, with mean zero, and standard deviation (for the smart_cle1_sh data) of about 9.0 days. We know this because the residual standard error is specified as 8.968 later in the linear model output. Thus, by the definition of a Normal distribution, we’d expect\nabout 68% of the residuals to be between -9 and +9 days,\nabout 95% of the residuals to be between -18 and +18 days,\nabout all (99.7%) of the residuals to be between -27 and +27 days.\n\n\n\n8.5.2.2 Coefficients section\nThe summary for a linear model shows Estimates, Standard Errors, t values and p values for each coefficient fit.\n\nsummary(model_A)\n\n\nCall:\nlm(formula = physhealth ~ bmi, data = smart_cle1_sh)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-10.5258  -4.5943  -3.5608  -0.5106  29.2965 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) -2.81208    1.21672  -2.311    0.021 *  \nbmi          0.26433    0.04188   6.312 3.95e-10 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 8.968 on 1131 degrees of freedom\nMultiple R-squared:  0.03403,   Adjusted R-squared:  0.03317 \nF-statistic: 39.84 on 1 and 1131 DF,  p-value: 3.95e-10\n\n\n\nThe Estimates are the point estimates of the intercept and slope of bmi in our model.\nIn this case, our estimated slope is 0.265, which implies that if Harry’s BMI is 20 and Sally’s BMI is 21, we predict that Sally’s physhealth will be 0.265 days larger than Harry’s.\nThe Standard Errors are also provided for each estimate. We can create rough 95% uncertainty intervals for these estimated coefficients by adding and subtracting two standard errors from each coefficient, or we can get a slightly more accurate answer with the confint function.\nHere, the 95% uncertainty interval for the slope of bmi is estimated to be (0.18, 0.35). This is a good measure of the uncertainty in the slope that is captured by our model. We are 95% confident in the process of building this interval, but this doesn’t mean we’re 95% sure that the true slope is actually in that interval.\n\nAlso available are a t value (just the Estimate divided by the Standard Error) and the appropriate p value for testing the null hypothesis that the true value of the coefficient is 0 against a two-tailed alternative.\n\nIf a slope coefficient is statistically detectably different from 0, this implies that 0 will not be part of the uncertainty interval obtained through confint.\nIf the slope was zero, it would suggest that bmi would add no predictive value to the model. But that’s unlikely here.\n\nIf the bmi slope coefficient is associated with a small p value, as in the case of our model_A, it suggests that the model including bmi is statistically detectably better at predicting physhealth than the model without bmi.\n\nWithout bmi our model_A would become an intercept-only model, in this case, which would predict the mean physhealth for everyone, regardless of any other information.\n\n\n\n8.5.2.3 Model Fit Summaries\n\nsummary(model_A)\n\n\nCall:\nlm(formula = physhealth ~ bmi, data = smart_cle1_sh)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-10.5258  -4.5943  -3.5608  -0.5106  29.2965 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) -2.81208    1.21672  -2.311    0.021 *  \nbmi          0.26433    0.04188   6.312 3.95e-10 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 8.968 on 1131 degrees of freedom\nMultiple R-squared:  0.03403,   Adjusted R-squared:  0.03317 \nF-statistic: 39.84 on 1 and 1131 DF,  p-value: 3.95e-10\n\n\nThe summary of a linear model also displays:\n\nThe residual standard error and associated degrees of freedom for the residuals.\nFor a simple (one-predictor) least regression like this, the residual degrees of freedom will be the sample size minus 2.\nThe multiple R-squared (or coefficient of determination)\nThis is interpreted as the proportion of variation in the outcome (physhealth) accounted for by the model, and will always fall between 0 and 1 as a result.\nOur model_A accounts for a mere 3.4% of the variation in physhealth.\nThe Adjusted R-squared value “adjusts” for the size of our model in terms of the number of coefficients included in the model.\nThe adjusted R-squared will always be smaller than the Multiple R-squared.\nWe still hope to find models with relatively large adjusted \\(R^2\\) values.\nIn particular, we hope to find models where the adjusted \\(R^2\\) isn’t substantially less than the Multiple R-squared.\nThe adjusted R-squared is usually a better estimate of likely performance of our model in new data than is the Multiple R-squared.\nThe adjusted R-squared result is no longer interpretable as a proportion of anything - in fact, it can fall below 0.\nWe can obtain the adjusted \\(R^2\\) from the raw \\(R^2\\), the number of observations N and the number of predictors p included in the model, as follows:\n\n\\[\nR^2_{adj} = 1 - \\frac{(1 - R^2)(N - 1)}{N - p - 1},\n\\]\n\nThe F statistic and p value from a global ANOVA test of the model.\n\nObtaining a statistically significant result here is usually pretty straightforward, since the comparison is between our model, and a model which simply predicts the mean value of the outcome for everyone.\nIn a simple (one-predictor) linear regression like this, the t statistic for the slope is just the square root of the F statistic, and the resulting p values for the slope’s t test and for the global F test will be identical.\n\nTo see the complete ANOVA F test for this model, we can run anova(model_A).\n\n\nanova(model_A)\n\nAnalysis of Variance Table\n\nResponse: physhealth\n            Df Sum Sq Mean Sq F value   Pr(>F)    \nbmi          1   3204  3204.4   39.84 3.95e-10 ***\nResiduals 1131  90968    80.4                     \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\n\n\n8.5.3 Using the broom package\nThe broom package has three functions of particular use in a linear regression model:\n\n8.5.3.1 The tidy function\ntidy builds a data frame/tibble containing information about the coefficients in the model, their standard errors, t statistics and p values.\n\ntidy(model_A)\n\n# A tibble: 2 × 5\n  term        estimate std.error statistic  p.value\n  <chr>          <dbl>     <dbl>     <dbl>    <dbl>\n1 (Intercept)   -2.81     1.22       -2.31 2.10e- 2\n2 bmi            0.264    0.0419      6.31 3.95e-10\n\n\nIt’s often useful to include other summaries in this tidying, for instance:\n\ntidy(model_A, conf.int = TRUE, conf.level = 0.9) |>\n  select(term, estimate, conf.low, conf.high)\n\n# A tibble: 2 × 4\n  term        estimate conf.low conf.high\n  <chr>          <dbl>    <dbl>     <dbl>\n1 (Intercept)   -2.81    -4.82     -0.809\n2 bmi            0.264    0.195     0.333\n\n\n\n\n8.5.3.2 The glance function\nglance` builds a data frame/tibble containing summary statistics about the model, including\n\nthe (raw) multiple \\(R^2\\) and adjusted R^2\nsigma which is the residual standard error\nthe F statistic, p.value model df and df.residual associated with the global ANOVA test, plus\nseveral statistics that will be useful in comparing models down the line:\nthe model’s log likelihood function value, logLik\nthe model’s Akaike’s Information Criterion value, AIC\nthe model’s Bayesian Information Criterion value, BIC\nand the model’s deviance statistic\n\n\nglance(model_A)\n\n# A tibble: 1 × 12\n  r.squared adj.r.squa…¹ sigma stati…²  p.value    df logLik   AIC   BIC devia…³\n      <dbl>        <dbl> <dbl>   <dbl>    <dbl> <dbl>  <dbl> <dbl> <dbl>   <dbl>\n1    0.0340       0.0332  8.97    39.8 3.95e-10     1 -4092. 8190. 8205.  90968.\n# … with 2 more variables: df.residual <int>, nobs <int>, and abbreviated\n#   variable names ¹​adj.r.squared, ²​statistic, ³​deviance\n\n\n\n\n8.5.3.3 The augment function\naugment builds a data frame/tibble which adds fitted values, residuals and other diagnostic summaries that describe each observation to the original data used to fit the model, and this includes\n\n.fitted and .resid, the fitted and residual values, in addition to\n.hat, the leverage value for this observation\n.cooksd, the Cook’s distance measure of influence for this observation\n.stdresid, the standardized residual (think of this as a z-score - a measure of the residual divided by its associated standard deviation .sigma)\nand se.fit which will help us generate prediction intervals for the model downstream\n\nNote that each of the new columns begins with . to avoid overwriting any data.\n\nhead(augment(model_A))\n\n# A tibble: 6 × 8\n  physhealth   bmi .fitted .resid     .hat .sigma    .cooksd .std.resid\n       <dbl> <dbl>   <dbl>  <dbl>    <dbl>  <dbl>      <dbl>      <dbl>\n1          4  27.9    4.57 -0.572 0.000886   8.97 0.00000181    -0.0638\n2          0  23.0    3.28 -3.28  0.00149    8.97 0.000100      -0.366 \n3          0  26.9    4.31 -4.31  0.000927   8.97 0.000107      -0.480 \n4          0  26.5    4.20 -4.20  0.000956   8.97 0.000105      -0.468 \n5          0  24.2    3.60 -3.60  0.00125    8.97 0.000101      -0.401 \n6          2  27.7    4.51 -2.51  0.000891   8.97 0.0000351     -0.281 \n\n\nFor more on the broom package, you may want to look at this vignette.\n\n\n\n8.5.4 How does the model do? (Residuals vs. Fitted Values)\n\nRemember that the \\(R^2\\) value was about 3.4%.\n\n\nplot(model_A, which = 1)\n\n\n\n\nThis is a plot of residuals vs. fitted values. The goal here is for this plot to look like a random scatter of points, perhaps like a “fuzzy football”, and that’s not what we have. Why?\nIf you prefer, here’s a ggplot2 version of a similar plot, now looking at standardized residuals instead of raw residuals, and adding a loess smooth and a linear fit to the result.\n\nggplot(augment(model_A), aes(x = .fitted, y = .std.resid)) +\n    geom_point() +\n    geom_smooth(method = \"lm\", se = FALSE, col = \"red\", linetype = \"dashed\") +\n    geom_smooth(method = \"loess\", se = FALSE, col = \"navy\") +\n    theme_bw()\n\n`geom_smooth()` using formula = 'y ~ x'\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\nThe problem we’re having here becomes, I think, a little more obvious if we look at what we’re predicting. Does physhealth look like a good candidate for a linear model?\n\nggplot(smart_cle1_sh, aes(x = physhealth)) +\n  geom_histogram(bins = 30, fill = \"dodgerblue\", \n                 color = \"royalblue\")\n\n\n\n\n\nsmart_cle1_sh |> count(physhealth == 0, physhealth == 30)\n\n# A tibble: 3 × 3\n  `physhealth == 0` `physhealth == 30`     n\n  <lgl>             <lgl>              <int>\n1 FALSE             FALSE                343\n2 FALSE             TRUE                 100\n3 TRUE              FALSE                690\n\n\nNo matter what model we fit, if we are predicting physhealth, and most of the data are values of 0 and 30, we have limited variation in our outcome, and so our linear model will be somewhat questionable just on that basis.\nA normal Q-Q plot of the standardized residuals for our model_A shows this problem, too.\n\nplot(model_A, which = 2)\n\n\n\n\nWe’re going to need a method to deal with this sort of outcome, that has both a floor and a ceiling. We’ll get there eventually, but linear regression alone doesn’t look promising.\nAll right, so that didn’t go anywhere great. We’ll try again, with a new outcome, in the next chapter."
  },
  {
    "objectID": "09-anovasmart.html#r-setup-used-here",
    "href": "09-anovasmart.html#r-setup-used-here",
    "title": "9  Analysis of Variance with SMART",
    "section": "9.1 R Setup Used Here",
    "text": "9.1 R Setup Used Here\n\nknitr::opts_chunk$set(comment = NA)\n\nlibrary(janitor)\nlibrary(broom)\nlibrary(knitr)\nlibrary(mosaic)\nlibrary(tidyverse) \n\ntheme_set(theme_bw())\n\n\n9.1.1 Data Load\n\nsmart_cle1_sh <- read_rds(\"data/smart_cle1_sh.Rds\")\n\nThe variables we’ll look at in this chapter are as follows.\n\n\n\n\n\n\n\nVariable\nDescription\n\n\n\n\nSEQNO\nrespondent identification number (all begin with 2016)\n\n\nbmi\nBody mass index, in kg/m2\n\n\nfemale\nSex, 1 = female, 0 = male\n\n\nsmoke100\nHave you smoked at least 100 cigarettes in your life? (1 = yes, 0 = no)\n\n\nactivity\nPhysical activity (Highly Active, Active, Insufficiently Active, Inactive)\n\n\ndrinks_wk\nOn average, how many drinks of alcohol do you consume in a week?\n\n\nphyshealth\nNow thinking about your physical health, which includes physical illness and injury, for how many days during the past 30 days was your physical health not good?"
  },
  {
    "objectID": "09-anovasmart.html#a-one-factor-analysis-of-variance",
    "href": "09-anovasmart.html#a-one-factor-analysis-of-variance",
    "title": "9  Analysis of Variance with SMART",
    "section": "9.2 A One-Factor Analysis of Variance",
    "text": "9.2 A One-Factor Analysis of Variance\nWe’ll be predicting body mass index, at first using a single factor as a predictor: the activity level.\n\n9.2.1 Can activity be used to predict bmi?\n\nggplot(smart_cle1_sh, aes(x = activity, y = bmi, \n                          fill = activity)) +\n  geom_violin(alpha = 0.3) +\n  geom_boxplot(width = 0.3, notch = TRUE) + \n  guides(fill = \"none\") +\n  coord_flip() +\n  labs(title = \"BMI as a function of Activity Level\",\n       subtitle = \"Subjects in the SMART CLE data\",\n       x = \"\", y = \"Body Mass Index\")\n\n\n\n\nHere’s a numerical summary of the distributions of bmi within each activity group.\n\nfavstats(bmi ~ activity, data = smart_cle1_sh)\n\n               activity   min      Q1   median     Q3   max     mean       sd\n1         Highly_Active 13.30 23.6275 26.99000 28.930 50.46 27.02253 5.217496\n2                Active 17.07 24.2400 27.06930 29.520 44.67 27.36157 5.151796\n3 Insufficiently_Active 17.49 25.0500 27.93776 32.180 49.98 29.04328 6.051823\n4              Inactive 13.64 25.2150 28.34000 33.775 70.56 30.15978 7.832675\n    n missing\n1 428       0\n2 173       0\n3 201       0\n4 331       0\n\n\n\n\n9.2.2 Should we transform bmi?\nThe analysis of variance is something of a misnomer. What we’re doing is using the variance to say something about population means. In light of the apparent right skew of the bmi results in each activity group, might it be a better choice to use a logarithmic transformation? We’ll use the natural logarithm here, which in R, is symbolized by log.\n\nggplot(smart_cle1_sh, aes(x = activity, y = log(bmi), \n                          fill = activity)) +\n  geom_violin(alpha = 0.3) +\n  geom_boxplot(width = 0.3, notch = TRUE) + \n  guides(fill = \"none\") +\n  coord_flip() +\n  labs(title = \"log(BMI) as a function of Activity Level\",\n       subtitle = \"Subjects in the SMART CLE data\",\n       x = \"\", y = \"log(Body Mass Index)\")\n\n\n\n\nThe logarithmic transformation yields distributions that look much more symmetric in each activity group, so we’ll proceed to build our regression model predicting log(bmi) using activity. Here’s the numerical summary of these logged results:\n\nfavstats(log(bmi) ~ activity, data = smart_cle1_sh)\n\n               activity      min       Q1   median       Q3      max     mean\n1         Highly_Active 2.587764 3.162411 3.295466 3.364879 3.921181 3.279246\n2                Active 2.837323 3.188004 3.298400 3.385068 3.799302 3.292032\n3 Insufficiently_Active 2.861629 3.220874 3.329979 3.471345 3.911623 3.348383\n4              Inactive 2.613007 3.227439 3.344274 3.519721 4.256463 3.376468\n         sd   n missing\n1 0.1851478 428       0\n2 0.1850568 173       0\n3 0.2007241 201       0\n4 0.2411196 331       0\n\n\n\n\n9.2.3 Building the ANOVA model\n\nmodel_5a <- lm(log(bmi) ~ activity, data = smart_cle1_sh)\n\nmodel_5a\n\n\nCall:\nlm(formula = log(bmi) ~ activity, data = smart_cle1_sh)\n\nCoefficients:\n                  (Intercept)                 activityActive  \n                      3.27925                        0.01279  \nactivityInsufficiently_Active               activityInactive  \n                      0.06914                        0.09722  \n\n\nThe activity data is categorical and there are four levels. The model equation is:\nlog(bmi) = 3.279 + 0.013 (activity = Active)\n                 + 0.069 (activity = Insufficiently Active)\n                 + 0.097 (activity = Inactive)\nwhere, for example, (activity = Active) is 1 if activity is Active, and 0 otherwise. The fourth level (Highly Active) is not shown here and is used as a baseline. Thus the model above can be interpreted as follows.\n\n\n\nactivity\nPredicted log(bmi)\nPredicted bmi\n\n\n\n\nHighly Active\n3.279\nexp(3.279) = 26.55\n\n\nActive\n3.279 + 0.013 = 3.292\nexp(3.292) = 26.90\n\n\nInsufficiently Active\n3.279 + 0.069 = 3.348\nexp(3.348) = 28.45\n\n\nInactive\n3.279 + 0.097 = 3.376\nexp(3.376) = 29.25\n\n\n\nThose predicted log(bmi) values should look familiar. They are just the means of log(bmi) in each group, but I’m sure you’ll also notice that the predicted bmi values are not exact matches for the observed means of bmi.\n\nsmart_cle1_sh |> group_by(activity) |>\n  summarise(mean(log(bmi)), mean(bmi))\n\n# A tibble: 4 × 3\n  activity              `mean(log(bmi))` `mean(bmi)`\n  <fct>                            <dbl>       <dbl>\n1 Highly_Active                     3.28        27.0\n2 Active                            3.29        27.4\n3 Insufficiently_Active             3.35        29.0\n4 Inactive                          3.38        30.2\n\n\n\n\n9.2.4 The ANOVA table\nNow, let’s press on to look at the ANOVA results for this model.\n\nanova(model_5a)\n\nAnalysis of Variance Table\n\nResponse: log(bmi)\n            Df Sum Sq Mean Sq F value    Pr(>F)    \nactivity     3  2.060 0.68652  16.225 2.496e-10 ***\nResiduals 1129 47.772 0.04231                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\nThe total variation in log(bmi), our outcome, is captured by the sums of squares here. SS(Total) = 2.058 + 47.770 = 49.828\nHere, the activity variable (with 4 levels, so 4-1 = 3 degrees of freedom) accounts for 4.13% (2.058 / 49.828) of the variation in log(bmi). Another way of saying this is that the model \\(R^2\\) or \\(\\eta^2\\) is 0.0413.\nThe variation accounted for by the activity categories meets the standard for a statistically detectable result, according to the ANOVA F test, although that’s not really important.\nThe square root of the Mean Square(Residuals) is the residual standard error, \\(\\sigma\\), we’ve seen in the past. MS(Residual) estimates the variance (0.0423), so the residual standard error is \\(\\sqrt{0.0423} \\approx 0.206\\).\n\n\n\n9.2.5 The Model Coefficients\nTo address the question of effect size for the various levels of activity on log(bmi), we could look directly at the regression model coefficients. For that, we might look at the model summary.\n\nsummary(model_5a)\n\n\nCall:\nlm(formula = log(bmi) ~ activity, data = smart_cle1_sh)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.76346 -0.12609 -0.00286  0.11055  0.88000 \n\nCoefficients:\n                              Estimate Std. Error t value Pr(>|t|)    \n(Intercept)                   3.279246   0.009943 329.806  < 2e-16 ***\nactivityActive                0.012785   0.018532   0.690     0.49    \nactivityInsufficiently_Active 0.069137   0.017589   3.931 8.99e-05 ***\nactivityInactive              0.097221   0.015056   6.457 1.58e-10 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.2057 on 1129 degrees of freedom\nMultiple R-squared:  0.04133,   Adjusted R-squared:  0.03878 \nF-statistic: 16.22 on 3 and 1129 DF,  p-value: 2.496e-10\n\n\nIf we want to see the confidence intervals around these estimates, we could use\n\nconfint(model_5a, conf.level = 0.95)\n\n                                    2.5 %     97.5 %\n(Intercept)                    3.25973769 3.29875522\nactivityActive                -0.02357630 0.04914707\nactivityInsufficiently_Active  0.03462572 0.10364764\nactivityInactive               0.06767944 0.12676300\n\n\nThe model suggests, based on these 1133 subjects, that (remember that the baseline category is Highly Active)\n\na 95% confidence (uncertainty) interval for the difference between Active and Highly Active subjects in log(BMI) ranges from -0.024 to 0.049\na 95% confidence (uncertainty) interval for the difference between Insufficiently Active and Highly Active subjects in log(BMI) ranges from 0.035 to 0.104\na 95% confidence (uncertainty) interval for the difference between Inactive and Highly Active subjects in log(BMI) ranges from 0.068 to 0.127\nthe model accounts for 4.13% of the variation in log(BMI), so that knowing the respondent’s activity level somewhat reduces the size of the prediction errors as compared to an intercept only model that would predict the overall mean log(BMI), regardless of activity level, for all subjects.\nfrom the summary of residuals, we see that one subject had a residual of 0.88 - that means they were predicted to have a log(BMI) 0.88 lower than their actual log(BMI) and one subject had a log(BMI) that is 0.76 larger than their actual log(BMI), at the extremes.\n\n\n\n9.2.6 Using tidy to explore the coefficients\nA better strategy for displaying the coefficients in any regression model is to use the tidy function from the broom package.\n\ntidy(model_5a, conf.int = TRUE, conf.level = 0.95) |>\n  kable(digits = 3)\n\n\n\n\n\n\n\n\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\nconf.low\nconf.high\n\n\n\n\n(Intercept)\n3.279\n0.010\n329.806\n0.00\n3.260\n3.299\n\n\nactivityActive\n0.013\n0.019\n0.690\n0.49\n-0.024\n0.049\n\n\nactivityInsufficiently_Active\n0.069\n0.018\n3.931\n0.00\n0.035\n0.104\n\n\nactivityInactive\n0.097\n0.015\n6.457\n0.00\n0.068\n0.127\n\n\n\n\n\n\n\n9.2.7 Using glance to summarize the model’s fit\n\nglance(model_5a) |> select(1:3) |> \n  kable(digits = c(4, 4, 3))\n\n\n\n\nr.squared\nadj.r.squared\nsigma\n\n\n\n\n0.0413\n0.0388\n0.206\n\n\n\n\n\n\nThe r.squared or \\(R^2\\) value is interpreted for a linear model as the percentage of variation in the outcome (here, log(bmi)) that is accounted for by the model.\nThe adj.r.squared or adjusted \\(R^2\\) value incorporates a small penalty for the number of predictors included in the model. Adjusted \\(R^2\\) is useful for models with more than one predictor, not simple regression models like this one. Like \\(R^2\\) and most of these other summaries, its primary value comes when making comparisons between models for the same outcome.\nThe sigma or \\(\\sigma\\) is the residual standard error. Doubling this value gives us a good idea of the range of errors made by the model (approximately 95% of the time if the normal distribution assumption for the residuals holds perfectly.)\n\n\nglance(model_5a) |> select(4:7) |>\n  kable(digits = c(2, 3, 0, 2))\n\n\n\n\nstatistic\np.value\ndf\nlogLik\n\n\n\n\n16.22\n0\n3\n185.99\n\n\n\n\n\n\nThe statistic and p.value shown here refer to the ANOVA F test and p value. They test the null hypothesis that the activity information is of no use in separating out the bmi data, or, equivalently, that the true \\(R^2\\) is 0.\nThe df indicates the model degrees of freedom, and in this case simply specifies the number of parameters fitted attributed to the model. Models that require more df for estimation require larger sample sizes.\nThe logLik is the log likelihood for the model. This is a function of the sample size, but we can compare the fit of multiple models by comparing this value across different models for the same outcome. You want to maximize the log-likelihood.\n\n\nglance(model_5a) |> select(8:9) |>\n  kable(digits = 2)\n\n\n\n\nAIC\nBIC\n\n\n\n\n-361.98\n-336.82\n\n\n\n\n\n\nThe AIC (or Akaike information criterion) and BIC (Bayes information criterion) are also used only to compare models. You want to minimize AIC and BIC in selecting a model. AIC and BIC are unique only up to a constant, so different packages or routines in R may give differing values, but in comparing two models - the difference in AIC (or BIC) should be consistent.\n\n\n\n9.2.8 Using augment to make predictions\nWe can obtain residuals and predicted (fitted) values for the points used to fit the model with augment from the broom package.\n\naugment(model_5a, se_fit = TRUE) |> \n  select(1:5) |> slice(1:4) |>\n  kable(digits = 3)\n\n\n\n\nlog(bmi)\nactivity\n.fitted\n.se.fit\n.resid\n\n\n\n\n3.330\nInactive\n3.376\n0.011\n-0.047\n\n\n3.138\nInactive\n3.376\n0.011\n-0.239\n\n\n3.293\nInsufficiently_Active\n3.348\n0.015\n-0.055\n\n\n3.278\nHighly_Active\n3.279\n0.010\n-0.002\n\n\n\n\n\n\nThe .fitted value is the predicted value of log(bmi) for this subject.\nThe .se.fit value shows the standard error associated with the fitted value.\nThe .resid is the residual value (observed - fitted log(bmi))\n\n\naugment(model_5a, se_fit = TRUE) |> \n  select(1:2, 6:9) |> slice(1:4) |>\n  kable(digits = 3)\n\n\n\n\nlog(bmi)\nactivity\n.hat\n.sigma\n.cooksd\n.std.resid\n\n\n\n\n3.330\nInactive\n0.003\n0.206\n0.000\n-0.227\n\n\n3.138\nInactive\n0.003\n0.206\n0.001\n-1.163\n\n\n3.293\nInsufficiently_Active\n0.005\n0.206\n0.000\n-0.269\n\n\n3.278\nHighly_Active\n0.002\n0.206\n0.000\n-0.008\n\n\n\n\n\n\nThe .hat value shows the leverage index associated with the observation (this is a function of the predictors - higher leveraged points have more unusual predictor values)\nThe .sigma value shows the estimate of the residual standard deviation if this observation were to be dropped from the model, and thus indexes how much of an outlier this observation’s residual is.\nThe .cooksd or Cook’s distance value shows the influence that the observation has on the model - it is one of a class of leave-one-out diagnostic measures. Larger values of Cook’s distance indicate more influential points.\nThe .std.resid shows the standardized residual (which is designed to have mean 0 and standard deviation 1, facilitating comparisons across models for differing outcomes)"
  },
  {
    "objectID": "09-anovasmart.html#a-two-factor-anova-without-interaction",
    "href": "09-anovasmart.html#a-two-factor-anova-without-interaction",
    "title": "9  Analysis of Variance with SMART",
    "section": "9.3 A Two-Factor ANOVA (without Interaction)",
    "text": "9.3 A Two-Factor ANOVA (without Interaction)\nLet’s add race_eth to the predictor set for log(BMI).\n\nmodel_5b <- lm(log(bmi) ~ activity + race_eth, data = smart_cle1_sh)\n\nanova(model_5b)\n\nAnalysis of Variance Table\n\nResponse: log(bmi)\n            Df Sum Sq Mean Sq F value    Pr(>F)    \nactivity     3  2.060 0.68652 16.5090 1.676e-10 ***\nrace_eth     4  0.989 0.24716  5.9435 9.843e-05 ***\nResiduals 1125 46.783 0.04158                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nNotice that the ANOVA model assesses these variables sequentially, so the SS(activity) = 2.058 is accounted for before we consider the SS(race_eth) = 0.990. Thus, in total, the model accounts for 2.058 + 0.990 = 3.048 of the sums of squares in log(bmi) in these data.\nIf we flip the order in the model, like this:\n\nlm(log(bmi) ~ race_eth + activity, data = smart_cle1_sh) |> \n  anova()\n\nAnalysis of Variance Table\n\nResponse: log(bmi)\n            Df Sum Sq Mean Sq F value    Pr(>F)    \nrace_eth     4  1.119 0.27981  6.7287 2.371e-05 ***\nactivity     3  1.929 0.64299 15.4620 7.332e-10 ***\nResiduals 1125 46.783 0.04158                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\nAfter flipping the order of the predictors, race_eth accounts for a larger Sum of Squares than it did previously, but activity accounts for a smaller amount, and the total between race_eth and activity remains the same, as 1.121 + 1.927 is still 3.048.\n\n\n9.3.1 Model Coefficients\nThe model coefficients are unchanged regardless of the order of the variables in our two-factor ANOVA model.\n\ntidy(model_5b, conf.int = TRUE, conf.level = 0.95) |>\n  select(term, estimate, std.error, conf.low, conf.high) |>\n  kable(digits = 3)\n\n\n\n\n\n\n\n\n\n\n\nterm\nestimate\nstd.error\nconf.low\nconf.high\n\n\n\n\n(Intercept)\n3.268\n0.010\n3.247\n3.288\n\n\nactivityActive\n0.012\n0.018\n-0.024\n0.048\n\n\nactivityInsufficiently_Active\n0.073\n0.018\n0.039\n0.108\n\n\nactivityInactive\n0.092\n0.015\n0.063\n0.122\n\n\nrace_ethBlack non-Hispanic\n0.066\n0.015\n0.036\n0.096\n\n\nrace_ethOther race non-Hispanic\n-0.086\n0.042\n-0.169\n-0.002\n\n\nrace_ethMultiracial non-Hispanic\n0.020\n0.042\n-0.063\n0.103\n\n\nrace_ethHispanic\n0.012\n0.035\n-0.057\n0.082\n\n\n\n\n\nThe model_5b equation is:\nlog(BMI) = 3.268\n      + 0.012 (activity = Active)\n      + 0.073 (activity = Insufficiently Active)\n      + 0.092 (activity = Inactive)\n      + 0.066 (race_eth = Black non-Hispanic)\n      - 0.086 (race_eth = Other race non-Hispanic)\n      + 0.020 (race_eth = Multiracial non-Hispanic)\n      + 0.012 (race_eth = Hispanic)\nand we can make predictions by filling in appropriate 1s and 0s for the indicator variables in parentheses.\nFor example, the predicted log(BMI) for a White Highly Active person is 3.268, as White and Highly Active are the baseline categories in our two factors.\nFor all other combinations, we can make predictions as follows:\n\nnew_dat = tibble(\n  race_eth = rep(c(\"White non-Hispanic\",\n                   \"Black non-Hispanic\",\n                   \"Other race non-Hispanic\",\n                   \"Multiracial non-Hispanic\",\n                   \"Hispanic\"), 4),\n  activity = c(rep(\"Highly_Active\", 5),\n               rep(\"Active\", 5),\n               rep(\"Insufficiently_Active\", 5),\n               rep(\"Inactive\", 5))\n  )\n\naugment(model_5b, newdata = new_dat)\n\n# A tibble: 20 × 3\n   race_eth                 activity              .fitted\n   <chr>                    <chr>                   <dbl>\n 1 White non-Hispanic       Highly_Active            3.27\n 2 Black non-Hispanic       Highly_Active            3.33\n 3 Other race non-Hispanic  Highly_Active            3.18\n 4 Multiracial non-Hispanic Highly_Active            3.29\n 5 Hispanic                 Highly_Active            3.28\n 6 White non-Hispanic       Active                   3.28\n 7 Black non-Hispanic       Active                   3.35\n 8 Other race non-Hispanic  Active                   3.19\n 9 Multiracial non-Hispanic Active                   3.30\n10 Hispanic                 Active                   3.29\n11 White non-Hispanic       Insufficiently_Active    3.34\n12 Black non-Hispanic       Insufficiently_Active    3.41\n13 Other race non-Hispanic  Insufficiently_Active    3.26\n14 Multiracial non-Hispanic Insufficiently_Active    3.36\n15 Hispanic                 Insufficiently_Active    3.35\n16 White non-Hispanic       Inactive                 3.36\n17 Black non-Hispanic       Inactive                 3.43\n18 Other race non-Hispanic  Inactive                 3.27\n19 Multiracial non-Hispanic Inactive                 3.38\n20 Hispanic                 Inactive                 3.37\n\n\n\naugment(model_5b, newdata = new_dat) |>\n  mutate(race_eth = fct_relevel(factor(race_eth),\n                                \"White non-Hispanic\",\n                                \"Black non-Hispanic\",\n                                \"Other race non-Hispanic\",\n                                \"Multiracial non-Hispanic\",\n                                \"Hispanic\"),\n         activity = fct_relevel(factor(activity),\n                                \"Highly_Active\",\n                                \"Active\",\n                                \"Insufficiently_Active\",\n                                \"Inactive\")) %>%\n  ggplot(., aes(x = activity, y = .fitted, \n                col = race_eth, group = race_eth)) +\n  geom_point(size = 2) + \n  geom_line() + \n  labs(title = \"Model 5b predictions for log(BMI)\",\n       subtitle = \"race_eth and activity, no interaction so lines are parallel\",\n       y = \"Model Predicted log(BMI)\",\n       x = \"\")\n\n\n\n\nThe lines joining the points for each race_eth category are parallel to each other. The groups always hold the same position relative to each other, regardless of their activity levels, and vice versa. There is no interaction in this model allowing the predicted effects of, say, activity on log(BMI) values to differ for the various race_eth groups. To do that, we’d have to fit the two-factor ANOVA model incorporating an interaction term."
  },
  {
    "objectID": "09-anovasmart.html#a-two-factor-anova-with-interaction",
    "href": "09-anovasmart.html#a-two-factor-anova-with-interaction",
    "title": "9  Analysis of Variance with SMART",
    "section": "9.4 A Two-Factor ANOVA (with Interaction)",
    "text": "9.4 A Two-Factor ANOVA (with Interaction)\nLet’s add the interaction of activity and race_eth (symbolized in R by activity * race_eth) to the model for log(BMI).\n\nmodel_5c <-  \n  lm(log(bmi) ~ activity * race_eth, data = smart_cle1_sh)\n\nanova(model_5c)\n\nAnalysis of Variance Table\n\nResponse: log(bmi)\n                    Df Sum Sq Mean Sq F value    Pr(>F)    \nactivity             3  2.060 0.68652 16.4468 1.839e-10 ***\nrace_eth             4  0.989 0.24716  5.9211 0.0001026 ***\nactivity:race_eth   12  0.324 0.02700  0.6469 0.8028368    \nResiduals         1113 46.459 0.04174                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nThe ANOVA model shows that the SS(interaction) = SS(activity:race_eth) is 0.324, and uses 12 degrees of freedom. The model including the interaction term now accounts for 2.058 + 0.990 + 0.324 = 3.372, which is 6.8% of the variation in log(BMI) overall (which is calculated as SS(Total) = 2.058 + 0.990 + 0.324 + 46.456 = 49.828.)\n\n9.4.1 Model Coefficients\nThe model coefficients now include additional product terms that incorporate indicator variables for both activity and race_eth. For each of the product terms to take effect, both their activity and race_eth status must yield a 1 in the indicator variables.\n\ntidy(model_5c, conf.int = TRUE, conf.level = 0.95) |>\n  select(term, estimate, std.error, conf.low, conf.high) |>\n  kable(digits = 3)\n\n\n\n\n\n\n\n\n\n\n\nterm\nestimate\nstd.error\nconf.low\nconf.high\n\n\n\n\n(Intercept)\n3.264\n0.011\n3.242\n3.287\n\n\nactivityActive\n0.021\n0.021\n-0.021\n0.062\n\n\nactivityInsufficiently_Active\n0.079\n0.020\n0.039\n0.118\n\n\nactivityInactive\n0.097\n0.018\n0.063\n0.132\n\n\nrace_ethBlack non-Hispanic\n0.062\n0.026\n0.011\n0.113\n\n\nrace_ethOther race non-Hispanic\n-0.070\n0.078\n-0.223\n0.083\n\n\nrace_ethMultiracial non-Hispanic\n0.067\n0.060\n-0.051\n0.185\n\n\nrace_ethHispanic\n0.110\n0.060\n-0.008\n0.228\n\n\nactivityActive:race_ethBlack non-Hispanic\n-0.001\n0.048\n-0.096\n0.094\n\n\nactivityInsufficiently_Active:race_ethBlack non-Hispanic\n0.005\n0.046\n-0.086\n0.096\n\n\nactivityInactive:race_ethBlack non-Hispanic\n0.008\n0.037\n-0.065\n0.080\n\n\nactivityActive:race_ethOther race non-Hispanic\n-0.065\n0.165\n-0.389\n0.259\n\n\nactivityInsufficiently_Active:race_ethOther race non-Hispanic\n-0.035\n0.101\n-0.233\n0.163\n\n\nactivityInactive:race_ethOther race non-Hispanic\n0.033\n0.129\n-0.221\n0.287\n\n\nactivityActive:race_ethMultiracial non-Hispanic\n-0.208\n0.134\n-0.470\n0.054\n\n\nactivityInsufficiently_Active:race_ethMultiracial non-Hispanic\n-0.050\n0.120\n-0.285\n0.184\n\n\nactivityInactive:race_ethMultiracial non-Hispanic\n-0.056\n0.110\n-0.272\n0.160\n\n\nactivityActive:race_ethHispanic\n-0.104\n0.096\n-0.291\n0.084\n\n\nactivityInsufficiently_Active:race_ethHispanic\n-0.240\n0.214\n-0.660\n0.179\n\n\nactivityInactive:race_ethHispanic\n-0.169\n0.082\n-0.331\n-0.008\n\n\n\n\n\nThe model_5c equation is:\nlog(BMI) = 3.264\n  + 0.021 (activity = Active)\n  + 0.079 (activity = Insufficiently Active)\n  + 0.097 (activity = Inactive)\n  + 0.062 (race_eth = Black non-Hispanic)\n  - 0.070 (race_eth = Other race non-Hispanic)\n  + 0.067 (race_eth = Multiracial non-Hispanic)\n  + 0.110 (race_eth = Hispanic)\n  - 0.002 (activity = Active)(race_eth = Black non-Hispanic)\n  + 0.005 (Insufficiently Active)(Black non-Hispanic)\n  + 0.008 (Inactive)(Black non-Hispanic)\n  - 0.065 (Active)(Other race non-Hispanic)\n  - 0.035 (Insufficiently Active)(Other race non-Hispanic)\n  + 0.033 (Inactive)(Other race non-Hispanic)\n  - 0.208 (Active)(Multiracial non-Hispanic)\n  - 0.050 (Insufficiently Active)(Multiracial non-Hispanic)\n  - 0.056 (Inactive)(Multiracial non-Hispanic)\n  - 0.104 (Active)(Hispanic)\n  - 0.240 (Insufficiently Active)(Hispanic)\n  - 0.169 (Inactive)(Hispanic)\n  \nand again, we can make predictions by filling in appropriate 1s and 0s for the indicator variables in parentheses.\nFor example, the predicted log(BMI) for a White Highly Active person is 3.264, as White and Highly Active are the baseline categories in our two factors.\nBut the predicted log(BMI) for a Hispanic Inactive person would be 3.264 + 0.097 + 0.110 - 0.169 = 3.302.\nAgain, we’ll plot the predicted log(BMI) predictions for each possible combination.\n\nnew_dat = tibble(\n  race_eth = rep(c(\"White non-Hispanic\",\n                   \"Black non-Hispanic\",\n                   \"Other race non-Hispanic\",\n                   \"Multiracial non-Hispanic\",\n                   \"Hispanic\"), 4),\n  activity = c(rep(\"Highly_Active\", 5),\n               rep(\"Active\", 5),\n               rep(\"Insufficiently_Active\", 5),\n               rep(\"Inactive\", 5))\n  )\n\naugment(model_5c, newdata = new_dat) |>\n  mutate(race_eth = fct_relevel(factor(race_eth),\n                                \"White non-Hispanic\",\n                                \"Black non-Hispanic\",\n                                \"Other race non-Hispanic\",\n                                \"Multiracial non-Hispanic\",\n                                \"Hispanic\"),\n         activity = fct_relevel(factor(activity),\n                                \"Highly_Active\",\n                                \"Active\",\n                                \"Insufficiently_Active\",\n                                \"Inactive\")) %>%\n  ggplot(., aes(x = activity, y = .fitted, \n                col = race_eth, group = race_eth)) +\n  geom_point(size = 2) + \n  geom_line() + \n  labs(title = \"Model 5c predictions for log(BMI)\",\n       subtitle = \"race_eth and activity, with interaction\",\n       y = \"Model Predicted log(BMI)\",\n       x = \"\")\n\n\n\n\nNote that the lines joining the points for each race_eth category are no longer parallel to each other. The race-ethnicity group relative positions on log(BMI) is now changing depending on the activity status.\n\n\n9.4.2 Is the interaction term necessary?\nWe can assess this in three ways, in order of importance:\n\nWith an interaction plot\nBy assessing the fraction of the variation in the outcome accounted for by the interaction\nBy assessing whether the interaction accounts for statistically detectable outcome variation\n\n\n9.4.2.1 The Interaction Plot\nA simple interaction plot is just a plot of the unadjusted outcome means, stratified by the two factors. For example, consider this plot for our two-factor ANOVA model. To obtain this plot, we first summarize the means within each group.\n\nsummaries_5 <- smart_cle1_sh |> \n  group_by(activity, race_eth) |>\n  summarize(n = n(), mean = mean(log(bmi)), \n            sd = sd(log(bmi)))\n\n`summarise()` has grouped output by 'activity'. You can override using the\n`.groups` argument.\n\nsummaries_5\n\n# A tibble: 20 × 5\n# Groups:   activity [4]\n   activity              race_eth                     n  mean      sd\n   <fct>                 <fct>                    <int> <dbl>   <dbl>\n 1 Highly_Active         White non-Hispanic         320  3.26  0.176 \n 2 Highly_Active         Black non-Hispanic          77  3.33  0.190 \n 3 Highly_Active         Other race non-Hispanic      7  3.19  0.198 \n 4 Highly_Active         Multiracial non-Hispanic    12  3.33  0.187 \n 5 Highly_Active         Hispanic                    12  3.37  0.296 \n 6 Active                White non-Hispanic         129  3.28  0.173 \n 7 Active                Black non-Hispanic          31  3.35  0.224 \n 8 Active                Other race non-Hispanic      2  3.15  0.0845\n 9 Active                Multiracial non-Hispanic     3  3.14  0.121 \n10 Active                Hispanic                     8  3.29  0.213 \n11 Insufficiently_Active White non-Hispanic         150  3.34  0.194 \n12 Insufficiently_Active Black non-Hispanic          35  3.41  0.213 \n13 Insufficiently_Active Other race non-Hispanic     11  3.24  0.137 \n14 Insufficiently_Active Multiracial non-Hispanic     4  3.36  0.374 \n15 Insufficiently_Active Hispanic                     1  3.21 NA     \n16 Inactive              White non-Hispanic         225  3.36  0.238 \n17 Inactive              Black non-Hispanic          83  3.43  0.247 \n18 Inactive              Other race non-Hispanic      4  3.32  0.238 \n19 Inactive              Multiracial non-Hispanic     5  3.37  0.129 \n20 Inactive              Hispanic                    14  3.30  0.264 \n\n\n\nggplot(summaries_5, aes(x = activity, y = mean, \n                        color = race_eth, \n                        group = race_eth)) +\n  geom_point(size = 3) +\n  geom_line() +\n  labs(title = \"Simple Interaction Plot for log(BMI)\",\n       subtitle = \"SMART CLE means by activity and race_eth\",\n       x = \"\", y = \"Mean of log(BMI)\")\n\n\n\n\nThe interaction plot suggests that there is a modest interaction here. The White non-Hispanic and Black non-Hispanic groups appear pretty parallel (and they are the two largest groups) and Other race non-Hispanic has a fairly similar pattern, but the other two groups (Hispanic and Multiracial non-Hispanic) bounce around quite a bit based on activity level.\nAn alternative would be to include a small “dodge” for each point and include error bars (means \\(\\pm\\) standard deviation) for each combination.\n\npd = position_dodge(0.2)\nggplot(summaries_5, aes(x = activity, y = mean, \n                        color = race_eth, \n                        group = race_eth)) +\n  geom_errorbar(aes(ymin = mean - sd,\n                    ymax = mean + sd),\n                width = 0.2, position = pd) +\n  geom_point(size = 3, position = pd) +\n  geom_line(position = pd) +\n  labs(title = \"Interaction Plot for log(BMI) with Error Bars\",\n       subtitle = \"SMART CLE means by activity and race_eth\",\n       x = \"\", y = \"Mean of log(BMI)\")\n\n\n\n\nHere, we see a warning flag because we have one combination (which turns out to be Insufficiently Active and Hispanic) with only one observation in it, so a standard deviation cannot be calculated. In general, I’ll stick with the simpler means plot most of the time.\n\n\n9.4.2.2 Does the interaction account for substantial variation?\nIn this case, we can look at the fraction of the overall sums of squares accounted for by the interaction.\n\nanova(model_5c)\n\nAnalysis of Variance Table\n\nResponse: log(bmi)\n                    Df Sum Sq Mean Sq F value    Pr(>F)    \nactivity             3  2.060 0.68652 16.4468 1.839e-10 ***\nrace_eth             4  0.989 0.24716  5.9211 0.0001026 ***\nactivity:race_eth   12  0.324 0.02700  0.6469 0.8028368    \nResiduals         1113 46.459 0.04174                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nHere we have\n\\[\n\\eta^2(Interaction) = \\frac{0.324}{2.058+0.990+0.324+46.456} = 0.0065\n\\]\nso the interaction accounts for 0.65% of the variation in bmi. That looks pretty modest.\n\n\n9.4.2.3 Does the interaction account for statistically detectable variation?\nWe can test this directly with the p value from the ANOVA table, which shows p = 0.803, which is far above any of our usual standards for a statistically detectable effect.\nOn the whole, I don’t think the interaction term is especially helpful in improving this model.\nIn the next chapter, we’ll look at two different examples of ANOVA models, now in more designed experiments. We’ll also add some additional details on how the analyses might proceed.\nWe’ll return to the SMART CLE data later in these Notes."
  },
  {
    "objectID": "10-anovaexamples.html#r-setup-used-here",
    "href": "10-anovaexamples.html#r-setup-used-here",
    "title": "10  Two-Way ANOVA and Interaction",
    "section": "10.1 R Setup Used Here",
    "text": "10.1 R Setup Used Here\n\nknitr::opts_chunk$set(comment = NA)\n\nlibrary(janitor)\nlibrary(broom)\nlibrary(ggridges)\nlibrary(glue)\nlibrary(gt)\nlibrary(mosaic)\nlibrary(patchwork)\nlibrary(tidyverse) \n\ntheme_set(theme_bw())\n\n\n10.1.1 Data Load\n\nbonding <- read_csv(\"data/bonding.csv\", show_col_types = FALSE) \ncortisol <- read_csv(\"data/cortisol.csv\", show_col_types = FALSE)"
  },
  {
    "objectID": "10-anovaexamples.html#the-bonding-data-a-designed-dental-experiment",
    "href": "10-anovaexamples.html#the-bonding-data-a-designed-dental-experiment",
    "title": "10  Two-Way ANOVA and Interaction",
    "section": "10.2 The bonding data: A Designed Dental Experiment",
    "text": "10.2 The bonding data: A Designed Dental Experiment\nThe bonding data describe a designed experiment into the properties of four different resin types (resin = A, B, C, D) and two different curing light sources (light = Halogen, LED) as they relate to the resulting bonding strength (measured in MPa1) on the surface of teeth. The source is @Kim2014.\nThe experiment involved making measurements of bonding strength under a total of 80 experimental setups, or runs, with 10 runs completed at each of the eight combinations of a light source and a resin type. The data are gathered in the bonding.csv file.\n\nbonding\n\n# A tibble: 80 × 4\n   run_ID light   resin strength\n   <chr>  <chr>   <chr>    <dbl>\n 1 R101   LED     B         12.8\n 2 R102   Halogen B         22.2\n 3 R103   Halogen B         24.6\n 4 R104   LED     A         17  \n 5 R105   LED     C         32.2\n 6 R106   Halogen B         27.1\n 7 R107   LED     A         23.4\n 8 R108   Halogen A         23.5\n 9 R109   Halogen D         37.3\n10 R110   Halogen A         19.7\n# … with 70 more rows"
  },
  {
    "objectID": "10-anovaexamples.html#a-one-factor-analysis-of-variance",
    "href": "10-anovaexamples.html#a-one-factor-analysis-of-variance",
    "title": "10  Two-Way ANOVA and Interaction",
    "section": "10.3 A One-Factor Analysis of Variance",
    "text": "10.3 A One-Factor Analysis of Variance\nSuppose we are interested in the distribution of the strength values for the four different types of resin.\n\nbonding |> group_by(resin) |> \n  summarise(n = n(), mean(strength), median(strength))\n\n# A tibble: 4 × 4\n  resin     n `mean(strength)` `median(strength)`\n  <chr> <int>            <dbl>              <dbl>\n1 A        20             18.4               18.0\n2 B        20             22.2               22.7\n3 C        20             25.2               25.7\n4 D        20             32.1               35.3\n\n\nI’d begin serious work with a plot.\n\n10.3.1 Look at the Data!\n\nggplot(bonding, aes(x = resin, y = strength)) +\n    geom_violin(aes(fill = resin)) +\n    geom_boxplot(width = 0.2)\n\n\n\n\nAnother good plot for this purpose is a ridgeline plot.\n\nggplot(bonding, aes(x = strength, y = resin, fill = resin)) +\n    geom_density_ridges2() +\n    guides(fill = \"none\")\n\nPicking joint bandwidth of 3.09\n\n\n\n\n\n\n\n10.3.2 Table of Summary Statistics\nWith the small size of this experiment (n = 20 for each resin type), graphical summaries may not perform as well as they often do. We’ll also produce a quick table of summary statistics for strength within each resin type.\n\nfavstats(strength ~ resin, data = bonding)\n\n  resin  min     Q1 median    Q3  max   mean       sd  n missing\n1     A  9.3 15.725  17.95 20.40 28.0 18.415 4.805948 20       0\n2     B 11.8 18.450  22.70 25.75 35.2 22.230 6.748263 20       0\n3     C 14.5 20.650  25.70 30.70 34.5 25.155 6.326425 20       0\n4     D 17.3 21.825  35.30 40.15 47.2 32.075 9.735063 20       0\n\n\nSince the means and medians within each group are fairly close, and the distributions (with the possible exception of resin D) are reasonably well approximated by the Normal, I’ll fit an ANOVA model2.\n\nanova(lm(strength ~ resin, data = bonding))\n\nAnalysis of Variance Table\n\nResponse: strength\n          Df Sum Sq Mean Sq F value   Pr(>F)    \nresin      3 1999.7  666.57  13.107 5.52e-07 ***\nResiduals 76 3865.2   50.86                     \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nIt appears that the resin types have a significant association with mean strength of the bonds. Can we identify which resin types have generally higher or lower strength?\n\nt_bond <- TukeyHSD(aov(strength ~ resin, data = bonding), \n                ordered = TRUE, conf.level = 0.90)\n\ntidy(t_bond) |> \n  select(-c(term, null.value)) |>\n  mutate(across(.cols = -contrast, num, digits = 3)) |>\n  arrange(desc(estimate)) |>\n  gt() |>\n  tab_header(title = \"Comparing Mean Bond Strength across pairs of resin types\",\n             subtitle = \"90% Tukey HSD Confidence Intervals\") |>\n  tab_footnote(footnote = glue(nrow(bonding), \" teeth in bonding data\"))\n\n\n\n\n\n  \n    \n      Comparing Mean Bond Strength across pairs of resin types\n    \n    \n      90% Tukey HSD Confidence Intervals\n    \n  \n  \n    \n      contrast\n      estimate\n      conf.low\n      conf.high\n      adj.p.value\n    \n  \n  \n    D-A\n13.660\n8.403\n18.917\n0.000\n    D-B\n9.845\n4.588\n15.102\n0.000\n    D-C\n6.920\n1.663\n12.177\n0.015\n    C-A\n6.740\n1.483\n11.997\n0.019\n    B-A\n3.815\n-1.442\n9.072\n0.335\n    C-B\n2.925\n-2.332\n8.182\n0.568\n  \n  \n  \n    \n       80 teeth in bonding data\n    \n  \n\n\n\ntidy(t_bond) |>\n  mutate(contrast = fct_reorder(contrast, estimate, .desc = TRUE)) %>%\n  ggplot(., aes(x = contrast, y = estimate, ymin = conf.low, ymax = conf.high)) +\n  geom_pointrange() +\n  geom_hline(yintercept = 0, col = \"red\", linetype = \"dashed\") +\n  geom_label(aes(label = round_half_up(estimate, 2))) +\n  labs(title = \"Comparing Mean Bond Strength across pairs of resin types\",\n       subtitle = \"90% Tukey HSD Confidence intervals\",\n       caption = glue(nrow(bonding), \" teeth in bonding data\"),\n       x = \"Pairwise Difference between resin types\",\n       y = \"Difference in Mean Bond Strength\")\n\n\n\n\nBased on these confidence intervals (which have a family-wise 90% confidence level), we see that D shows arger mean strength than A or B or C, and that C is also associated with larger mean strength than A."
  },
  {
    "objectID": "10-anovaexamples.html#a-two-way-anova-looking-at-two-factors",
    "href": "10-anovaexamples.html#a-two-way-anova-looking-at-two-factors",
    "title": "10  Two-Way ANOVA and Interaction",
    "section": "10.4 A Two-Way ANOVA: Looking at Two Factors",
    "text": "10.4 A Two-Way ANOVA: Looking at Two Factors\nNow, we’ll now add consideration of the light source into our study. We can look at the distribution of the strength values at the combinations of both light and resin, with a plot like this one.\n\nggplot(bonding, aes(x = resin, y = strength, color = light)) +\n    geom_point(size = 2, alpha = 0.5) +\n    facet_wrap(~ light) +\n    guides(color = \"none\") +\n    scale_color_manual(values = c(\"purple\", \"darkorange\")) +\n    theme_bw()"
  },
  {
    "objectID": "10-anovaexamples.html#a-means-plot-with-standard-deviations-to-check-for-interaction",
    "href": "10-anovaexamples.html#a-means-plot-with-standard-deviations-to-check-for-interaction",
    "title": "10  Two-Way ANOVA and Interaction",
    "section": "10.5 A Means Plot (with standard deviations) to check for interaction",
    "text": "10.5 A Means Plot (with standard deviations) to check for interaction\nSometimes, we’ll instead look at a plot simply of the means (and, often, the standard deviations) of strength at each combination of light and resin. We’ll start by building up a data set with the summaries we want to plot.\n\nbond.sum <- bonding |> \n    group_by(resin, light) |>\n    summarize(mean.str = mean(strength), sd.str = sd(strength))\n\n`summarise()` has grouped output by 'resin'. You can override using the\n`.groups` argument.\n\nbond.sum\n\n# A tibble: 8 × 4\n# Groups:   resin [4]\n  resin light   mean.str sd.str\n  <chr> <chr>      <dbl>  <dbl>\n1 A     Halogen     17.8   4.02\n2 A     LED         19.1   5.63\n3 B     Halogen     19.9   5.62\n4 B     LED         24.6   7.25\n5 C     Halogen     22.5   6.19\n6 C     LED         27.8   5.56\n7 D     Halogen     40.3   4.15\n8 D     LED         23.8   5.70\n\n\nNow, we’ll use this new data set to plot the means and standard deviations of strength at each combination of resin and light.\n\n## The error bars will overlap unless we adjust the position.\npd <- position_dodge(0.2) # move them .1 to the left and right\n\nggplot(bond.sum, aes(x = resin, y = mean.str, col = light)) +\n    geom_errorbar(aes(ymin = mean.str - sd.str, \n                      ymax = mean.str + sd.str),\n                  width = 0.2, position = pd) +\n    geom_point(size = 2, position = pd) + \n    geom_line(aes(group = light), position = pd) +\n    scale_color_manual(values = c(\"purple\", \"darkorange\")) +\n    theme_bw() +\n    labs(y = \"Bonding Strength (MPa)\", x = \"Resin Type\",\n         title = \"Observed Means (+/- SD) of Bonding Strength\")\n\n\n\n\nIs there evidence of a meaningful interaction between the resin type and the light source on the bonding strength in this plot?\n\nSure. A meaningful interaction just means that the strength associated with different resin types depends on the light source.\n\nWith LED light, it appears that resin C leads to the strongest bonding strength.\nWith Halogen light, though, it seems that resin D is substantially stronger.\n\nNote that the lines we see here connecting the light sources aren’t in parallel (as they would be if we had zero interaction between resin and light), but rather, they cross.\n\n\n10.5.1 Summarizing the data after grouping by resin and light\nWe might want to look at a numerical summary of the strengths within these groups, too.\n\nfavstats(strength ~ resin + light, data = bonding) |>\n    select(resin.light, median, mean, sd, n, missing)\n\n  resin.light median  mean       sd  n missing\n1   A.Halogen  18.35 17.77 4.024108 10       0\n2   B.Halogen  21.75 19.90 5.617631 10       0\n3   C.Halogen  21.30 22.54 6.191069 10       0\n4   D.Halogen  40.40 40.30 4.147556 10       0\n5       A.LED  17.80 19.06 5.625181 10       0\n6       B.LED  24.45 24.56 7.246792 10       0\n7       C.LED  28.45 27.77 5.564980 10       0\n8       D.LED  21.45 23.85 5.704043 10       0"
  },
  {
    "objectID": "10-anovaexamples.html#fitting-the-two-way-anova-model-with-interaction",
    "href": "10-anovaexamples.html#fitting-the-two-way-anova-model-with-interaction",
    "title": "10  Two-Way ANOVA and Interaction",
    "section": "10.6 Fitting the Two-Way ANOVA model with Interaction",
    "text": "10.6 Fitting the Two-Way ANOVA model with Interaction\n\nc3_m1 <- lm(strength ~ resin * light, data = bonding)\n\nsummary(c3_m1)\n\n\nCall:\nlm(formula = strength ~ resin * light, data = bonding)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-11.760  -3.663  -0.320   3.697  11.250 \n\nCoefficients:\n                Estimate Std. Error t value Pr(>|t|)    \n(Intercept)       17.770      1.771  10.033 2.57e-15 ***\nresinB             2.130      2.505   0.850   0.3979    \nresinC             4.770      2.505   1.904   0.0609 .  \nresinD            22.530      2.505   8.995 2.13e-13 ***\nlightLED           1.290      2.505   0.515   0.6081    \nresinB:lightLED    3.370      3.542   0.951   0.3446    \nresinC:lightLED    3.940      3.542   1.112   0.2697    \nresinD:lightLED  -17.740      3.542  -5.008 3.78e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 5.601 on 72 degrees of freedom\nMultiple R-squared:  0.6149,    Adjusted R-squared:  0.5775 \nF-statistic: 16.42 on 7 and 72 DF,  p-value: 9.801e-13\n\n\n\n10.6.1 The ANOVA table for our model\nIn a two-way ANOVA model, we begin by assessing the interaction term. If it’s important, then our best model is the model including the interaction. If it’s not important, we will often move on to consider a new model, fit without an interaction.\nThe ANOVA table is especially helpful in this case, because it lets us look specifically at the interaction effect.\n\nanova(c3_m1)\n\nAnalysis of Variance Table\n\nResponse: strength\n            Df  Sum Sq Mean Sq F value    Pr(>F)    \nresin        3 1999.72  666.57 21.2499 5.792e-10 ***\nlight        1   34.72   34.72  1.1067    0.2963    \nresin:light  3 1571.96  523.99 16.7043 2.457e-08 ***\nResiduals   72 2258.52   31.37                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\n\n10.6.2 Is the interaction important?\nIn this case, the interaction:\n\nis evident in the means plot, and\nis highly statistically significant, and\naccounts for a sizable fraction (27%) of the overall variation\n\n\\[\n\\eta^2_{interaction} = \\frac{\\mbox{SS(resin:light)}}{SS(Total)}\n= \\frac{1571.96}{1999.72 + 34.72 + 1571.96 + 2258.52} = 0.268\n\\]\nIf the interaction were either large or significant we would be inclined to keep it in the model. In this case, it’s both, so there’s no real reason to remove it.\n\n\n10.6.3 Interpreting the Interaction\nRecall the model equation, which is:\n\nc3_m1\n\n\nCall:\nlm(formula = strength ~ resin * light, data = bonding)\n\nCoefficients:\n    (Intercept)           resinB           resinC           resinD  \n          17.77             2.13             4.77            22.53  \n       lightLED  resinB:lightLED  resinC:lightLED  resinD:lightLED  \n           1.29             3.37             3.94           -17.74  \n\n\nSo, if light = Halogen, our equation is:\n\\[\nstrength = 17.77 + 2.13 resinB + 4.77 resinC + 22.53 resinD\n\\]\nAnd if light = LED, our equation is:\n\\[\nstrength = 19.06 + 5.50 resinB + 8.71 resinC + 4.79 resinD\n\\]\nNote that both the intercept and the slopes change as a result of the interaction. The model yields a different prediction for every possible combination of a resin type and a light source."
  },
  {
    "objectID": "10-anovaexamples.html#comparing-individual-combinations-of-resin-and-light",
    "href": "10-anovaexamples.html#comparing-individual-combinations-of-resin-and-light",
    "title": "10  Two-Way ANOVA and Interaction",
    "section": "10.7 Comparing Individual Combinations of resin and light",
    "text": "10.7 Comparing Individual Combinations of resin and light\nTo make comparisons between individual combinations of a resin type and a light source, using something like Tukey’s HSD approach for multiple comparisons, we first refit the model using the aov structure, rather than lm.\n\nc3m1_aov <- aov(strength ~ resin * light, data = bonding)\n\nsummary(c3m1_aov)\n\n            Df Sum Sq Mean Sq F value   Pr(>F)    \nresin        3 1999.7   666.6  21.250 5.79e-10 ***\nlight        1   34.7    34.7   1.107    0.296    \nresin:light  3 1572.0   524.0  16.704 2.46e-08 ***\nResiduals   72 2258.5    31.4                     \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nAnd now, we can obtain Tukey HSD comparisons (which will maintain an overall 90% family-wise confidence level) across the resin types, the light sources, and the combinations, with the TukeyHSD command. This approach is only completely appropriate if these comparisons are pre-planned, and if the design is balanced (as this is, with the same sample size for each combination of a light source and resin type.)\n\nTukeyHSD(c3m1_aov, conf.level = 0.9)\n\n  Tukey multiple comparisons of means\n    90% family-wise confidence level\n\nFit: aov(formula = strength ~ resin * light, data = bonding)\n\n$resin\n      diff        lwr       upr     p adj\nB-A  3.815 -0.3176052  7.947605 0.1461960\nC-A  6.740  2.6073948 10.872605 0.0016436\nD-A 13.660  9.5273948 17.792605 0.0000000\nC-B  2.925 -1.2076052  7.057605 0.3568373\nD-B  9.845  5.7123948 13.977605 0.0000026\nD-C  6.920  2.7873948 11.052605 0.0011731\n\n$light\n               diff       lwr       upr     p adj\nLED-Halogen -1.3175 -3.404306 0.7693065 0.2963128\n\n$`resin:light`\n                      diff         lwr        upr     p adj\nB:Halogen-A:Halogen   2.13  -4.9961962   9.256196 0.9893515\nC:Halogen-A:Halogen   4.77  -2.3561962  11.896196 0.5525230\nD:Halogen-A:Halogen  22.53  15.4038038  29.656196 0.0000000\nA:LED-A:Halogen       1.29  -5.8361962   8.416196 0.9995485\nB:LED-A:Halogen       6.79  -0.3361962  13.916196 0.1361092\nC:LED-A:Halogen      10.00   2.8738038  17.126196 0.0037074\nD:LED-A:Halogen       6.08  -1.0461962  13.206196 0.2443200\nC:Halogen-B:Halogen   2.64  -4.4861962   9.766196 0.9640100\nD:Halogen-B:Halogen  20.40  13.2738038  27.526196 0.0000000\nA:LED-B:Halogen      -0.84  -7.9661962   6.286196 0.9999747\nB:LED-B:Halogen       4.66  -2.4661962  11.786196 0.5818695\nC:LED-B:Halogen       7.87   0.7438038  14.996196 0.0473914\nD:LED-B:Halogen       3.95  -3.1761962  11.076196 0.7621860\nD:Halogen-C:Halogen  17.76  10.6338038  24.886196 0.0000000\nA:LED-C:Halogen      -3.48 -10.6061962   3.646196 0.8591455\nB:LED-C:Halogen       2.02  -5.1061962   9.146196 0.9922412\nC:LED-C:Halogen       5.23  -1.8961962  12.356196 0.4323859\nD:LED-C:Halogen       1.31  -5.8161962   8.436196 0.9995004\nA:LED-D:Halogen     -21.24 -28.3661962 -14.113804 0.0000000\nB:LED-D:Halogen     -15.74 -22.8661962  -8.613804 0.0000006\nC:LED-D:Halogen     -12.53 -19.6561962  -5.403804 0.0001014\nD:LED-D:Halogen     -16.45 -23.5761962  -9.323804 0.0000002\nB:LED-A:LED           5.50  -1.6261962  12.626196 0.3665620\nC:LED-A:LED           8.71   1.5838038  15.836196 0.0185285\nD:LED-A:LED           4.79  -2.3361962  11.916196 0.5471915\nC:LED-B:LED           3.21  -3.9161962  10.336196 0.9027236\nD:LED-B:LED          -0.71  -7.8361962   6.416196 0.9999920\nD:LED-C:LED          -3.92 -11.0461962   3.206196 0.7690762\n\n\nOne conclusion from this is that the combination of D and Halogen appears stronger than each of the other seven combinations."
  },
  {
    "objectID": "10-anovaexamples.html#the-bonding-model-without-interaction",
    "href": "10-anovaexamples.html#the-bonding-model-without-interaction",
    "title": "10  Two-Way ANOVA and Interaction",
    "section": "10.8 The bonding model without Interaction",
    "text": "10.8 The bonding model without Interaction\nIt seems incorrect in this situation to fit a model without the interaction term, but we’ll do so just so you can see what’s involved.\n\nc3_m2 <- lm(strength ~ resin + light, data = bonding)\n\nsummary(c3_m2)\n\n\nCall:\nlm(formula = strength ~ resin + light, data = bonding)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-14.1163  -4.9531   0.1187   4.4613  14.4663 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)   19.074      1.787  10.676  < 2e-16 ***\nresinB         3.815      2.260   1.688  0.09555 .  \nresinC         6.740      2.260   2.982  0.00386 ** \nresinD        13.660      2.260   6.044 5.39e-08 ***\nlightLED      -1.317      1.598  -0.824  0.41229    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 7.147 on 75 degrees of freedom\nMultiple R-squared:  0.3469,    Adjusted R-squared:  0.312 \nF-statistic: 9.958 on 4 and 75 DF,  p-value: 1.616e-06\n\n\nIn the no-interaction model, if light = Halogen, our equation is:\n\\[\nstrength = 19.07 + 3.82 resinB + 6.74 resinC + 13.66 resinD\n\\]\nAnd if light = LED, our equation is:\n\\[\nstrength = 17.75 + 3.82 resinB + 6.74 resinC + 13.66 resinD\n\\]\nSo, in the no-interaction model, only the intercept changes.\n\nanova(c3_m2)\n\nAnalysis of Variance Table\n\nResponse: strength\n          Df Sum Sq Mean Sq F value    Pr(>F)    \nresin      3 1999.7  666.57 13.0514 6.036e-07 ***\nlight      1   34.7   34.72  0.6797    0.4123    \nResiduals 75 3830.5   51.07                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nAnd, it appears, if we ignore the interaction, then resin type has a large impact on strength but light source doesn’t. This is clearer when we look at boxplots of the separated light and resin groups.\n\np1 <- ggplot(bonding, aes(x = light, y = strength)) + \n    geom_boxplot()\np2 <- ggplot(bonding, aes(x = resin, y = strength)) +\n    geom_boxplot()\n\np1 + p2"
  },
  {
    "objectID": "10-anovaexamples.html#cortisol-a-hypothetical-clinical-trial",
    "href": "10-anovaexamples.html#cortisol-a-hypothetical-clinical-trial",
    "title": "10  Two-Way ANOVA and Interaction",
    "section": "10.9 cortisol: A Hypothetical Clinical Trial",
    "text": "10.9 cortisol: A Hypothetical Clinical Trial\n156 adults who complained of problems with a high-stress lifestyle were enrolled in a hypothetical clinical trial of the effectiveness of a behavioral intervention designed to help reduce stress levels, as measured by salivary cortisol.\nThe subjects were randomly assigned to one of three intervention groups (usual care, low dose, and high dose.) The “low dose” subjects received a one-week intervention with a follow-up at week 5. The “high dose” subjects received a more intensive three-week intervention, with follow up at week 5.\nSince cortisol levels rise and fall with circadian rhythms, the cortisol measurements were taken just after rising for all subjects. These measurements were taken at baseline, and again at five weeks. The difference (baseline - week 5) in cortisol level (in micrograms / l) serves as the primary outcome.\n\n10.9.1 Codebook and Raw Data for cortisol\nThe data are gathered in the cortisol data set. Included are:\n\n\n\nVariable\nDescription\n\n\n\n\nsubject\nsubject identification code\n\n\ninterv\nintervention group (UC = usual care, Low, High)\n\n\nwaist\nwaist circumference at baseline (in inches)\n\n\nsex\nmale or female\n\n\ncort.1\nsalivary cortisol level (microg/l) week 1\n\n\ncort.5\nsalivary cortisol level (microg/l) week 5\n\n\n\n\ncortisol\n\n# A tibble: 156 × 6\n   subject interv waist sex   cort.1 cort.5\n     <dbl> <chr>  <dbl> <chr>  <dbl>  <dbl>\n 1    1001 UC      48.3 M       13.4   13.3\n 2    1002 Low     58.3 M       17.8   16.6\n 3    1003 High    43   M       14.4   12.7\n 4    1004 Low     44.9 M        9      9.8\n 5    1005 High    46.1 M       14.2   14.2\n 6    1006 UC      41.3 M       14.8   15.1\n 7    1007 Low     51   F       13.7   16  \n 8    1008 UC      42   F       17.3   18.7\n 9    1009 Low     24.7 F       15.3   15.8\n10    1010 Low     59.4 M       12.4   11.7\n# … with 146 more rows"
  },
  {
    "objectID": "10-anovaexamples.html#creating-a-factor-combining-sex-and-waist",
    "href": "10-anovaexamples.html#creating-a-factor-combining-sex-and-waist",
    "title": "10  Two-Way ANOVA and Interaction",
    "section": "10.10 Creating a factor combining sex and waist",
    "text": "10.10 Creating a factor combining sex and waist\nNext, we’ll put the waist and sex data in the cortisol example together. We want to build a second categorical variable (called fat_est) combining this information, to indicate “healthy” vs. “unhealthy” levels of fat around the waist.\n\nMale subjects whose waist circumference is 40 inches or more, and\nFemale subjects whose waist circumference is 35 inches or more, will fall in the “unhealthy” group.\n\n\ncortisol <- cortisol |>\n    mutate(\n        fat_est = factor(case_when(\n            sex == \"M\" & waist >= 40 ~ \"unhealthy\",\n            sex == \"F\" & waist >= 35 ~ \"unhealthy\",\n            TRUE                     ~ \"healthy\")),\n        cort_diff = cort.1 - cort.5)\n\nsummary(cortisol)\n\n    subject        interv              waist           sex           \n Min.   :1001   Length:156         Min.   :20.80   Length:156        \n 1st Qu.:1040   Class :character   1st Qu.:33.27   Class :character  \n Median :1078   Mode  :character   Median :40.35   Mode  :character  \n Mean   :1078                      Mean   :40.42                     \n 3rd Qu.:1117                      3rd Qu.:47.77                     \n Max.   :1156                      Max.   :59.90                     \n     cort.1           cort.5          fat_est      cort_diff      \n Min.   : 6.000   Min.   : 4.2   healthy  : 56   Min.   :-2.3000  \n 1st Qu.: 9.675   1st Qu.: 9.6   unhealthy:100   1st Qu.:-0.5000  \n Median :12.400   Median :12.6                   Median : 0.2000  \n Mean   :12.686   Mean   :12.4                   Mean   : 0.2821  \n 3rd Qu.:16.025   3rd Qu.:15.7                   3rd Qu.: 1.2000  \n Max.   :19.000   Max.   :19.7                   Max.   : 2.0000"
  },
  {
    "objectID": "10-anovaexamples.html#a-means-plot-for-the-cortisol-trial-with-standard-errors",
    "href": "10-anovaexamples.html#a-means-plot-for-the-cortisol-trial-with-standard-errors",
    "title": "10  Two-Way ANOVA and Interaction",
    "section": "10.11 A Means Plot for the cortisol trial (with standard errors)",
    "text": "10.11 A Means Plot for the cortisol trial (with standard errors)\nAgain, we’ll start by building up a data set with the summaries we want to plot.\n\ncort.sum <- cortisol |> \n    group_by(interv, fat_est) |>\n    summarize(mean.cort = mean(cort_diff), \n              se.cort = sd(cort_diff)/sqrt(n()))\n\n`summarise()` has grouped output by 'interv'. You can override using the\n`.groups` argument.\n\ncort.sum\n\n# A tibble: 6 × 4\n# Groups:   interv [3]\n  interv fat_est   mean.cort se.cort\n  <chr>  <fct>         <dbl>   <dbl>\n1 High   healthy       0.695   0.217\n2 High   unhealthy     0.352   0.150\n3 Low    healthy       0.5     0.182\n4 Low    unhealthy     0.327   0.190\n5 UC     healthy       0.347   0.225\n6 UC     unhealthy    -0.226   0.155\n\n\nNow, we’ll use this new data set to plot the means and standard errors.\n\n## The error bars will overlap unless we adjust the position.\npd <- position_dodge(0.2) # move them .1 to the left and right\n\nggplot(cort.sum, aes(x = interv, y = mean.cort, col = fat_est)) +\n    geom_errorbar(aes(ymin = mean.cort - se.cort, \n                      ymax = mean.cort + se.cort),\n                  width = 0.2, position = pd) +\n    geom_point(size = 2, position = pd) + \n    geom_line(aes(group = fat_est), position = pd) +\n    scale_color_manual(values = c(\"royalblue\", \"darkred\")) +\n    theme_bw() +\n    labs(y = \"Salivary Cortisol Level\", x = \"Intervention Group\",\n         title = \"Observed Means (+/- SE) of Salivary Cortisol\")"
  },
  {
    "objectID": "10-anovaexamples.html#a-two-way-anova-model-for-cortisol-with-interaction",
    "href": "10-anovaexamples.html#a-two-way-anova-model-for-cortisol-with-interaction",
    "title": "10  Two-Way ANOVA and Interaction",
    "section": "10.12 A Two-Way ANOVA model for cortisol with Interaction",
    "text": "10.12 A Two-Way ANOVA model for cortisol with Interaction\n\nc3_m3 <- lm(cort_diff ~ interv * fat_est, data = cortisol)\n\nanova(c3_m3)\n\nAnalysis of Variance Table\n\nResponse: cort_diff\n                Df  Sum Sq Mean Sq F value  Pr(>F)  \ninterv           2   7.847  3.9235  4.4698 0.01301 *\nfat_est          1   4.614  4.6139  5.2564 0.02326 *\ninterv:fat_est   2   0.943  0.4715  0.5371 0.58554  \nResiduals      150 131.666  0.8778                  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nDoes it seem like we need the interaction term in this case?\n\nsummary(c3_m3)\n\n\nCall:\nlm(formula = cort_diff ~ interv * fat_est, data = cortisol)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-2.62727 -0.75702  0.08636  0.84848  2.12647 \n\nCoefficients:\n                           Estimate Std. Error t value Pr(>|t|)   \n(Intercept)                  0.6950     0.2095   3.317  0.00114 **\nintervLow                   -0.1950     0.3001  -0.650  0.51689   \nintervUC                    -0.3479     0.3091  -1.126  0.26206   \nfat_estunhealthy            -0.3435     0.2655  -1.294  0.19774   \nintervLow:fat_estunhealthy   0.1708     0.3785   0.451  0.65256   \nintervUC:fat_estunhealthy   -0.2300     0.3846  -0.598  0.55068   \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.9369 on 150 degrees of freedom\nMultiple R-squared:  0.0924,    Adjusted R-squared:  0.06214 \nF-statistic: 3.054 on 5 and 150 DF,  p-value: 0.01179\n\n\n\n10.12.1 Notes on this Question\nWhen we’re evaluating a two-factor ANOVA model with an interaction, we are choosing between models with either:\n\njust one factor\nboth factors, but only as main effects\nboth factors and an interaction between them\n\nBut we don’t get to pick models that include any other combination of terms. For this two-way ANOVA, then, our choices are:\n\na model with `interv only\na model with `fat_est only\na model with both interv and fat_est but not their interaction\na model with interv and fat_est and their interaction\n\nThose are the only modeling options available to us.\nFirst, consider the ANOVA table, repeated below…\n\nanova(c3_m3)\n\nAnalysis of Variance Table\n\nResponse: cort_diff\n                Df  Sum Sq Mean Sq F value  Pr(>F)  \ninterv           2   7.847  3.9235  4.4698 0.01301 *\nfat_est          1   4.614  4.6139  5.2564 0.02326 *\ninterv:fat_est   2   0.943  0.4715  0.5371 0.58554  \nResiduals      150 131.666  0.8778                  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nThe conclusions here are as follows:\n\nThe interaction effect (interv:fat_est) has a large p value (0.58554) and assesses whether the two interaction terms (product terms) included in the model add detectable predictive value to the main effects model that includes only interv and fat_est alone. You are right to say that this ANOVA is sequential, which means that the p value for the interaction effect is looking at the additional effect of the interaction once we already have the main effects interv and fat_est included.\nThe interv and fat_est terms aren’t usually evaluated with hypothesis tests or interpreted in the ANOVA for this setting, since if we intend to include the interaction term (as this model does) we also need these main effects. If we wanted to look at those terms individually in a model without the interaction, then we’d want to fit that model (without the interaction term) to do so.\n\nNext, let’s look at the summary of the c3_m3 model, specifically the coefficients…\n\nsummary(c3_m3)\n\n\nCall:\nlm(formula = cort_diff ~ interv * fat_est, data = cortisol)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-2.62727 -0.75702  0.08636  0.84848  2.12647 \n\nCoefficients:\n                           Estimate Std. Error t value Pr(>|t|)   \n(Intercept)                  0.6950     0.2095   3.317  0.00114 **\nintervLow                   -0.1950     0.3001  -0.650  0.51689   \nintervUC                    -0.3479     0.3091  -1.126  0.26206   \nfat_estunhealthy            -0.3435     0.2655  -1.294  0.19774   \nintervLow:fat_estunhealthy   0.1708     0.3785   0.451  0.65256   \nintervUC:fat_estunhealthy   -0.2300     0.3846  -0.598  0.55068   \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.9369 on 150 degrees of freedom\nMultiple R-squared:  0.0924,    Adjusted R-squared:  0.06214 \nF-statistic: 3.054 on 5 and 150 DF,  p-value: 0.01179\n\n\nSo here, we see two p values associated with the interaction terms (the two product terms at the bottom of the regression) but these aren’t especially helpful, because we’re either going to include the interaction (in which case both of these terms will be in the model) or we’re not going to include the interaction (in which case neither of these terms will be in the model.)\nSo the p values provided here aren’t very helpful - like all such p values for t tests, they are looking at the value of the term in their row as the last predictor in to the model, essentially comparing the full model to the model without that specific component, but none of those tests enable us to decide which of the 4 available model choices is our best fit.\nNow, let’s consider the reason why, for example, the p value for fat_est in the summary() which is looking at comparing the following models …\n\na model including interv (which has 2 coefficients to account for its 3 categories), fat_est (which has 1 coefficient to account for its 2 categories), and the interv*fat_est interaction terms (which are 2 terms)\na model including interv and the interv*fat_est interaction (but somehow not the main effect of fat_est, which actually makes no sense: if we include the interaction we always include the main effect)\n\nto the p value for fat_est in the ANOVA which is looking at comparing\n\nthe model with interv alone to\nthe model with interv and fat_est as main effects, but no interaction\n\nOnly the ANOVA p value is therefore in any way useful, and it suggests that once you have the main effect of interv, adding fat_est’s main effect adds statistically detectable value (p = 0.023)"
  },
  {
    "objectID": "10-anovaexamples.html#a-two-way-anova-model-for-cortisol-without-interaction",
    "href": "10-anovaexamples.html#a-two-way-anova-model-for-cortisol-without-interaction",
    "title": "10  Two-Way ANOVA and Interaction",
    "section": "10.13 A Two-Way ANOVA model for cortisol without Interaction",
    "text": "10.13 A Two-Way ANOVA model for cortisol without Interaction\n\n10.13.1 The Graph\n\np1 <- ggplot(cortisol, aes(x = interv, y = cort_diff)) + \n    geom_boxplot()\np2 <- ggplot(cortisol, aes(x = fat_est, y = cort_diff)) +\n    geom_boxplot()\n\np1 + p2\n\n\n\n\n\n\n10.13.2 The ANOVA Model\n\nc3_m4 <- lm(cort_diff ~ interv + fat_est, data = cortisol)\n\nanova(c3_m4)\n\nAnalysis of Variance Table\n\nResponse: cort_diff\n           Df  Sum Sq Mean Sq F value  Pr(>F)  \ninterv      2   7.847  3.9235  4.4972 0.01266 *\nfat_est     1   4.614  4.6139  5.2886 0.02283 *\nResiduals 152 132.609  0.8724                  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nHow do these results compare to those we saw in the model with interaction?\n\n\n10.13.3 The Regression Summary\n\nsummary(c3_m4)\n\n\nCall:\nlm(formula = cort_diff ~ interv + fat_est, data = cortisol)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-2.55929 -0.74527  0.05457  0.86456  2.05489 \n\nCoefficients:\n                 Estimate Std. Error t value Pr(>|t|)    \n(Intercept)       0.70452    0.16093   4.378 2.22e-05 ***\nintervLow        -0.08645    0.18232  -0.474  0.63606    \nintervUC         -0.50063    0.18334  -2.731  0.00707 ** \nfat_estunhealthy -0.35878    0.15601  -2.300  0.02283 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.934 on 152 degrees of freedom\nMultiple R-squared:  0.0859,    Adjusted R-squared:  0.06785 \nF-statistic: 4.761 on 3 and 152 DF,  p-value: 0.00335\n\n\n\n\n10.13.4 Tukey HSD Comparisons\nWithout the interaction term, we can make direct comparisons between levels of the intervention, and between levels of the fat_est variable. This is probably best done here in a Tukey HSD comparison.\n\nTukeyHSD(aov(cort_diff ~ interv + fat_est, data = cortisol), conf.level = 0.9)\n\n  Tukey multiple comparisons of means\n    90% family-wise confidence level\n\nFit: aov(formula = cort_diff ~ interv + fat_est, data = cortisol)\n\n$interv\n                diff        lwr         upr     p adj\nLow-High -0.09074746 -0.4677566  0.28626166 0.8724916\nUC-High  -0.51642619 -0.8952964 -0.13755598 0.0150150\nUC-Low   -0.42567873 -0.8063312 -0.04502625 0.0570728\n\n$fat_est\n                        diff        lwr       upr     p adj\nunhealthy-healthy -0.3582443 -0.6162415 -0.100247 0.0229266\n\n\nWhat conclusions can we draw here?"
  },
  {
    "objectID": "90-incomplete.html",
    "href": "90-incomplete.html",
    "title": "11  NOTE: Work in progress.",
    "section": "",
    "text": "These materials are not yet complete. Additional materials will appear as the semester progresses. We thank you for your patience.\nIf you find any typographical or other errors, please let Dr. Love know about them, either through the Typos folder in Campuswire or by email to Thomas dot Love at case dot edu.\nThanks again."
  }
]