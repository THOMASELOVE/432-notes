[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Data Science for Biological, Medical and Health Research",
    "section": "",
    "text": "Introduction\nThese Notes provide a series of examples using R to work through issues that are likely to come up in PQHS/CRSP/MPHP 432.\nWhile these Notes share some of the features of a textbook, they are neither comprehensive nor completely original. The main purpose is to give students in 432 a set of common materials on which to draw during the course. In class, we will sometimes:\n\nreiterate points made in this document,\namplify what is here,\nsimplify the presentation of things done here,\nuse new examples to show some of the same techniques,\nrefer to issues not mentioned in this document,\n\nbut what we don’t (always) do is follow these notes very precisely. We assume instead that you will read the materials and try to learn from them, just as you will attend classes and try to learn from them. We welcome feedback of all kinds on this document or anything else via our Campuswire discussion forum.\nWhat you will mostly find are brief explanations of a key idea or summary, accompanied (most of the time) by R code and a demonstration of the results of applying that code.\nEverything you see here is available to you as HTML or PDF. You will also have access to the Quarto files, which contain the code which generates everything in the document, including all of the R results. We will demonstrate the use of Quarto and R Studio (the “program” which we use to interface with the R language) in class.\nTo download the data and R code related to these notes, visit the 432-data page."
  },
  {
    "objectID": "setup.html#general-theme-for-ggplot-work",
    "href": "setup.html#general-theme-for-ggplot-work",
    "title": "R Setup",
    "section": "General Theme for ggplot work",
    "text": "General Theme for ggplot work\nDr. Love prefers theme_bw() to the default choice.\n\ntheme_set(theme_bw())"
  },
  {
    "objectID": "setup.html#data-used-in-these-notes",
    "href": "setup.html#data-used-in-these-notes",
    "title": "R Setup",
    "section": "Data used in these notes",
    "text": "Data used in these notes\nAll data sets used in these notes are available on our 432-data website.\nDr. Love is in the process of moving all of the data loads below to their individual chapters.\n\nprost <- read_csv(\"data/prost.csv\", show_col_types = FALSE) \npollution <- read_csv(\"data/pollution.csv\", show_col_types = FALSE) \n\nbonding <- read_csv(\"data/bonding.csv\", show_col_types = FALSE) \ncortisol <- read_csv(\"data/cortisol.csv\", show_col_types = FALSE) \nemphysema <- read_csv(\"data/emphysema.csv\", show_col_types = FALSE) \nresect <- read_csv(\"data/resect.csv\", show_col_types = FALSE) \ncolscr <- read_csv(\"data/screening.csv\", show_col_types = FALSE) \ncolscr2 <- read_csv(\"data/screening2.csv\", show_col_types = FALSE) \nauthorship <- read_csv(\"data/authorship.csv\", show_col_types = FALSE) \nhem <- read_csv(\"data/hem.csv\", show_col_types = FALSE) \nleukem <- read_csv(\"data/leukem.csv\", show_col_types = FALSE)"
  },
  {
    "objectID": "01-nhanes.html#r-setup",
    "href": "01-nhanes.html#r-setup",
    "title": "1  Building the nh432 example",
    "section": "1.1 R Setup",
    "text": "1.1 R Setup\n\nknitr::opts_chunk$set(comment = NA)\n\nlibrary(janitor) \nlibrary(gt)\nlibrary(gtsummary)\nlibrary(Hmisc)\nlibrary(naniar)\nlibrary(nhanesA)\n\nlibrary(tidyverse) \n\ntheme_set(theme_bw())"
  },
  {
    "objectID": "01-nhanes.html#selecting-nhanes-variables",
    "href": "01-nhanes.html#selecting-nhanes-variables",
    "title": "1  Building the nh432 example",
    "section": "1.2 Selecting NHANES Variables",
    "text": "1.2 Selecting NHANES Variables\nWe’ll focus on NHANES data describing\n\nparticipating adults ages 30-59 years who\ncompleted both an NHANES interview and medical examination, and who\nalso completed an oral health examination, and who\nalso reported their overall health as either Excellent, Very Good, Good, Fair, or Poor\n\nWe will pull the following NHANES data elements using the nhanesA package:\n\n1.2.1 Demographics and Sample Weights\nFrom the Demographic Variables and Sample Weights database (P_DEMO), we collect the following variables.\n\n\n\n\n\n\n\n\nVariable\nDescription\nValues\n\n\n\n\nSEQN\nRespondent Sequence Number  (participant ID)\nlink across databases\n\n\nRIDSTATR\nInterview and MEC exam status\n1 = Interviewed only  2 = Interviewed & MEC examined\n\n\nRIDAGEYR\nAge at screening (years)  We will require ages 30-59.\nTop coded at 801\n\n\nRIDRETH3\nRace/Hispanic origin\n1 = Mexican American  2 = Other Hispanic  3 = Non-Hispanic White  4 = Non-Hispanic Black  6 = Non-Hispanic Asian  7 = Other Race, including Multi-Racial\n\n\nDMDEDUC2\nEducation Level\n1 = Less than 9th grade  2 = 9-11th grade  3 = High school graduate  4 = Some college or AA  5 = College graduate or above  7 = Refused (will treat as NA)  9 = Don’t Know (will treat as NA)\n\n\nRIAGENDR\nSex\n1 = Male, 2 = Female\n\n\nWTINTPRP\nFull sample interview weight\nSampling weight\n\n\nWTMECPRP\nFull sample MEC examination weight\nSampling weight\n\n\n\nNote As part of our inclusion criteria, we will require that all participants in our analytic data have RIDAGEYR values of 30-59 and thus drop the participants whose responses to that item are missing or outside that range.\nHere’s my code to select these variables from the P_DEMO data.\n\np_demo <- nhanes('P_DEMO') |>\n  select(SEQN, RIDSTATR, RIDAGEYR, RIDRETH3, DMDEDUC2, RIAGENDR,\n         WTINTPRP, WTMECPRP) \n\ndim(p_demo) # gives number of rows (participants) and columns (variables)\n\n[1] 15560     8\n\n\nDo we have any duplicate SEQN values?\n\np_demo |> get_dupes(SEQN)\n\nNo duplicate combinations found of: SEQN\n\n\n[1] SEQN       dupe_count RIDSTATR   RIDAGEYR   RIDRETH3   DMDEDUC2   RIAGENDR  \n[8] WTINTPRP   WTMECPRP  \n<0 rows> (or 0-length row.names)\n\n\nGood.\n\n\n1.2.2 Oral Health\nFrom the Oral Health - Recommendation of Care (P_OHXREF), we collect the following variables.\n\n\n\n\n\n\n\n\nVariable\nDescription\nValues\n\n\n\n\nSEQN\nRespondent Sequence Number  (participant ID)\nlink across databases\n\n\nOHDEXSTS\nOverall Oral Health Exam Status\n1 = Complete  2 = Partial  3 = Not Done\n\n\nOHAREC\nOverall Recommendation for Dental Care\n1 = See a dentist immediately  2 = See a dentist within the next 2 weeks  3 = See a dentist at your earliest convenience  4 = Continue your regular routine care\n\n\n\nNote In addition to requiring that all participants in our analytic data have OHDEXSTS = 1, we will (later) collapse values 1 and 2 in OHAREC because there are only a few participants with code 1 in OHAREC.\nHere’s my code to select these variables from the P_OHXREF data.\n\np_ohxref <- nhanes('P_OHXREF') |>\n  select(SEQN, OHDEXSTS, OHAREC)\n\ndim(p_ohxref)\n\n[1] 13772     3\n\n\n\n\n1.2.3 Hospital Utilization & Access to Care\nFrom the Questionnaire on Hospital Utilization & Access to Care (P_HUQ) we collect the following variables.\n\n\n\n\n\n\n\n\nVariable\nDescription\nValues\n\n\n\n\nSEQN\nRespondent Sequence Number  (participant ID)\nlink across databases\n\n\nHUQ010\nGeneral health condition  we require a 1-5 response\n1 = Excellent  2 = Very Good  3 = Good  4 = Fair  5 = Poor  7 = Refused (to be dropped)  9 = Don’t Know (to be dropped)\n\n\nHUQ071\nOvernight hospital patient in past 12 months\n1 = Yes, 2 = No  7 = Refused (will treat as NA)  9 = Don’t Know (will treat as NA)\n\n\nHUQ090\nSeen mental health professional in past 12 months\n1 = Yes, 2 = No  7 = Refused (will treat as NA)  9 = Don’t Know (will treat as NA)\n\n\n\nNote As part of our inclusion criteria, we will require that all participants in our analytic data have HUQ010 values of 1-5 and drop participants whose responses are missing, Refused or Don’t Know for that item.\nHere’s my code to select these variables from the P_HUQ data.\n\np_huq <- nhanes('P_HUQ') |>\n  select(SEQN, HUQ010, HUQ071, HUQ090)\n\ndim(p_huq)\n\n[1] 15560     4\n\n\n\n\n1.2.4 Body Measures\nFrom the Body Measures database (P_BMX), we collect the following variables.\n\n\n\n\n\n\n\n\nVariable\nDescription\nValues\n\n\n\n\nSEQN\nRespondent Sequence Number  (participant ID)\nlink across databases\n\n\nBMXWT\nBody weight (kg)\nMeasured in examination\n\n\nBMXHT\nStanding height (cm)\nMeasured in examination\n\n\nBMXWAIST\nWaist Circumference (cm)\nMeasured in examination\n\n\n\nHere’s my code to select these variables from the P_BMX data.\n\np_bmx <- nhanes('P_BMX') |>\n  select(SEQN, BMXWT, BMXHT, BMXWAIST)\n\ndim(p_bmx)\n\n[1] 14300     4\n\n\n\n\n1.2.5 Blood Pressure\nFrom the Blood Pressure - Oscillometric Measurement (P_BPXO), we collect the following variables.\n\n\n\n\n\n\n\n\nVariable\nDescription\nValues\n\n\n\n\nSEQN\nRespondent Sequence Number  (participant ID)\nlink across databases\n\n\nBPXOSY2\nSystolic BP  (2nd reading, in mm Hg)\nMeasured in examination\n\n\nBPXODI2\nDiastolic BP  (2nd reading, in mm Hg)\nMeasured in examination\n\n\nBPXOPLS1\nPulse  (1st reading, beats/minute)\nMeasured in examination\n\n\nBPXOPLS2\nPulse  (2nd reading, beats/minute)\nMeasured in examination\n\n\n\n\nA “normal” blood pressure for most adults is < 120 systolic and < 80 diastolic.\nA “normal” resting pulse rate for most adults is between 60 and 100 beats/minute.\n\nHere’s my code to select these variables from the P_BPXO data.\n\np_bpxo <- nhanes('P_BPXO') |>\n  select(SEQN, BPXOSY2, BPXODI2, BPXOPLS1, BPXOPLS2)\n\ndim(p_bpxo)\n\n[1] 11656     5\n\n\n\n\n1.2.6 Complete Blood Count\nFrom the Complete Blood Count with 5-Part Differential in Whole Blood (P_CBC), we collect the following variables.\n\n\n\n\n\n\n\n\nVariable\nDescription\nValues\n\n\n\n\nSEQN\nRespondent Sequence Number  (participant ID)\nlink across databases\n\n\nLBXWBCSI\nWhite blood cell count  in 1000 cells/uL\nnormal range: 4.5 - 11\n\n\nLBXPLTSI\nPlatelet count  in 1000 cells/uL\nnormal range: 150-450\n\n\n\nHere’s my code to select these variables from the P_CBC data.\n\np_cbc <- nhanes('P_CBC') |>\n  select(SEQN, LBXWBCSI, LBXPLTSI)\n\ndim(p_cbc)\n\n[1] 13772     3\n\n\n\n\n1.2.7 C-Reactive Protein\nFrom the High-Sensitivity C-Reactive Protein (P_HSCRP) we collect the following variables.\n\n\n\n\n\n\n\n\nVariable\nDescription\nValues\n\n\n\n\nSEQN\nRespondent Sequence Number  (participant ID)\nlink across databases\n\n\nLBXHSCRP\nHigh-Sensitivity  C-Reactive Protein (mg/L)\nnormal range: 1.0 - 3.0\n\n\n\nHere’s my code to select these variables from the P_HSCRP data.\n\np_hscrp <- nhanes('P_HSCRP') |>\n  select(SEQN, LBXHSCRP)\n\ndim(p_hscrp)\n\n[1] 13772     2\n\n\n\n\n1.2.8 Alcohol Use\nFrom the Questionnaire on Alcohol Use (P_ALQ) we collect the following variables.\n\n\n\n\n\n\n\n\nVariable\nDescription\nValues\n\n\n\n\nSEQN\nRespondent Sequence Number  (participant ID)\nlink across databases\n\n\nALQ111\nEver had a drink of alcohol?\n1 = Yes, 2 = No  7 = Refused (treat as NA)  9 = Don’t Know (treat as NA)\n\n\nALQ130\nAverage drinks per day  in past 12 months  (Top coded at 152)\ncount (set to 0 if ALQ111 is No)  777 = Refused (treat as NA)  999 = Don’t Know (treat as NA)\n\n\n\nHere’s my code to select these variables from the P_ALQ data.\n\np_alq <- nhanes('P_ALQ') |>\n  select(SEQN, ALQ111, ALQ130)\n\ndim(p_alq)\n\n[1] 8965    3\n\n\nAs noted above, we set the value of ALQ130 to be 0 if the response to ALQ111 is 2 (No).\n\np_alq <- p_alq |>\n  mutate(ALQ130 = ifelse(ALQ111 == 2, 0, ALQ130))\n\np_alq |> count(ALQ130, ALQ111)\n\n   ALQ130 ALQ111    n\n1       0      2  867\n2       1      1 2126\n3       2      1 1769\n4       3      1  843\n5       4      1  431\n6       5      1  231\n7       6      1  201\n8       7      1   44\n9       8      1   65\n10      9      1   13\n11     10      1   42\n12     11      1    3\n13     12      1   53\n14     13      1    3\n15     15      1   29\n16    777      1    1\n17    999      1    9\n18     NA      1 1640\n19     NA     NA  595\n\n\n\n\n1.2.9 Dermatology\nFrom the Questionnaire on Dermatology (P_DEQ) we collect the following variables.\n\n\n\n\n\n\n\n\nVariable\nDescription\nValues\n\n\n\n\nSEQN\nRespondent Sequence Number  (participant ID)\nlink across databases\n\n\nDEQ034D\nUse sunscreen if outside on very sunny day?\n1 = Always  2 = Most of the time  3 = Sometimes  4 = Rarely  5 = Never  7 = Refused (treat as NA)  9 = Don’t Know (treat as NA)\n\n\n\nHere’s my code to select these variables from the P_DEQ data.\n\np_deq <- nhanes('P_DEQ') |>\n  select(SEQN, DEQ034D)\n\ndim(p_deq)\n\n[1] 5810    2\n\n\n\n\n1.2.10 Depression Screener\nFrom the Questionnaire on Mental Health - Depression Screener (P_DPQ) we collect the following variables.\n\n\n\n\n\n\n\n\nVariable\nDescription\nValues\n\n\n\n\nSEQN\nRespondent Sequence Number  (participant ID)\nlink across databases\n\n\nDPQ010\nHave little interest in doing things\n0-3: codes below\n\n\nDPQ020\nFeeling down, depressed or hopeless\n0-3: codes below\n\n\nDPQ030\nTrouble sleeping or sleeping too much\n0-3: codes below\n\n\nDPQ040\nFeeling tired or having little energy\n0-3: codes below\n\n\nDPQ050\nPoor appetite or overeating\n0-3: codes below\n\n\nDPQ060\nFeeling bad about yourself\n0-3: codes below\n\n\nDPQ070\nTrouble concentrating on things\n0-3: codes below\n\n\nDPQ080\nMoving or speaking slowly or too fast\n0-3: codes below\n\n\nDPQ090\nThoughts you would be better off dead\n0-3: codes below\n\n\nDPQ100\nDifficulty these problems have caused\n0-3: codes below\n\n\n\n\nFor DPQ010 - DPQ090, the codes are 0 = Not at all, 1 = Several days in the past two weeks, 2 = More than half the days in the past two weeks, 3 = Nearly every day in the past two weeks, with 7 = Refused and 9 = Don’t Know which we will treat as NA.\nFor DPQ100, the codes are 0 = Not at all difficult, 1 = Somewhat difficult, 2 = Very difficult, 3 = Extremely difficult, with 7 = Refused and 9 = Don’t Know which we will treat as NA. Also, the DPQ100 score should be 0 if the scores on DPQ010 through DPQ090 are all zero.\n\nLater, we will sum the scores in DPQ010 - DPQ090 to produce a PHQ-9 score for each participant.\nHere’s my code to select these variables from the P_DPQ data.\n\np_dpq <- nhanes('P_DPQ') # we're actually pulling all available variables\n\ndim(p_dpq)\n\n[1] 8965   11\n\n\n\n\n1.2.11 Diet Behavior\nFrom the Questionnaire on Diet Behavior and Nutrition (P_DBQ) we collect the following variables.\n\n\n\n\n\n\n\n\nVariable\nDescription\nValues\n\n\n\n\nSEQN\nRespondent Sequence Number  (participant ID)\nlink across databases\n\n\nDBQ700\nHow healthy is your diet?\n1 = Excellent  2 = Very Good  3 = Good  4 = Fair  5 = Poor  7 = Refused (treat as NA)  9 = Don’t Know (treat as NA)\n\n\n\nHere’s my code to select these variables from the P_DBQ data.\n\np_dbq <- nhanes('P_DBQ') |>\n  select(SEQN, DBQ700)\n\ndim(p_dbq)\n\n[1] 15560     2\n\n\n\n\n1.2.12 Food Security\nFrom the Questionnaire on Food Security (P_FSQ) we collect the following variables.\n\n\n\n\n\n\n\n\nVariable\nDescription\nValues\n\n\n\n\nSEQN\nRespondent Sequence Number  (participant ID)\nlink across databases\n\n\nFSDAD\nAdult food security category for last 12m\n1 = Full food security  2 = Marginal  3 = Low  4 = Very low\n\n\n\nHere’s my code to select these variables from the P_FSQ data.\n\np_fsq <- nhanes('P_FSQ') |>\n  select(SEQN, FSDAD)\n\ndim(p_fsq)\n\n[1] 15560     2\n\n\n\n\n1.2.13 Health Insurance\nFrom the Questionnaire on Health Insurance (P_HIQ) we collect the following variables.\n\n\n\n\n\n\n\n\nVariable\nDescription\nValues\n\n\n\n\nSEQN\nRespondent Sequence Number  (participant ID)\nlink across databases\n\n\nHIQ011\nCovered by health insurance now?\n1 = Yes, 2 = No  7 = Refused (treat as NA)  9 = Don’t Know (treat as NA)\n\n\nHIQ210\nTime when no insurance in past year?\n1 = Yes, 2 = No  7 = Refused (treat as NA)  9 = Don’t Know (treat as NA)  (set to Yes if HIQ011 is No.)\n\n\n\nHere’s my code to select these variables from the P_HIQ data.\n\np_hiq <- nhanes('P_HIQ') |>\n  select(SEQN, HIQ011, HIQ210)\n\ndim(p_hiq)\n\n[1] 15560     3\n\n\nAs noted above, we set the value of HIQ210 to be 1 (Yes) if HIQ011 is 2 (No).\n\np_hiq <- p_hiq |>\n  mutate(HIQ210 = ifelse(HIQ011 == 2, 1, HIQ210))\n\np_hiq |> count(HIQ210, HIQ011)\n\n  HIQ210 HIQ011     n\n1      1      1   960\n2      1      2  1852\n3      2      1 12682\n4      7      1     2\n5      9      1    25\n6     NA      1     2\n7     NA      7     8\n8     NA      9    29\n\n\n\n\n1.2.14 Medical Conditions\nFrom the Questionnaire on Medical Conditions (P_MCQ) we collect the following variables.\n\n\n\n\n\n\n\n\nVariable\nDescription\nValues\n\n\n\n\nSEQN\nRespondent Sequence Number  (participant ID)\nlink across databases\n\n\nMCQ366A\nDoctor told you to control/lose weight in the past 12 months?\n1 = Yes, 2 = No  7 = Refused (treat as NA)  9 = Don’t Know (treat as NA)\n\n\nMCQ366B\nDoctor told you to exercise  in the past 12 months?\n1 = Yes, 2 = No  7 = Refused (treat as NA)  9 = Don’t Know (treat as NA)\n\n\nMCQ371A\nAre you now controlling or losing weight?\n1 = Yes, 2 = No  7 = Refused (treat as NA)  9 = Don’t Know (treat as NA)\n\n\nMCQ371B\nAre you now increasing exercise?\n1 = Yes, 2 = No  7 = Refused (treat as NA)  9 = Don’t Know (treat as NA)\n\n\n\nHere’s my code to select these variables from the P_MCQ data.\n\np_mcq <- nhanes('P_MCQ') |>\n  select(SEQN, MCQ366A, MCQ366B, MCQ371A, MCQ371B)\n\ndim(p_mcq)\n\n[1] 14986     5\n\n\n\n\n1.2.15 Oral Health\nFrom the Questionnaire on Oral Health (P_OHQ) we collect the following variables.\n\n\n\n\n\n\n\n\nVariable\nDescription\nValues\n\n\n\n\nSEQN\nRespondent Sequence Number  (participant ID)\nlink across databases\n\n\nOHQ870\nDays using dental floss  (in the last week)\ncount (0-7)  9 = Unknown (treat as NA)  99 = Don’t Know (treat as NA)\n\n\n\nHere’s my code to select these variables from the P_OHQ data.\n\np_ohq <- nhanes('P_OHQ') |>\n  select(SEQN, OHQ870)\n\ndim(p_ohq)\n\n[1] 14986     2\n\n\n\n\n1.2.16 Physical Activity\nFrom the Questionnaire on Physical Activity (P_PAQ) we collect the following variables.\n\n\n\n\n\n\n\n\nVariable\nDescription\nValues\n\n\n\n\nSEQN\nRespondent Sequence Number  (participant ID)\nlink across databases\n\n\nPAQ605\nVigorous work activity for 10 min/week?\n1 = Yes, 2 = No  7 = Refused (treat as NA)  9 = Don’t Know (treat as NA)\n\n\nPAQ610\n# of days of vigorous work activity  in past week\ncount (1-7)  77 = Refused (treat as NA)  99 = Don’t Know (treat as NA)  (set to 0 if PAQ605 is No.)\n\n\nPAQ650\nVigorous recreational activity for 10 min/week?\n1 = Yes, 2 = No  7 = Refused (treat as NA)  9 = Don’t Know (treat as NA)\n\n\nPAQ655\n# of days of vigorous recreational  activity in past week\ncount (1-7)  77 = Refused (treat as NA)  99 = Don’t Know (treat as NA)  (set to 0 if PAQ650 is No.)\n\n\nPAD680\nMinutes of sedentary activity (min/day)\nexcludes sleeping  7777 = Refused (treat as NA)  9999 = Don’t Know (treat as NA)\n\n\n\nHere’s my code to select these variables from the P_PAQ data.\n\np_paq <- nhanes('P_PAQ') |>\n  select(SEQN, PAQ605, PAQ610, PAQ650, PAQ655, PAD680)\n\ndim(p_paq)\n\n[1] 9693    6\n\n\nNow, let’s set the value of PAQ610 to be 0 if PAQ605 is 2 (No).\n\np_paq <- p_paq |>\n  mutate(PAQ610 = ifelse(PAQ605 == 2, 0, PAQ610))\n\nFinally, we set the value of PAQ655 to be 0 if PAQ650 is 2 (No).\n\np_paq <- p_paq |>\n  mutate(PAQ655 = ifelse(PAQ650 == 2, 0, PAQ655))\n\n\n\n1.2.17 Reproductive Health\nFrom the Questionnaire on Reproductive Health (P_RHQ) we collect the following variables.\n\n\n\n\n\n\n\n\nVariable\nDescription\nValues\n\n\n\n\nSEQN\nRespondent Sequence Number  (participant ID)\nlink across databases\n\n\nRHQ131\nEver been pregnant?\n1 = Yes, 2 = No  7 = Refused (treat as NA)  9 = Don’t Know (treat as NA)\n\n\nRHQ160\nHow many times have you been pregnant?\ncount (1-11)  77 = Refused (treat as NA)  99 = Don’t Know (treat as NA)  (set to 0 if RHQ131 is No.)\n\n\n\nHere’s my code to select these variables from the P_RHQ data.\n\np_rhq <- nhanes('P_RHQ') |>\n  select(SEQN, RHQ131, RHQ160)\n\ndim(p_rhq)\n\n[1] 5314    3\n\n\nNow, let’s set the value of RHQ160 to be 0 if RHQ131 is 2 (No).\n\np_rhq <- p_rhq |>\n  mutate(RHQ160 = ifelse(RHQ131 == 2, 0, RHQ160))\n\n\n\n1.2.18 Sleep Disorders\nFrom the Questionnaire on Sleep Disorders (P_SLQ) we collect the following variables.\n\n\n\n\n\n\n\n\nVariable\nDescription\nValues\n\n\n\n\nSEQN\nRespondent Sequence Number  (participant ID)\nlink across databases\n\n\nSLD012\nUsual hours of sleep (weekdays)\nhours limited to 2-14\n\n\nSLD013\nUsual hours of sleep (weekends)\nhours limited to 2-14\n\n\nSLQ030\nHow often do you snore in the past 12 months?\n0 = Never  1 = Rarely (1-2 nights/week)  2 = Occasionally (3-4 nights/week)  3 = Frequently (5+ nights/week)  7 = Refused (treat as NA)  9 = Don’t Know (treat as NA)\n\n\nSLQ050\nHave you ever told a doctor you had trouble sleeping?\n1 = Yes, 2 = No  7 = Refused (treat as NA)  9 = Don’t Know (treat as NA)\n\n\n\nHere’s my code to select these variables from the P_SLQ data.\n\np_slq <- nhanes('P_SLQ') |>\n  select(SEQN, SLD012, SLD013, SLQ030, SLQ050)\n\ndim(p_slq)\n\n[1] 10195     5\n\n\n\n\n1.2.19 Smoking Cigarettes\nFrom the Questionnaire on Smoking - Cigarette Use (P_SMQ) we collect the following variables.\n\n\n\n\n\n\n\n\nVariable\nDescription\nValues\n\n\n\n\nSEQN\nRespondent Sequence Number  (participant ID)\nlink across databases\n\n\nSMQ020\nSmoked at least 100 cigarettes in your life?\n1 = Yes, 2 = No  7 = Refused (treat as NA)  9 = Don’t Know (treat as NA)\n\n\nSMD641\nDays (in past 30) when you smoked a cigarette?\ncount (0-30)  77 = Refused (treat as NA)  99 = Don’t Know (treat as NA)  (set to 0 if SMQ020 is No.)\n\n\n\nHere’s my code to select these variables from the P_SMQ data.\n\np_smq <- nhanes('P_SMQ') |>\n  select(SEQN, SMQ020, SMD641)\n\ndim(p_smq)\n\n[1] 11137     3\n\n\nNow, let’s set the value of SMD641 to be 0 if SMQ020 is 2 (No).\n\np_smq <- p_smq |>\n  mutate(SMD641 = ifelse(SMQ020 == 2, 0, SMD641))\n\n\n\n1.2.20 Secondhand Smoke\nFrom the Questionnaire on Smoking - Secondhand Smoke Exposure (P_SMQSHS) we collect the following variables.\n\n\n\n\n\n\n\n\nVariable\nDescription\nValues\n\n\n\n\nSEQN\nRespondent Sequence Number  (participant ID)\nlink across databases\n\n\nSMQ856\nLast 7 days worked at a job not at home?\n1 = Yes, 2 = No  7 = Refused (treat as NA)  9 = Don’t Know (treat as NA)\n\n\nSMQ860\nLast 7 days spent time in a restaurant?\n1 = Yes, 2 = No  7 = Refused (treat as NA)  9 = Don’t Know (treat as NA)\n\n\nSMQ866\nLast 7 days spent time in a bar?\n1 = Yes, 2 = No  7 = Refused (treat as NA)  9 = Don’t Know (treat as NA)\n\n\n\nHere’s my code to select these variables from the P_SMQSHS data.\n\np_smqshs <- nhanes('P_SMQSHS') |>\n  select(SEQN, SMQ856, SMQ860, SMQ866)\n\ndim(p_smqshs)\n\n[1] 15560     4\n\n\n\n\n1.2.21 Weight History\nFrom the Questionnaire on Weight History (P_WHQ) we collect the following variables.\n\n\n\n\n\n\n\n\nVariable\nDescription\nValues\n\n\n\n\nSEQN\nRespondent Sequence Number  (participant ID)\nlink across databases\n\n\nWHD010\nCurrent self-reported height (in inches)\n49 to 82  7777 = Refused (treat as NA)  9999 = Don’t Know (treat as NA)\n\n\nWHD020\nCurrent self-reported weight (in pounds)\n67 to 578  7777 = Refused (treat as NA)  9999 = Don’t Know (treat as NA)\n\n\nWHQ040\nLike to weigh more, less, or same\n1 = More  2 = Less  3 = Stay about the same  7 = Refused (treat as NA)  9 = Don’t Know (treat as NA)\n\n\n\nHere’s my code to select these variables from the P_WHQ data.\n\np_whq <- nhanes('P_WHQ') |>\n  select(SEQN, WHD010, WHD020, WHQ040)\n\ndim(p_whq)\n\n[1] 10195     4"
  },
  {
    "objectID": "01-nhanes.html#filtering-for-inclusion",
    "href": "01-nhanes.html#filtering-for-inclusion",
    "title": "1  Building the nh432 example",
    "section": "1.3 Filtering for Inclusion",
    "text": "1.3 Filtering for Inclusion\nFirst, I’ll filter the demographic data (p_demo) to the participants with known ages (RIDAGEYR here) between 30 and 59 years (inclusive), and to those who were both interviewed and examined (so RIDSTATR is 2) to match our inclusion criteria.\n\np_demo <- p_demo |>\n  filter(RIDAGEYR >= 30 & RIDAGEYR <= 59,\n         RIDSTATR == 2)\n\ndim(p_demo)\n\n[1] 4133    8\n\n\nSecond, I’ll restrict the p_ohxref sample to the participants who had a complete oral health exam (so OHDEXSTS is 1) which is also part of our inclusion criteria.\n\np_ohxref <- p_ohxref |>\n  filter(OHDEXSTS == 1)\n\ndim(p_ohxref)\n\n[1] 13271     3\n\n\nThird, I’ll restrict the p_hug sample to the participants who gave one of our five available responses (codes 1-5) to the general health condition question in HUQ010, which is the final element of our inclusion criteria.\n\np_huq <- p_huq |>\n  filter(HUQ010 <= 5)\n\ndim(p_huq)\n\n[1] 15550     4\n\n\nSubjects that meet all of these requirements will be included in our analytic data. To achieve that end, we’ll begin merging the individual data bases."
  },
  {
    "objectID": "01-nhanes.html#merging-the-data",
    "href": "01-nhanes.html#merging-the-data",
    "title": "1  Building the nh432 example",
    "section": "1.4 Merging the Data",
    "text": "1.4 Merging the Data\n\n1.4.1 Merging Two Data Frames at a Time\nWe have two ways to merge our data. We can merge data sets two at a time. In this case, we’ll use inner_join() from the dplyr package to include only those participants with data in each of the two data frames we’re merging. For example, we’ll create temp01 to include data from both p_demo and p_ohxref for all participants (identified by their SEQN) that appear in each of those two data frames. Then, we’ll merge the resulting temp01 with p_huq to create temp02 in a similar way.\n\ntemp01 <- inner_join(p_demo, p_ohxref, by = \"SEQN\")\ntemp02 <- inner_join(temp01, p_huq, by = \"SEQN\")\n\ndim(temp02)\n\n[1] 3931   13\n\n\nNote that we now have 3931 participants in our data, and this should be the case after we merge in all of the other data sets, too. Rather than using inner_join() we will switch now to using left_join() many more times so that we always add new information only on those subjects who meet our inclusion criteria (as identified in temp02. For more on the various types of joins we can use from the dplyr package, visit <https://dplyr.tidyverse.org/reference/mutate-joins.html. The problem is that that approach would force us to create lots of new temporary files as we add in each new variable.\n\n\n1.4.2 Merging Many Data Frames Together\nA better approach is to use the reduce() function in the purrr package3, which will let us join this temp02 data frame with our remaining 17 data frames using left_join() in a much more streamlined way. We’ll also ensure that the final result (which we’ll call nh_raw) is a tibble, rather than just a data frame.\n\ndf_list <- list(temp02, p_bmx, p_bpxo, p_cbc, p_hscrp, \n                p_alq, p_deq, p_dpq, p_dbq, p_fsq, \n                p_hiq, p_mcq, p_ohq, p_paq, p_rhq,\n                p_slq, p_smqshs, p_smq, p_whq)\n\nnh_raw <- df_list |> \n  reduce(left_join, by = 'SEQN') |>\n  as_tibble()\n\ndim(nh_raw)\n\n[1] 3931   64"
  },
  {
    "objectID": "01-nhanes.html#the-raw-data",
    "href": "01-nhanes.html#the-raw-data",
    "title": "1  Building the nh432 example",
    "section": "1.5 The “Raw” Data",
    "text": "1.5 The “Raw” Data\nWhat does the data in nh_raw look like? Normally, I wouldn’t include this sort of intermediate description in a published bit of work, but it may be helpful to compare this description to the one we’ll generate at the end of the cleaning process in this case.\n\nsummary(nh_raw)\n\n      SEQN           RIDSTATR    RIDAGEYR        RIDRETH3        DMDEDUC2   \n Min.   :109271   Min.   :2   Min.   :30.00   Min.   :1.000   Min.   :1.00  \n 1st Qu.:113103   1st Qu.:2   1st Qu.:37.00   1st Qu.:3.000   1st Qu.:3.00  \n Median :117059   Median :2   Median :45.00   Median :3.000   Median :4.00  \n Mean   :117074   Mean   :2   Mean   :44.79   Mean   :3.561   Mean   :3.64  \n 3rd Qu.:121040   3rd Qu.:2   3rd Qu.:53.00   3rd Qu.:4.000   3rd Qu.:5.00  \n Max.   :124818   Max.   :2   Max.   :59.00   Max.   :7.000   Max.   :7.00  \n                                                                            \n    RIAGENDR        WTINTPRP         WTMECPRP         OHDEXSTS     OHAREC     \n Min.   :1.000   Min.   :  2467   Min.   :  2589   Min.   :1   Min.   :1.000  \n 1st Qu.:1.000   1st Qu.: 10615   1st Qu.: 11365   1st Qu.:1   1st Qu.:3.000  \n Median :2.000   Median : 17358   Median : 18422   Median :1   Median :4.000  \n Mean   :1.533   Mean   : 28434   Mean   : 30353   Mean   :1   Mean   :3.455  \n 3rd Qu.:2.000   3rd Qu.: 31476   3rd Qu.: 33155   3rd Qu.:1   3rd Qu.:4.000  \n Max.   :2.000   Max.   :311265   Max.   :321574   Max.   :1   Max.   :4.000  \n                                                                              \n     HUQ010          HUQ071          HUQ090          BMXWT       \n Min.   :1.000   Min.   :1.000   Min.   :1.000   Min.   : 36.90  \n 1st Qu.:2.000   1st Qu.:2.000   1st Qu.:2.000   1st Qu.: 69.30  \n Median :3.000   Median :2.000   Median :2.000   Median : 82.10  \n Mean   :2.741   Mean   :1.913   Mean   :1.883   Mean   : 86.31  \n 3rd Qu.:3.000   3rd Qu.:2.000   3rd Qu.:2.000   3rd Qu.: 99.10  \n Max.   :5.000   Max.   :2.000   Max.   :9.000   Max.   :254.30  \n                                                 NA's   :28      \n     BMXHT          BMXWAIST        BPXOSY2         BPXODI2      \n Min.   :135.3   Min.   : 57.9   Min.   : 69.0   Min.   : 31.00  \n 1st Qu.:160.0   1st Qu.: 89.1   1st Qu.:110.0   1st Qu.: 69.00  \n Median :166.9   Median : 99.2   Median :120.0   Median : 76.00  \n Mean   :167.4   Mean   :101.5   Mean   :121.5   Mean   : 77.03  \n 3rd Qu.:174.7   3rd Qu.:111.7   3rd Qu.:131.0   3rd Qu.: 84.00  \n Max.   :198.7   Max.   :178.0   Max.   :222.0   Max.   :136.00  \n NA's   :30      NA's   :149     NA's   :346     NA's   :346     \n    BPXOPLS1        BPXOPLS2         LBXWBCSI         LBXPLTSI    \n Min.   : 38.0   Min.   : 37.00   Min.   : 2.300   Min.   : 47.0  \n 1st Qu.: 62.0   1st Qu.: 63.00   1st Qu.: 5.700   1st Qu.:210.0  \n Median : 69.0   Median : 70.00   Median : 6.900   Median :246.0  \n Mean   : 70.3   Mean   : 70.96   Mean   : 7.254   Mean   :253.3  \n 3rd Qu.: 77.0   3rd Qu.: 78.00   3rd Qu.: 8.400   3rd Qu.:290.0  \n Max.   :126.0   Max.   :121.00   Max.   :22.800   Max.   :818.0  \n NA's   :615     NA's   :617      NA's   :176      NA's   :176    \n    LBXHSCRP           ALQ111          ALQ130          DEQ034D     \n Min.   :  0.110   Min.   :1.000   Min.   : 0.000   Min.   :1.000  \n 1st Qu.:  0.890   1st Qu.:1.000   1st Qu.: 1.000   1st Qu.:3.000  \n Median :  2.090   Median :1.000   Median : 2.000   Median :4.000  \n Mean   :  4.326   Mean   :1.089   Mean   : 2.345   Mean   :3.675  \n 3rd Qu.:  4.740   3rd Qu.:1.000   3rd Qu.: 3.000   3rd Qu.:5.000  \n Max.   :182.820   Max.   :2.000   Max.   :15.000   Max.   :5.000  \n NA's   :267       NA's   :205     NA's   :789      NA's   :19     \n     DPQ010           DPQ020           DPQ030           DPQ040      \n Min.   :0.0000   Min.   :0.0000   Min.   :0.0000   Min.   :0.0000  \n 1st Qu.:0.0000   1st Qu.:0.0000   1st Qu.:0.0000   1st Qu.:0.0000  \n Median :0.0000   Median :0.0000   Median :0.0000   Median :1.0000  \n Mean   :0.3907   Mean   :0.3732   Mean   :0.6529   Mean   :0.7612  \n 3rd Qu.:1.0000   3rd Qu.:1.0000   3rd Qu.:1.0000   3rd Qu.:1.0000  \n Max.   :9.0000   Max.   :7.0000   Max.   :9.0000   Max.   :9.0000  \n NA's   :212      NA's   :212      NA's   :212      NA's   :212     \n     DPQ050           DPQ060           DPQ070           DPQ080      \n Min.   :0.0000   Min.   :0.0000   Min.   :0.0000   Min.   :0.0000  \n 1st Qu.:0.0000   1st Qu.:0.0000   1st Qu.:0.0000   1st Qu.:0.0000  \n Median :0.0000   Median :0.0000   Median :0.0000   Median :0.0000  \n Mean   :0.4117   Mean   :0.2504   Mean   :0.2924   Mean   :0.1622  \n 3rd Qu.:1.0000   3rd Qu.:0.0000   3rd Qu.:0.0000   3rd Qu.:0.0000  \n Max.   :9.0000   Max.   :3.0000   Max.   :3.0000   Max.   :3.0000  \n NA's   :212      NA's   :213      NA's   :213      NA's   :213     \n     DPQ090          DPQ100           DBQ700         FSDAD      \n Min.   :0.000   Min.   :0.0000   Min.   :1.00   Min.   :1.000  \n 1st Qu.:0.000   1st Qu.:0.0000   1st Qu.:2.00   1st Qu.:1.000  \n Median :0.000   Median :0.0000   Median :3.00   Median :1.000  \n Mean   :0.053   Mean   :0.3447   Mean   :3.11   Mean   :1.737  \n 3rd Qu.:0.000   3rd Qu.:1.0000   3rd Qu.:4.00   3rd Qu.:2.000  \n Max.   :3.000   Max.   :3.0000   Max.   :9.00   Max.   :4.000  \n NA's   :214     NA's   :1448                    NA's   :231    \n     HIQ011          HIQ210         MCQ366A         MCQ366B     \n Min.   :1.000   Min.   :1.000   Min.   :1.000   Min.   :1.000  \n 1st Qu.:1.000   1st Qu.:1.000   1st Qu.:1.000   1st Qu.:1.000  \n Median :1.000   Median :2.000   Median :2.000   Median :2.000  \n Mean   :1.211   Mean   :1.742   Mean   :1.699   Mean   :1.574  \n 3rd Qu.:1.000   3rd Qu.:2.000   3rd Qu.:2.000   3rd Qu.:2.000  \n Max.   :9.000   Max.   :9.000   Max.   :9.000   Max.   :9.000  \n                 NA's   :12                                     \n    MCQ371A         MCQ371B          OHQ870           PAQ605     \n Min.   :1.000   Min.   :1.000   Min.   : 0.000   Min.   :1.000  \n 1st Qu.:1.000   1st Qu.:1.000   1st Qu.: 0.000   1st Qu.:1.000  \n Median :1.000   Median :1.000   Median : 3.000   Median :2.000  \n Mean   :1.371   Mean   :1.399   Mean   : 3.503   Mean   :1.723  \n 3rd Qu.:2.000   3rd Qu.:2.000   3rd Qu.: 7.000   3rd Qu.:2.000  \n Max.   :9.000   Max.   :9.000   Max.   :99.000   Max.   :9.000  \n                                 NA's   :1                       \n     PAQ610           PAQ650          PAQ655            PAD680      \n Min.   : 0.000   Min.   :1.000   Min.   : 0.0000   Min.   :   2.0  \n 1st Qu.: 0.000   1st Qu.:1.000   1st Qu.: 0.0000   1st Qu.: 180.0  \n Median : 0.000   Median :2.000   Median : 0.0000   Median : 300.0  \n Mean   : 1.244   Mean   :1.731   Mean   : 0.9201   Mean   : 363.7  \n 3rd Qu.: 2.000   3rd Qu.:2.000   3rd Qu.: 1.0000   3rd Qu.: 480.0  \n Max.   :99.000   Max.   :2.000   Max.   :99.0000   Max.   :9999.0  \n NA's   :4                                          NA's   :11      \n     RHQ131          RHQ160           SLD012           SLD013      \n Min.   :1.000   Min.   : 0.000   Min.   : 2.000   Min.   : 2.000  \n 1st Qu.:1.000   1st Qu.: 2.000   1st Qu.: 6.500   1st Qu.: 7.000  \n Median :1.000   Median : 3.000   Median : 7.500   Median : 8.000  \n Mean   :1.114   Mean   : 3.159   Mean   : 7.359   Mean   : 8.231  \n 3rd Qu.:1.000   3rd Qu.: 4.000   3rd Qu.: 8.000   3rd Qu.: 9.000  \n Max.   :7.000   Max.   :77.000   Max.   :14.000   Max.   :14.000  \n NA's   :1970    NA's   :1972     NA's   :34       NA's   :34      \n     SLQ030          SLQ050          SMQ856          SMQ860     \n Min.   :0.000   Min.   :1.000   Min.   :1.000   Min.   :1.000  \n 1st Qu.:1.000   1st Qu.:1.000   1st Qu.:1.000   1st Qu.:1.000  \n Median :2.000   Median :2.000   Median :1.000   Median :1.000  \n Mean   :2.015   Mean   :1.711   Mean   :1.323   Mean   :1.423  \n 3rd Qu.:3.000   3rd Qu.:2.000   3rd Qu.:2.000   3rd Qu.:2.000  \n Max.   :9.000   Max.   :9.000   Max.   :2.000   Max.   :9.000  \n                                                                \n     SMQ866          SMQ020        SMD641           WHD010         WHD020      \n Min.   :1.000   Min.   :1.0   Min.   : 0.000   Min.   :  50   Min.   :  86.0  \n 1st Qu.:2.000   1st Qu.:1.0   1st Qu.: 0.000   1st Qu.:  63   1st Qu.: 152.0  \n Median :2.000   Median :2.0   Median : 0.000   Median :  66   Median : 180.0  \n Mean   :1.846   Mean   :1.6   Mean   : 6.859   Mean   : 247   Mean   : 353.9  \n 3rd Qu.:2.000   3rd Qu.:2.0   3rd Qu.: 5.000   3rd Qu.:  70   3rd Qu.: 220.0  \n Max.   :2.000   Max.   :7.0   Max.   :99.000   Max.   :9999   Max.   :9999.0  \n                               NA's   :724      NA's   :24                     \n     WHQ040    \n Min.   :1.00  \n 1st Qu.:2.00  \n Median :2.00  \n Mean   :2.18  \n 3rd Qu.:2.00  \n Max.   :9.00"
  },
  {
    "objectID": "01-nhanes.html#cleaning-tasks",
    "href": "01-nhanes.html#cleaning-tasks",
    "title": "1  Building the nh432 example",
    "section": "1.6 Cleaning Tasks",
    "text": "1.6 Cleaning Tasks\nWe now have a tibble called nh_raw containing 3931 NHANES participants in the rows and 64 variables in the columns. What must we do to clean up the data?\n\nCheck that every identifier (here, SEQN) is unique.\nEnsure that all coded values for “Refused”, “Don’t Know” or “Missing” are interpreted as missing values by R.\nBe sure all quantitative variables have plausible minimum and maximum values.\nReplace the RIAGENDR variable with a new factor variable called SEX with levels Male and Female.\nConvert all binary Yes/No variables to 1/0 numeric variables where 1 = Yes, 0 = No.\nCreate the PHQ-9 score from the nine relevant items in the depression screener (P_DPQ).\nUse meaningful level names for all multi-categorical variables, and be sure R uses factors to represent them.\nClean and adjust the names of the variables to something more useful, as desired. (Usually, I will do this first, but in this case, I’ve decided to do it last.)\n\nOnce we’ve accomplished these cleaning tasks, we’ll save the resulting tibble as an R data set we can use later, and we’ll summarize our final analytic variables in a proper codebook."
  },
  {
    "objectID": "01-nhanes.html#our-identifying-variable",
    "href": "01-nhanes.html#our-identifying-variable",
    "title": "1  Building the nh432 example",
    "section": "1.7 Our identifying variable",
    "text": "1.7 Our identifying variable\nSEQN is meant to identify the rows (participants) in these data, with one row per SEQN. Is every SEQN unique?\n\nnrow(nh_raw)\n\n[1] 3931\n\nn_distinct(nh_raw$SEQN)\n\n[1] 3931\n\n\nIt looks like the number of rows in our tibble matches the number of unique (distinct) SEQN values, so we’re OK. I prefer to specify that the SEQN be maintained by R as a character variable, which reduces the chance of my accidentally including it in a model as if it were something meaningful.\n\nnh_fixing <- nh_raw |> mutate(SEQN = as.character(SEQN))"
  },
  {
    "objectID": "01-nhanes.html#refused-dont-know",
    "href": "01-nhanes.html#refused-dont-know",
    "title": "1  Building the nh432 example",
    "section": "1.8 “Refused” & “Don’t Know”",
    "text": "1.8 “Refused” & “Don’t Know”\nSome of our variables have “hidden” missing values coded as “Refused” or “Don’t Know”. We must ensure that R sees these values as missing.\n\nThe following variables use code 7 for Refused and 9 for Don’t Know:\n\nDMDEDUC2, HUQ071, HUQ090, ALQ111, DEQ034D,\nDPQ010, DPQ020, DPQ030, DPQ040, DPQ050,\nDPQ060, DPQ070, DPQ080, DPQ090, DPQ100,\nDBQ700, HIQ011, HIQ210, MCQ366A, MCQ366B,\nMCQ371A, MCQ371B, PAQ605, PAQ650, RHQ131,\nSLQ030, SLQ050, SMQ020, SMQ856, SMQ860,\nSMQ866, WHQ040\n\nThe following variables use code 9 for Unknown and 99 for Don’t Know:\n\nOHQ870\n\nThe following variables use code 77 for Refused and 99 for Unknown:\n\nPAQ610, PAQ655, RHQ160, SMD641\n\nThe following variables use code 777 for Refused and 999 for Don’t Know:\n\nALQ130\n\nThe following variables use code 7777 for Refused and 9999 for Don’t Know:\n\nPAD680, WHD010, WHD020\n\n\nThe replace_with_na() set of functions from the naniar package can be very helpful here4.\n\nnh_fixing <- nh_fixing %>%\n  replace_with_na_at(.vars = c(\"DMDEDUC2\", \"HUQ071\", \"HUQ090\", \"ALQ111\", \n                               \"DEQ034D\", \"DPQ010\", \"DPQ020\", \"DPQ030\",\n                               \"DPQ040\", \"DPQ050\", \"DPQ060\", \"DPQ070\",\n                               \"DPQ080\", \"DPQ090\", \"DPQ100\", \"DBQ700\",\n                               \"HIQ011\", \"HIQ210\", \"MCQ366A\", \"MCQ366B\",\n                               \"MCQ371A\", \"MCQ371B\", \"PAQ605\", \"PAQ650\", \n                               \"RHQ131\", \"SLQ030\", \"SLQ050\", \"SMQ020\",\n                               \"SMQ856\", \"SMQ860\", \"SMQ866\", \"WHQ040\"),\n                     condition = ~.x %in% c(7, 9)) %>%\n  replace_with_na_at(.vars = c(\"OHQ870\"),\n                     condition = ~.x %in% c(9, 99)) %>%\n  replace_with_na_at(.vars = c(\"PAQ610\", \"PAQ655\", \"RHQ160\", \"SMD641\"),\n                     condition = ~.x %in% c(77, 99)) %>%\n  replace_with_na_at(.vars = c(\"ALQ130\"),\n                     condition = ~.x %in% c(777, 999)) %>%\n  replace_with_na_at(.vars = c(\"PAD680\", \"WHD010\", \"WHD020\"),\n                     condition = ~.x %in% c(7777, 9999))"
  },
  {
    "objectID": "01-nhanes.html#variables-without-variation",
    "href": "01-nhanes.html#variables-without-variation",
    "title": "1  Building the nh432 example",
    "section": "1.9 Variables without Variation",
    "text": "1.9 Variables without Variation\nNote first that we have two variables which now have the same value for all participants.\n\n\n\nVariable\nDescription\nCodes\n\n\n\n\nRIDSTATR\nInterview and examination status\n2 = Both\n\n\nOHDEXSTS\nComplete oral health exam?\n1 = Yes\n\n\n\n\nnh_fixing |> count(RIDSTATR, OHDEXSTS)\n\n# A tibble: 1 × 3\n  RIDSTATR OHDEXSTS     n\n     <dbl>    <dbl> <int>\n1        2        1  3931\n\n\nWe won’t use these variables in our analyses, now that we’ve verified them."
  },
  {
    "objectID": "01-nhanes.html#quantitative-variables",
    "href": "01-nhanes.html#quantitative-variables",
    "title": "1  Building the nh432 example",
    "section": "1.10 Quantitative Variables",
    "text": "1.10 Quantitative Variables\nHere are our quantitative variables, and some key information about the values we observe, along with their units of measurement. Our job here is to check the ranges of these variables, and be sure we have no unreasonable values. It’s also helpful to keep an eye on how much missingness we might have to deal with.\nWe’re also going to rename each of these variables, as indicated.\n\n\n\n\n\n\n\n\n\n\n\nNew  Name\nNHANES  Name\nDescription\nUnits\n# NA\nRange\n\n\n\n\nAGE\nRIDAGEYR\nAge at screening\nyears\n0\n30, 59\n\n\nWEIGHT\nBMXWT\nBody weight\nkg\n28\n36.9, 254.3\n\n\nHEIGHT\nBMXHT\nStanding height\ncm\n30\n135.3, 198.7\n\n\nWAIST\nBMXWAIST\nWaist Circumference\ncm\n149\n57.9, 178\n\n\nSBP\nBPXOSY2\nSystolic BP (2nd reading)\nmm Hg\n346\n69, 222\n\n\nDBP\nBPXODI2\nDiastolic BP (2nd reading)\nmm Hg\n346\n31, 136\n\n\nPULSE1\nBPXOPLS1\nPulse (1st reading)\n\\(\\frac{beats}{minute}\\)\n615\n38, 126\n\n\nPULSE2\nBPXOPLS2\nPulse (2nd reading)\n\\(\\frac{beats}{minute}\\)\n617\n37, 121\n\n\nWBC\nLBXWBCSI\nWhite blood cell count\n\\(\\frac{\\mbox{1000 cells}}{uL}\\)\n176\n2.3, 22.8\n\n\nPLATELET\nLBXPLTSI\nPlatelet count\n\\(\\frac{\\mbox{1000 cells}}{uL}\\)\n176\n47, 818\n\n\nHSCRP\nLBXHSCRP\nHigh-Sensitivity  C-Reactive Protein\n\\(mg/L\\)\n267\n0.11, 182.82\n\n\nDRINKS\nALQ130\nAverage daily  Alcoholic drinks\ndrinks\n789\n0, 15\n\n\nFLOSS\nOHQ870\nDays using dental floss  in past week\ndays\n4\n0, 7\n\n\nVIGWK_D\nPAQ610\nAverage days per week  with Vigorous work\ndays\n5\n0, 7\n\n\nVIGREC_D\nPAQ655\nAverage days per week  with Vigorous recreation\ndays\n1\n0, 7\n\n\nSEDATE\nPAD680\nAverage daily  Sedentary activity\nminutes\n24\n2, 1320\n\n\nPREGNANT\nRHQ160\nPregnancies\ntimes\n1975\n0, 11\n\n\nSLPWKDAY\nSLD012\nUsual sleep (weekdays)\nhours\n34\n2, 14\n\n\nSLPWKEND\nSLD013\nUsual sleep (weekends)\nhours\n34\n2, 14\n\n\nSMOKE30\nSMD641\nDays in past 30 when  you smoked a cigarette\ndays\n726\n0, 30\n\n\nESTHT\nWHD010\nSelf-reported height\ninches\n95\n50, 81\n\n\nESTWT\nWHD020\nSelf-reported weight\npounds\n68\n86, 578\n\n\n\nTo insert the number of missing values and range (minimum, maximum among non-missing values) into the table, I used inline R code like this:\n\nn_miss(nh_fixing$BMXWT)\n\n[1] 28\n\nrange(nh_fixing$BMXWT, na.rm = TRUE)\n\n[1]  36.9 254.3\n\n\n\n1.10.1 Renaming the Quantities\nHere’s the renaming code:\n\nnh_fixing <- nh_fixing |>\n  rename(AGE = RIDAGEYR, WEIGHT = BMXWT, HEIGHT = BMXHT,\n         WAIST = BMXWAIST, SBP = BPXOSY2, DBP = BPXODI2,\n         PULSE1 = BPXOPLS1, PULSE2 = BPXOPLS2, WBC = LBXWBCSI,\n         PLATELET = LBXPLTSI, HSCRP = LBXHSCRP, DRINKS = ALQ130,\n         FLOSS = OHQ870, VIGWK_D = PAQ610, VIGREC_D = PAQ655,\n         SEDATE = PAD680, PREGS = RHQ160, SLPWKDAY = SLD012, \n         SLPWKEND = SLD013, SMOKE30 = SMD641, \n         ESTHT = WHD010, ESTWT = WHD020)\n\n\n\n1.10.2 Sampling Weights\nHere are the sampling weights, which I think of as unitless, typically, though they represent people.\n\n\n\nVariable\nDescription\n# NA\nRange\n\n\n\n\nWTINTPRP\nSampling Weight (interviews)\n0\n2467.1, 311265.2\n\n\nWTMECPRP\nSampling Weight (examinations)\n0\n2589.2, 321573.5\n\n\n\nNote that to obtain these ranges formatted like this, I had to use some additional code in the table:\n\nformat(round_half_up(range(nh_fixing$WTINTPRP, na.rm = TRUE),1), scientific = FALSE)\n\n[1] \"  2467.1\" \"311265.2\""
  },
  {
    "objectID": "01-nhanes.html#binary-variables",
    "href": "01-nhanes.html#binary-variables",
    "title": "1  Building the nh432 example",
    "section": "1.11 Binary Variables",
    "text": "1.11 Binary Variables\n\n1.11.1 Sex (RIAGENDR)\nTo start, let’s do something about the variable describing the participant’s biological sex (so we’ll rename it to a more useful name), and then we’ll recode the values of the SEX variable to more useful choices.\n\n\n\nNew Name\nNHANES Name\nDescription\n\n\n\n\nSEX\nRIAGENDR\nSex\n\n\n\nNote that we have more female than male subjects, and no missing values, as it turns out.\n\nnh_fixing |> count(RIAGENDR)\n\n# A tibble: 2 × 2\n  RIAGENDR     n\n     <dbl> <int>\n1        1  1837\n2        2  2094\n\n\nNow, let’s convert the information in RIAGENDR to SEX.\n\nnh_fixing <- nh_fixing |>\n  rename(SEX = RIAGENDR) |>\n  mutate(SEX = factor(ifelse(SEX == 1, \"Male\", \"Female\")))\n\nAnd we’ll run a little “sanity check” here to ensure that we’ve recoded this variable properly5.\n\nnh_fixing |> count(SEX)\n\n# A tibble: 2 × 2\n  SEX        n\n  <fct>  <int>\n1 Female  2094\n2 Male    1837\n\n\n\n\n1.11.2 Yes/No variables\nNow, let’s tackle the variables with code 1 = Yes, and 2 = No, and (potentially) some missing values. I’ll summarize each with the percentage of “Yes” responses (out of those with code 1 or 2) and the number of missing values.\n\n\n\n\n\n\n\n\n\n\nNew NAME\nNHANES NAME\nDescription\n% Yes\n# NA\n\n\n\n\nHOSPITAL\nHUQ071\nOvernight hospital patient in past 12m?\n8.7\n0\n\n\nMENTALH\nHUQ090\nSeen mental health professional past 12m?\n12.1\n2\n\n\nEVERALC\nALQ111\nEver had a drink of alcohol?\n91.1\n205\n\n\nINSURNOW\nHIQ011\nCovered by health insurance now?\n80.8\n10\n\n\nNOINSUR\nHIQ210\nTime when no insurance in past year?\n26.6\n16\n\n\nDR_LOSE\nMCQ366A\nDoctor told you to control/lose weight  in the past 12 months?\n30.3\n1\n\n\nDR_EXER\nMCQ366B\nDoctor told you to exercise  in the past 12 months?\n42.7\n1\n\n\nNOW_LOSE\nMCQ371A\nAre you now controlling or losing weight?\n63.2\n2\n\n\nNOW_EXER\nMCQ371B\nAre you now increasing exercise?\n60.3\n1\n\n\nWORK_V\nPAQ605\nVigorous work activity for 10 min/week?\n28.4\n4\n\n\nREC_V\nPAQ650\nVigorous recreational activity for 10 min/week?\n26.9\n0\n\n\nEVERPREG\nRHQ131\nEver been pregnant?\n89.2\n1972\n\n\nSLPTROUB\nSLQ050\nEver told a doctor you had trouble sleeping?\n29.5\n3\n\n\nCIG100\nSMQ020\nSmoked at least 100 cigarettes in your life?\n40.1\n1\n\n\nAWAYWORK\nSMQ856\nLast 7 days worked at a job not at home?\n67.7\n0\n\n\nAWAYREST\nSMQ860\nLast 7 days spent time in a restaurant?\n58.1\n2\n\n\nAWAYBAR\nSMQ866\nLast 7 days spent time in a bar?\n15.4\n0\n\n\n\nThe inline code I used in the tables was, for example:\n\nround_half_up(100 * sum(nh_fixing$HUQ090 == \"1\", na.rm = TRUE) / \n                sum(nh_fixing$HUQ090 %in% c(\"1\",\"2\"), na.rm = TRUE), 1)\n\n[1] 12.1\n\nn_miss(nh_fixing$HUQ090)\n\n[1] 2\n\n\nTo clean these (1 = Yes, 2 = No) variables, I’ll subtract the values from 2, to obtain variables where 1 = Yes and 0 = No. I’ll use the across() function within my mutate() statement so as to avoid having to type out each change individually6.\n\nnh_fixing <- nh_fixing |>\n  mutate(across(c(HUQ071, HUQ090, ALQ111, HIQ011, HIQ210,\n                  MCQ366A, MCQ366B, MCQ371A, MCQ371B, PAQ605,\n                  PAQ650, RHQ131, SLQ050, SMQ020, SMQ856,\n                  SMQ860, SMQ866), \n                ~ 2 - .x))\n\nLet’s do just one of the relevant sanity checks here. In addition to verifying that our new variable has the values 0 and 1 (instead of 2 and 1), we want to be certain that we’ve maintained any missing values.\n\nnh_fixing |> count(SLQ050)\n\n# A tibble: 3 × 2\n  SLQ050     n\n   <dbl> <int>\n1      0  2769\n2      1  1159\n3     NA     3\n\n\n\n\n1.11.3 Renaming Binary Variables\nHere’s the renaming code.\n\nnh_fixing <- nh_fixing |>\n  rename(HOSPITAL = HUQ071, MENTALH = HUQ090, EVERALC = ALQ111,\n         INSURNOW = HIQ011, NOINSUR = HIQ210, DR_LOSE = MCQ366A,\n         DR_EXER = MCQ366B, NOW_LOSE = MCQ371A, NOW_EXER = MCQ371B,\n         WORK_V = PAQ605, REC_V = PAQ650, EVERPREG = RHQ131, \n         SLPTROUB = SLQ050, CIG100 =  SMQ020, AWAYWORK = SMQ856,\n         AWAYREST = SMQ860, AWAYBAR = SMQ866)"
  },
  {
    "objectID": "01-nhanes.html#create-phq-9-scores",
    "href": "01-nhanes.html#create-phq-9-scores",
    "title": "1  Building the nh432 example",
    "section": "1.12 Create PHQ-9 Scores",
    "text": "1.12 Create PHQ-9 Scores\nThe questions below are asked to assess depression severity, following the prompt “Over the last two weeks, how often have you been bothered by any of the following problems?”\n\n\n\n\n\n\n\n\nVariable\nDescription\nValues\n\n\n\n\nSEQN\nRespondent Sequence Number  (participant ID)\nlink across databases\n\n\nDPQ010\nHave little interest in doing things\n0-3: see below\n\n\nDPQ020\nFeeling down, depressed or hopeless\n0-3: see below\n\n\nDPQ030\nTrouble sleeping or sleeping too much\n0-3: see below\n\n\nDPQ040\nFeeling tired or having little energy\n0-3: see below\n\n\nDPQ050\nPoor appetite or overeating\n0-3: see below\n\n\nDPQ060\nFeeling bad about yourself\n0-3: see below\n\n\nDPQ070\nTrouble concentrating on things\n0-3: see below\n\n\nDPQ080\nMoving or speaking slowly or too fast\n0-3: see below\n\n\nDPQ090\nThoughts you would be better off dead\n0-3: see below\n\n\n\n\nFor DPQ010 - DPQ090, the codes are 0 = Not at all, 1 = Several days in the past two weeks, 2 = More than half the days in the past two weeks, 3 = Nearly every day in the past two weeks.\n\n\n1.12.1 Forming the PHQ-9 Score\nOne way to use this information is to sum the scores from items DPQ010 through DPQ090 to obtain a result on a scale from 0 - 27. Cutoffs of 5, 10, 15, and 20 then represent mild, moderate, moderately severe, and severe levels of depressive symptoms, respectively7. If we had no missing values in our responses, then this would be relatively straightforward.\n\ntemp <- nh_fixing |>\n  mutate(PHQ9 = DPQ010 + DPQ020 + DPQ030 + DPQ040 + DPQ050 +\n           DPQ060 + DPQ070 + DPQ080 + DPQ090)\n\ntemp |> count(PHQ9) |> tail()\n\n# A tibble: 6 × 2\n   PHQ9     n\n  <dbl> <int>\n1    22     9\n2    23     5\n3    24     2\n4    25     2\n5    26     3\n6    NA   221\n\n\nIt turns out that this formulation of PHQ9 regards as missing the result for any participant who failed to answer all 9 questions. A common approach to dealing with missing data in creating PHQ-9 scores8 is to score all questionnaires with up to two missing values, replacing any missing values with the average score of the completed items.\nSo how many of our subjects are missing only one or two of the 9 items?\n\ntemp2 <- temp |>\n  select(SEQN, DPQ010, DPQ020, DPQ030, DPQ040, DPQ050, DPQ060, \n         DPQ070, DPQ080, DPQ090) \n\nmiss_case_table(temp2)\n\n# A tibble: 5 × 3\n  n_miss_in_case n_cases pct_cases\n           <int>   <int>     <dbl>\n1              0    3710   94.4   \n2              1       5    0.127 \n3              2       3    0.0763\n4              4       1    0.0254\n5              9     212    5.39  \n\n\nWith a little googling I found an R script online that will perform this task, and create three new variables:\n\nnvalid_phq9 = Number of Valid Responses (out of 9) to the PHQ-9 items\nPHQ9 = PHQ-9 score (0-27 scale, higher values indicate more depression)\nPHQ9_CAT = factor describing PHQ-9 score\n\nPHQ9 > 20 means PHQ9_CAT is”severe”,\n15-19 = “moderately severe”,\n10-14 = “moderate”\n5-9 = “mild”\n0-4 = “minimal”\n\n\n\nscoring_phq9 <- function(data, items.phq9) {\n  data %>%\n    mutate(nvalid_phq9 = rowSums(!is.na(select(., items.phq9))),\n           nvalid_phq9 = as.integer(nvalid_phq9),\n           mean.temp = rowSums(select(., items.phq9), na.rm = TRUE)/nvalid_phq9,\n           phq.01.temp = as.integer(unlist(data[items.phq9[1]])),\n           phq.02.temp = as.integer(unlist(data[items.phq9[2]])),\n           phq.03.temp = as.integer(unlist(data[items.phq9[3]])),\n           phq.04.temp = as.integer(unlist(data[items.phq9[4]])),\n           phq.05.temp = as.integer(unlist(data[items.phq9[5]])),\n           phq.06.temp = as.integer(unlist(data[items.phq9[6]])),\n           phq.07.temp = as.integer(unlist(data[items.phq9[7]])),\n           phq.08.temp = as.integer(unlist(data[items.phq9[8]])),\n           phq.09.temp = as.integer(unlist(data[items.phq9[9]]))) %>%\n    mutate_at(vars(phq.01.temp:phq.09.temp),\n              funs(ifelse(is.na(.), round(mean.temp), .))) %>%\n    mutate(score.temp = rowSums(select(., phq.01.temp:phq.09.temp), na.rm = TRUE),\n           PHQ9 = ifelse(nvalid_phq9 >= 7, as.integer(round(score.temp)), NA),\n           PHQ9_CAT = case_when(\n             PHQ9 >= 20 ~ 'severe',\n             PHQ9 >= 15 ~ 'moderately severe',\n             PHQ9 >= 10 ~ 'moderate',\n             PHQ9 >= 5 ~ 'mild',\n             PHQ9 < 5 ~ 'minimal'),\n             PHQ9_CAT = factor(PHQ9_CAT, levels = c('minimal', 'mild',\n                                                          'moderate', 'moderately severe',\n                                                          'severe'))) %>%\n    select(-ends_with(\"temp\"))\n \n}\n\nApplying this script to our nh_fixing data, our result is:\n\nitems.phq9 <- c(\"DPQ010\", \"DPQ020\", \"DPQ030\", \"DPQ040\", \"DPQ050\",\n                \"DPQ060\", \"DPQ070\", \"DPQ080\", \"DPQ090\")\nnh_fixing <- nh_fixing %>% scoring_phq9(., all_of(items.phq9))\n\nnh_fixing |> count(nvalid_phq9, PHQ9, PHQ9_CAT)\n\n# A tibble: 36 × 4\n   nvalid_phq9  PHQ9 PHQ9_CAT     n\n         <int> <int> <fct>    <int>\n 1           0    NA <NA>       212\n 2           5    NA <NA>         1\n 3           7     1 minimal      1\n 4           7     2 minimal      1\n 5           7     7 mild         1\n 6           8     1 minimal      2\n 7           8     2 minimal      1\n 8           8     3 minimal      1\n 9           8     8 mild         1\n10           9     0 minimal   1233\n# … with 26 more rows\n\n\n\n\n1.12.2 Distribution of PHQ-9 Score\nHere’s a quick look at the distribution of PHQ-9 scores in our nh_fixing data.\n\nnh_fixing |> filter(complete.cases(PHQ9, PHQ9_CAT)) %>%\n  ggplot(., aes(x = PHQ9, fill = PHQ9_CAT)) +\n  geom_histogram(binwidth = 1) +\n  scale_fill_viridis_d() + \n  labs(title = \"PHQ-9 Scores for subjects in `nh_fixing`\")\n\n\n\n\n\n\n1.12.3 Fixing the DPQ100 variable\nThe DPQ100 variable should be 0 (Not at all difficult) if the PHQ-9 score is zero. We need to fix this, because NHANES participants who answered 0 (Not at all) to each of the nine elements contained in the PHQ-9 were not asked the DPQ100 question. So, we set the value of DPQ100 to be 0 if PHQ9 is 0.\n\nnh_fixing <- nh_fixing |>\n  mutate(DPQ100 = ifelse(PHQ9 == 0, 0, DPQ100))\n\nThis will eliminate the “automatic missing” values in DPQ100.\n\nnh_fixing |> tabyl(DPQ100)\n\n DPQ100    n    percent valid_percent\n      0 3039 0.77308573    0.81781485\n      1  541 0.13762401    0.14558665\n      2   93 0.02365810    0.02502691\n      3   43 0.01093869    0.01157158\n     NA  215 0.05469346            NA"
  },
  {
    "objectID": "01-nhanes.html#multi-categorical-variables",
    "href": "01-nhanes.html#multi-categorical-variables",
    "title": "1  Building the nh432 example",
    "section": "1.13 Multi-Categorical Variables",
    "text": "1.13 Multi-Categorical Variables\nOur remaining categorical variables with more than two levels are:\n\n\n\n\n\n\n\n\n\nVariable\nDescription\nCodes\n# NAs\n\n\n\n\nRIDRETH3\nRace/Hispanic origin\n1, 2, 3, 4, 6, 7\n0\n\n\nDMDEDUC2\nEducation Level\n1, 2, 3, 4, 5\n1\n\n\nOHAREC\nOverall Recommendation for Care\n1, 2, 3, 4\n0\n\n\nHUQ010\nGeneral health condition\n1, 2, 3, 4, 5\n0\n\n\nDEQ034D\nUse sunscreen if outside on very sunny day?\n1, 2, 3, 4, 5\n19\n\n\nDPQ100\nDifficulty depression problems have caused\n0, 1, 2, 3\n215\n\n\nDBQ700\nHow healthy is your diet?\n1, 2, 3, 4, 5\n1\n\n\nFSDAD\nAdult food security in last 12m\n1, 2, 3, 4\n231\n\n\nSLQ030\nHow often do you snore?\n0, 1, 2, 3\n219\n\n\nWHQ040\nLike to weigh more, less, or same?\n1, 2, 3\n3\n\n\n\n\n1.13.1 Creating RACEETH from RIDRETH3\nAt the moment, our RIDRETH3 data look like this:\n\nt_ridreth3 <- nh_fixing |> tabyl(RIDRETH3) |> adorn_pct_formatting() |>\n  mutate(Code = c(\"Mexican American\", \"Other Hispanic\", \"Non-Hispanic White\", \n                  \"Non-Hispanic Black\", \"Non-Hispanic Asian\", \"Other Race\"))\n\ngt(t_ridreth3)\n\n\n\n\n\n  \n  \n    \n      RIDRETH3\n      n\n      percent\n      Code\n    \n  \n  \n    1\n500\n12.7%\nMexican American\n    2\n403\n10.3%\nOther Hispanic\n    3\n1192\n30.3%\nNon-Hispanic White\n    4\n1049\n26.7%\nNon-Hispanic Black\n    6\n588\n15.0%\nNon-Hispanic Asian\n    7\n199\n5.1%\nOther Race\n  \n  \n  \n\n\n\n\nNow, we’ll turn this RIDRETH3 variable into a new factor called RACEETH with meaningful levels, and then sort those levels by their frequency in the data. We’ll also collapse together the Mexican American and Other Hispanic levels, not because the distinction is irrelevant, but more to demonstrate how this might be done.\n\nnh_fixing <- nh_fixing |>\n  mutate(RACEETH = \n           fct_recode(\n             factor(RIDRETH3), \n             \"Hispanic\" = \"1\", \n             \"Hispanic\" = \"2\", \n             \"Non-H White\" = \"3\",\n             \"Non-H Black\" = \"4\",\n             \"Non-H Asian\" = \"6\",\n             \"Other Race\" = \"7\")) |>\n  mutate(RACEETH = fct_infreq(RACEETH))\n\nI’m using fct_infreq() here to sort the (nominal) Race and Ethnicity data so that the most common column appears first, and will thus be treated as the “baseline” level in models. Here is the resulting order.\n\nnh_fixing |> count(RACEETH)\n\n# A tibble: 5 × 2\n  RACEETH         n\n  <fct>       <int>\n1 Non-H White  1192\n2 Non-H Black  1049\n3 Hispanic      903\n4 Non-H Asian   588\n5 Other Race    199\n\n\nNow, let’s check9 to see if RACEETH and RIDRETH3 include the same information (after collapsing the Mexican American and Other Hispanic categories.)\n\nnh_fixing |> count(RACEETH, RIDRETH3)\n\n# A tibble: 6 × 3\n  RACEETH     RIDRETH3     n\n  <fct>          <dbl> <int>\n1 Non-H White        3  1192\n2 Non-H Black        4  1049\n3 Hispanic           1   500\n4 Hispanic           2   403\n5 Non-H Asian        6   588\n6 Other Race         7   199\n\n\n\n\n1.13.2 Creating EDUC from DMDEDUC2\nAt the moment, our DMDEDUC2 data look like this:\n\nt_dmdeduc2 <- nh_fixing |> tabyl(DMDEDUC2) |> adorn_pct_formatting() |>\n  mutate(Code = c(\"Less than 9th grade\", \"9th-11th grade\", \"High School Grad\", \n                  \"Some College / AA\", \"College Grad\", \"Missing\"))\n\ngt(t_dmdeduc2)\n\n\n\n\n\n  \n  \n    \n      DMDEDUC2\n      n\n      percent\n      valid_percent\n      Code\n    \n  \n  \n    1\n272\n6.9%\n6.9%\nLess than 9th grade\n    2\n424\n10.8%\n10.8%\n9th-11th grade\n    3\n850\n21.6%\n21.6%\nHigh School Grad\n    4\n1287\n32.7%\n32.7%\nSome College / AA\n    5\n1097\n27.9%\n27.9%\nCollege Grad\n    NA\n1\n0.0%\n-\nMissing\n  \n  \n  \n\n\n\n\nNow, we’ll turn this DMDEDUC2 variable into a new factor called EDUC with meaningful levels.\n\nnh_fixing <- nh_fixing |>\n  mutate(EDUC = \n           fct_recode(\n             factor(DMDEDUC2), \n             \"Less than 9th Grade\" = \"1\", \n             \"9th - 11th Grade\" = \"2\", \n             \"High School Grad\" = \"3\",\n             \"Some College / AA\" = \"4\",\n             \"College Grad\" = \"5\")) \n\nOnce again, checking our work…\n\nnh_fixing |> tabyl(EDUC, DMDEDUC2) |> gt()\n\n\n\n\n\n  \n  \n    \n      EDUC\n      1\n      2\n      3\n      4\n      5\n      NA_\n    \n  \n  \n    Less than 9th Grade\n272\n0\n0\n0\n0\n0\n    9th - 11th Grade\n0\n424\n0\n0\n0\n0\n    High School Grad\n0\n0\n850\n0\n0\n0\n    Some College / AA\n0\n0\n0\n1287\n0\n0\n    College Grad\n0\n0\n0\n0\n1097\n0\n    NA\n0\n0\n0\n0\n0\n1\n  \n  \n  \n\n\n\n\n\n\n1.13.3 Creating DENTAL from OHAREC\n\nt_oharec <- nh_fixing |> tabyl(OHAREC) |> adorn_pct_formatting() |>\n  mutate(Code = c(\"See a dentist immediately\", \"See a dentist within the next 2 weeks\", \"See a dentist at your earliest convenience\", \"Continue your regular routine care\"))\n\ngt(t_oharec)\n\n\n\n\n\n  \n  \n    \n      OHAREC\n      n\n      percent\n      Code\n    \n  \n  \n    1\n4\n0.1%\nSee a dentist immediately\n    2\n230\n5.9%\nSee a dentist within the next 2 weeks\n    3\n1671\n42.5%\nSee a dentist at your earliest convenience\n    4\n2026\n51.5%\nContinue your regular routine care\n  \n  \n  \n\n\n\n\nWe’ll collapse categories 1 and 2 together since they are quite small.\n\nnh_fixing <- nh_fixing |>\n  mutate(DENTAL = \n           fct_recode(\n             factor(OHAREC), \n             \"See dentist urgently\" = \"1\", \n             \"See dentist urgently\" = \"2\", \n             \"See dentist soon\" = \"3\",\n             \"Regular Routine\" = \"4\")) \n\nOnce again, checking our work…\n\nnh_fixing |> tabyl(DENTAL, OHAREC) |> gt()\n\n\n\n\n\n  \n  \n    \n      DENTAL\n      1\n      2\n      3\n      4\n    \n  \n  \n    See dentist urgently\n4\n230\n0\n0\n    See dentist soon\n0\n0\n1671\n0\n    Regular Routine\n0\n0\n0\n2026\n  \n  \n  \n\n\n\n\n\n\n1.13.4 Creating SROH from HUQ010\n\n\n\n\n\n\n\n\n\nVariable\nDescription\nCodes\n# NAs\n\n\n\n\nHUQ010\nGeneral health condition\n1 = Excellent  2 = Very Good  3 = Good  4 = Fair  5 = Poor\n0\n\n\n\n\nnh_fixing <- nh_fixing |>\n  mutate(SROH = \n           fct_recode(\n             factor(HUQ010), \n             \"Excellent\" = \"1\", \n             \"Very Good\" = \"2\", \n             \"Good\" = \"3\",\n             \"Fair\" = \"4\",\n             \"Poor\" = \"5\"))\n\nChecking our work…\n\nnh_fixing |> tabyl(SROH, HUQ010) |> gt()\n\n\n\n\n\n  \n  \n    \n      SROH\n      1\n      2\n      3\n      4\n      5\n    \n  \n  \n    Excellent\n495\n0\n0\n0\n0\n    Very Good\n0\n1071\n0\n0\n0\n    Good\n0\n0\n1462\n0\n0\n    Fair\n0\n0\n0\n765\n0\n    Poor\n0\n0\n0\n0\n138\n  \n  \n  \n\n\n\n\n\n\n1.13.5 Creating SUNSCR from DEQ034D\n\n\n\n\n\n\n\n\n\nVariable\nDescription\nCodes\n# NAs\n\n\n\n\nDEQ034D\nUse sunscreen if outside on very sunny day?\n1 = Always  2 = Most of the time  3 = Sometimes  4 = Rarely  5 = Never\n19\n\n\n\n\nnh_fixing <- nh_fixing |>\n  mutate(SUNSCR = \n           fct_recode(\n             factor(DEQ034D), \n             \"Always\" = \"1\", \n             \"Most of the time\" = \"2\", \n             \"Sometimes\" = \"3\",\n             \"Rarely\" = \"4\",\n             \"Never\" = \"5\")) \n\n\nnh_fixing |> tabyl(SUNSCR, DEQ034D) |> gt()\n\n\n\n\n\n  \n  \n    \n      SUNSCR\n      1\n      2\n      3\n      4\n      5\n      NA_\n    \n  \n  \n    Always\n351\n0\n0\n0\n0\n0\n    Most of the time\n0\n485\n0\n0\n0\n0\n    Sometimes\n0\n0\n831\n0\n0\n0\n    Rarely\n0\n0\n0\n662\n0\n0\n    Never\n0\n0\n0\n0\n1583\n0\n    NA\n0\n0\n0\n0\n0\n19\n  \n  \n  \n\n\n\n\n\n\n1.13.6 Creating DEPRDIFF from DPQ100\n\n\n\n\n\n\n\n\n\nVariable\nDescription\nCodes\n# NAs\n\n\n\n\nDPQ100\nDifficulty depression problems have caused\n0 = Not at all difficult  1 = Somewhat difficult  2 = Very difficult  3 = Extremely difficult\n215\n\n\n\n\nnh_fixing <- nh_fixing |>\n  mutate(DEPRDIFF = \n           fct_recode(\n             factor(DPQ100), \n             \"Not at all\" = \"0\", \n             \"Somewhat\" = \"1\", \n             \"Very\" = \"2\",\n             \"Extremely\" = \"3\")) \n\n\nnh_fixing |> tabyl(DEPRDIFF, DPQ100) |> gt()\n\n\n\n\n\n  \n  \n    \n      DEPRDIFF\n      0\n      1\n      2\n      3\n      NA_\n    \n  \n  \n    Not at all\n3039\n0\n0\n0\n0\n    Somewhat\n0\n541\n0\n0\n0\n    Very\n0\n0\n93\n0\n0\n    Extremely\n0\n0\n0\n43\n0\n    NA\n0\n0\n0\n0\n215\n  \n  \n  \n\n\n\n\n\n\n1.13.7 Creating DIETQUAL from DBQ700\n\n\n\n\n\n\n\n\n\nVariable\nDescription\nCodes\n# NAs\n\n\n\n\nDBQ700\nHow healthy is your diet?\n1 = Excellent  2 = Very Good  3 = Good  4 = Fair  5 = Poor\n1\n\n\n\n\nnh_fixing <- nh_fixing |>\n  mutate(DIETQUAL = \n           fct_recode(\n             factor(DBQ700), \n             \"Excellent\" = \"1\", \n             \"Very Good\" = \"2\", \n             \"Good\" = \"3\",\n             \"Fair\" = \"4\",\n             \"Poor\" = \"5\")) \n\n\nnh_fixing |> tabyl(DIETQUAL, DBQ700) |> gt()\n\n\n\n\n\n  \n  \n    \n      DIETQUAL\n      1\n      2\n      3\n      4\n      5\n      NA_\n    \n  \n  \n    Excellent\n260\n0\n0\n0\n0\n0\n    Very Good\n0\n758\n0\n0\n0\n0\n    Good\n0\n0\n1519\n0\n0\n0\n    Fair\n0\n0\n0\n1082\n0\n0\n    Poor\n0\n0\n0\n0\n311\n0\n    NA\n0\n0\n0\n0\n0\n1\n  \n  \n  \n\n\n\n\n\n\n1.13.8 Creating FOODSEC from FSDAD\n\n\n\n\n\n\n\n\n\nVariable\nDescription\nCodes\n# NAs\n\n\n\n\nFSDAD\nAdult food security category for last 12m\n1 = Full food security  2 = Marginal food security  3 = Low food security  4 = Very low food security\n231\n\n\n\n\nnh_fixing <- nh_fixing |>\n  mutate(FOODSEC = \n           fct_recode(\n             factor(FSDAD), \n             \"Full\" = \"1\", \n             \"Marginal\" = \"2\", \n             \"Low\" = \"3\",\n             \"Very Low\" = \"4\")) \n\n\nnh_fixing |> tabyl(FOODSEC, FSDAD) |> gt()\n\n\n\n\n\n  \n  \n    \n      FOODSEC\n      1\n      2\n      3\n      4\n      NA_\n    \n  \n  \n    Full\n2247\n0\n0\n0\n0\n    Marginal\n0\n565\n0\n0\n0\n    Low\n0\n0\n503\n0\n0\n    Very Low\n0\n0\n0\n385\n0\n    NA\n0\n0\n0\n0\n231\n  \n  \n  \n\n\n\n\n\n\n1.13.9 Creating SNORE from SLQ030\n\n\n\n\n\n\n\n\n\nVariable\nDescription\nCodes\n# NAs\n\n\n\n\nSLQ030\nHow often do you snore?\n0 = Never  1 = Rarely (1-2 nights/week)  2 = Occasionally (3-4 nights/week)  3 = Frequently (5+ nights/week)\n219\n\n\n\n\nnh_fixing <- nh_fixing |>\n  mutate(SNORE = \n           fct_recode(\n             factor(SLQ030), \n             \"Never\" = \"0\", \n             \"Rarely\" = \"1\", \n             \"Occasionally\" = \"2\",\n             \"Frequently\" = \"3\")) \n\n\nnh_fixing |> tabyl(SNORE, SLQ030) |> gt()\n\n\n\n\n\n  \n  \n    \n      SNORE\n      0\n      1\n      2\n      3\n      NA_\n    \n  \n  \n    Never\n855\n0\n0\n0\n0\n    Rarely\n0\n959\n0\n0\n0\n    Occasionally\n0\n0\n700\n0\n0\n    Frequently\n0\n0\n0\n1198\n0\n    NA\n0\n0\n0\n0\n219\n  \n  \n  \n\n\n\n\n\n\n1.13.10 Creating WTGOAL from WHQ040\n\n\n\n\n\n\n\n\n\nVariable\nDescription\nCodes\n# NAs\n\n\n\n\nWHQ040\nLike to weigh more, less, or same?\n1 = More  2 = Less  3 = Stay about the same\n3\n\n\n\nSince there’s a natural ordering here (more then same then less) I’ll adapt it using the fct_relevel() function from the forcats package10 for this variable.\n\nnh_fixing <- nh_fixing |>\n  mutate(WTGOAL = \n           fct_recode(\n             factor(WHQ040), \n             \"More\" = \"1\", \n             \"Less\" = \"2\", \n             \"Same\" = \"3\")) |>\n  mutate(WTGOAL = fct_relevel(WTGOAL, \"More\", \"Same\", \"Less\"))\n\n\nnh_fixing |> tabyl(WTGOAL, WHQ040) |> gt()\n\n\n\n\n\n  \n  \n    \n      WTGOAL\n      1\n      2\n      3\n      NA_\n    \n  \n  \n    More\n289\n0\n0\n0\n    Same\n0\n0\n974\n0\n    Less\n0\n2665\n0\n0\n    NA\n0\n0\n0\n3"
  },
  {
    "objectID": "01-nhanes.html#dropping-variables",
    "href": "01-nhanes.html#dropping-variables",
    "title": "1  Building the nh432 example",
    "section": "1.14 Dropping Variables",
    "text": "1.14 Dropping Variables\nWe’ll drop the following variables before saving an analytic tibble.\n\nOur two variables with no variation\n\nRIDSTATR, OHDEXSTS\n\nElements of the PHQ-9 we no longer need\n\nnvalid_phq, DPQ010, DPQ020, DPQ030, DPQ040\nDPQ050, DPQ060, DPQ070, DPQ080, DPQ090\n\nMulti-categorical variables that we renamed\n\nRIDRETH3, DMDEDUC2, OHAREC, HUQ010, DEQ034D\nDPQ100, DBQ700, FSDAD, SLQ030, WHQ040\n\n\n\nnh_fixing <- nh_fixing |>\n  select(-c(RIDSTATR, OHDEXSTS, nvalid_phq9, DPQ010, \n            DPQ020, DPQ030, DPQ040, DPQ050, DPQ060, \n            DPQ070, DPQ080, DPQ090, RIDRETH3, DMDEDUC2, \n            OHAREC, HUQ010, DEQ034D, DPQ100, DBQ700, \n            FSDAD, SLQ030, WHQ040))"
  },
  {
    "objectID": "01-nhanes.html#resorting-variables",
    "href": "01-nhanes.html#resorting-variables",
    "title": "1  Building the nh432 example",
    "section": "1.15 Resorting Variables",
    "text": "1.15 Resorting Variables\nI’d like to have the variables in the following order:\n\nnh432 <- nh_fixing |>\n  select(SEQN, AGE, RACEETH, EDUC, SEX, INSURNOW, \n         NOINSUR, SROH, WEIGHT, HEIGHT, WAIST, \n         SBP, DBP, PULSE1, PULSE2, WBC, PLATELET, HSCRP, \n         DR_LOSE, DR_EXER, NOW_LOSE, NOW_EXER,\n         ESTHT, ESTWT, WTGOAL, DIETQUAL, FOODSEC, \n         WORK_V, VIGWK_D, REC_V, VIGREC_D, SEDATE, \n         PHQ9, PHQ9_CAT, DEPRDIFF, MENTALH, \n         SLPWKDAY, SLPWKEND, SLPTROUB, SNORE,\n         HOSPITAL, EVERALC, DRINKS, CIG100, SMOKE30,\n         AWAYWORK, AWAYREST, AWAYBAR, DENTAL, FLOSS, \n         EVERPREG, PREGS, SUNSCR, WTINTPRP, WTMECPRP)"
  },
  {
    "objectID": "01-nhanes.html#nh432-analytic-tibble",
    "href": "01-nhanes.html#nh432-analytic-tibble",
    "title": "1  Building the nh432 example",
    "section": "1.16 nh432 analytic tibble",
    "text": "1.16 nh432 analytic tibble\n\nnh432\n\n# A tibble: 3,931 × 55\n   SEQN     AGE RACEETH    EDUC  SEX   INSUR…¹ NOINSUR SROH  WEIGHT HEIGHT WAIST\n   <chr>  <dbl> <fct>      <fct> <fct>   <dbl>   <dbl> <fct>  <dbl>  <dbl> <dbl>\n 1 109271    49 Non-H Whi… 9th … Male        1       0 Fair    98.8   182. 120. \n 2 109273    36 Non-H Whi… Some… Male        1       1 Good    74.3   184.  86.8\n 3 109284    44 Hispanic   9th … Fema…       0       1 Fair    91.1   153. 103. \n 4 109291    42 Non-H Asi… Coll… Fema…       1       0 Fair    81.4   161.  NA  \n 5 109292    58 Hispanic   High… Male        1       0 Very…   86     168. 108. \n 6 109293    44 Non-H Whi… High… Male        1       0 Good    99.4   182. 107  \n 7 109295    54 Hispanic   Less… Fema…       1       0 Good    61.7   157.  90.5\n 8 109297    30 Non-H Asi… Some… Fema…       1       0 Very…   55.4   155.  73.2\n 9 109300    54 Non-H Asi… Coll… Fema…       1       0 Exce…   62     145.  84.8\n10 109305    55 Non-H Asi… Coll… Male        1       0 Good    64     175.  82.5\n# … with 3,921 more rows, 44 more variables: SBP <dbl>, DBP <dbl>,\n#   PULSE1 <dbl>, PULSE2 <dbl>, WBC <dbl>, PLATELET <dbl>, HSCRP <dbl>,\n#   DR_LOSE <dbl>, DR_EXER <dbl>, NOW_LOSE <dbl>, NOW_EXER <dbl>, ESTHT <dbl>,\n#   ESTWT <dbl>, WTGOAL <fct>, DIETQUAL <fct>, FOODSEC <fct>, WORK_V <dbl>,\n#   VIGWK_D <dbl>, REC_V <dbl>, VIGREC_D <dbl>, SEDATE <dbl>, PHQ9 <int>,\n#   PHQ9_CAT <fct>, DEPRDIFF <fct>, MENTALH <dbl>, SLPWKDAY <dbl>,\n#   SLPWKEND <dbl>, SLPTROUB <dbl>, SNORE <fct>, HOSPITAL <dbl>, …\n\n\n\n1.16.1 Saving the tibble as nh432.Rds\n\nwrite_rds(nh432, \"data/nh432.Rds\")"
  },
  {
    "objectID": "02-nh432cb.html#r-setup",
    "href": "02-nh432cb.html#r-setup",
    "title": "2  Codebook for nh432",
    "section": "2.1 R Setup",
    "text": "2.1 R Setup\n\nknitr::opts_chunk$set(comment = NA)\n\nlibrary(janitor) \nlibrary(gt)\nlibrary(gtsummary)\nlibrary(Hmisc)\nlibrary(mosaic)\nlibrary(naniar)\nlibrary(tidyverse) \n\ntheme_set(theme_bw())\n\n\n2.1.1 Data Load\n\nnh432 <- read_rds(\"data/nh432.Rds\")"
  },
  {
    "objectID": "02-nh432cb.html#quantitative-variables-in-nh432",
    "href": "02-nh432cb.html#quantitative-variables-in-nh432",
    "title": "2  Codebook for nh432",
    "section": "2.2 Quantitative Variables in nh432",
    "text": "2.2 Quantitative Variables in nh432\n\nt1_quantitative <- df_stats(~ AGE + WEIGHT + HEIGHT + WAIST + SBP + DBP +\n           PULSE1 + PULSE2 + WBC + PLATELET + HSCRP +\n           ESTHT + ESTWT + VIGWK_D + VIGREC_D + SEDATE + PHQ9 +\n           SLPWKDAY + SLPWKEND + DRINKS + SMOKE30 + \n           FLOSS + PREGS, data = nh432) |>\n  mutate(across(.cols = -c(response, n, missing), \n              round_half_up, digits = 1)) |> \n  rename(med = median, \"NA\" = missing)\n\nt1_quantitative |>\n  mutate(description = \n           c(\"Age (years)\", \"Weight (kg)\", \"Height (cm)\", \n             \"Waist circumference (cm)\", \"Systolic BP (mm Hg)\", \n             \"Diastolic BP (mm Hg)\", \"1st Pulse (beats/min)\", \n             \"2nd Pulse (beats/min)\", \"White Blood Cell Count (1000 cells/uL)\",\n             \"Platelets (1000 cells/uL)\", \n             \"High-Sensitivity C-Reactive Protein (mg/L)\",\n             \"Self Estimate: Height (in)\", \"Self-Estimate: Weight (lb)\",\n             \"Vigorous Work per week (days)\", \n             \"Vigorous Recreation per week (days)\",\n             \"Sedentary Activity per day (minutes)\",\n             \"PHQ-9 Depression Screener Score (points)\",\n             \"Average weekday sleep (hours)\", \"Average weekend sleep (hours)\",\n             \"Average Alcohol per day (drinks)\", \n             \"Days smoked cigarette in last 30\",\n             \"Days Flossed in last 7\", \"Pregnancies\")) |>\n  select(response, description, everything()) |>\n  gt() |>\n  tab_header(title = \"Quantitative Variables in nh432\")\n\n\n\n\n\n  \n    \n      Quantitative Variables in nh432\n    \n    \n  \n  \n    \n      response\n      description\n      min\n      Q1\n      med\n      Q3\n      max\n      mean\n      sd\n      n\n      NA\n    \n  \n  \n    AGE\nAge (years)\n30.0\n37.0\n45.0\n53.0\n59.0\n44.8\n8.7\n3931\n0\n    WEIGHT\nWeight (kg)\n36.9\n69.3\n82.1\n99.1\n254.3\n86.3\n24.6\n3903\n28\n    HEIGHT\nHeight (cm)\n135.3\n160.0\n166.9\n174.7\n198.7\n167.4\n10.1\n3901\n30\n    WAIST\nWaist circumference (cm)\n57.9\n89.1\n99.2\n111.7\n178.0\n101.5\n17.7\n3782\n149\n    SBP\nSystolic BP (mm Hg)\n69.0\n110.0\n120.0\n131.0\n222.0\n121.5\n17.0\n3585\n346\n    DBP\nDiastolic BP (mm Hg)\n31.0\n69.0\n76.0\n84.0\n136.0\n77.0\n11.7\n3585\n346\n    PULSE1\n1st Pulse (beats/min)\n38.0\n62.0\n69.0\n77.0\n126.0\n70.3\n11.6\n3316\n615\n    PULSE2\n2nd Pulse (beats/min)\n37.0\n63.0\n70.0\n78.0\n121.0\n71.0\n11.6\n3314\n617\n    WBC\nWhite Blood Cell Count (1000 cells/uL)\n2.3\n5.7\n6.9\n8.4\n22.8\n7.3\n2.2\n3755\n176\n    PLATELET\nPlatelets (1000 cells/uL)\n47.0\n210.0\n246.0\n290.0\n818.0\n253.3\n66.4\n3755\n176\n    HSCRP\nHigh-Sensitivity C-Reactive Protein (mg/L)\n0.1\n0.9\n2.1\n4.7\n182.8\n4.3\n8.3\n3664\n267\n    ESTHT\nSelf Estimate: Height (in)\n50.0\n63.0\n66.0\n69.0\n81.0\n66.5\n4.2\n3836\n95\n    ESTWT\nSelf-Estimate: Weight (lb)\n86.0\n150.0\n180.0\n216.0\n578.0\n188.1\n52.2\n3863\n68\n    VIGWK_D\nVigorous Work per week (days)\n0.0\n0.0\n0.0\n2.0\n7.0\n1.2\n2.1\n3926\n5\n    VIGREC_D\nVigorous Recreation per week (days)\n0.0\n0.0\n0.0\n1.0\n7.0\n0.9\n1.7\n3930\n1\n    SEDATE\nSedentary Activity per day (minutes)\n2.0\n180.0\n300.0\n480.0\n1320.0\n332.7\n210.2\n3907\n24\n    PHQ9\nPHQ-9 Depression Screener Score (points)\n0.0\n0.0\n2.0\n5.0\n26.0\n3.3\n4.3\n3718\n213\n    SLPWKDAY\nAverage weekday sleep (hours)\n2.0\n6.5\n7.5\n8.0\n14.0\n7.4\n1.6\n3897\n34\n    SLPWKEND\nAverage weekend sleep (hours)\n2.0\n7.0\n8.0\n9.0\n14.0\n8.2\n1.8\n3897\n34\n    DRINKS\nAverage Alcohol per day (drinks)\n0.0\n1.0\n2.0\n3.0\n15.0\n2.3\n2.2\n3142\n789\n    SMOKE30\nDays smoked cigarette in last 30\n0.0\n0.0\n0.0\n5.0\n30.0\n6.8\n12.1\n3205\n726\n    FLOSS\nDays Flossed in last 7\n0.0\n0.0\n3.0\n7.0\n7.0\n3.5\n2.9\n3927\n4\n    PREGS\nPregnancies\n0.0\n2.0\n3.0\n4.0\n11.0\n3.0\n2.1\n1956\n1975"
  },
  {
    "objectID": "02-nh432cb.html#two-category-10-variables-in-nh432",
    "href": "02-nh432cb.html#two-category-10-variables-in-nh432",
    "title": "2  Codebook for nh432",
    "section": "2.3 Two-Category (1/0) Variables in nh432",
    "text": "2.3 Two-Category (1/0) Variables in nh432\n\nnh_dich_vars <- nh432 |>\n  select(HOSPITAL, MENTALH, EVERALC, INSURNOW, NOINSUR, DR_LOSE,\n         DR_EXER, NOW_LOSE, NOW_EXER, WORK_V, REC_V, EVERPREG,\n         SLPTROUB, CIG100, AWAYWORK, AWAYREST, AWAYBAR) \n\ntemp1 <- nh_dich_vars |>  summarise(across(.cols = everything(), \n                                           ~ sum(.x, na.rm = TRUE)))\n\ntemp2 <- nh_dich_vars |>  summarise(across(.cols = everything(), \n                           ~ round_half_up(100*mean(.x, na.rm = TRUE), 1)))\n\ntemp3 <- nh_dich_vars |>  summarise(across(.cols = everything(), \n                                           ~ n_miss(.x)))\n\nnh_dichotomous_summary <- bind_rows(temp1, temp2, temp3) |>\n  mutate(summary = c(\"Yes\", \"% Yes\", \"# NA\")) |>\n  relocate(summary) |>\n  pivot_longer(!summary, names_to = \"variable\") |>\n  pivot_wider(names_from = summary) |>\n  mutate(Description = \n           c(\"Overnight hospital patient in past 12m?\",\n             \"Seen mental health professional past 12m?\",\n             \"Ever had a drink of alcohol?\",\n             \"Covered by health insurance now?\",\n             \"Time when no insurance in past year?\",\n             \"Doctor said to control/lose weight past 12m?\",\n             \"Doctor said to exercise in past 12m?\",\n             \"Are you now controlling or losing weight?\",\n             \"Are you now increasing exercise?\",\n             \"Vigorous work activity for 10 min/week?\",\n             \"Vigorous recreational activity for 10 min/week?\",\n             \"Ever been pregnant?\",\n             \"Ever told a doctor you had trouble sleeping?\",\n             \"Smoked at least 100 cigarettes in your life?\",\n             \"Last 7 days worked at a job not at home?\",\n             \"Last 7 days spent time in a restaurant?\",\n             \"Last 7 days spent time in a bar?\"))\n\nnh_dichotomous_summary |> \n  gt()\n\n\n\n\n\n  \n  \n    \n      variable\n      Yes\n      % Yes\n      # NA\n      Description\n    \n  \n  \n    HOSPITAL\n343\n8.7\n0\nOvernight hospital patient in past 12m?\n    MENTALH\n475\n12.1\n2\nSeen mental health professional past 12m?\n    EVERALC\n3393\n91.1\n205\nEver had a drink of alcohol?\n    INSURNOW\n3169\n80.8\n10\nCovered by health insurance now?\n    NOINSUR\n1041\n26.6\n16\nTime when no insurance in past year?\n    DR_LOSE\n1189\n30.3\n1\nDoctor said to control/lose weight past 12m?\n    DR_EXER\n1680\n42.7\n1\nDoctor said to exercise in past 12m?\n    NOW_LOSE\n2485\n63.2\n2\nAre you now controlling or losing weight?\n    NOW_EXER\n2369\n60.3\n1\nAre you now increasing exercise?\n    WORK_V\n1116\n28.4\n4\nVigorous work activity for 10 min/week?\n    REC_V\n1056\n26.9\n0\nVigorous recreational activity for 10 min/week?\n    EVERPREG\n1747\n89.2\n1972\nEver been pregnant?\n    SLPTROUB\n1159\n29.5\n3\nEver told a doctor you had trouble sleeping?\n    CIG100\n1576\n40.1\n1\nSmoked at least 100 cigarettes in your life?\n    AWAYWORK\n2663\n67.7\n0\nLast 7 days worked at a job not at home?\n    AWAYREST\n2283\n58.1\n2\nLast 7 days spent time in a restaurant?\n    AWAYBAR\n605\n15.4\n0\nLast 7 days spent time in a bar?"
  },
  {
    "objectID": "02-nh432cb.html#factor-variables-in-nh432",
    "href": "02-nh432cb.html#factor-variables-in-nh432",
    "title": "2  Codebook for nh432",
    "section": "2.4 Factor Variables in nh432",
    "text": "2.4 Factor Variables in nh432\n\nnh_factor_vars <- nh432 |>\n  select(where(~ is.factor(.x)))\n\ntbl_summary(nh_factor_vars,\n            label = c(RACEETH = \"RACEETH: Race/Ethnicity\",\n                      EDUC = \"EDUC: Educational Attainment\",\n                      SROH = \"SROH: Self-reported Overall Health\",\n                      WTGOAL = \"WTGOAL: Like to weigh more/less/the same?\",\n                      DIETQUAL = \"DIETQUAL: How healthy is your diet?\",\n                      FOODSEC = \"FOODSEC: Adult food security (last 12m)\",\n                      PHQ9_CAT = \"PHQ9_CAT: Depression Screen Category\",\n                      DEPRDIFF = \"DEPRDIFF: Difficulty with Depression?\",\n                      SNORE = \"SNORE: How often do you snore?\",\n                      DENTAL = \"DENTAL: Recommendation for Dental Care?\",\n                      SUNSCR = \"SUNSCR: Use sunscreen on very sunny day?\"),\n            missing_text = \"(# NA)\")\n\n\n\n\n\n  \n  \n    \n      Characteristic\n      N = 3,9311\n    \n  \n  \n    RACEETH: Race/Ethnicity\n\n        Non-H White\n1,192 (30%)\n        Non-H Black\n1,049 (27%)\n        Hispanic\n903 (23%)\n        Non-H Asian\n588 (15%)\n        Other Race\n199 (5.1%)\n    EDUC: Educational Attainment\n\n        Less than 9th Grade\n272 (6.9%)\n        9th - 11th Grade\n424 (11%)\n        High School Grad\n850 (22%)\n        Some College / AA\n1,287 (33%)\n        College Grad\n1,097 (28%)\n        (# NA)\n1\n    SEX\n\n        Female\n2,094 (53%)\n        Male\n1,837 (47%)\n    SROH: Self-reported Overall Health\n\n        Excellent\n495 (13%)\n        Very Good\n1,071 (27%)\n        Good\n1,462 (37%)\n        Fair\n765 (19%)\n        Poor\n138 (3.5%)\n    WTGOAL: Like to weigh more/less/the same?\n\n        More\n289 (7.4%)\n        Same\n974 (25%)\n        Less\n2,665 (68%)\n        (# NA)\n3\n    DIETQUAL: How healthy is your diet?\n\n        Excellent\n260 (6.6%)\n        Very Good\n758 (19%)\n        Good\n1,519 (39%)\n        Fair\n1,082 (28%)\n        Poor\n311 (7.9%)\n        (# NA)\n1\n    FOODSEC: Adult food security (last 12m)\n\n        Full\n2,247 (61%)\n        Marginal\n565 (15%)\n        Low\n503 (14%)\n        Very Low\n385 (10%)\n        (# NA)\n231\n    PHQ9_CAT: Depression Screen Category\n\n        minimal\n2,748 (74%)\n        mild\n621 (17%)\n        moderate\n220 (5.9%)\n        moderately severe\n91 (2.4%)\n        severe\n38 (1.0%)\n        (# NA)\n213\n    DEPRDIFF: Difficulty with Depression?\n\n        Not at all\n3,039 (82%)\n        Somewhat\n541 (15%)\n        Very\n93 (2.5%)\n        Extremely\n43 (1.2%)\n        (# NA)\n215\n    SNORE: How often do you snore?\n\n        Never\n855 (23%)\n        Rarely\n959 (26%)\n        Occasionally\n700 (19%)\n        Frequently\n1,198 (32%)\n        (# NA)\n219\n    DENTAL: Recommendation for Dental Care?\n\n        See dentist urgently\n234 (6.0%)\n        See dentist soon\n1,671 (43%)\n        Regular Routine\n2,026 (52%)\n    SUNSCR: Use sunscreen on very sunny day?\n\n        Always\n351 (9.0%)\n        Most of the time\n485 (12%)\n        Sometimes\n831 (21%)\n        Rarely\n662 (17%)\n        Never\n1,583 (40%)\n        (# NA)\n19\n  \n  \n  \n    \n      1 n (%)"
  },
  {
    "objectID": "02-nh432cb.html#detailed-numerical-description-for-nh432",
    "href": "02-nh432cb.html#detailed-numerical-description-for-nh432",
    "title": "2  Codebook for nh432",
    "section": "2.5 Detailed Numerical Description for nh432",
    "text": "2.5 Detailed Numerical Description for nh432\n\ndescribe(nh432)\n\nnh432 \n\n 55  Variables      3931  Observations\n--------------------------------------------------------------------------------\nSEQN \n       n  missing distinct \n    3931        0     3931 \n\nlowest : 109271 109273 109284 109291 109292, highest: 124807 124810 124813 124815 124818\n--------------------------------------------------------------------------------\nAGE \n       n  missing distinct     Info     Mean      Gmd      .05      .10 \n    3931        0       30    0.999    44.79    10.09       31       33 \n     .25      .50      .75      .90      .95 \n      37       45       53       57       58 \n\nlowest : 30 31 32 33 34, highest: 55 56 57 58 59\n--------------------------------------------------------------------------------\nRACEETH \n       n  missing distinct \n    3931        0        5 \n\nlowest : Non-H White Non-H Black Hispanic    Non-H Asian Other Race \nhighest: Non-H White Non-H Black Hispanic    Non-H Asian Other Race \n                                                                      \nValue      Non-H White Non-H Black    Hispanic Non-H Asian  Other Race\nFrequency         1192        1049         903         588         199\nProportion       0.303       0.267       0.230       0.150       0.051\n--------------------------------------------------------------------------------\nEDUC \n       n  missing distinct \n    3930        1        5 \n\nlowest : Less than 9th Grade 9th - 11th Grade    High School Grad    Some College / AA   College Grad       \nhighest: Less than 9th Grade 9th - 11th Grade    High School Grad    Some College / AA   College Grad       \n                                                                      \nValue      Less than 9th Grade    9th - 11th Grade    High School Grad\nFrequency                  272                 424                 850\nProportion               0.069               0.108               0.216\n                                                  \nValue        Some College / AA        College Grad\nFrequency                 1287                1097\nProportion               0.327               0.279\n--------------------------------------------------------------------------------\nSEX \n       n  missing distinct \n    3931        0        2 \n                        \nValue      Female   Male\nFrequency    2094   1837\nProportion  0.533  0.467\n--------------------------------------------------------------------------------\nINSURNOW \n       n  missing distinct     Info      Sum     Mean      Gmd \n    3921       10        2    0.465     3169   0.8082   0.3101 \n\n--------------------------------------------------------------------------------\nNOINSUR \n       n  missing distinct     Info      Sum     Mean      Gmd \n    3915       16        2    0.586     1041   0.2659   0.3905 \n\n--------------------------------------------------------------------------------\nSROH \n       n  missing distinct \n    3931        0        5 \n\nlowest : Excellent Very Good Good      Fair      Poor     \nhighest: Excellent Very Good Good      Fair      Poor     \n                                                            \nValue      Excellent Very Good      Good      Fair      Poor\nFrequency        495      1071      1462       765       138\nProportion     0.126     0.272     0.372     0.195     0.035\n--------------------------------------------------------------------------------\nWEIGHT \n       n  missing distinct     Info     Mean      Gmd      .05      .10 \n    3903       28      969        1    86.31    26.59    54.20    58.82 \n     .25      .50      .75      .90      .95 \n   69.30    82.10    99.10   119.30   131.49 \n\nlowest :  36.9  39.4  39.6  39.8  39.9, highest: 204.4 204.6 210.8 242.6 254.3\n--------------------------------------------------------------------------------\nHEIGHT \n       n  missing distinct     Info     Mean      Gmd      .05      .10 \n    3901       30      484        1    167.4    11.45    152.0    154.8 \n     .25      .50      .75      .90      .95 \n   160.0    166.9    174.7    180.8    184.6 \n\nlowest : 135.3 138.3 139.7 141.4 141.9, highest: 195.8 195.9 196.6 198.3 198.7\n--------------------------------------------------------------------------------\nWAIST \n       n  missing distinct     Info     Mean      Gmd      .05      .10 \n    3782      149      781        1    101.5    19.68     75.9     80.4 \n     .25      .50      .75      .90      .95 \n    89.1     99.2    111.7    125.4    134.5 \n\nlowest :  57.9  62.7  63.2  64.5  64.9, highest: 166.0 167.1 170.8 173.1 178.0\n--------------------------------------------------------------------------------\nSBP \n       n  missing distinct     Info     Mean      Gmd      .05      .10 \n    3585      346      116        1    121.5    18.61       98      102 \n     .25      .50      .75      .90      .95 \n     110      120      131      143      152 \n\nlowest :  69  72  77  79  80, highest: 199 200 211 219 222\n--------------------------------------------------------------------------------\nDBP \n       n  missing distinct     Info     Mean      Gmd      .05      .10 \n    3585      346       81    0.999    77.03    13.01     59.2     63.0 \n     .25      .50      .75      .90      .95 \n    69.0     76.0     84.0     92.0     97.0 \n\nlowest :  31  44  45  46  47, highest: 121 122 126 127 136\n--------------------------------------------------------------------------------\nPULSE1 \n       n  missing distinct     Info     Mean      Gmd      .05      .10 \n    3316      615       76    0.999     70.3    12.92       53       57 \n     .25      .50      .75      .90      .95 \n      62       69       77       86       91 \n\nlowest :  38  40  41  42  44, highest: 114 115 120 121 126\n--------------------------------------------------------------------------------\nPULSE2 \n       n  missing distinct     Info     Mean      Gmd      .05      .10 \n    3314      617       80    0.999    70.96    12.92       54       57 \n     .25      .50      .75      .90      .95 \n      63       70       78       86       91 \n\nlowest :  37  39  40  41  42, highest: 117 118 119 120 121\n--------------------------------------------------------------------------------\nWBC \n       n  missing distinct     Info     Mean      Gmd      .05      .10 \n    3755      176      136        1    7.254    2.387      4.3      4.8 \n     .25      .50      .75      .90      .95 \n     5.7      6.9      8.4     10.1     11.3 \n\nlowest :  2.3  2.5  2.6  2.7  2.8, highest: 17.2 17.4 17.6 20.6 22.8\n--------------------------------------------------------------------------------\nPLATELET \n       n  missing distinct     Info     Mean      Gmd      .05      .10 \n    3755      176      372        1    253.3    72.04      159      179 \n     .25      .50      .75      .90      .95 \n     210      246      290      337      371 \n\nlowest :  47  48  54  57  61, highest: 583 602 638 662 818\n--------------------------------------------------------------------------------\nHSCRP \n       n  missing distinct     Info     Mean      Gmd      .05      .10 \n    3664      267     1065        1    4.326    5.271    0.350    0.470 \n     .25      .50      .75      .90      .95 \n   0.890    2.090    4.740    9.217   13.630 \n\nlowest :   0.11   0.16   0.17   0.18   0.19, highest: 102.94 104.48 109.81 138.81 182.82\n--------------------------------------------------------------------------------\nDR_LOSE \n       n  missing distinct     Info      Sum     Mean      Gmd \n    3930        1        2    0.633     1189   0.3025   0.4221 \n\n--------------------------------------------------------------------------------\nDR_EXER \n       n  missing distinct     Info      Sum     Mean      Gmd \n    3930        1        2    0.734     1680   0.4275   0.4896 \n\n--------------------------------------------------------------------------------\nNOW_LOSE \n       n  missing distinct     Info      Sum     Mean      Gmd \n    3929        2        2    0.697     2485   0.6325    0.465 \n\n--------------------------------------------------------------------------------\nNOW_EXER \n       n  missing distinct     Info      Sum     Mean      Gmd \n    3930        1        2    0.718     2369   0.6028    0.479 \n\n--------------------------------------------------------------------------------\nESTHT \n       n  missing distinct     Info     Mean      Gmd      .05      .10 \n    3836       95       29    0.995    66.46    4.722       60       61 \n     .25      .50      .75      .90      .95 \n      63       66       69       72       74 \n\nlowest : 50 53 54 55 56, highest: 76 77 78 79 81\n--------------------------------------------------------------------------------\nESTWT \n       n  missing distinct     Info     Mean      Gmd      .05      .10 \n    3863       68      255        1    188.1    56.51    120.0    130.0 \n     .25      .50      .75      .90      .95 \n   150.0    180.0    216.0    258.0    281.9 \n\nlowest :  86  88  90  93  95, highest: 416 434 450 457 578\n--------------------------------------------------------------------------------\nWTGOAL \n       n  missing distinct \n    3928        3        3 \n                            \nValue       More  Same  Less\nFrequency    289   974  2665\nProportion 0.074 0.248 0.678\n--------------------------------------------------------------------------------\nDIETQUAL \n       n  missing distinct \n    3930        1        5 \n\nlowest : Excellent Very Good Good      Fair      Poor     \nhighest: Excellent Very Good Good      Fair      Poor     \n                                                            \nValue      Excellent Very Good      Good      Fair      Poor\nFrequency        260       758      1519      1082       311\nProportion     0.066     0.193     0.387     0.275     0.079\n--------------------------------------------------------------------------------\nFOODSEC \n       n  missing distinct \n    3700      231        4 \n                                              \nValue          Full Marginal      Low Very Low\nFrequency      2247      565      503      385\nProportion    0.607    0.153    0.136    0.104\n--------------------------------------------------------------------------------\nWORK_V \n       n  missing distinct     Info      Sum     Mean      Gmd \n    3927        4        2     0.61     1116   0.2842    0.407 \n\n--------------------------------------------------------------------------------\nVIGWK_D \n       n  missing distinct     Info     Mean      Gmd \n    3926        5        8    0.632     1.22    1.904 \n\nlowest : 0 1 2 3 4, highest: 3 4 5 6 7\n                                                          \nValue          0     1     2     3     4     5     6     7\nFrequency   2811    79   127   184   112   352   132   129\nProportion 0.716 0.020 0.032 0.047 0.029 0.090 0.034 0.033\n--------------------------------------------------------------------------------\nREC_V \n       n  missing distinct     Info      Sum     Mean      Gmd \n    3931        0        2    0.589     1056   0.2686    0.393 \n\n--------------------------------------------------------------------------------\nVIGREC_D \n       n  missing distinct     Info     Mean      Gmd \n    3930        1        8    0.608   0.8952    1.436 \n\nlowest : 0 1 2 3 4, highest: 3 4 5 6 7\n                                                          \nValue          0     1     2     3     4     5     6     7\nFrequency   2875   126   211   301   166   154    46    51\nProportion 0.732 0.032 0.054 0.077 0.042 0.039 0.012 0.013\n--------------------------------------------------------------------------------\nSEDATE \n       n  missing distinct     Info     Mean      Gmd      .05      .10 \n    3907       24       44     0.99    332.7    232.2       60      120 \n     .25      .50      .75      .90      .95 \n     180      300      480      600      720 \n\nlowest :    2    3    5    8    9, highest:  960 1020 1080 1200 1320\n--------------------------------------------------------------------------------\nPHQ9 \n       n  missing distinct     Info     Mean      Gmd      .05      .10 \n    3718      213       27    0.958    3.324    4.201        0        0 \n     .25      .50      .75      .90      .95 \n       0        2        5        9       13 \n\nlowest :  0  1  2  3  4, highest: 22 23 24 25 26\n--------------------------------------------------------------------------------\nPHQ9_CAT \n       n  missing distinct \n    3718      213        5 \n\nlowest : minimal           mild              moderate          moderately severe severe           \nhighest: minimal           mild              moderate          moderately severe severe           \n                                                                \nValue                minimal              mild          moderate\nFrequency               2748               621               220\nProportion             0.739             0.167             0.059\n                                              \nValue      moderately severe            severe\nFrequency                 91                38\nProportion             0.024             0.010\n--------------------------------------------------------------------------------\nDEPRDIFF \n       n  missing distinct \n    3716      215        4 \n                                                      \nValue      Not at all   Somewhat       Very  Extremely\nFrequency        3039        541         93         43\nProportion      0.818      0.146      0.025      0.012\n--------------------------------------------------------------------------------\nMENTALH \n       n  missing distinct     Info      Sum     Mean      Gmd \n    3929        2        2    0.319      475   0.1209   0.2126 \n\n--------------------------------------------------------------------------------\nSLPWKDAY \n       n  missing distinct     Info     Mean      Gmd      .05      .10 \n    3897       34       22    0.984    7.359    1.735      5.0      5.5 \n     .25      .50      .75      .90      .95 \n     6.5      7.5      8.0      9.0     10.0 \n\nlowest :  2.0  3.0  3.5  4.0  4.5, highest: 11.0 11.5 12.0 13.0 14.0\n--------------------------------------------------------------------------------\nSLPWKEND \n       n  missing distinct     Info     Mean      Gmd      .05      .10 \n    3897       34       24    0.983    8.231    1.928        5        6 \n     .25      .50      .75      .90      .95 \n       7        8        9       10       11 \n\nlowest :  2.0  3.0  3.5  4.0  4.5, highest: 12.0 12.5 13.0 13.5 14.0\n--------------------------------------------------------------------------------\nSLPTROUB \n       n  missing distinct     Info      Sum     Mean      Gmd \n    3928        3        2    0.624     1159   0.2951   0.4161 \n\n--------------------------------------------------------------------------------\nSNORE \n       n  missing distinct \n    3712      219        4 \n                                                              \nValue             Never       Rarely Occasionally   Frequently\nFrequency           855          959          700         1198\nProportion        0.230        0.258        0.189        0.323\n--------------------------------------------------------------------------------\nHOSPITAL \n       n  missing distinct     Info      Sum     Mean      Gmd \n    3931        0        2    0.239      343  0.08726   0.1593 \n\n--------------------------------------------------------------------------------\nEVERALC \n       n  missing distinct     Info      Sum     Mean      Gmd \n    3726      205        2    0.244     3393   0.9106   0.1628 \n\n--------------------------------------------------------------------------------\nDRINKS \n       n  missing distinct     Info     Mean      Gmd      .05      .10 \n    3142      789       15    0.948    2.345    2.051        0        0 \n     .25      .50      .75      .90      .95 \n       1        2        3        5        6 \n\nlowest :  0  1  2  3  4, highest: 10 11 12 13 15\n                                                                            \nValue          0     1     2     3     4     5     6     7     8     9    10\nFrequency    333   912   903   412   228   123   105    21    37     4    19\nProportion 0.106 0.290 0.287 0.131 0.073 0.039 0.033 0.007 0.012 0.001 0.006\n                                  \nValue         11    12    13    15\nFrequency      1    26     2    16\nProportion 0.000 0.008 0.001 0.005\n--------------------------------------------------------------------------------\nCIG100 \n       n  missing distinct     Info      Sum     Mean      Gmd \n    3930        1        2    0.721     1576    0.401   0.4805 \n\n--------------------------------------------------------------------------------\nSMOKE30 \n       n  missing distinct     Info     Mean      Gmd      .05      .10 \n    3205      726       26    0.594    6.808     10.5        0        0 \n     .25      .50      .75      .90      .95 \n       0        0        5       30       30 \n\nlowest :  0  1  2  3  4, highest: 26 27 28 29 30\n--------------------------------------------------------------------------------\nAWAYWORK \n       n  missing distinct     Info      Sum     Mean      Gmd \n    3931        0        2    0.656     2663   0.6774   0.4371 \n\n--------------------------------------------------------------------------------\nAWAYREST \n       n  missing distinct     Info      Sum     Mean      Gmd \n    3929        2        2     0.73     2283   0.5811    0.487 \n\n--------------------------------------------------------------------------------\nAWAYBAR \n       n  missing distinct     Info      Sum     Mean      Gmd \n    3931        0        2    0.391      605   0.1539   0.2605 \n\n--------------------------------------------------------------------------------\nDENTAL \n       n  missing distinct \n    3931        0        3 \n                                                                         \nValue      See dentist urgently     See dentist soon      Regular Routine\nFrequency                   234                 1671                 2026\nProportion                0.060                0.425                0.515\n--------------------------------------------------------------------------------\nFLOSS \n       n  missing distinct     Info     Mean      Gmd \n    3927        4        8    0.934    3.476    3.248 \n\nlowest : 0 1 2 3 4, highest: 3 4 5 6 7\n                                                          \nValue          0     1     2     3     4     5     6     7\nFrequency   1104   288   395   329   230   179    44  1358\nProportion 0.281 0.073 0.101 0.084 0.059 0.046 0.011 0.346\n--------------------------------------------------------------------------------\nEVERPREG \n       n  missing distinct     Info      Sum     Mean      Gmd \n    1959     1972        2     0.29     1747   0.8918   0.1931 \n\n--------------------------------------------------------------------------------\nPREGS \n       n  missing distinct     Info     Mean      Gmd      .05      .10 \n    1956     1975       12    0.973    3.046     2.24        0        0 \n     .25      .50      .75      .90      .95 \n       2        3        4        6        7 \n\nlowest :  0  1  2  3  4, highest:  7  8  9 10 11\n                                                                            \nValue          0     1     2     3     4     5     6     7     8     9    10\nFrequency    212   205   421   420   297   191    95    58    22    10     9\nProportion 0.108 0.105 0.215 0.215 0.152 0.098 0.049 0.030 0.011 0.005 0.005\n                \nValue         11\nFrequency     16\nProportion 0.008\n--------------------------------------------------------------------------------\nSUNSCR \n       n  missing distinct \n    3912       19        5 \n\nlowest : Always           Most of the time Sometimes        Rarely           Never           \nhighest: Always           Most of the time Sometimes        Rarely           Never           \n                                                                              \nValue                Always Most of the time        Sometimes           Rarely\nFrequency               351              485              831              662\nProportion            0.090            0.124            0.212            0.169\n                           \nValue                 Never\nFrequency              1583\nProportion            0.405\n--------------------------------------------------------------------------------\nWTINTPRP \n       n  missing distinct     Info     Mean      Gmd      .05      .10 \n    3931        0     3677        1    28434    27437     5911     7199 \n     .25      .50      .75      .90      .95 \n   10615    17358    31476    65098    94422 \n\nlowest :   2467.054   2779.464   2833.287   2917.413   2967.271\nhighest: 246249.502 248091.496 264719.137 282883.648 311265.152\n--------------------------------------------------------------------------------\nWTMECPRP \n       n  missing distinct     Info     Mean      Gmd      .05      .10 \n    3931        0     3701        1    30353    29409     6217     7634 \n     .25      .50      .75      .90      .95 \n   11365    18422    33155    68569   102038 \n\nlowest :   2589.175   2782.738   3003.518   3009.532   3016.643\nhighest: 267064.352 268878.570 273958.374 308014.509 321573.519\n--------------------------------------------------------------------------------"
  },
  {
    "objectID": "02-nh432cb.html#missingness-in-nh432",
    "href": "02-nh432cb.html#missingness-in-nh432",
    "title": "2  Codebook for nh432",
    "section": "2.6 Missingness in nh432",
    "text": "2.6 Missingness in nh432\n\nmiss_case_table(nh432) |> gt()\n\n\n\n\n\n  \n  \n    \n      n_miss_in_case\n      n_cases\n      pct_cases\n    \n  \n  \n    0\n907\n23.0730094\n    1\n533\n13.5588909\n    2\n1030\n26.2019842\n    3\n591\n15.0343424\n    4\n307\n7.8097176\n    5\n161\n4.0956500\n    6\n87\n2.2131773\n    7\n106\n2.6965149\n    8\n68\n1.7298397\n    9\n18\n0.4578988\n    10\n20\n0.5087764\n    11\n39\n0.9921140\n    12\n27\n0.6868481\n    13\n14\n0.3561435\n    14\n9\n0.2289494\n    15\n14\n0.3561435\n  \n  \n  \n\n\n\n\n\ngg_miss_var(nh432)\n\n\n\n\n\nmiss_var_summary(nh432) |> gt()\n\n\n\n\n\n  \n  \n    \n      variable\n      n_miss\n      pct_miss\n    \n  \n  \n    PREGS\n1975\n50.24166879\n    EVERPREG\n1972\n50.16535233\n    DRINKS\n789\n20.07122869\n    SMOKE30\n726\n18.46858306\n    PULSE2\n617\n15.69575172\n    PULSE1\n615\n15.64487408\n    SBP\n346\n8.80183160\n    DBP\n346\n8.80183160\n    HSCRP\n267\n6.79216484\n    FOODSEC\n231\n5.87636734\n    SNORE\n219\n5.57110150\n    DEPRDIFF\n215\n5.46934622\n    PHQ9\n213\n5.41846858\n    PHQ9_CAT\n213\n5.41846858\n    EVERALC\n205\n5.21495803\n    WBC\n176\n4.47723226\n    PLATELET\n176\n4.47723226\n    WAIST\n149\n3.79038413\n    ESTHT\n95\n2.41668787\n    ESTWT\n68\n1.72983974\n    SLPWKDAY\n34\n0.86491987\n    SLPWKEND\n34\n0.86491987\n    HEIGHT\n30\n0.76316459\n    WEIGHT\n28\n0.71228695\n    SEDATE\n24\n0.61053167\n    SUNSCR\n19\n0.48333757\n    NOINSUR\n16\n0.40702111\n    INSURNOW\n10\n0.25438820\n    VIGWK_D\n5\n0.12719410\n    WORK_V\n4\n0.10175528\n    FLOSS\n4\n0.10175528\n    WTGOAL\n3\n0.07631646\n    SLPTROUB\n3\n0.07631646\n    NOW_LOSE\n2\n0.05087764\n    MENTALH\n2\n0.05087764\n    AWAYREST\n2\n0.05087764\n    EDUC\n1\n0.02543882\n    DR_LOSE\n1\n0.02543882\n    DR_EXER\n1\n0.02543882\n    NOW_EXER\n1\n0.02543882\n    DIETQUAL\n1\n0.02543882\n    VIGREC_D\n1\n0.02543882\n    CIG100\n1\n0.02543882\n    SEQN\n0\n0.00000000\n    AGE\n0\n0.00000000\n    RACEETH\n0\n0.00000000\n    SEX\n0\n0.00000000\n    SROH\n0\n0.00000000\n    REC_V\n0\n0.00000000\n    HOSPITAL\n0\n0.00000000\n    AWAYWORK\n0\n0.00000000\n    AWAYBAR\n0\n0.00000000\n    DENTAL\n0\n0.00000000\n    WTINTPRP\n0\n0.00000000\n    WTMECPRP\n0\n0.00000000\n  \n  \n  \n\n\n\n\n\nmiss_var_table(nh432) |> gt()\n\n\n\n\n\n  \n  \n    \n      n_miss_in_var\n      n_vars\n      pct_vars\n    \n  \n  \n    0\n12\n21.818182\n    1\n7\n12.727273\n    2\n3\n5.454545\n    3\n2\n3.636364\n    4\n2\n3.636364\n    5\n1\n1.818182\n    10\n1\n1.818182\n    16\n1\n1.818182\n    19\n1\n1.818182\n    24\n1\n1.818182\n    28\n1\n1.818182\n    30\n1\n1.818182\n    34\n2\n3.636364\n    68\n1\n1.818182\n    95\n1\n1.818182\n    149\n1\n1.818182\n    176\n2\n3.636364\n    205\n1\n1.818182\n    213\n2\n3.636364\n    215\n1\n1.818182\n    219\n1\n1.818182\n    231\n1\n1.818182\n    267\n1\n1.818182\n    346\n2\n3.636364\n    615\n1\n1.818182\n    617\n1\n1.818182\n    726\n1\n1.818182\n    789\n1\n1.818182\n    1972\n1\n1.818182\n    1975\n1\n1.818182"
  },
  {
    "objectID": "03-431review1.html#r-setup",
    "href": "03-431review1.html#r-setup",
    "title": "3  431 Review: Comparing Means",
    "section": "3.1 R Setup",
    "text": "3.1 R Setup\n\nknitr::opts_chunk$set(comment = NA)\n\nlibrary(janitor) \nlibrary(broom)\nlibrary(car)\nlibrary(glue)\nlibrary(gt)\nlibrary(gtsummary)\nlibrary(lmboot)\nlibrary(MKinfer)\nlibrary(mosaic)\nlibrary(naniar)\nlibrary(patchwork)\nlibrary(rstatix)\n\nlibrary(tidyverse) \n\ntheme_set(theme_bw())"
  },
  {
    "objectID": "03-431review1.html#data-ingest",
    "href": "03-431review1.html#data-ingest",
    "title": "3  431 Review: Comparing Means",
    "section": "3.2 Data Ingest",
    "text": "3.2 Data Ingest\nSince we’ve already got the nh432 file formatted as an R data set, we’ll use that.\n\nnh432 <- read_rds(\"data/nh432.Rds\")"
  },
  {
    "objectID": "03-431review1.html#testing-or-summarizing-normality",
    "href": "03-431review1.html#testing-or-summarizing-normality",
    "title": "3  431 Review: Comparing Means",
    "section": "3.3 Testing or Summarizing Normality?",
    "text": "3.3 Testing or Summarizing Normality?\nAs we’ll see, the two most useful strategies for dealing with problematic non-Normality when comparing means are (1) transformation of the outcome to make the assumption of Normality more tenable, and (2) alternate inference approaches (for example, using a bootstrap or rank-based procedure instead of a t test.)\nWhile it is possible to obtain numerical summaries of deviations from Normality, perhaps a measure of skewness (asymmetry) or kurtosis (heavy-tailed behavior), in practical work, I never use such summaries to overrule my assessment of the plots. It’s critical instead to focus on the pictures of a distribution, most especially Normal Q-Q plots.\nPerhaps the simplest skewness summary is \\(skew_1\\) = (mean-median)/(standard deviation), where values below -0.2 are meant to indicate (meaningful) left skew, and values above +0.2 indicate (meaningful) right skew. Unfortunately, this approach works poorly with many distributions (for example, multimodal distributions) and so do many other (more sophisticated) measures1.\nIt is also possible to develop hypothesis tests of whether a particular batch of data follows a Normal distribution, for example, the Kolmogorov-Smirnov test2, or the Shapiro-Wilk test3, but again, I find these to be without value in practical work and cannot recommend their use."
  },
  {
    "objectID": "03-431review1.html#comparing-two-means-using-paired-samples",
    "href": "03-431review1.html#comparing-two-means-using-paired-samples",
    "title": "3  431 Review: Comparing Means",
    "section": "3.4 Comparing Two Means using Paired Samples",
    "text": "3.4 Comparing Two Means using Paired Samples\nNow, we’ll demonstrate some approaches to comparing two means coming from paired samples. This will include:\n\na paired t test (one-sample t test on the paired differences), which we can obtain from a linear model, or from t.test()\n\nThese procedures based on the t distribution for paired samples require that the distribution of the sample paired differences is well-approximated by a Normal model. As an alternative without that requirement, we’ll focus primarily on a bootstrap comparison (not assuming Normality) from boot.t.test(), which comes from the MKinfer package. It is also possible to generate rank-based inference, such as using the Wilcoxon signed rank approach, but this introduces the major weakness of not estimating the population mean (or even the population median.)\nWe’ll assume a Missing Completely at Random (MCAR) mechanism for missing data, so that a complete case analysis makes sense, and we’ll also use functions from the broom package to tidy our output, and from the gt package to help present it in an attractive table."
  },
  {
    "objectID": "03-431review1.html#comparing-pulse1-to-pulse2",
    "href": "03-431review1.html#comparing-pulse1-to-pulse2",
    "title": "3  431 Review: Comparing Means",
    "section": "3.5 Comparing PULSE1 to PULSE2",
    "text": "3.5 Comparing PULSE1 to PULSE2\nWe have two measurements of pulse rate (in beats per minute) in nh432 for each participant. Let’s compare the two for all participants with two PULSE readings. Since we have a value of PULSE1 and PULSE2 for each participant, it makes sense to treat these as paired samples, and study the paired differences in pulse rate.\n\ndat1 <- nh432 |> select(SEQN, PULSE1, PULSE2) |>\n  drop_na() |>\n  mutate(PULSEDIFF = PULSE2 - PULSE1)\n\nsummary(dat1 |> select(-SEQN))\n\n     PULSE1          PULSE2         PULSEDIFF       \n Min.   : 38.0   Min.   : 37.00   Min.   :-22.0000  \n 1st Qu.: 62.0   1st Qu.: 63.00   1st Qu.: -1.0000  \n Median : 69.0   Median : 70.00   Median :  1.0000  \n Mean   : 70.3   Mean   : 70.96   Mean   :  0.6533  \n 3rd Qu.: 77.0   3rd Qu.: 78.00   3rd Qu.:  2.0000  \n Max.   :126.0   Max.   :121.00   Max.   : 26.0000  \n\ndf_stats(~ PULSE1 + PULSE2 + PULSEDIFF, data = dat1) |> \n  mutate(across(.cols = -c(response, n, missing), num, digits = 2)) |>\n  gt()\n\n\n\n\n\n  \n  \n    \n      response\n      min\n      Q1\n      median\n      Q3\n      max\n      mean\n      sd\n      n\n      missing\n    \n  \n  \n    PULSE1\n38.00\n62.00\n69.00\n77.00\n126.00\n70.30\n11.61\n3314\n0\n    PULSE2\n37.00\n63.00\n70.00\n78.00\n121.00\n70.96\n11.57\n3314\n0\n    PULSEDIFF\n-22.00\n-1.00\n1.00\n2.00\n26.00\n0.65\n3.43\n3314\n0\n  \n  \n  \n\n\n\n\n\n3.5.1 Distribution of Paired Differences\n\np1 <- ggplot(dat1, aes(sample = PULSEDIFF)) +\n  geom_qq(col = \"dodgerblue\") + geom_qq_line(col = \"navy\") + \n  theme(aspect.ratio = 1) + \n  labs(title = \"Normal Q-Q plot\")\n\np2 <- ggplot(dat1, aes(x = PULSEDIFF)) +\n  geom_histogram(aes(y = after_stat(density)), \n                 bins = 25, fill = \"dodgerblue\", col = \"white\") +\n  stat_function(fun = dnorm, \n                args = list(mean = mean(dat1$PULSEDIFF), \n                            sd = sd(dat1$PULSEDIFF)),\n                col = \"navy\", linewidth = 1.5) +\n  labs(title = \"Histogram and Normal Density\")\n\np3 <- ggplot(dat1, aes(x = PULSEDIFF, y = \"\")) +\n  geom_boxplot(fill = \"dodgerblue\", outlier.color = \"dodgerblue\") + \n  stat_summary(fun = \"mean\", geom = \"point\", \n               shape = 23, size = 3, fill = \"white\") +\n  labs(title = \"Boxplot with mean\", y = \"\")\n\np1 + (p2 / p3 + plot_layout(heights = c(4,1))) +\n  plot_annotation(title = \"Pulse 2 - Pulse 1 difference in nh432\",\n                  subtitle = glue(nrow(dat1), \" NHANES Participants ages 30-59 in nh432 data\"))\n\n\n\n\nThese data appear to come from a distribution that is essentially symmetric, but extremely heavy-tailed, with many outlier candidates on both the low and high end of the distribution. It seems unwise to assume a Normal distribution for these differences in pulse rate.\n\n\n3.5.2 Using t.test to obtain a 90% CI for the mean pulse difference\nNote that I use 90% as my confidence level here, mostly to make sure that we don’t always simply default to 95% without engaging our brains.\n\ntt1 <- t.test(dat1$PULSEDIFF)\n\ntt1\n\n\n    One Sample t-test\n\ndata:  dat1$PULSEDIFF\nt = 10.98, df = 3313, p-value < 2.2e-16\nalternative hypothesis: true mean is not equal to 0\n95 percent confidence interval:\n 0.5366358 0.7699424\nsample estimates:\nmean of x \n0.6532891 \n\ntidy(tt1, conf.int = TRUE, conf = 0.90) |>\n  select(estimate, low.90 = conf.low, hi.90 = conf.high, \n         \"Paired t\" = statistic, p.value, method) |>\n  mutate(across(.cols = -method, num, digits = 3)) |>\n  gt() |>\n  tab_header(title = \"Pulse 2 - Pulse 1 Difference Estimate with 90% T-based CI\",\n             subtitle = glue(nrow(dat1), \" NHANES Participants ages 30-59 in nh432 data\"))\n\n\n\n\n\n  \n    \n      Pulse 2 - Pulse 1 Difference Estimate with 90% T-based CI\n    \n    \n      3314 NHANES Participants ages 30-59 in nh432 data\n    \n  \n  \n    \n      estimate\n      low.90\n      hi.90\n      Paired t\n      p.value\n      method\n    \n  \n  \n    0.653\n0.537\n0.770\n10.980\n0.000\nOne Sample t-test\n  \n  \n  \n\n\n\n\n\n\n3.5.3 Using linear regression to obtain a 90% CI for the mean pulse difference\nA linear regression model predicting the paired differences with an intercept alone produces the same result as the paired t test.\n\nlm1 <- lm(PULSEDIFF ~ 1, data = dat1)\n\nsummary(lm1)\n\n\nCall:\nlm(formula = PULSEDIFF ~ 1, data = dat1)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-22.6533  -1.6533   0.3467   1.3467  25.3467 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)   0.6533     0.0595   10.98   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.425 on 3313 degrees of freedom\n\ntidy(lm1, conf.int = TRUE, conf = 0.90) |>\n  mutate(method = c(\"Linear Model\")) |>\n  select(estimate, low.90 = conf.low, hi.90 = conf.high, \n         \"Paired t\" = statistic, p.value, method) |>\n  mutate(across(.cols = -method, num, digits = 3)) |>\n  gt() |>\n  tab_header(title = \"Pulse 2 - Pulse 1 Difference Estimate with 90% T-based CI\",\n             subtitle = glue(nrow(dat1), \" NHANES Participants ages 30-59 in nh432 data\"))\n\n\n\n\n\n  \n    \n      Pulse 2 - Pulse 1 Difference Estimate with 90% T-based CI\n    \n    \n      3314 NHANES Participants ages 30-59 in nh432 data\n    \n  \n  \n    \n      estimate\n      low.90\n      hi.90\n      Paired t\n      p.value\n      method\n    \n  \n  \n    0.653\n0.555\n0.751\n10.980\n0.000\nLinear Model\n  \n  \n  \n\n\n\n\n\n\n3.5.4 Using the bootstrap to obtain a 90% CI for the mean pulse difference\nThis is a better choice than the t test if the distribution of the paired differences veer far away from a Normal distribution, but you are still interested in making inferences about the population mean. This is a different approach to obtaining a bootstrap than I have used in the past, but I prefer it because it works well with the tidy() function in the broom package.\n\nset.seed(4321)\nbs1 <- boot.t.test(dat1$PULSEDIFF, conf.level = 0.90, \n                           boot = TRUE, R = 999)\n\nbs1\n\n\n    Bootstrap One Sample t-test\n\ndata:  dat1$PULSEDIFF\nbootstrap p-value < 2.2e-16 \nbootstrap mean of x (SE) = 0.6554904 (0.05954107) \n90 percent bootstrap percentile confidence interval:\n 0.5554617 0.7526554\n\nResults without bootstrap:\nt = 10.98, df = 3313, p-value < 2.2e-16\nalternative hypothesis: true mean is not equal to 0\n90 percent confidence interval:\n 0.5553989 0.7511792\nsample estimates:\nmean of x \n0.6532891 \n\ntidy(bs1, conf.int = TRUE, conf = 0.90) |>\n  select(estimate, low.90 = conf.low, hi.90 = conf.high, \n         \"Bootstrap t\" = statistic, p.value, method) |>\n  mutate(across(.cols = -method, num, digits = 3)) |>\n  gt() |>\n  tab_header(title = \"Pulse 2 - Pulse 1 Difference Estimate with 90% Bootstrap CI\",\n             subtitle = glue(nrow(dat1), \" NHANES Participants ages 30-59 in nh432 data\"))\n\n\n\n\n\n  \n    \n      Pulse 2 - Pulse 1 Difference Estimate with 90% Bootstrap CI\n    \n    \n      3314 NHANES Participants ages 30-59 in nh432 data\n    \n  \n  \n    \n      estimate\n      low.90\n      hi.90\n      Bootstrap t\n      p.value\n      method\n    \n  \n  \n    0.653\n0.555\n0.751\n10.980\n0.000\nBootstrap One Sample t-test\n  \n  \n  \n\n\n\n\nGiven our large sample size, it is perhaps not overly surprising that even a small difference in mean pulse rate (0.653 beats per minute) turns out to have a 90% confidence interval well above the value (0) that would occur if there were no difference at all between the groups.\n\n\n3.5.5 Wilcoxon signed rank approach to comparing pulse rates\nWe can obtain a 90% confidence interval for the pseudo-median of our paired differences in pulse rate with the Wilcoxon signed rank approach.\n\nwt1 <- wilcox.test(dat1$PULSEDIFF, conf.int = TRUE, conf.level = 0.90)\n\nwt1\n\n\n    Wilcoxon signed rank test with continuity correction\n\ndata:  dat1$PULSEDIFF\nV = 2449203, p-value < 2.2e-16\nalternative hypothesis: true location is not equal to 0\n90 percent confidence interval:\n 0.5000466 0.9999290\nsample estimates:\n(pseudo)median \n     0.9999809 \n\n\nBut this is of limited value, even though it doesn’t assume Normality of the distribution of paired differences, because the summary statistic is a pseudo-median4, which isn’t straightforward to interpret, unless the true distribution of the paired differences is symmetric, in which case the pseudo-median and the median have the same value.\nLet’s consider another example using two paired samples to compare means, this time with a somewhat smaller sample size."
  },
  {
    "objectID": "03-431review1.html#comparing-weight-to-estwt",
    "href": "03-431review1.html#comparing-weight-to-estwt",
    "title": "3  431 Review: Comparing Means",
    "section": "3.6 Comparing WEIGHT to ESTWT",
    "text": "3.6 Comparing WEIGHT to ESTWT\nWe have two assessments of each participant’s weight in nh432: their WEIGHT (as measured using a scale, in kilograms) and their ESTWT (self-reported weight via questionnaire, in pounds.) First, let’s create a data set containing those values, and converting pounds to kilograms for the ESTWT results so that we can compare the two assessments fairly. To shrink the sample size a bit, let’s only look at people whose age is 43, and who describe their overall health as either Good or Fair.\n\ndat2 <- nh432 |> select(SEQN, AGE, SROH, WEIGHT, ESTWT) |>\n  filter(AGE == 43, SROH %in% c(\"Good\", \"Fair\")) |>\n  drop_na() |>\n  mutate(ESTWTKG = ESTWT*0.45359,\n         WTDIFF = WEIGHT - ESTWTKG)\n\nglimpse(dat2)\n\nRows: 70\nColumns: 7\n$ SEQN    <chr> \"109342\", \"109602\", \"109805\", \"110286\", \"110645\", \"111149\", \"1…\n$ AGE     <dbl> 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43…\n$ SROH    <fct> Good, Good, Good, Fair, Good, Good, Good, Good, Good, Fair, Go…\n$ WEIGHT  <dbl> 92.1, 76.5, 133.0, 86.8, 119.3, 74.1, 75.8, 106.8, 102.1, 77.0…\n$ ESTWT   <dbl> 200, 167, 260, 198, 230, 145, 167, 240, 223, 172, 150, 265, 22…\n$ ESTWTKG <dbl> 90.71800, 75.74953, 117.93340, 89.81082, 104.32570, 65.77055, …\n$ WTDIFF  <dbl> 1.38200, 0.75047, 15.06660, -3.01082, 14.97430, 8.32945, 0.050…\n\ndf_stats(~ WEIGHT + ESTWTKG + WTDIFF, data = dat2) |> \n  mutate(across(.cols = -c(response, n, missing), num, digits = 3)) |>\n  gt()\n\n\n\n\n\n  \n  \n    \n      response\n      min\n      Q1\n      median\n      Q3\n      max\n      mean\n      sd\n      n\n      missing\n    \n  \n  \n    WEIGHT\n36.900\n74.525\n89.400\n106.175\n204.600\n93.040\n29.211\n70\n0\n    ESTWTKG\n45.359\n74.842\n89.130\n103.759\n204.115\n92.532\n27.869\n70\n0\n    WTDIFF\n-9.871\n-2.256\n-0.028\n1.923\n15.067\n0.508\n4.671\n70\n0\n  \n  \n  \n\n\n\n\n\n3.6.1 Plotting The Paired Difference in Weight\n\np1 <- ggplot(dat2, aes(sample = WTDIFF)) +\n  geom_qq(col = \"seagreen\") + geom_qq_line(col = \"deeppink\") + \n  theme(aspect.ratio = 1) + \n  labs(title = \"Normal Q-Q plot\")\n\np2 <- ggplot(dat2, aes(x = WTDIFF)) +\n  geom_histogram(aes(y = after_stat(density)), \n                   bins = 15, fill = \"seagreen\", col = \"white\") +\n  stat_function(fun = dnorm, \n                args = list(mean = mean(dat2$WTDIFF), \n                            sd = sd(dat2$WTDIFF)),\n                col = \"deeppink\", linewidth = 1.5) +\n  labs(title = \"Histogram and Normal Density\")\n\np3 <- ggplot(dat2, aes(x = WTDIFF, y = \"\")) +\n  geom_boxplot(fill = \"seagreen\", outlier.color = \"seagreen\") + \n  stat_summary(fun = \"mean\", geom = \"point\", \n               shape = 23, size = 3, fill = \"white\") +\n  labs(title = \"Boxplot with mean\", y = \"\")\n\np1 + (p2 / p3 + plot_layout(heights = c(4,1))) +\n  plot_annotation(title = \"Measured - Self-reported Weight (in kilograms)\",\n                  subtitle = glue(nrow(dat2), \" participants in Good or Fair Health aged 43 in nh432\"))\n\n\n\n\nAs we saw with the differences in pulse rate, the differences in weight for this sample appear to come from a distribution that might be symmetric, but that still has several outlier candidates, especially on the high end of the distribution. We may want to consider whether the assumption of a t-based confidence interval is reasonable here, and whether we might be better off using a bootstrap approach.\n\n\n3.6.2 t.test 90% CI for the mean weight difference\n\ntt2 <- t.test(dat2$WTDIFF)\n\ntidy(tt2, conf.int = TRUE, conf = 0.90) |>\n  select(estimate, low.90 = conf.low, hi.90 = conf.high, \n         \"Paired t\" = statistic, p.value, method) |>\n  mutate(across(.cols = -method, num, digits = 3)) |>\n  gt() |>\n  tab_header(title = \"Measured - Self-reported Weight with 90% T-based CI\",\n             subtitle = glue(nrow(dat2), \" NHANES Participants aged 43 in Good or Fair Health in nh432 data\"))\n\n\n\n\n\n  \n    \n      Measured - Self-reported Weight with 90% T-based CI\n    \n    \n      70 NHANES Participants aged 43 in Good or Fair Health in nh432 data\n    \n  \n  \n    \n      estimate\n      low.90\n      hi.90\n      Paired t\n      p.value\n      method\n    \n  \n  \n    0.508\n-0.606\n1.621\n0.909\n0.366\nOne Sample t-test\n  \n  \n  \n\n\n\n\n\n\n3.6.3 Linear Regression: 90% CI for mean weight difference\n\nlm2 <- lm(WTDIFF ~ 1, data = dat2)\n\ntidy(lm2, conf.int = TRUE, conf = 0.90) |>\n  mutate(method = c(\"Linear Model\")) |>\n  select(estimate, low.90 = conf.low, hi.90 = conf.high, \n         \"Paired t\" = statistic, p.value, method) |>\n  mutate(across(.cols = -method, num, digits = 3)) |>\n  gt() |>\n  tab_header(title = \"Measured - Self-reported Weight with 90% T-based CI\",\n             subtitle = glue(nrow(dat2), \" NHANES Participants aged 43 in Good or Fair Health in nh432 data\"))\n\n\n\n\n\n  \n    \n      Measured - Self-reported Weight with 90% T-based CI\n    \n    \n      70 NHANES Participants aged 43 in Good or Fair Health in nh432 data\n    \n  \n  \n    \n      estimate\n      low.90\n      hi.90\n      Paired t\n      p.value\n      method\n    \n  \n  \n    0.508\n-0.423\n1.438\n0.909\n0.366\nLinear Model\n  \n  \n  \n\n\n\n\n\n\n3.6.4 Bootstrap 90% CI for the mean weight difference\n\nset.seed(4322)\nbs2 <- boot.t.test(dat2$WTDIFF, conf.level = 0.90, \n                           boot = TRUE, R = 999)\n\ntidy(bs2, conf.int = TRUE, conf = 0.90) |>\n  select(estimate, low.90 = conf.low, hi.90 = conf.high, \n         \"Bootstrap t\" = statistic, p.value, method) |>\n  mutate(across(.cols = -method, num, digits = 3)) |>\n  gt() |>\n  tab_header(title = \"Measured - Self-reported Weight\",\n             subtitle = \"with 90% Bootstrap CI\") |>\n  tab_footnote(footnote = glue(nrow(dat1), \" NHANES Participants age 43 in Good or Fair Health in nh432 data\"))\n\n\n\n\n\n  \n    \n      Measured - Self-reported Weight\n    \n    \n      with 90% Bootstrap CI\n    \n  \n  \n    \n      estimate\n      low.90\n      hi.90\n      Bootstrap t\n      p.value\n      method\n    \n  \n  \n    0.508\n-0.423\n1.438\n0.909\n0.366\nBootstrap One Sample t-test\n  \n  \n  \n    \n       3314 NHANES Participants age 43 in Good or Fair Health in nh432 data\n    \n  \n\n\n\n\nIn light of the clear issue with outliers in the plots of the weight differences, I think I would choose the bootstrap confidence interval, which clearly includes both negative and positive values as plausible estimates of the population mean difference.\n\n\n3.6.5 Wilcoxon signed rank approach to comparing weight estimates\nWe can obtain a 90% confidence interval for the pseudo-median of our paired differences in weight with the Wilcoxon signed rank approach.\n\nwt2 <- wilcox.test(dat2$WTDIFF, conf.int = TRUE, conf.level = 0.90)\n\nwt2\n\n\n    Wilcoxon signed rank test with continuity correction\n\ndata:  dat2$WTDIFF\nV = 1262, p-value = 0.9115\nalternative hypothesis: true location is not equal to 0\n90 percent confidence interval:\n -0.6347756  0.7941699\nsample estimates:\n(pseudo)median \n    0.06155979 \n\n\nBut this is of limited value, even though it doesn’t assume Normality of the distribution of paired differences, because the summary statistic is a pseudo-median5, which isn’t straightforward to interpret, unless the true distribution of the paired differences is symmetric, in which case the pseudo-median and the median have the same value."
  },
  {
    "objectID": "03-431review1.html#comparing-two-means-using-independent-samples",
    "href": "03-431review1.html#comparing-two-means-using-independent-samples",
    "title": "3  431 Review: Comparing Means",
    "section": "3.7 Comparing Two Means using Independent Samples",
    "text": "3.7 Comparing Two Means using Independent Samples\nNow, we’ll demonstrate some approaches to comparing two means coming from independent samples. This will include:\n\na pooled t test (t test assuming equal population variances), which we can obtain from a linear model, or from t.test()\na Welch t test (t test not assuming equal population variances), from t.test()\n\nEach of these t tests requires the distribution of each of our two independent samples to be well-approximated by a Normal model. As an alternative without that requirement, we’ll focus on a bootstrap comparison (not assuming equal variances or Normality) from boot.t.test() (again from the MKinfer package.) Once more, it is also possible to generate rank-based inference, such as using the Wilcoxon-Mann-Whitney rank sum approach, but again this does not provide us with estimates of either the difference in population means or medians, which limits its utility."
  },
  {
    "objectID": "03-431review1.html#comparing-white-blood-cell-count-by-hospitalization-status",
    "href": "03-431review1.html#comparing-white-blood-cell-count-by-hospitalization-status",
    "title": "3  431 Review: Comparing Means",
    "section": "3.8 Comparing White Blood Cell Count by Hospitalization Status",
    "text": "3.8 Comparing White Blood Cell Count by Hospitalization Status\nNow, we’ll use independent samples to compare subjects who were hospitalized in the past year to those who were not, in terms of their white blood cell count. The normal range of WBCs in the blood is 4.5 to 11 on the scale (1000 cells per microliter) our data is available.\n\n3.8.1 Exploring the Data\n\ndat3 <- nh432 |>\n  select(SEQN, HOSPITAL, WBC) |>\n  drop_na()\n\nggplot(dat3, aes(x = factor(HOSPITAL), y = WBC)) +\n  geom_violin(aes(fill = factor(HOSPITAL))) +\n  geom_boxplot(width = 0.3, notch = TRUE) +\n  stat_summary(aes(fill = factor(HOSPITAL)), fun = \"mean\", geom = \"point\", \n               shape = 23, size = 3) +\n  guides(fill = \"none\", col = \"none\") +\n  scale_fill_viridis_d(option = \"cividis\", alpha = 0.3) +\n  coord_flip() +\n  labs(x = \"Hospitalized in Past Year? (0 - No, 1 = Yes)\",\n       y = \"White blood cell count (1000 cells / uL)\",\n       title = \"White Blood Cell Count by Hospitalization Status\",\n       subtitle = glue(nrow(dat3), \" NHANES participants in nh432\"))\n\n\n\n\nEach of these distributions shows some signs of right skew, or at least more than a few outlier candidates on the upper end of the white blood cell count’s distribution, according to the boxplot. A pair of Normal Q-Q plots should help clarify issues for us.\n\nggplot(dat3, aes(sample = WBC)) +\n  geom_qq(aes(col = factor(HOSPITAL))) + geom_qq_line(col = \"red\") + \n  facet_wrap(~ HOSPITAL, labeller = \"label_both\") +\n  guides(col = \"none\") +\n  scale_color_viridis_d(option = \"cividis\", end = 0.5) +\n  labs(x = \"Expectation Under Standard Normal Distribution\",\n       y = \"Observed White Blood Cell Count (1000 cells/uL)\",\n       title = \"Normal Q-Q plots of White Blood Cell Count\",\n       subtitle = \"By Hospitalization Status in the Past Year\",\n       caption = glue(nrow(dat3), \" NHANES participants in nh432\"))\n\n\n\n\nIt seems unreasonable to assume that each of these samples comes from a distribution that is well-approximated by the Normal. There’s just too much skew here. Here are some key numerical summaries of the data in each sample.\n\nfavstats(WBC ~ HOSPITAL, data = dat3) |>\n  mutate(across(.cols = c(mean, sd), num, digits = 3)) |>\n  gt()\n\n\n\n\n\n  \n  \n    \n      HOSPITAL\n      min\n      Q1\n      median\n      Q3\n      max\n      mean\n      sd\n      n\n      missing\n    \n  \n  \n    0\n2.3\n5.7\n7.0\n8.4\n22.8\n7.246\n2.170\n3430\n0\n    1\n2.5\n5.6\n6.9\n8.7\n17.1\n7.332\n2.392\n325\n0\n  \n  \n  \n\n\n\n\n\n\n3.8.2 Pooled t test (assumes equal variances) via linear model\nThe pooled t test for comparison of two population means using independent samples assumes:\n\nthat the WBC (outcome) in each of the two HOSPITAL (exposure) groups follows a Normal distribution, and\nthat the population variances are equal in the two groups\n\nThe “equal population variances” assumption can be relaxed and a pooled t test used if we have a balanced design, with the same number of subjects in each exposure group.\nIn our setting, we shouldn’t be particularly comfortable with the assumption of Normality, as mentioned above. Were we able to get past that, though, we can see that the two distributions have fairly similar sample variances (remember this is just the square of the standard deviation.) The sample sizes are wildly different, with many more non-hospitalized subjects than hospitalized ones.\nFor completeness, though, we’ll start by running the pooled t test.\n\nlm3 <- lm(WBC ~ HOSPITAL, data = dat3)\n\nsummary(lm3)\n\n\nCall:\nlm(formula = WBC ~ HOSPITAL, data = dat3)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-4.9463 -1.5463 -0.3463  1.1537 15.5537 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  7.24633    0.03740 193.760   <2e-16 ***\nHOSPITAL     0.08567    0.12712   0.674      0.5    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.19 on 3753 degrees of freedom\nMultiple R-squared:  0.000121,  Adjusted R-squared:  -0.0001454 \nF-statistic: 0.4542 on 1 and 3753 DF,  p-value: 0.5004\n\nconfint(lm3, level = 0.90)\n\n                   5 %      95 %\n(Intercept)  7.1847964 7.3078567\nHOSPITAL    -0.1234734 0.2948204\n\ntidy(lm3, conf.int = TRUE, conf.level = 0.90) |>\n  filter(term == \"HOSPITAL\") |>\n  mutate(method = \"Pooled t\") |>\n  select(estimate, low.90 = conf.low, hi.90 = conf.high, \n         \"Pooled t\" = statistic, p.value, method) |>\n  mutate(across(.cols = -method, num, digits = 3)) |>\n  gt() |>\n  tab_header(title = \"White Blood Cell Count (Hospitalized - Non-Hospitalized)\",\n             subtitle = \"with 90% Pooled t-based CI via Linear Model\") |>\n  tab_footnote(footnote = glue(nrow(dat3), \" NHANES Participants ages 30-59 in nh432 data\"))\n\n\n\n\n\n  \n    \n      White Blood Cell Count (Hospitalized - Non-Hospitalized)\n    \n    \n      with 90% Pooled t-based CI via Linear Model\n    \n  \n  \n    \n      estimate\n      low.90\n      hi.90\n      Pooled t\n      p.value\n      method\n    \n  \n  \n    0.086\n-0.123\n0.295\n0.674\n0.500\nPooled t\n  \n  \n  \n    \n       3755 NHANES Participants ages 30-59 in nh432 data\n    \n  \n\n\n\n\n\n\n3.8.3 Pooled t test (assumes equal variances) via t.test\nNote that this approach estimates the difference with Not Hospitalized - Hospitalized, as opposed to the approach used in the linear model. Be careful to check the sample estimates provided in your output against the original summary of the sample data to avoid making a mistake.\n\ntt3p <- t.test(WBC ~ HOSPITAL, data = dat3, var.equal = TRUE)\n\ntt3p\n\n\n    Two Sample t-test\n\ndata:  WBC by HOSPITAL\nt = -0.67395, df = 3753, p-value = 0.5004\nalternative hypothesis: true difference in means between group 0 and group 1 is not equal to 0\n95 percent confidence interval:\n -0.3349062  0.1635593\nsample estimates:\nmean in group 0 mean in group 1 \n       7.246327        7.332000 \n\ntidy(tt3p, conf.int = TRUE, conf.level = 0.90) |>\n  select(estimate, low.90 = conf.low, hi.90 = conf.high, \n         \"Pooled t\" = statistic, p.value, method) |>\n  mutate(across(.cols = -method, num, digits = 3)) |>\n  gt() |>\n  tab_header(title = \"White Blood Cell Count (Non-Hospitalized minus Hospitalized)\",\n             subtitle = \"with 90% Pooled t-based Confidence Interval\") |>\n  tab_footnote(footnote = glue(nrow(dat3), \" NHANES Participants ages 30-59 in nh432 data\"))\n\n\n\n\n\n  \n    \n      White Blood Cell Count (Non-Hospitalized minus Hospitalized)\n    \n    \n      with 90% Pooled t-based Confidence Interval\n    \n  \n  \n    \n      estimate\n      low.90\n      hi.90\n      Pooled t\n      p.value\n      method\n    \n  \n  \n    -0.086\n-0.335\n0.164\n-0.674\n0.500\nTwo Sample t-test\n  \n  \n  \n    \n       3755 NHANES Participants ages 30-59 in nh432 data\n    \n  \n\n\n\n\nAgain, note that the t.test() approach estimates Non-Hospitalized - Hospitalized (so that the sample mean is negative.)\n\n\n3.8.4 Welch t test (doesn’t assume equal variance) via t.test\nThe Welch t test (which is actually the default t.test in R) assumes that the two groups each follow a Normal distribution, but does not require that those distributions have the same population variance.\n\ntt3w <- t.test(WBC ~ HOSPITAL, data = dat3)\n\ntt3w\n\n\n    Welch Two Sample t-test\n\ndata:  WBC by HOSPITAL\nt = -0.6218, df = 376.28, p-value = 0.5345\nalternative hypothesis: true difference in means between group 0 and group 1 is not equal to 0\n95 percent confidence interval:\n -0.3565964  0.1852494\nsample estimates:\nmean in group 0 mean in group 1 \n       7.246327        7.332000 \n\ntidy(tt3w, conf.int = TRUE, conf.level = 0.90) |>\n  select(estimate, low.90 = conf.low, hi.90 = conf.high, \n         \"Welch t\" = statistic, p.value, method) |>\n  mutate(across(.cols = -method, num, digits = 3)) |>\n  gt() |>\n  tab_header(title = \"White Blood Cell Count (Non-Hospitalized minus Hospitalized)\",\n             subtitle = \"with 90% Welch t-based Confidence Interval\") |>\n  tab_footnote(footnote = glue(nrow(dat3), \" NHANES Participants ages 30-59 in nh432 data\"))\n\n\n\n\n\n  \n    \n      White Blood Cell Count (Non-Hospitalized minus Hospitalized)\n    \n    \n      with 90% Welch t-based Confidence Interval\n    \n  \n  \n    \n      estimate\n      low.90\n      hi.90\n      Welch t\n      p.value\n      method\n    \n  \n  \n    -0.086\n-0.357\n0.185\n-0.622\n0.534\nWelch Two Sample t-test\n  \n  \n  \n    \n       3755 NHANES Participants ages 30-59 in nh432 data\n    \n  \n\n\n\n\n\n\n3.8.5 Bootstrap comparison of WBC by HOSPITAL\nThe bootstrap approach is appealing in part because it neither assumes Normality or equal population variances.\n\nset.seed(4323)\nbs3 <- boot.t.test(WBC ~ HOSPITAL, data = dat3, \n                   R = 999, conf.level = 0.90)\n\nbs3\n\n\n    Bootstrap Welch Two Sample t-test\n\ndata:  WBC by HOSPITAL\nbootstrap p-value = 0.5325 \nbootstrap difference of means (SE) = -0.08734076 (0.1376246) \n90 percent bootstrap percentile confidence interval:\n -0.3137606  0.1414317\n\nResults without bootstrap:\nt = -0.6218, df = 376.28, p-value = 0.5345\nalternative hypothesis: true difference in means is not equal to 0\n90 percent confidence interval:\n -0.3128672  0.1415202\nsample estimates:\nmean in group 0 mean in group 1 \n       7.246327        7.332000 \n\ntidy(bs3, conf.int = TRUE, conf.level = 0.90) |>\n  mutate(diff = estimate1 - estimate2) |>\n  select(est1 = estimate1, est2 = estimate2, diff, \n         low.90 = conf.low, hi.90 = conf.high, \n         p.value, method) |>\n  mutate(across(.cols = -method, num, digits = 3)) |>\n  gt() |>\n  tab_header(title = \"White Blood Cell Count (Non-Hospitalized minus Hospitalized)\",\n             subtitle = \"with 90% Bootstrap Confidence Interval\") |>\n  tab_footnote(footnote = glue(nrow(dat3), \" NHANES Participants ages 30-59 in nh432 data\"))\n\n\n\n\n\n  \n    \n      White Blood Cell Count (Non-Hospitalized minus Hospitalized)\n    \n    \n      with 90% Bootstrap Confidence Interval\n    \n  \n  \n    \n      est1\n      est2\n      diff\n      low.90\n      hi.90\n      p.value\n      method\n    \n  \n  \n    7.246\n7.332\n-0.086\n-0.313\n0.142\n0.534\nBootstrap Welch Two Sample t-test\n  \n  \n  \n    \n       3755 NHANES Participants ages 30-59 in nh432 data\n    \n  \n\n\n\n\nIn any case, though, we come to the same basic conclusion - both positive and negative differences in WBC count are plausible.\nGiven the huge imbalance between the two groups in terms of sample size, and the apparent skew in the distribution of each sample, I would probably be most comfortable with the bootstrap approach here than the t-based intervals.\n\n\n3.8.6 Transforming the WBC Counts\nSince the White Blood Cell counts are far from Normally distributed, and in fact appear to be substantially skewed (asymmetric) we might want to consider a transformation of the data. The Box-Cox approach can be used to suggest potential transformations even in a simple case like this. We can use the boxCox() function from the car package, for example.\n\nm3 <- lm(WBC ~ HOSPITAL, data = dat3)\n\nboxCox(m3)\n\n\n\n\nThe estimated power (\\(\\lambda\\)) shown in the plot is close to 0. The ladder of power transformations looks like this:\n\n\n\n$\nTransformation\nFormula\n\n\n\n\n-2\ninverse square\n\\(1/y^2\\)\n\n\n-1\ninverse\n\\(1/y\\)\n\n\n-0.5\ninverse square root\n\\(1/\\sqrt{y}\\)\n\n\n0\nlogarithm\n\\(log y\\)\n\n\n0.5\nsquare root\n\\(\\sqrt{y}\\)\n\n\n1\nno transformation\ny\n\n\n2\nsquare\n\\(y^2\\)\n\n\n3\ncube\n\\(y^3\\)\n\n\n\nSo in this case, the Box-Cox approach is suggesting we try the logarithm (we use the natural logarithm, with base e, here) of WBC.\nLet’s redraw our Normal Q-Q plots with this transformation applied.\n\nggplot(dat3, aes(sample = log(WBC))) +\n  geom_qq(aes(col = factor(HOSPITAL))) + geom_qq_line(col = \"red\") + \n  facet_wrap(~ HOSPITAL, labeller = \"label_both\") +\n  guides(col = \"none\") +\n  scale_color_viridis_d(option = \"cividis\", end = 0.5) +\n  labs(x = \"Expectation Under Standard Normal Distribution\",\n       y = \"Log of White Blood Cell Count (1000 cells/uL)\",\n       title = \"Normal Q-Q plots of log(WBC)\",\n       subtitle = \"By Hospitalization Status in the Past Year\",\n       caption = glue(nrow(dat3), \" NHANES participants in nh432\"))\n\n\n\n\nThe assumption of Normality now looks much more plausible for each of our samples. So we might try building a 90% confidence interval for the mean of log(WBC), as follows:\n\nfavstats(log(WBC) ~ HOSPITAL, data = dat3) |>\n  mutate(across(.cols = -c(HOSPITAL, n, missing), num, digits = 3)) |>\n  gt() |>\n  tab_header(title = \"log(WBC) by Hospitalization Status\",\n             subtitle = \"NHANES participants in nh432\")\n\n\n\n\n\n  \n    \n      log(WBC) by Hospitalization Status\n    \n    \n      NHANES participants in nh432\n    \n  \n  \n    \n      HOSPITAL\n      min\n      Q1\n      median\n      Q3\n      max\n      mean\n      sd\n      n\n      missing\n    \n  \n  \n    0\n0.833\n1.740\n1.946\n2.128\n3.127\n1.938\n0.293\n3430\n0\n    1\n0.916\n1.723\n1.932\n2.163\n2.839\n1.941\n0.322\n325\n0\n  \n  \n  \n\n\n\n\nWe see that there’s essentially no difference at all in the means of the log(WBC) values across the two levels of hospitalization status.\n\ntt3log <- t.test(log(WBC) ~ HOSPITAL, data = dat3, var.equal = TRUE)\n\ntidy(tt3log, conf.int = TRUE, conf.level = 0.90) |>\n  select(estimate1, estimate2, estimate, conf.low, conf.high, p.value)\n\n# A tibble: 1 × 6\n  estimate1 estimate2 estimate conf.low conf.high p.value\n      <dbl>     <dbl>    <dbl>    <dbl>     <dbl>   <dbl>\n1      1.94      1.94 -0.00320  -0.0369    0.0305   0.852\n\n\nLet’s consider a second example for comparing means from independent samples."
  },
  {
    "objectID": "03-431review1.html#comparing-waist-circumference-by-sleep-trouble",
    "href": "03-431review1.html#comparing-waist-circumference-by-sleep-trouble",
    "title": "3  431 Review: Comparing Means",
    "section": "3.9 Comparing Waist Circumference by Sleep Trouble",
    "text": "3.9 Comparing Waist Circumference by Sleep Trouble\nNow, we’ll restrict ourselves to NHANES participants who rated their overall health as “Fair”, and we’ll compare the mean waist circumference (WAIST, in cm) of people in that group who responded Yes (vs. No) to the question of whether they had told a doctor that they had trouble sleeping (gathered in the SLPTROUB variable.)\n\n3.9.1 Summarizing the Data\n\ndat4 <- nh432 |>\n  select(SEQN, SROH, SLPTROUB, WAIST) |>\n  filter(SROH == \"Fair\") |>\n  drop_na()\n\nggplot(dat4, aes(x = factor(SLPTROUB), y = WAIST)) +\n  geom_violin(aes(fill = factor(SLPTROUB))) +\n  geom_boxplot(width = 0.3, notch = TRUE) +\n  stat_summary(fill = \"red\", fun = \"mean\", geom = \"point\", \n               shape = 23, size = 3) +\n  guides(fill = \"none\", col = \"none\") +\n  scale_fill_brewer(palette = \"Accent\") +\n  labs(x = \"Reported Sleep Trouble to a Doctor? (0 - No, 1 = Yes)\",\n       y = \"Waist circumference (cm)\",\n       title = \"Waist Circumference by Sleep Trouble\",\n       subtitle = glue(nrow(dat4), \" NHANES participants in Fair health in nh432\"))\n\n\n\n\n\nggplot(dat4, aes(sample = WAIST)) +\n  geom_qq(aes(col = factor(SLPTROUB))) + geom_qq_line(col = \"red\") + \n  facet_wrap(~ SLPTROUB, labeller = \"label_both\") +\n  guides(col = \"none\") +\n  scale_color_brewer(palette = \"Accent\") +\n  labs(x = \"Expectation Under Standard Normal Distribution\",\n       y = \"Observed Waist Circumference (in cm)\",\n       title = \"Normal Q-Q plots of Waist Circumference\",\n       subtitle = \"By Reported Sleep Trouble\",\n       caption = glue(nrow(dat4), \" NHANES participants in Fair health in nh432\"))\n\n\n\n\nHere’s a situation where we might be willing to consider a t test, since a Normal distribution is a much better fit for the data in each of our two samples. Let’s look at some brief numerical summaries, too.\n\nfavstats(WAIST ~ SLPTROUB, data = dat4) |>\n  mutate(across(.cols = c(mean, sd), num, digits = 2)) |>\n  gt()\n\n\n\n\n\n  \n  \n    \n      SLPTROUB\n      min\n      Q1\n      median\n      Q3\n      max\n      mean\n      sd\n      n\n      missing\n    \n  \n  \n    0\n65.8\n91.7\n102.1\n117.5\n178\n104.71\n18.35\n425\n0\n    1\n69.6\n97.4\n109.4\n124.0\n166\n110.69\n18.86\n309\n0\n  \n  \n  \n\n\n\n\n\n\n3.9.2 Pooled t test (assumes equal variances) via linear model\nHere’s the pooled t test via linear model.\n\nlm4 <- lm(WAIST ~ SLPTROUB, data = dat4)\n\nsummary(lm4)\n\n\nCall:\nlm(formula = WAIST ~ SLPTROUB, data = dat4)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-41.093 -13.293  -2.293  12.790  73.290 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 104.7099     0.9006  116.27  < 2e-16 ***\nSLPTROUB      5.9830     1.3881    4.31 1.85e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 18.57 on 732 degrees of freedom\nMultiple R-squared:  0.02475,   Adjusted R-squared:  0.02342 \nF-statistic: 18.58 on 1 and 732 DF,  p-value: 1.853e-05\n\nconfint(lm4, level = 0.90)\n\n                   5 %       95 %\n(Intercept) 103.226621 106.193144\nSLPTROUB      3.696944   8.269052\n\ntidy(lm4, conf.int = TRUE, conf.level = 0.90) |>\n  filter(term == \"SLPTROUB\") |>\n  mutate(method = \"Pooled t\") |>\n  select(estimate, low.90 = conf.low, hi.90 = conf.high, \n         \"Pooled t\" = statistic, p.value, method) |>\n  mutate(across(.cols = -method, num, digits = 2)) |>\n  gt() |>\n  tab_header(title = \"Waist Circumference by Sleep Trouble\",\n             subtitle = \"with 90% Pooled t-based CI via Linear Model\") |>\n  tab_footnote(footnote = glue(nrow(dat4), \" NHANES Participants in Fair Health in nh432 data\"))\n\n\n\n\n\n  \n    \n      Waist Circumference by Sleep Trouble\n    \n    \n      with 90% Pooled t-based CI via Linear Model\n    \n  \n  \n    \n      estimate\n      low.90\n      hi.90\n      Pooled t\n      p.value\n      method\n    \n  \n  \n    5.98\n3.70\n8.27\n4.31\n0.00\nPooled t\n  \n  \n  \n    \n       734 NHANES Participants in Fair Health in nh432 data\n    \n  \n\n\n\n\n\n\n3.9.3 Pooled t test (assumes equal variances) via t.test\n\ntt4p <- t.test(WAIST ~ SLPTROUB, data = dat4, var.equal = TRUE)\n\ntt4p\n\n\n    Two Sample t-test\n\ndata:  WAIST by SLPTROUB\nt = -4.3103, df = 732, p-value = 1.853e-05\nalternative hypothesis: true difference in means between group 0 and group 1 is not equal to 0\n95 percent confidence interval:\n -8.708058 -3.257938\nsample estimates:\nmean in group 0 mean in group 1 \n       104.7099        110.6929 \n\ntidy(tt4p, conf.int = TRUE, conf.level = 0.90) |>\n  select(estimate, low.90 = conf.low, hi.90 = conf.high, \n         \"Pooled t\" = statistic, p.value, method) |>\n  mutate(across(.cols = -method, num, digits = 2)) |>\n  gt() |>\n  tab_header(title = \"Waist Circumference by Sleep Trouble\",\n             subtitle = \"with 90% Pooled t-based Confidence Interval\") |>\n  tab_footnote(footnote = glue(nrow(dat4), \" NHANES Participants in Fair Health in nh432 data\"))\n\n\n\n\n\n  \n    \n      Waist Circumference by Sleep Trouble\n    \n    \n      with 90% Pooled t-based Confidence Interval\n    \n  \n  \n    \n      estimate\n      low.90\n      hi.90\n      Pooled t\n      p.value\n      method\n    \n  \n  \n    -5.98\n-8.71\n-3.26\n-4.31\n0.00\nTwo Sample t-test\n  \n  \n  \n    \n       734 NHANES Participants in Fair Health in nh432 data\n    \n  \n\n\n\n\n\n\n3.9.4 Welch t test (doesn’t assume equal variance) via t.test\n\ntt4w <- t.test(WAIST ~ SLPTROUB, data = dat4)\n\ntt4w\n\n\n    Welch Two Sample t-test\n\ndata:  WAIST by SLPTROUB\nt = -4.2919, df = 653.24, p-value = 2.04e-05\nalternative hypothesis: true difference in means between group 0 and group 1 is not equal to 0\n95 percent confidence interval:\n -8.720327 -3.245669\nsample estimates:\nmean in group 0 mean in group 1 \n       104.7099        110.6929 \n\ntidy(tt4w, conf.int = TRUE, conf.level = 0.90) |>\n  select(estimate, low.90 = conf.low, hi.90 = conf.high, \n         \"Welch t\" = statistic, p.value, method) |>\n  mutate(across(.cols = -method, num, digits = 2)) |>\n  gt() |>\n  tab_header(title = \"Waist Circumference by Sleep Trouble\",\n             subtitle = \"with 90% Welch t-based Confidence Interval\") |>\n  tab_footnote(footnote = glue(nrow(dat4), \" NHANES Participants in Fair Health in nh432 data\"))\n\n\n\n\n\n  \n    \n      Waist Circumference by Sleep Trouble\n    \n    \n      with 90% Welch t-based Confidence Interval\n    \n  \n  \n    \n      estimate\n      low.90\n      hi.90\n      Welch t\n      p.value\n      method\n    \n  \n  \n    -5.98\n-8.72\n-3.25\n-4.29\n0.00\nWelch Two Sample t-test\n  \n  \n  \n    \n       734 NHANES Participants in Fair Health in nh432 data\n    \n  \n\n\n\n\n\n\n3.9.5 Bootstrap comparison of WAIST by SLPTROUB\n\nset.seed(4324)\nbs4 <- boot.t.test(WAIST ~ SLPTROUB, data = dat4, \n                   R = 999, conf.level = 0.90)\n\nbs4\n\n\n    Bootstrap Welch Two Sample t-test\n\ndata:  WAIST by SLPTROUB\nbootstrap p-value < 2.2e-16 \nbootstrap difference of means (SE) = -5.991814 (1.389937) \n90 percent bootstrap percentile confidence interval:\n -8.207833 -3.747965\n\nResults without bootstrap:\nt = -4.2919, df = 653.24, p-value = 2.04e-05\nalternative hypothesis: true difference in means is not equal to 0\n90 percent confidence interval:\n -8.279237 -3.686759\nsample estimates:\nmean in group 0 mean in group 1 \n       104.7099        110.6929 \n\ntidy(bs4, conf.int = TRUE, conf.level = 0.90) |>\n  mutate(diff = estimate1 - estimate2) |>\n  select(est1 = estimate1, est2 = estimate2, diff, \n         low.90 = conf.low, hi.90 = conf.high, \n         p.value, method) |>\n  mutate(across(.cols = -method, num, digits = 2)) |>\n  gt() |>\n  tab_header(title = \"Waist Circumference by Sleep Trouble\",\n             subtitle = \"with 90% Bootstrap Confidence Interval\") |>\n  tab_footnote(footnote = glue(nrow(dat4), \" NHANES Participants in Fair Health in nh432 data\"))\n\n\n\n\n\n  \n    \n      Waist Circumference by Sleep Trouble\n    \n    \n      with 90% Bootstrap Confidence Interval\n    \n  \n  \n    \n      est1\n      est2\n      diff\n      low.90\n      hi.90\n      p.value\n      method\n    \n  \n  \n    104.71\n110.69\n-5.98\n-8.28\n-3.69\n0.00\nBootstrap Welch Two Sample t-test\n  \n  \n  \n    \n       734 NHANES Participants in Fair Health in nh432 data\n    \n  \n\n\n\n\n\n\n3.9.6 Wilcoxon-Mann-Whitney Rank Sum Approach\nThe Wilcoxon-Mann-Whitney rank sum approach also allows us (like the bootstrap) to avoid the assumptions of Normality and equal population variances, but at the cost of no longer yielding direct inference about the population mean.\n\nwt4 <- wilcox.test(WAIST ~ SLPTROUB, data = dat4, \n            conf.int = TRUE, conf.level = 0.90, paired = FALSE)\nwt4\n\n\n    Wilcoxon rank sum test with continuity correction\n\ndata:  WAIST by SLPTROUB\nW = 53322, p-value = 1.355e-05\nalternative hypothesis: true location shift is not equal to 0\n90 percent confidence interval:\n -8.599997 -3.900026\nsample estimates:\ndifference in location \n             -6.299963 \n\n\nNote that the estimated “difference in location” here is not the difference in the medians across the two groups, but instead the median of the difference between a sample from the SLPTROUB = Yes group and a sample from the SLPTROUB = No group.\nJust to prove my point, here are the sample median WAIST results in the two SLPTROUB groups. You can see that the difference between these medians does not match the “difference in location” estimate from the Wilcoxon-Mann-Whitney rank sum output.\n\ndat4 |> group_by(SLPTROUB) |> summarise(median(WAIST))\n\n# A tibble: 2 × 2\n  SLPTROUB `median(WAIST)`\n     <dbl>           <dbl>\n1        0            102.\n2        1            109.\n\n\nIn conclusion, the confidence intervals (from any of these approaches) suggest that plausible means of waist circumference are around 3-8 centimeters larger in the “told Dr. about sleep problems” group, which I suppose isn’t especially surprising, at least in terms of its direction."
  },
  {
    "objectID": "03-431review1.html#comparing-3-means-using-independent-samples-systolic-bp-by-weight-goal",
    "href": "03-431review1.html#comparing-3-means-using-independent-samples-systolic-bp-by-weight-goal",
    "title": "3  431 Review: Comparing Means",
    "section": "3.10 Comparing 3 Means using Independent Samples: Systolic BP by Weight Goal",
    "text": "3.10 Comparing 3 Means using Independent Samples: Systolic BP by Weight Goal\nWe’ll compare systolic blood pressure means across the three samples defined by WTGOAL (goal is to weigh more, less or stay about the same), restricting to our participants of Hispanic or Latinx ethnicity in nh432.\n\ndat5 <- nh432 |>\n  select(SEQN, RACEETH, SBP, WTGOAL) |>\n  filter(RACEETH == \"Hispanic\") |>\n  drop_na()\n\n\n3.10.1 Summarizing SBP by WTGOAL\n\nggplot(dat5, aes(x = SBP, y = WTGOAL)) + \n  geom_violin(aes(fill = WTGOAL)) +\n  geom_boxplot(width = 0.3, notch = TRUE) + \n  guides(fill = \"none\") + \n  labs(title = \"Comparing Mean Systolic BP by Weight Goal\",\n       subtitle = glue(\"among \", nrow(dat5), \" Hispanic participants in nh432\"),\n    x = \"Systolic Blood Pressure (mm Hg)\", y = \"Weight Goal\")\n\n\n\n\n\nfavstats(SBP ~ WTGOAL, data = dat5) |> \n  as_tibble() |>\n  mutate(across(.cols = c(\"mean\", \"sd\"), num, digits = 2)) |>\n  gt()\n\n\n\n\n\n  \n  \n    \n      WTGOAL\n      min\n      Q1\n      median\n      Q3\n      max\n      mean\n      sd\n      n\n      missing\n    \n  \n  \n    More\n84\n104.5\n114.0\n123\n150\n114.08\n14.76\n36\n0\n    Same\n87\n107.0\n116.5\n128\n200\n119.26\n16.49\n188\n0\n    Less\n80\n109.0\n119.0\n131\n199\n120.70\n17.17\n564\n0\n  \n  \n  \n\n\n\n\nThe analysis of variance is our primary tool for comparing more than two means (this is the extension of the pooled t test, with similar assumptions.) So the assumptions we might want to think about here are:\n\nSBP in each Weight Goal group is assumed to follow a Normal distribution\nSBP in each Weight Goal group is assumed to have the same population variance\n\nThe ANOVA, however, is far more robust to minor violations of these assumptions than is the pooled t test. So we might go ahead and fit the ANOVA model anyway, despite the apparent right skew in the “Less” group.\n\n\n3.10.2 Fitting an ANOVA Model\n\nm5 <- lm(SBP ~ WTGOAL, data = dat5)\n\nanova(m5)\n\nAnalysis of Variance Table\n\nResponse: SBP\n           Df Sum Sq Mean Sq F value  Pr(>F)  \nWTGOAL      2   1639  819.45  2.8656 0.05754 .\nResiduals 785 224477  285.96                  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nA small p value (remember we are using 90% confidence in our 432 work) like this isn’t really very important - usually it simply steers us towards trying to identify confidence intervals for differences between pairs of SBP means defined by WTGOAL.\n\n3.10.2.1 ANOVA without assuming Equal Variances?\nR will also fit an ANOVA-style model and produce a p value without the assumption of equal population SBP variance across the three groups of WTGOAL.\n\noneway.test(SBP ~ WTGOAL, data = dat5)\n\n\n    One-way analysis of means (not assuming equal variances)\n\ndata:  SBP and WTGOAL\nF = 3.5255, num df = 2.000, denom df = 93.753, p-value = 0.0334\n\n\nI don’t use this approach much, as ANOVA is pretty robust to the assumption of equal variance. The huge differences in sample size in this study (many more participants are in the Less group than the More, for instance) are most of the cause of the difference we see here.\n\n\n3.10.2.2 Testing for Equal Population Variance?\nSome people like to perform tests for equal population variance to help choose between ANOVA and the oneway.test() approach, but I do not. If I’m happy with the assumption of Normality, I virtually always just use ANOVA. There are many such tests of “equal variance”, including:\n\nBartlett’s test\nLevene’s test (which in R comes from the car package)\nFligner-Killeen test\n\nBartlett’s test is the least reliable of these when the data in at least one sample appear to be poorly described by the Normal distribution. Either Levene or Fligner-Killeen is a better choice in that setting, but again, I don’t use any of these in my work.\n\nbartlett.test(SBP ~ WTGOAL, data = dat5)\n\n\n    Bartlett test of homogeneity of variances\n\ndata:  SBP by WTGOAL\nBartlett's K-squared = 1.6722, df = 2, p-value = 0.4334\n\n\n\nleveneTest(SBP ~ WTGOAL, data = dat5)\n\nLevene's Test for Homogeneity of Variance (center = median)\n       Df F value Pr(>F)\ngroup   2  0.6557 0.5193\n      785               \n\n\n\nfligner.test(SBP ~ WTGOAL, data = dat5)\n\n\n    Fligner-Killeen test of homogeneity of variances\n\ndata:  SBP by WTGOAL\nFligner-Killeen:med chi-squared = 1.26, df = 2, p-value = 0.5326\n\n\n\n\n3.10.2.3 Is there a bootstrap one-way ANOVA approach?\nIf all you are looking for is a p value for the ANOVA model, then yes, there is a bootstrap approach available to perform one-way ANOVA testing. But I don’t actually use it, again usually preferring the usual ANOVA if the data seem reasonably likely to have been drawn from a Normal distribution, and the Kruskal-Wallis rank-based test otherwise. If you are willing to install the lmboot package, and use its ANOVA.boot() function, you can do so, like this.\n\nbs5 <- ANOVA.boot(SBP ~ WTGOAL, B = 1000, seed = 4325, data = dat5)\nbs5$`p-value`\n\n[1] 0.052\n\n\nIn this case, it doesn’t seem that we have a wildly different result than we got from the original ANOVA. That is often the case, and I have never actually used ANOVA.boot() in practical work.\n\n\n\n3.10.3 Tukey HSD Pairwise Comparisons\nWhen pairwise comparisons are pre-planned, especially when the design is close to balanced, my favorite choice for generating adjusted inferences about the means is Tukey’s Honestly Significant Differences (HSD) approach.\nHere, we generate confidence intervals for the pairwise differences in the SBP means by WTGOAL group with a 90% family-wise confidence level.\n\nth5 <- TukeyHSD(aov(SBP ~ WTGOAL, data = dat5), \n                ordered = TRUE, conf.level = 0.90)\n\ntidy(th5) |> \n  select(-c(term, null.value)) |>\n  mutate(across(.cols = -contrast, num, digits = 3)) |>\n  arrange(desc(estimate)) |>\n  gt() |>\n  tab_header(title = \"Comparing Mean Systolic BP across pairs of WTGOAL groups\",\n             subtitle = \"90% Tukey HSD Confidence Intervals\") |>\n  tab_footnote(footnote = glue(nrow(dat5), \" Hispanic participants in nh432\"))\n\n\n\n\n\n  \n    \n      Comparing Mean Systolic BP across pairs of WTGOAL groups\n    \n    \n      90% Tukey HSD Confidence Intervals\n    \n  \n  \n    \n      contrast\n      estimate\n      conf.low\n      conf.high\n      adj.p.value\n    \n  \n  \n    Less-More\n6.617\n0.642\n12.592\n0.060\n    Same-More\n5.172\n-1.151\n11.495\n0.213\n    Less-Same\n1.445\n-1.482\n4.372\n0.568\n  \n  \n  \n    \n       788 Hispanic participants in nh432\n    \n  \n\n\n\ntidy(th5) |>\n  mutate(contrast = fct_reorder(contrast, estimate, .desc = TRUE)) %>%\n  ggplot(., aes(x = contrast, y = estimate, ymin = conf.low, ymax = conf.high)) +\n  geom_crossbar() +\n  geom_hline(yintercept = 0, col = \"red\", linetype = \"dashed\") +\n  geom_label(aes(label = round_half_up(estimate, 2))) +\n  labs(title = \"Comparing Mean Systolic BP across pairs of WTGOAL groups\",\n       subtitle = \"90% Tukey HSD Confidence intervals\",\n       caption = glue(nrow(dat5), \" Hispanic participants in nh432\"),\n       x = \"Pairwise Difference between WTGOAL groups\",\n       y = \"Difference in Systolic Blood Pressure (mm Hg)\")\n\n\n\n\nThe main problems here are that:\n\nthe sample sizes in the various levels of WTGOAL are very different from one another, and\nthe SBP data are not especially well-described by a Normal distribution, at least in the “Less” group.\n\n\n\n3.10.4 Holm pairwise comparisons of means\nAnother approach to developing pairwise inferences would be to use either Bonferroni or (my preference) Holm-adjusted p values for the relevant t tests. First, we’ll run the appropriate Holm comparison of means assuming equal population variances of SBP across all three WTGOAL groups.\n\nht5 <- pairwise.t.test(dat5$SBP, dat5$WTGOAL, pool.sd = TRUE, \n                       p.adjust.method = \"holm\")\ntidy(ht5) |>\n  gt()\n\n\n\n\n\n  \n  \n    \n      group1\n      group2\n      p.value\n    \n  \n  \n    Same\nMore\n0.18625400\n    Less\nMore\n0.06929224\n    Less\nSame\n0.31056211\n  \n  \n  \n\n\n\n\nThe results are merely p-values, and not confidence intervals. There’s nothing being estimated here of interest. We can also perform these Holm comparisons without assuming equal population variances, as shown below.\n\nht5un <- pairwise.t.test(dat5$SBP, dat5$WTGOAL, pool.sd = FALSE, \n                       p.adjust.method = \"holm\")\ntidy(ht5un) |>\n  gt()\n\n\n\n\n\n  \n  \n    \n      group1\n      group2\n      p.value\n    \n  \n  \n    Same\nMore\n0.12866665\n    Less\nMore\n0.04046259\n    Less\nSame\n0.30396220\n  \n  \n  \n\n\n\n\nAgain, the problem with this approach is that it’s only producing a p value, which tempts us into talking about useless things like “statistical significance.” This is part of the reason I prefer Tukey HSD approaches when appropriate."
  },
  {
    "objectID": "03-431review1.html#comparing-4-means-using-independent-samples-weight-by-food-security",
    "href": "03-431review1.html#comparing-4-means-using-independent-samples-weight-by-food-security",
    "title": "3  431 Review: Comparing Means",
    "section": "3.11 Comparing 4 Means using Independent Samples: Weight by Food Security",
    "text": "3.11 Comparing 4 Means using Independent Samples: Weight by Food Security\n\ndat6 <- nh432 |>\n  select(SEQN, WEIGHT, FOODSEC) |>\n  drop_na()\n\n\n3.11.1 Summarizing the Data\n\nggplot(dat6, aes(x = FOODSEC, y = WEIGHT)) + \n  geom_violin(aes(fill = FOODSEC)) +\n  geom_boxplot(width = 0.3, notch = TRUE) + \n  guides(fill = \"none\") + \n  scale_fill_viridis_d(option = \"rocket\") +\n  labs(title = \"Comparing Mean Weight by Food Security\",\n       subtitle = glue(\"among \", nrow(dat6), \" participants in nh432\"),\n    x = \"Food Security Category\", y = \"Weight (kg)\")\n\n\n\n\n\nfavstats(WEIGHT ~ FOODSEC, data = dat6) |> \n  as_tibble() |>\n  mutate(across(.cols = -c(\"FOODSEC\", \"n\", \"missing\"), num, digits = 1)) |>\n  gt()\n\n\n\n\n\n  \n  \n    \n      FOODSEC\n      min\n      Q1\n      median\n      Q3\n      max\n      mean\n      sd\n      n\n      missing\n    \n  \n  \n    Full\n36.9\n68.6\n81.0\n97.7\n210.8\n85.2\n24.5\n2233\n0\n    Marginal\n39.9\n69.2\n83.0\n100.2\n242.6\n87.0\n25.0\n560\n0\n    Low\n40.9\n70.4\n83.6\n102.3\n201.0\n88.2\n24.6\n501\n0\n    Very Low\n46.1\n73.4\n85.8\n101.3\n254.3\n90.1\n24.4\n379\n0\n  \n  \n  \n\n\n\n\n\n\n3.11.2 Fitting the ANOVA model\n\nm6 <- lm(WEIGHT ~ FOODSEC, data = dat6)\n\nanova(m6)\n\nAnalysis of Variance Table\n\nResponse: WEIGHT\n            Df  Sum Sq Mean Sq F value    Pr(>F)    \nFOODSEC      3   10021  3340.3   5.525 0.0008786 ***\nResiduals 3669 2218162   604.6                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nDoes the bootstrap ANOVA give a meaningfully different result? No.\n\nbs6 <- ANOVA.boot(WEIGHT ~ FOODSEC, B = 5000, seed = 4326, data = dat6)\nbs6$`p-value`\n\n[1] 8e-04\n\n\n\n\n3.11.3 Tukey HSD Pairwise Comparisons\n\nth6 <- TukeyHSD(aov(WEIGHT ~ FOODSEC, data = dat6), \n                ordered = TRUE, conf.level = 0.90)\n\ntidy(th6) |> \n  select(-c(term, null.value)) |>\n  mutate(across(.cols = -contrast, num, digits = 3)) |>\n  arrange(desc(estimate)) |>\n  gt() |>\n  tab_header(title = \"Comparing Mean Weight across pairs of Food Security groups\",\n             subtitle = \"90% Tukey HSD Confidence Intervals\") |>\n  tab_footnote(footnote = glue(nrow(dat6), \" participants in nh432\"))\n\n\n\n\n\n  \n    \n      Comparing Mean Weight across pairs of Food Security groups\n    \n    \n      90% Tukey HSD Confidence Intervals\n    \n  \n  \n    \n      contrast\n      estimate\n      conf.low\n      conf.high\n      adj.p.value\n    \n  \n  \n    Very Low-Full\n4.877\n1.746\n8.008\n0.002\n    Very Low-Marginal\n3.103\n-0.646\n6.851\n0.229\n    Low-Full\n2.930\n0.144\n5.717\n0.075\n    Very Low-Low\n1.946\n-1.891\n5.783\n0.650\n    Marginal-Full\n1.774\n-0.889\n4.438\n0.422\n    Low-Marginal\n1.156\n-2.310\n4.622\n0.870\n  \n  \n  \n    \n       3673 participants in nh432\n    \n  \n\n\n\ntidy(th6) |> \n  mutate(contrast = fct_reorder(contrast, estimate, .desc = TRUE)) %>%\n  ggplot(., aes(x = contrast, y = estimate, ymin = conf.low, ymax = conf.high)) +\n  geom_pointrange() +\n  geom_hline(yintercept = 0, col = \"blue\", linetype = \"dashed\") +\n  geom_label(aes(label = round_half_up(estimate, 2))) +\n  labs(title = \"Comparing Mean Weight across pairs of Food Security groups\",\n       subtitle = \"90% Tukey HSD Confidence intervals\",\n       caption = glue(nrow(dat5), \" non-Hispanic Black participants in nh432\"),\n       x = \"Pairwise Difference between FOODSEC groups\",\n       y = \"Difference in Weight (kg)\")\n\n\n\n\n\n\n3.11.4 Kruskal-Wallis Test\nWhen the assumption of Normality is really unreasonable, many people (including me) will instead use a rank-based method, called the Kruskal-Wallis test to compare the locations of WEIGHT across levels of FOODSEC.\n\nkruskal.test(WEIGHT ~ FOODSEC, data = dat6)\n\n\n    Kruskal-Wallis rank sum test\n\ndata:  WEIGHT by FOODSEC\nKruskal-Wallis chi-squared = 21.102, df = 3, p-value = 0.0001003\n\n\n\n\n3.11.5 Dunn Test for Pairwise Comparisons after Kruskal-Wallis Test\nShould you develop a Kruskal-Wallis test result which implies that running a set of pairwise comparisons is important, I would suggest the use of the Dunn test, available in the dunn_test() function from the rstatix package.\n\ndunn_test(data = dat6, WEIGHT ~ FOODSEC, \n                   p.adjust.method = \"holm\", detailed = TRUE) |>\n  select(group1, group2, p.adj, n1, n2, estimate1, estimate2, estimate) |>\n  mutate(across(.cols = -c(group1, group2, n1, n2), num, digits = 3)) |>\n  gt() |>\n  tab_header(title = \"Dunn Tests comparing WEIGHT by FOODSEC\",\n             subtitle = \"Pairwise Comparisons after Kruskal-Wallis test\") |>\n  tab_footnote(footnote = glue(nrow(dat6), \" participants in nh432\"))\n\n\n\n\n\n  \n    \n      Dunn Tests comparing WEIGHT by FOODSEC\n    \n    \n      Pairwise Comparisons after Kruskal-Wallis test\n    \n  \n  \n    \n      group1\n      group2\n      p.adj\n      n1\n      n2\n      estimate1\n      estimate2\n      estimate\n    \n  \n  \n    Full\nMarginal\n0.269\n2233\n560\n1780.595\n1865.621\n85.026\n    Full\nLow\n0.049\n2233\n501\n1780.595\n1916.147\n135.551\n    Full\nVery Low\n0.000\n2233\n379\n1780.595\n2022.412\n241.816\n    Marginal\nLow\n0.438\n560\n501\n1865.621\n1916.147\n50.525\n    Marginal\nVery Low\n0.105\n560\n379\n1865.621\n2022.412\n156.790\n    Low\nVery Low\n0.282\n501\n379\n1916.147\n2022.412\n106.265\n  \n  \n  \n    \n       3673 participants in nh432\n    \n  \n\n\n\n\nAgain, a problem with this approach is that all it provides is a set of adjusted p values for these comparisons, but if we’re not willing to assume even very approximate Normality (and thus use an ANOVA approach) this is what we’ll have to cope with."
  },
  {
    "objectID": "04-431review2.html#r-setup",
    "href": "04-431review2.html#r-setup",
    "title": "4  431 Review: Comparing Rates",
    "section": "4.1 R Setup",
    "text": "4.1 R Setup\n\nknitr::opts_chunk$set(comment = NA)\n\nlibrary(janitor) \nlibrary(broom)\nlibrary(DescTools)\nlibrary(Epi)\nlibrary(gt)\nlibrary(Hmisc)\nlibrary(vcd)\n\nlibrary(tidyverse) \n\ntheme_set(theme_bw())\n\n\n4.1.1 Data Load\n\nnh432 <- read_rds(\"data/nh432.Rds\")"
  },
  {
    "objectID": "04-431review2.html#x2-contingency-table-dr_lose-and-nowlose",
    "href": "04-431review2.html#x2-contingency-table-dr_lose-and-nowlose",
    "title": "4  431 Review: Comparing Rates",
    "section": "4.2 2x2 Contingency Table: DR_LOSE and NOWLOSE",
    "text": "4.2 2x2 Contingency Table: DR_LOSE and NOWLOSE\nLet’s compare the probability that NOWLOSE is 1 (The subject is currently working on losing or controlling their body weight) between NHANES participants who have (vs. who have not) been told by a doctor to lose or control their weight in the past 12 months (DR_LOSE). Each of these (DR_LOSE and NOWLOSE) is stored in R as a numeric variable with non-missing values equal to 0 or 1.\n\ntemp <- nh432 |> \n  select(SEQN, DR_LOSE, NOW_LOSE) |> \n  drop_na()\n\nAs with any categorical variable, we start by counting, and the natural way to display the counts of these two variables (DR_LOSE and NOW_LOSE) is in a table, rather than a graph, I think.\n\ntemp |> \n  tabyl(DR_LOSE, NOW_LOSE) |> \n  adorn_totals(where = c(\"row\", \"col\")) |> \n  adorn_title() \n\n         NOW_LOSE           \n DR_LOSE        0    1 Total\n       0     1198 1541  2739\n       1      246  943  1189\n   Total     1444 2484  3928\n\n\nNow that we have a 2x2 table, we could consider obtaining some more detailed summary statistics, with a tool like the twoby2() function in the Epi package. There is a problem with this, though.\n\ntwoby2(temp$DR_LOSE, temp$NOW_LOSE)\n\n2 by 2 table analysis: \n------------------------------------------------------ \nOutcome   : 0 \nComparing : 0 vs. 1 \n\n     0    1    P(0) 95% conf. interval\n0 1198 1541  0.4374    0.4189   0.4560\n1  246  943  0.2069    0.1848   0.2309\n\n                                   95% conf. interval\n             Relative Risk: 2.1140    1.8766   2.3815\n         Sample Odds Ratio: 2.9801    2.5412   3.4949\nConditional MLE Odds Ratio: 2.9793    2.5350   3.5096\n    Probability difference: 0.2305    0.2002   0.2594\n\n             Exact P-value: 0.0000 \n        Asymptotic P-value: 0.0000 \n------------------------------------------------------\n\n\nThe code runs fine, but the table isn’t really in a useful format. This table shows the probability that NOWLOSE = 0 (“No”) comparing DR_LOSE = 0 (“No”) to DR_LOSE = 1 (“Yes”), and that’s just confusing.\nIt would be much better if we did two things:\n\nused factors with meaningful labels to represent the 1/0 variables for this table\nset up the table in standard epidemiological format, and then made a better choice as to what combination should be in the top left of the 2x2 table.\n\nSo let’s do that.\n\n4.2.1 Standard Epidemiological Format\nStandard Epidemiological Format for a 2x2 table places the exposure in the rows, and the outcome in the columns, with the top left representing the combination of interest when we obtain things like an odds ratio or probability difference. Typically this means we want to put the “Yes” and “Yes” combination in the top left.\nFirst, let’s create factor versions (with more meaningful labels than 1 and 0) out of the two variables of interest: DR_LOSE and NOW_LOSE.\n\ndat1 <- nh432 |> \n  select(SEQN, DR_LOSE, NOW_LOSE) |> \n  drop_na() |>\n  mutate(DR_LOSE_f = fct_recode(factor(DR_LOSE), \"Dr_said_Lose_Wt\" = \"1\", No = \"0\"),\n         DR_LOSE_f = fct_relevel(DR_LOSE_f, \"Dr_said_Lose_Wt\", \"No\"),\n         NOW_LOSE_f = fct_recode(factor(NOW_LOSE), \"Now_losing_Wt\" = \"1\", No = \"0\"),\n         NOW_LOSE_f = fct_relevel(NOW_LOSE_f, \"Now_losing_Wt\", \"No\"))\n\nNote that after recoding the levels to more meaningful labels, we also re-leveled the factors so that the “Yes” result comes first rather than last.\nThis produces the following table, which is now in standard epidemiological format, where we are using the DR_LOSE_f information to predict NOW_LOSE_f.\n\ndat1 |> \n  tabyl(DR_LOSE_f, NOW_LOSE_f) |> \n  adorn_totals(where = c(\"row\", \"col\")) |> \n  adorn_title() \n\n                    NOW_LOSE_f           \n       DR_LOSE_f Now_losing_Wt   No Total\n Dr_said_Lose_Wt           943  246  1189\n              No          1541 1198  2739\n           Total          2484 1444  3928\n\n\nWe could, I suppose, make the table even prettier.\n\ntab1 <- dat1 |> \n  tabyl(DR_LOSE_f, NOW_LOSE_f) |> \n  adorn_totals(where = c(\"row\", \"col\")) \n\ngt(tab1, rowname_col = \"DR_LOSE_f\") |>\n  tab_header(title = \"DR_LOSE vs. NOW_LOSE\",\n             subtitle = \"Standard Epidemiological Format\") |>\n  tab_stubhead(label = \"Dr said Lose Weight?\") |>\n  tab_spanner(label = \"Currently Losing Weight?\", \n              columns = c(Now_losing_Wt, No))\n\n\n\n\n\n  \n    \n      DR_LOSE vs. NOW_LOSE\n    \n    \n      Standard Epidemiological Format\n    \n  \n  \n    \n      Dr said Lose Weight?\n      \n        Currently Losing Weight?\n      \n      Total\n    \n    \n      Now_losing_Wt\n      No\n    \n  \n  \n    Dr_said_Lose_Wt\n943\n246\n1189\n    No\n1541\n1198\n2739\n    Total\n2484\n1444\n3928\n  \n  \n  \n\n\n\n\n\n\n4.2.2 Obtaining Key Summaries with twoby2()\nAnd, finally, we can obtain necessary summaries (including estimates and confidence intervals) using the twoby2() function.\n\ntwoby2(dat1$DR_LOSE_f, dat1$NOW_LOSE_f, conf.level = 0.90)\n\n2 by 2 table analysis: \n------------------------------------------------------ \nOutcome   : Now_losing_Wt \nComparing : Dr_said_Lose_Wt vs. No \n\n                Now_losing_Wt   No    P(Now_losing_Wt) 90% conf. interval\nDr_said_Lose_Wt           943  246              0.7931    0.7731   0.8118\nNo                       1541 1198              0.5626    0.5470   0.5781\n\n                                   90% conf. interval\n             Relative Risk: 1.4097    1.3586   1.4627\n         Sample Odds Ratio: 2.9801    2.6071   3.4065\nConditional MLE Odds Ratio: 2.9793    2.5998   3.4195\n    Probability difference: 0.2305    0.2052   0.2548\n\n             Exact P-value: 0.0000 \n        Asymptotic P-value: 0.0000 \n------------------------------------------------------\n\n\nSome brief descriptions of these results:\n\nThe probability that a participant is now losing weight (NOW_LOSE is 1) is estimated to be 0.79 (with 90% CI 0.77, 0.81) if the participant has been told to lose weight by a doctor in the past 12 months (DR_LOSE = 1), but only 0.56 (with 90% CI 0.55, 0.58) if the participant has not been told this.\nThe relative risk of a participant now losing weight is estimated to be \\(\\frac{0.7931}{0.5626}\\) = 1.41 (with 90% CI 1.36, 1.46) for a participant who has been told to lose weight vs. a participant who has not.\nThe odds of a participant now losing weight are \\(\\frac{0.7931(1-0.5626)}{0.5626(1-0.7931)}\\) = 2.98 times as high for a participant who has been told to lose weight than for one who has not, with 90% CI (2.61, 3.41).\nThe difference in probability is estimated to be 0.7931 - 0.5626 = 0.2305 (90% CI: 0.21, 0.25), indicating again that the true probability of now losing weight is higher in participants who have been told to lose weight than in those who have not.\n\nThe “exact” p-value listed comes from the Fisher exact test, while the “asymptotic” p-value comes from a Pearson \\(\\chi^2\\) (chi-squared) test. I would focus on the meaningful estimates (those with confidence intervals) in making comparisons, rather than on trying to determine “statistical significance” with the p-values."
  },
  {
    "objectID": "04-431review2.html#x2-table-sedate-category-and-now_exer",
    "href": "04-431review2.html#x2-table-sedate-category-and-now_exer",
    "title": "4  431 Review: Comparing Rates",
    "section": "4.3 2x2 Table: SEDATE category and NOW_EXER",
    "text": "4.3 2x2 Table: SEDATE category and NOW_EXER\nLet’s now look at another example, where we compare the probability that a participant is “now exercising” (NOW_EXER = 1) on the basis of their level of sedentary activity in a typical day (collected in the SEDATE variable, in minutes.)\n\ndat2 <- nh432 |> \n  select(SEQN, SEDATE, NOW_EXER) |> \n  drop_na()\n\nsummary(dat2 |> select(-SEQN))\n\n     SEDATE          NOW_EXER     \n Min.   :   2.0   Min.   :0.0000  \n 1st Qu.: 180.0   1st Qu.:0.0000  \n Median : 300.0   Median :1.0000  \n Mean   : 332.8   Mean   :0.6019  \n 3rd Qu.: 480.0   3rd Qu.:1.0000  \n Max.   :1320.0   Max.   :1.0000  \n\n\nAs you can see above, the information in SEDATE is quantitative, and suppose we want to compare a High SEDATE group vs. a Low SEDATE group.\n\n4.3.1 Creating a Low and High Group on SEDATE\nWe can use the cut2() function from the Hmisc package to partition the data by the SEDATE variable into three groups of equal sample size. At the same time, we’ll make NOW_EXER into a more useful (for tabulation) factor with more meaningful level descriptions.\n\ndat2 <- dat2 |>\n  mutate(SED_f = cut2(SEDATE, g = 3),\n         NOW_EXER_f = fct_recode(factor(NOW_EXER), \"Now_exercising\" = \"1\", No = \"0\"),\n         NOW_EXER_f = fct_relevel(NOW_EXER_f, \"Now_exercising\", \"No\"))\n\nAs you can see, we now have three groups defined by their SEDATE values, of roughly equal sample sizes.\n\ndat2 |> tabyl(SED_f)\n\n      SED_f    n   percent\n [  2, 200) 1323 0.3387097\n [200, 420) 1301 0.3330773\n [420,1320] 1282 0.3282130\n\n\nThe group labeled [2, 200) contains the 1323 subjects who had SEDATE values ranging from 2 up to (but not including) 200 minutes, for example.\n\nggplot(dat2, aes(x = SEDATE)) +\n  geom_histogram(aes(fill = SED_f), col = \"black\", bins = 25) +\n  scale_fill_manual(values = c(\"seagreen\", \"white\", \"seagreen\")) +\n  labs(title = \"Comparing Low SEDATE to High SEDATE\",\n       subtitle = \"Identification of Groups\")\n\n\n\n\nNow, we want to compare the Lowest SEDATE group (SED_F = [2, 200)) to the Highest SEDATE group (SED_F = [420, 1320]). To do that, we’ll drop the middle group, and then look at the cross-tabulation of our two remaining SEDATE groups with our outcome: NOW_EXER (in factor form.)\n\ndat2 <- dat2 |>\n  filter(SED_f != \"[200, 420)\") |>\n  mutate(SED_f = fct_drop(SED_f))\n\ndat2 |> tabyl(SED_f, NOW_EXER_f)\n\n      SED_f Now_exercising  No\n [  2, 200)            776 547\n [420,1320]            789 493\n\n\n\n\n4.3.2 Two-by-Two Table Summaries\nLet’s look at the analytic results for this table.\n\ntwoby2(dat2$SED_f, dat2$NOW_EXER_f) \n\n2 by 2 table analysis: \n------------------------------------------------------ \nOutcome   : Now_exercising \nComparing : [  2, 200) vs. [420,1320] \n\n           Now_exercising  No    P(Now_exercising) 95% conf. interval\n[  2, 200)            776 547               0.5865    0.5598   0.6128\n[420,1320]            789 493               0.6154    0.5885   0.6417\n\n                                    95% conf. interval\n             Relative Risk:  0.9530    0.8952   1.0146\n         Sample Odds Ratio:  0.8864    0.7577   1.0371\nConditional MLE Odds Ratio:  0.8865    0.7553   1.0403\n    Probability difference: -0.0289   -0.0664   0.0087\n\n             Exact P-value: 0.1388 \n        Asymptotic P-value: 0.1322 \n------------------------------------------------------\n\n\nUh, oh. There’s a bit of a problem here now. We have the right rows and the right columns, but they’re not in the best possible order, since the estimated probability of Now Exercising for the group on top (SED = [2, 200)) is smaller than it is for the people in the high group in terms of sedentary activity As a result of this problem with ordering, our relative risk and odds ratio estimates are less than 1, and our probability difference is negative.\n\n\n4.3.3 Flipping Levels\nSince which exposure goes at the top is an arbitrary decision, let’s switch the factor levels in SED_f, so that the people with high sedentary activity and who are now exercising are shown in the top left cell of the table. This should flip the point estimates of the relative risk and odds ratio above 1, and the estimated probability difference to a positive number. Note the use of the fct_rev() function from the forcats package to accomplish this.\n\ndat2 <- dat2 |>\n  mutate(SED_f = fct_rev(SED_f))\n\ndat2 |> tabyl(SED_f, NOW_EXER_f) |> \n  adorn_totals(where = c(\"row\", \"col\")) |> \n  adorn_title() \n\n                NOW_EXER_f           \n      SED_f Now_exercising   No Total\n [420,1320]            789  493  1282\n [  2, 200)            776  547  1323\n      Total           1565 1040  2605\n\ntwoby2(dat2$SED_f, dat2$NOW_EXER_f, conf.level = 0.90)\n\n2 by 2 table analysis: \n------------------------------------------------------ \nOutcome   : Now_exercising \nComparing : [420,1320] vs. [  2, 200) \n\n           Now_exercising  No    P(Now_exercising) 90% conf. interval\n[420,1320]            789 493               0.6154    0.5929   0.6375\n[  2, 200)            776 547               0.5865    0.5641   0.6086\n\n                                   90% conf. interval\n             Relative Risk: 1.0493    0.9956   1.1059\n         Sample Odds Ratio: 1.1281    0.9889   1.2869\nConditional MLE Odds Ratio: 1.1281    0.9858   1.2910\n    Probability difference: 0.0289   -0.0027   0.0604\n\n             Exact P-value: 0.1388 \n        Asymptotic P-value: 0.1322 \n------------------------------------------------------\n\n\nWe conclude now that the participants who were in the high SEDATION group (as compared to those in the low SEDATION group) had:\n\na relative risk of 1.05 (90% CI: 0.995, 1.106) for Now exercising,\na sample odds ratio of 1.13 (90% CI: 0.989, 1.287) for Now exercising,\nand probability for Now exercising that was 0.029 higher (-0.003, 0.060) than for those in the low SEDATION group."
  },
  {
    "objectID": "04-431review2.html#a-larger-5x3-2-way-table-dietqual-and-wtgoal-in-lighter-men",
    "href": "04-431review2.html#a-larger-5x3-2-way-table-dietqual-and-wtgoal-in-lighter-men",
    "title": "4  431 Review: Comparing Rates",
    "section": "4.4 A Larger (5x3) 2-Way Table: DIETQUAL and WTGOAL in Lighter Men",
    "text": "4.4 A Larger (5x3) 2-Way Table: DIETQUAL and WTGOAL in Lighter Men\nHere, we’ll look at Male participants who weighed less than 100 kg (approximately 220 pounds) and ask whether their DIETQUAL (diet quality: self-rated as Excellent to poor in 5 categories) response is associated with their response to WTGOAL (would you like to weigh more, about the same, or less than you do now: 3 categories.)\nThe resulting two-way contingency table includes 5 rows and 3 columns. We are interested in evaluating the relationship between the rows and the columns. It’s called a two-way table because there are two categorical variables (DIETQUAL and WTGOAL) under study.\nIf the rows and columns were found to be independent of one another, this would mean that the probabilities of falling in each column do not change, regardless of what row of the table we look at.\nIf the rows and columns are associated, then the probabilities of falling in each column do depend on which row we’re looking at.\n\ndat3 <- nh432 |> \n  select(SEQN, DIETQUAL, WTGOAL, WEIGHT, SEX) |>\n  filter(WEIGHT < 100 & SEX == \"Male\") |>\n  drop_na()\n\ndat3 |> \n  tabyl(DIETQUAL, WTGOAL)\n\n  DIETQUAL More Same Less\n Excellent   16   53   32\n Very Good   37  153  117\n      Good   68  179  238\n      Fair   44  111  144\n      Poor   15   18   43\n\n\nIf we want a graphical representation of a two-way table, the most common choice is probably a mosaic plot.\n\nvcd::mosaic(~ DIETQUAL + WTGOAL, data = dat3,\n            highlighting = \"WTGOAL\")\n\n\n\n\nLarger observed frequencies in the contingency table show up with larger tile areas in the in the mosaic plot. So, for instance, we see the larger proportion of “less” WTGOAL in the “Poor” DIETQUAL category, as compared to most of the other DIETQUAL categories.\n\n4.4.1 What would independence look like?\nA mosaic plot displaying perfect independence (using simulated data) might look something like this:\n\nvar1 <- c(rep(\"A\", 48), rep(\"B\", 54), rep(\"C\", 60), rep(\"D\", 24) )\nvar2 <- c( rep(c(\"G1\", \"G1\", \"G2\", \"G2\", \"G2\", \"G3\"), 31) )\ntemp_tab <- tibble(var1, var2); rm(var1, var2)\nvcd::mosaic(~ var1 + var2, data = temp_tab, highlighting = \"var1\")\n\n\n\n\nHere’s the table for our simulated data, where independence holds perfectly.\n\nxtabs(~ var1 + var2, data = temp_tab)\n\n    var2\nvar1 G1 G2 G3\n   A 16 24  8\n   B 18 27  9\n   C 20 30 10\n   D  8 12  4\n\n\nNote that in these simulated data, we have the same fraction of people in each of the four var1 categories (A, B, C, and D) regardless of which of the three var2 categories (G1, G2 and G3) we are in, and vice versa. That’s what it means for rows and columns to be independent.\n\n\n4.4.2 Back to the DIETQUAL and WTGOAL table\nNow, returning to our problem, to obtain detailed results from the Pearson \\(\\chi^2\\) test, I use the xtabs() function and then the chisq.test() function, like this:\n\nchi3 <- chisq.test(xtabs(~ DIETQUAL + WTGOAL, data = dat3))\n\nchi3\n\n\n    Pearson's Chi-squared test\n\ndata:  xtabs(~DIETQUAL + WTGOAL, data = dat3)\nX-squared = 32.603, df = 8, p-value = 7.261e-05\n\n\nThe null hypothesis being tested here is that DIETQUAL and WTGOAL are independent of each other. A small p value like this is indicative of an association between the two variables.\nThe chi3 object we have created also contains:\n\nthe observed frequencies in each cell, as well as\nthe expected frequencies under the hypothesis of independence of the rows and the columns1, and\nthe Pearson residuals \\((\\mbox{observed - expected})/\\sqrt{\\mbox{expected}}\\) for each cell, among other things.\n\n\nchi3$observed\n\n           WTGOAL\nDIETQUAL    More Same Less\n  Excellent   16   53   32\n  Very Good   37  153  117\n  Good        68  179  238\n  Fair        44  111  144\n  Poor        15   18   43\n\nchi3$expected\n\n           WTGOAL\nDIETQUAL        More      Same      Less\n  Excellent 14.33754  40.94164  45.72082\n  Very Good 43.58044 124.44637 138.97319\n  Good      68.84858 196.60095 219.55047\n  Fair      42.44479 121.20347 135.35174\n  Poor      10.78864  30.80757  34.40379\n\nchi3$residuals # Pearson residuals\n\n           WTGOAL\nDIETQUAL          More       Same       Less\n  Excellent  0.4390501  1.8845411 -2.0291917\n  Very Good -0.9968028  2.5595886 -1.8639211\n  Good      -0.1022694 -1.2552875  1.2451396\n  Fair       0.2387127 -0.9268093  0.7433564\n  Poor       1.2821492 -2.3074805  1.4655618\n\n\nAn association plot presents a graphical description of the Pearson residuals, with the area of each box shown proportional to the difference between the observed and expected frequencies.\n\nIf the observed frequency of a cell is greater than the expectation under the hypothesis of independence, then the box rises above the baseline.\n\nAn example here is the (DIETQUAL = Very Good, WTGOAL = Same) which had an observed frequency of 153 but an expected frequency of 124.4, yielding the largest positive Pearson residual at 2.56.\n\nBoxes shown below the baseline indicate that the observed frequency was less than the expectation under the independence hypothesis.\n\nThe largest negative Pearson residual is the (DIETQUAL = Poor, WTGOAL = Same) cell, where we observed 18 observations but the independence model would predict 30.8, yielding a Pearson residual of -2.31.\n\n\n\nvcd::assoc(~ DIETQUAL + WTGOAL, data = dat3)\n\n\n\n\nSome people also like to calculate a correlation between categorical variables. If each of your categorical variables is ordinal (as in this case) then Kendall’s tau (version b) is probably the best choice. As with a Pearson correlation for quantities, the value for this measure ranges from -1 to 1, with -1 indicating a strong negative correlation, and +1 a strong positive correlation, with 0 indicating no correlation.\nTo use this approach, though, we first have to be willing to treat our multi-categorical variables as if they were numeric, which may or may not be reasonable.\n\ndat3 <- dat3 |>\n  mutate(DIETQUAL_num = as.numeric(DIETQUAL))\n\ndat3 |> tabyl(DIETQUAL_num, DIETQUAL)\n\n DIETQUAL_num Excellent Very Good Good Fair Poor\n            1       101         0    0    0    0\n            2         0       307    0    0    0\n            3         0         0  485    0    0\n            4         0         0    0  299    0\n            5         0         0    0    0   76\n\n\n\ndat3 <- dat3 |>\n  mutate(WTGOAL_num = as.numeric(WTGOAL))\n\ndat3 |> tabyl(WTGOAL_num, WTGOAL)\n\n WTGOAL_num More Same Less\n          1  180    0    0\n          2    0  514    0\n          3    0    0  574\n\n\n\ncor(dat3$DIETQUAL_num, dat3$WTGOAL_num, method = \"kendall\")\n\n[1] 0.07193663\n\n\nIf you want to obtain a confidence interval for this correlation coefficient, then you would need to use the KendallTauB() function from the DescTools package.\n\nKendallTauB(dat3$DIETQUAL_num, dat3$WTGOAL_num, conf.level = 0.90)\n\n     tau_b     lwr.ci     upr.ci \n0.07193663 0.03147130 0.11240196 \n\n\nAgain, it’s just a number, and not especially valuable."
  },
  {
    "objectID": "04-431review2.html#phq9-category-and-raceethnicity",
    "href": "04-431review2.html#phq9-category-and-raceethnicity",
    "title": "4  431 Review: Comparing Rates",
    "section": "4.5 PHQ9 Category and Race/Ethnicity",
    "text": "4.5 PHQ9 Category and Race/Ethnicity\nLet’s look next at the association of race-ethnicity (RACEETH, which has 5 levels) and the depression category (minimal, mild, moderate, moderately severe, or severe) available in PHQ9_CAT, which we derived from the PHQ-9 depression screener score. We’ll restrict this small analysis to NHANES participants who did not receive care from a mental health provider (so MENTALH is 0) in the last 12 months.\n\ntemp <- nh432 |> \n  select(SEQN, RACEETH, PHQ9_CAT, MENTALH) |>\n  filter(MENTALH == 0) |>\n  drop_na()\n\nSo here’s our first attempt at a 5x5 table describing this association.\n\ntemp |> \n  tabyl(RACEETH, PHQ9_CAT)\n\n     RACEETH minimal mild moderate moderately severe severe\n Non-H White     770  151       41                20      8\n Non-H Black     668  143       34                13      1\n    Hispanic     581  133       36                12      5\n Non-H Asian     428   49       11                 1      1\n  Other Race     105   34       12                 5      1\n\n\nWe note some very small observed frequencies, especially in the bottom right of the table. Should we try to run a Pearson \\(\\chi^2\\) test on these results, we will generate a warning that the Chi-square approximation may be incorrect.\n\nxtabs( ~ RACEETH + PHQ9_CAT, data = temp ) |>\n  chisq.test()\n\nWarning in stats::chisq.test(x, y, ...): Chi-squared approximation may be\nincorrect\n\n\n\n    Pearson's Chi-squared test\n\ndata:  xtabs(~RACEETH + PHQ9_CAT, data = temp)\nX-squared = 49.288, df = 16, p-value = 2.974e-05\n\n\n\n4.5.1 The Cochran conditions\nR sets off this warning when the “Cochran conditions” are not met. The Cochran conditions require that we have:\n\nno cells with 0 counts\nat least 80% of the cells in our table with counts of 5 or higher\nexpected counts in each cell of the table should be 5 or more\n\nIn our table, we have four cells with observed counts below 5 (all have count 1) and two more with observed counts of exactly 5. If we look at the expected frequencies under the hypothesis of independence, what do we see?\n\ntemp_chi <- xtabs( ~ RACEETH + PHQ9_CAT, data = temp ) |>\n  chisq.test()\n\nWarning in stats::chisq.test(x, y, ...): Chi-squared approximation may be\nincorrect\n\ntemp_chi$expected\n\n             PHQ9_CAT\nRACEETH        minimal      mild  moderate moderately severe    severe\n  Non-H White 774.2813 154.73491 40.655838         15.473491 4.8544284\n  Non-H Black 671.8259 134.25988 35.276126         13.425988 4.2120748\n  Hispanic    599.8725 119.88048 31.498008         11.988048 3.7609562\n  Non-H Asian 383.2302  76.58596 20.122587          7.658596 2.4026969\n  Other Race  122.7901  24.53877  6.447441          2.453877 0.7698437\n\n\nEvery cell in the “severe” category has an expected frequency below 5, and we also have some generally small counts, in the Non-Hispanic Asian and Other Race categories, as well as the “moderately severe” category.\n\n\n4.5.2 Collapsing Categories\nSo what might we do about this?\nLet us consider two approaches that we’ll use simultaneously:\n\ndrop two of the RACEETH groups, and just use the top 3 (Non-H White, Non-H Black and Hispanic) using filter()\ncollapse together the two right-most levels of PHQ9_CAT (moderately severe and severe) into a new level which I’ll call “More Severe”, using fct_lump_n()\n\n\ndat5 <- nh432 |> \n  select(SEQN, RACEETH, PHQ9_CAT, MENTALH) |>\n  filter(MENTALH == 0) |>\n  filter(RACEETH %in% c(\"Non-H White\", \"Non-H Black\", \"Hispanic\")) |>\n  drop_na() |>\n  mutate(RACEETH = fct_drop(RACEETH),\n         PHQ9_CAT = fct_lump_n(PHQ9_CAT, 3, \n                               other_level = \"More Severe\"))\n\ndat5 |> \n  tabyl(RACEETH, PHQ9_CAT)\n\n     RACEETH minimal mild moderate More Severe\n Non-H White     770  151       41          28\n Non-H Black     668  143       34          14\n    Hispanic     581  133       36          17\n\n\nNow, we have at least 14 participants in every cell of the table.\n\n\n4.5.3 Pearson \\(\\chi^2\\) Analysis\nNow, let’s consider what the Pearson \\(\\chi^2\\) test suggests.\n\ntab5 <- xtabs(~ RACEETH + PHQ9_CAT, data = dat5)\n\ntab5\n\n             PHQ9_CAT\nRACEETH       minimal mild moderate More Severe\n  Non-H White     770  151       41          28\n  Non-H Black     668  143       34          14\n  Hispanic        581  133       36          17\n\nchisq.test(tab5)\n\n\n    Pearson's Chi-squared test\n\ndata:  tab5\nX-squared = 5.0157, df = 6, p-value = 0.5418\n\n\nNow we have no warning, and notice also how large a change this has meant in terms of the p-value, as compared to our original \\(\\chi^2\\) result.\n\n\n4.5.4 Mosaic Plot\nHere’s a mosaic plot2 of the table.\n\nvcd::mosaic(tab5, highlighting = \"PHQ9_CAT\")\n\n\n\n\n\n\n4.5.5 Examining the Fit\nWe’ll finish up with a look at the expected frequencies, and a table and association plot of the Pearson residuals.\n\nchisq.test(tab5)$observed\n\n             PHQ9_CAT\nRACEETH       minimal mild moderate More Severe\n  Non-H White     770  151       41          28\n  Non-H Black     668  143       34          14\n  Hispanic        581  133       36          17\n\nchisq.test(tab5)$expected\n\n             PHQ9_CAT\nRACEETH        minimal     mild moderate More Severe\n  Non-H White 764.0711 161.5940 42.00688    22.32798\n  Non-H Black 662.9667 140.2114 36.44839    19.37347\n  Hispanic    591.9622 125.1946 32.54472    17.29855\n\nchisq.test(tab5)$residuals\n\n             PHQ9_CAT\nRACEETH           minimal        mild    moderate More Severe\n  Non-H White  0.21449006 -0.83339100 -0.15535235  1.20036381\n  Non-H Black  0.19548040  0.23550271 -0.40554793 -1.22081874\n  Hispanic    -0.45055624  0.69759600  0.60567876 -0.07178083\n\nassoc(tab5)"
  },
  {
    "objectID": "05-431review3.html#r-setup",
    "href": "05-431review3.html#r-setup",
    "title": "5  431 Review: Fitting Linear Models",
    "section": "5.1 R Setup",
    "text": "5.1 R Setup\n\nknitr::opts_chunk$set(comment = NA)\n\nlibrary(janitor) \nlibrary(broom)\nlibrary(car)\nlibrary(equatiomatic)\nlibrary(GGally)\nlibrary(gt)\nlibrary(Hmisc)\nlibrary(patchwork)\n\nlibrary(tidyverse) \n\ntheme_set(theme_bw())\n\n\n5.1.1 Data Load\n\nnh432 <- read_rds(\"data/nh432.Rds\")"
  },
  {
    "objectID": "05-431review3.html#modeling-weekend-sleep-hours",
    "href": "05-431review3.html#modeling-weekend-sleep-hours",
    "title": "5  431 Review: Fitting Linear Models",
    "section": "5.2 Modeling Weekend Sleep Hours",
    "text": "5.2 Modeling Weekend Sleep Hours\nIn this example, we’ll try to build an effective model to predict our outcome: average weekend hours of sleep (SLPWKEND) on the basis of four predictors:\n\naverage weekday hours of sleep (SLPWKDAY)\nsystolic blood pressure (SBP)\nPHQ-9 depression screener score (PHQ9), and\nwhether or not the participant has mentioned trouble sleeping to a physician (SLPTROUB)\n\nWe’ll compare a model using all four of these predictors to a model using just the two directly related to sleep (SLPWKDAY and SLPTROUB), and we’ll restrict our analysis to those participants whose self-reported overall health (SROH) was “Good”.\n\ndat1 <- nh432 |>\n  select(SEQN, SLPWKEND, SLPWKDAY, SBP, PHQ9, SLPTROUB, SROH) |>\n  filter(SROH == \"Good\") |>\n  drop_na()\n\ndat1\n\n# A tibble: 1,293 × 7\n   SEQN   SLPWKEND SLPWKDAY   SBP  PHQ9 SLPTROUB SROH \n   <chr>     <dbl>    <dbl> <dbl> <int>    <dbl> <fct>\n 1 109273      8        6.5   110    15        1 Good \n 2 109293      6.5      7.5   130     3        0 Good \n 3 109295      7        7     161     0        1 Good \n 4 109305      6.5      6     125     0        0 Good \n 5 109307     11        7.5   114     0        0 Good \n 6 109315      5        5     123     1        0 Good \n 7 109336      8        4     148     1        1 Good \n 8 109342      8        6.5   107    16        1 Good \n 9 109365      9.5      9.5   133     7        0 Good \n10 109378      9        9     133     0        0 Good \n# … with 1,283 more rows\n\n\n\n5.2.1 Should we transform our outcome?\nWe can develop a Box-Cox plot to help us choose between potential transformations of our outcome, so as to improve the adherence to regression assumptions. To do so, we first fit our larger model.\n\nm1 <- lm(SLPWKEND ~ SLPWKDAY + SBP + PHQ9 + SLPTROUB, data = dat1)\n\nboxCox(m1)\n\n\n\n\nThe Box-Cox suggested set of transformations centers near \\(\\lambda = 1\\). As we saw back in Chapter 3, The ladder of power transformations looks like this:\n\n\n\n\\(\\lambda\\)\nTransformation\nFormula\n\n\n\n\n-2\ninverse square\n\\(1/y^2\\)\n\n\n-1\ninverse\n\\(1/y\\)\n\n\n-0.5\ninverse square root\n\\(1/\\sqrt{y}\\)\n\n\n0\nlogarithm\n\\(log y\\)\n\n\n0.5\nsquare root\n\\(\\sqrt{y}\\)\n\n\n1\nno transformation\ny\n\n\n2\nsquare\n\\(y^2\\)\n\n\n3\ncube\n\\(y^3\\)\n\n\n\nSo, in this case, the Box-Cox approach (again, with \\(\\lambda\\) near 1) suggests that we leave the existing SLPWKEND outcome alone.\n\n\n5.2.2 Scatterplot Matrix\n\nggpairs(dat1, columns = c(3:6, 2), switch = \"both\",\n        lower=list(combo=wrap(\"facethist\", bins=25)))\n\n\n\n\n\nThe reason I included column 2 (our outcome: SLPWKEND) last in this plot is so that the bottom row would include each of our predictors plotted on the X (horizontal) axis against the outcome on the Y (vertical) axis, next to a density plot of the outcome.\nI also switched the locations of the facet labels on both the x and y axis from their defaults, so that the labels are to the left and below the plots, since I find that a bit easier to work with.\nThe lower business is to avoid getting a warning about binwidths.\nThe binary variable (SLPTROUB) is included here as a 1-0 numeric variable, rather than a factor, which is why the scatterplot matrix looks as it does, rather than creating a series of boxplots (as we’ll see when we work with a factor later.)\n\n\n\n5.2.3 Collinearity?\nIn any multiple regression setting, two or more of the predictors might be highly correlated with one another, and this is referred to as multicollinearity or just collinearity. If we have a serious problem with collinearity, this can cause several problems, including difficulty fitting and interpreting the resulting model.\nIs collinearity a serious concern in our situation? Looking at the scatterplot matrix, we see that the largest observed correlation between two predictors is between PHQ9 and SLPTROUB. Does that rise to the level of a problem?\nI usually use the vif() function from the car package to help make this decision. The variance inflation factor (or VIF) measures how much the variance of a regression coefficient is inflated due to collinearity in the model. The smallest possible VIF value is 1, and VIFs near 1, as we’ll see here, indicate no problems with collinearity worth worrying about.\n\nvif(m1)\n\nSLPWKDAY      SBP     PHQ9 SLPTROUB \n1.002807 1.006778 1.102341 1.098187 \n\n\nShould we see a VIF (or generalized VIF, which is produced by the vif() function when we have factor variables in the model) above, say, 5, that would be an indication that the model would be improved by not including the variable that exhibits collinearity. Here, we have no such issues, and will proceed to fit the model including all of these predictors.\n\n\n5.2.4 Fitting and Displaying Model m1\nHere are the coefficients obtained from fitting the model m1.\n\nm1 <- lm(SLPWKEND ~ SLPWKDAY + SBP + PHQ9 + SLPTROUB, data = dat1)\n\nm1\n\n\nCall:\nlm(formula = SLPWKEND ~ SLPWKDAY + SBP + PHQ9 + SLPTROUB, data = dat1)\n\nCoefficients:\n(Intercept)     SLPWKDAY          SBP         PHQ9     SLPTROUB  \n    5.26266      0.52661     -0.00560     -0.02945     -0.20813  \n\n\nWe can use the extract_eq() function from the equatiomatic package to display the model attractively.\nextract_eq(m1, use_coefs = TRUE, coef_digits = 3,\n           wrap = TRUE, operator_location = \"start\")\n\n\\[\n\\begin{aligned}\n\\operatorname{\\widehat{SLPWKEND}} &= 5.263 + 0.527(\\operatorname{SLPWKDAY}) - 0.006(\\operatorname{SBP}) - 0.029(\\operatorname{PHQ9})\\\\\n&\\quad - 0.208(\\operatorname{SLPTROUB})\n\\end{aligned}\n\\]\n\nIf Harry and Sally have the same values of SLPWKDAY, SBP and SLPTROUB, but Harry’s PHQ9 is one point higher than Sally’s, then model m1 predicts that Harry will sleep 0.029 hours longer than Sally on the weekends.\nA summary of the regression model m1 provides lots of useful information about the parameters (including their standard errors) and the quality of fit (at least as measured by \\(R^2\\) and adjusted \\(R^2\\).)\n\nsummary(m1)\n\n\nCall:\nlm(formula = SLPWKEND ~ SLPWKDAY + SBP + PHQ9 + SLPTROUB, data = dat1)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-5.1173 -0.9609 -0.1005  0.9248  6.3659 \n\nCoefficients:\n             Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  5.262657   0.388427  13.549  < 2e-16 ***\nSLPWKDAY     0.526612   0.028950  18.190  < 2e-16 ***\nSBP         -0.005600   0.002515  -2.227  0.02613 *  \nPHQ9        -0.029450   0.010955  -2.688  0.00727 ** \nSLPTROUB    -0.208129   0.098758  -2.107  0.03527 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.554 on 1288 degrees of freedom\nMultiple R-squared:  0.2171,    Adjusted R-squared:  0.2147 \nF-statistic:  89.3 on 4 and 1288 DF,  p-value: < 2.2e-16\n\n\n\n\n5.2.5 Using broom functions on Model m1\nIf we want to actually use the information in the model summary elsewhere, we use the tidy() and glance() functions from the broom package to help us.\n\ntidy(m1, conf.int = TRUE, conf.level = 0.90)\n\n# A tibble: 5 × 7\n  term        estimate std.error statistic  p.value conf.low conf.high\n  <chr>          <dbl>     <dbl>     <dbl>    <dbl>    <dbl>     <dbl>\n1 (Intercept)  5.26      0.388       13.5  3.41e-39  4.62      5.90   \n2 SLPWKDAY     0.527     0.0290      18.2  5.52e-66  0.479     0.574  \n3 SBP         -0.00560   0.00251     -2.23 2.61e- 2 -0.00974  -0.00146\n4 PHQ9        -0.0294    0.0110      -2.69 7.27e- 3 -0.0475   -0.0114 \n5 SLPTROUB    -0.208     0.0988      -2.11 3.53e- 2 -0.371    -0.0456 \n\n\nHere’s a cleaner presentation of the tidy() output.\n\ntidy(m1, conf.int = TRUE, conf.level = 0.90) |> \n  mutate(across(.cols = -c(term), num, digits = 3)) |>\n  gt()\n\n\n\n\n\n  \n  \n    \n      term\n      estimate\n      std.error\n      statistic\n      p.value\n      conf.low\n      conf.high\n    \n  \n  \n    (Intercept)\n5.263\n0.388\n13.549\n0.000\n4.623\n5.902\n    SLPWKDAY\n0.527\n0.029\n18.190\n0.000\n0.479\n0.574\n    SBP\n-0.006\n0.003\n-2.227\n0.026\n-0.010\n-0.001\n    PHQ9\n-0.029\n0.011\n-2.688\n0.007\n-0.047\n-0.011\n    SLPTROUB\n-0.208\n0.099\n-2.107\n0.035\n-0.371\n-0.046\n  \n  \n  \n\n\n\n\nNote that none of the 90% confidence intervals here cross zero. This just means that we have a pretty good handle on the direction of effects - for example, our estimate for the slope of SLPWKDAY is positive, suggesting that people who sleep more during the week also sleep more on the weekend, after accounting for SBP, PHQ9 and SLPTROUB.\n\nglance(m1)\n\n# A tibble: 1 × 12\n  r.squared adj.r.squa…¹ sigma stati…²  p.value    df logLik   AIC   BIC devia…³\n      <dbl>        <dbl> <dbl>   <dbl>    <dbl> <dbl>  <dbl> <dbl> <dbl>   <dbl>\n1     0.217        0.215  1.55    89.3 4.93e-67     4 -2402. 4817. 4848.   3111.\n# … with 2 more variables: df.residual <int>, nobs <int>, and abbreviated\n#   variable names ¹​adj.r.squared, ²​statistic, ³​deviance\n\n\nModel m1 shows an \\(R^2\\) value of 0.217, which means that 21.7% of the variation in our outcome SLPWKEND is accounted for by the model using SLPWKDAY, SBP, PHQ9 and SLPTROUBLE.\nThe adjusted \\(R^2\\) value isn’t a percentage or proportion of anything, but it is a handy index when comparing two models fit to the same outcome for the same observations. It penalizes the raw \\(R^2\\) value for models that require more coefficients to be fit. If the raw \\(R^2\\) is much larger than the adjusted \\(R^2\\) value, this is also an indication that the model may be “overfit” - capitalizing on noise in the data more than we’d like, so that the amount of signal in the predictors may be overstated by raw \\(R^2\\).\nHere’s a cleaner presentation of some of the more important elements in the glance() output:\n\nglance(m1) |> \n  select(r2 = r.squared, adjr2 = adj.r.squared, \n         AIC, BIC, sigma, nobs, df, df.res = df.residual) |>\n  gt()\n\n\n\n\n\n  \n  \n    \n      r2\n      adjr2\n      AIC\n      BIC\n      sigma\n      nobs\n      df\n      df.res\n    \n  \n  \n    0.217108\n0.2146766\n4816.624\n4847.613\n1.55416\n1293\n4\n1288\n  \n  \n  \n\n\n\n\n\nAIC and BIC are measures used to compare models for the same outcome using the same data, so we’ll see those again when we fit a second model to these data. In those comparisons, smaller values of AIC and BIC indicate better fitting models.\nnobs is the number of observations used to actually fit our model m1,\ndf indicates the number of degrees of freedom used by the model, and represents the number of estimated coefficients fit, while\ndf.res = nobs - df - 1 = residual degrees of freedom.\n\n\n\n5.2.6 Residual Plots for Model m1\nThe key assumptions for a linear regression model include:\n\nLinearity of the association under study\nNormality of the residuals\nConstant Variance (Homoscedasticity)\nIndependence (not an issue with cross-sectional data like this)\n\nA residual for a point in a regression model is just the observed value of our outcome (here, SLPWKEND) minus the value predicted by the model based on the predictor values (also called the fitted value.)\nThe four key plots that R will generate for you to help assess these results are shown below for model m1.\n\n## I used \n## #| fig.height: 8 \n## at the top of this code chunk\n## to make the plots tall enough to see well\n\npar(mfrow = c(2,2)); plot(m1); par(mfrow = c(1,1))\n\n\n\n\n\n5.2.6.1 Residuals vs. Fitted values\nThe top left plot (Residuals vs. Fitted Values) helps us to assess the linearity and constant variance assumptions.\n\nWe want to see a “fuzzy football” shape.\nA clear curve is indicative of a problem with linearity, and suggests that perhaps a transformation of the outcome (or perhaps one or more predictors) may be in order\nA fan shape, with much more variation at one end of the fitted values (left or right) than the other indicates a problem with the constant variance assumption, and again a transformation may be needed.\n\nThe diagonal lines we see in the Residuals vs. Fitted plot are the result of the fact that both the outcome (SLPWKEND) and a key predictor (SLPWKDAYS) aren’t really continuous in the data, as most of the responses to those questions were either integers, or used 0.5 as the fraction. So those two variables are more discrete than we might have expected.\n\n\n5.2.6.2 Normal Q-Q plot of standardized residuals\nThe top right plot (Normal Q-Q) is a Normal Q-Q plot of the standardized regression residuals for our model m1. Substantial issues with skew (a curve in the plot) or a major problem with outliers (as indicated by a reverse S-shape) indicate potential concerns with the Normality assumption. Since the y-axis here shows standardized residuals, we can also assess whether what we’re seeing is especially surprising relative to our expectations for any standardized values (for example, we should see values above +3 or below -3 approximately 3 times in 1000).\n\nRemember that this plot represents nobs = 1293 residuals, so a few values near 3 in absolute value aren’t surprising.\nWe’re looking for big deviations from Normality here.\nThe plot() function in R will always identify three of the cases, by default, in these four plots.\n\nSuppose we wanted to look at the data for case 210, identified by these plots as a potential outlier, or at least a poorly fit point.\n\ndat1_aug <- augment(m1, data = dat1)\n\ndat1_aug |> slice(210) |>\n  select(SEQN, SLPWKEND, .fitted, .resid, .std.resid, everything())\n\n# A tibble: 1 × 13\n  SEQN  SLPWK…¹ .fitted .resid .std.…² SLPWK…³   SBP  PHQ9 SLPTR…⁴ SROH     .hat\n  <chr>   <dbl>   <dbl>  <dbl>   <dbl>   <dbl> <dbl> <int>   <dbl> <fct>   <dbl>\n1 1116…      13    6.63   6.37    4.11       4   126     1       0 Good  0.00504\n# … with 2 more variables: .sigma <dbl>, .cooksd <dbl>, and abbreviated\n#   variable names ¹​SLPWKEND, ²​.std.resid, ³​SLPWKDAY, ⁴​SLPTROUB\n\n\nWe see that row 210 contains SEQN 111688, whose\n\nobserved SLPWKEND is 13\nfitted SLPWKEND is 6.63\nyielding a residual of 6.37,\nor a standardized residual of 4.11\n\nWe can use the outlierTest() function in the car package to help assess whether this value is unusual enough to merit more careful consideration. This function actually works with the studentized residual, which is similar to the standardized residual we saw above. Here, this point (SEQN 111688) is fit poorly enough to be flagged by the Bonferroni outlier test as a mean-shift outlier.\n\noutlierTest(m1)\n\n    rstudent unadjusted p-value Bonferroni p\n210 4.131972         3.8289e-05     0.049508\n\n\nHaving seen that, though, I’m going to essentially ignore it for the moment, and press on to the rest of our residual analysis.\n\n\n5.2.6.3 Scale-Location plot\nThe bottom left plot in our set of four residual plots is the Scale-Location plot, which presents the square root of the standardized residuals against the fitted values. This plot provides another check on the “equal variance” assumption - if the plot shows a clear trend either up or down as we move left to right, then that indicates an issue with constant variance. While a loess smooth is provided (red curve) to help guide our thinking, it’s important not to get too excited about small changes or changes associated with small numbers of observations.\nYou’ll also note the presence of curves (in particular, little “V” shapes) formed by the points of the plot. Again, this is caused by the discrete nature of the outcome (and one of the key predictors) and wouldn’t be evident if our outcome was more continuous.\nDespite the drop in the red loess smooth as fitted values move from 5 to about 8, I don’t see much of a pattern here to indicate trouble with non-constant variance.\n\n\n5.2.6.4 Residuals vs. Leverage plot\nThe bottom-left plot is a plot of residuals vs. leverage, with influence contours.\nHighly leveraged points have unusual combinations of predictor values.\nHighly influential points have a big impact on the model, in that the model’s coefficients or quality of fit would change markedly were those points to be removed from the model. To measure influence, we combine leverage and residuals together, with a measure like Cook’s distance.\n\nTo look for points with substantial leverage on the model by virtue of having unusual values of the predictors - look for points whose leverage is at least 3 times as large as the average leverage value.\nThe average leverage is always k/n, where k is the number of coefficients fit by the model (including the slopes and intercept), and n is the number of observations in the model.\nTo obtain the leverage values, the augment() function stores them in .hat.\nTo look for points with substantial influence on the model, that is, removing them from the model would change it substantially, consider the Cook’s distance, plotted in contours here.\nAny Cook’s distance point > 1 will likely have a substantial impact on the model.\nAny points with Cook’s distance > 0.5, if any, will be indicated in the bottom-right (Residuals vs. Leverage) plot, and are worthy of investigation.\nIn model m1, we have no points with values of Cook’s distance > 0.5. To obtain the Cook’s distance values for each point, use the augment() function, which stores them in .cooksd.\n\nHere, for example, we identify the points with largest leverage and with largest Cook’s distance, across the points used to fit m1.\n\ndat1_aug <- augment(m1, data = dat1)\n\ndat1_aug |> slice_max(.hat) |>\n  select(SEQN, .hat, .resid, .fitted, .cooksd, everything())\n\n# A tibble: 1 × 13\n  SEQN     .hat .resid .fitted .cooksd SLPWK…¹ SLPWK…²   SBP  PHQ9 SLPTR…³ SROH \n  <chr>   <dbl>  <dbl>   <dbl>   <dbl>   <dbl>   <dbl> <dbl> <int>   <dbl> <fct>\n1 123474 0.0268  -1.03    8.03 0.00250       7       8   126    25       0 Good \n# … with 2 more variables: .sigma <dbl>, .std.resid <dbl>, and abbreviated\n#   variable names ¹​SLPWKEND, ²​SLPWKDAY, ³​SLPTROUB\n\ndat1_aug |> slice_max(.cooksd) |>\n  select(SEQN, .hat, .resid, .fitted, .cooksd, everything())\n\n# A tibble: 1 × 13\n  SEQN     .hat .resid .fitted .cooksd SLPWK…¹ SLPWK…²   SBP  PHQ9 SLPTR…³ SROH \n  <chr>   <dbl>  <dbl>   <dbl>   <dbl>   <dbl>   <dbl> <dbl> <int>   <dbl> <fct>\n1 122894 0.0113   5.38    5.62  0.0277      11       2   114     2       0 Good \n# … with 2 more variables: .sigma <dbl>, .std.resid <dbl>, and abbreviated\n#   variable names ¹​SLPWKEND, ²​SLPWKDAY, ³​SLPTROUB\n\n\nIt turns out that SEQN 123474 has the largest value of leverage (.hat) and SEQN 122894 has the largest value of influence (.cooksd) in our model. We will worry about .cooksd values above 0.5, but the largest value in this model is much smaller than that, so I think we’re OK for now.\n\n\n\n5.2.7 Fitting and Displaying Model m2\nWe will now move on to compare the results of this model (m1) to a smaller model.\nOur second model, m2 is a subset of m1, including only the two predictors directly related to sleep, SLPWKDAY and SLPTROUB.\n\nm2 <- lm(SLPWKEND ~ SLPWKDAY + SLPTROUB, data = dat1)\n\nm2\n\n\nCall:\nlm(formula = SLPWKEND ~ SLPWKDAY + SLPTROUB, data = dat1)\n\nCoefficients:\n(Intercept)     SLPWKDAY     SLPTROUB  \n     4.4813       0.5301      -0.2862  \n\n\nextract_eq(m2, use_coefs = TRUE, coef_digits = 3,\n           wrap = TRUE, operator_location = \"start\")\n\n\\[\n\\begin{aligned}\n\\operatorname{\\widehat{SLPWKEND}} &= 4.481 + 0.53(\\operatorname{SLPWKDAY}) - 0.286(\\operatorname{SLPTROUB})\n\\end{aligned}\n\\]\n\nNote that the slopes of both SLPWKDAY and SLPTROUB have changed from model m1 (although not very much), and that the intercept has changed more substantially.\n\n\n5.2.8 Using broom functions on m2\n\ntidy(m2, conf.int = TRUE, conf.level = 0.90) |> \n  mutate(across(.cols = -c(term), num, digits = 3)) |>\n  gt()\n\n\n\n\n\n  \n  \n    \n      term\n      estimate\n      std.error\n      statistic\n      p.value\n      conf.low\n      conf.high\n    \n  \n  \n    (Intercept)\n4.481\n0.219\n20.475\n0.000\n4.121\n4.842\n    SLPWKDAY\n0.530\n0.029\n18.268\n0.000\n0.482\n0.578\n    SLPTROUB\n-0.286\n0.095\n-3.025\n0.003\n-0.442\n-0.130\n  \n  \n  \n\n\n\n\n\nglance(m2) |> \n  select(r2 = r.squared, adjr2 = adj.r.squared, \n         AIC, BIC, sigma, nobs, df, df.res = df.residual) |>\n  gt()\n\n\n\n\n\n  \n  \n    \n      r2\n      adjr2\n      AIC\n      BIC\n      sigma\n      nobs\n      df\n      df.res\n    \n  \n  \n    0.2101295\n0.2089049\n4824.099\n4844.758\n1.559861\n1293\n2\n1290\n  \n  \n  \n\n\n\n\nSince we want to compare the fit of m1 to that of m2, we probably want to do so in a single table, like this:\n\ntemp1 <- glance(m1) |> \n  select(r2 = r.squared, adjr2 = adj.r.squared, \n         AIC, BIC, sigma, nobs, df, df.res = df.residual) |>\n  mutate(model = \"m1\") |>\n  relocate(model)\n\ntemp2 <- glance(m2) |> \n  select(r2 = r.squared, adjr2 = adj.r.squared, \n         AIC, BIC, sigma, nobs, df, df.res = df.residual) |>\n  mutate(model = \"m2\") |>\n  relocate(model)\n\nbind_rows(temp1, temp2) |>\n  gt()\n\n\n\n\n\n  \n  \n    \n      model\n      r2\n      adjr2\n      AIC\n      BIC\n      sigma\n      nobs\n      df\n      df.res\n    \n  \n  \n    m1\n0.2171080\n0.2146766\n4816.624\n4847.613\n1.554160\n1293\n4\n1288\n    m2\n0.2101295\n0.2089049\n4824.099\n4844.758\n1.559861\n1293\n2\n1290\n  \n  \n  \n\n\n\n\nEach model uses the same number of observations to predict the same outcome (SLPWKEND). So we can compare them directly. As compared to model m2, model m1 has:\n\nthe larger \\(R^2\\) (as it must, since model m2 includes a subset of the predictors in model m1),\nthe larger adjusted \\(R^2\\),\nthe smaller AIC (Akaike Information Criterion: smaller values are better),\nthe larger BIC (Bayes Information Criterion: again, smaller values are better),\n\nand the smaller residual standard error (\\(\\sigma\\)) (smaller values are better.)\n\nThe key realizations for these data are that the AIC, adjusted \\(R^2\\) and \\(\\sigma\\) results favor model m1 while the BIC favors model m2.\n\n\n5.2.9 Residual Plots for Model m2\n\n## I used #| fig.height: 8 in this code chunk\n## to make the plots tall enough to see well\n\npar(mfrow = c(2,2)); plot(m2); par(mfrow = c(1,1))\n\n\n\n\nThe residual plots here show (even more starkly than in model m1) the discrete nature of our outcome and the two variables we’re using to predict it. I see no especially serious problems with the assumptions of linearity or constant variance here, and while there are still some fairly poorly fit values, there are no highly influential points, so I’ll accept these residual plots as indicative of a fairly reasonable model on the whole.\n\n\n5.2.10 Conclusions\nThree of our four in-sample measures of fit quality (AIC, \\(\\sigma\\) and adjusted \\(R^2\\)) favor the larger model m1 over m2, but there’s not a lot to choose from here. Neither model showed important problems with regression assumptions, so I would probably wind up choosing m1 based on the analyses we’ve done in this Chapter.\nHowever, a more appropriate strategy for prediction assessment would be to partition the data into separate samples for model training (the development or building sample) and model testing. We adopt such a model validation strategy in our next little study."
  },
  {
    "objectID": "05-431review3.html#modeling-high-sensitivity-c-reactive-protein",
    "href": "05-431review3.html#modeling-high-sensitivity-c-reactive-protein",
    "title": "5  431 Review: Fitting Linear Models",
    "section": "5.3 Modeling High-Sensitivity C-Reactive Protein",
    "text": "5.3 Modeling High-Sensitivity C-Reactive Protein\nIn this, our second linear modeling example, we will try to predict High-Sensitivity C-Reactive Protein levels (HSCRP) on the basis of these three predictor variables:\n\nthe participant’s mean pulse rate, specifically the mean of the two gathered pulse rates, PULSE1 and PULSE2\nthe participant’s self-reported overall health (SROH, which is an ordinal factor with levels Excellent, Very Good, Good, Fair and Poor)\nHOSPITAL, a 1-0 binary variable indicating whether or not the participant was hospitalized in the past year.\n\nIn this case, we’ll use all NHANES participants with complete data on the relevant variables to fit the three-predictor model, and then a second model using mean pulse rate alone.\n\ndat2 <- nh432 |>\n  select(SEQN, HSCRP, PULSE1, PULSE2, SROH, HOSPITAL) |>\n  drop_na() |>\n  mutate(MEANPULSE = 0.5*(PULSE1 + PULSE2))\n\nglimpse(dat2)\n\nRows: 3,117\nColumns: 7\n$ SEQN      <chr> \"109271\", \"109273\", \"109291\", \"109292\", \"109293\", \"109295\", …\n$ HSCRP     <dbl> 28.68, 0.98, 5.31, 3.08, 15.10, 6.28, 0.56, 1.45, 0.32, 0.86…\n$ PULSE1    <dbl> 73, 71, 77, 93, 62, 93, 74, 59, 66, 83, 64, 55, 54, 63, 68, …\n$ PULSE2    <dbl> 71, 70, 76, 91, 64, 93, 74, 58, 64, 87, 68, 55, 54, 63, 70, …\n$ SROH      <fct> Fair, Good, Fair, Very Good, Good, Good, Very Good, Excellen…\n$ HOSPITAL  <dbl> 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ MEANPULSE <dbl> 72.0, 70.5, 76.5, 92.0, 63.0, 93.0, 74.0, 58.5, 65.0, 85.0, …\n\n\n\n5.3.1 Partitioning the Data\nBefore partitioning, it’s always a good idea to be sure that the number of rows in the tibble matches the number of distinct (unique) values in the identifier column.\n\nidentical(nrow(dat2), n_distinct(dat2 |> select(SEQN)))\n\n[1] TRUE\n\n\nOK. Now, be sure to set a seed so that we can replicate the selection. We’ll put 70% of the data in our training sample, setting aside the remaining 30% for the test sample.\n\nset.seed(432005)\n\ndat2_train <- slice_sample(dat2, prop = 0.70)\n\ndat2_test <- anti_join(dat2, dat2_train, by = \"SEQN\")\n\nc(nrow(dat2), nrow(dat2_train), nrow(dat2_test))\n\n[1] 3117 2181  936\n\n\nIn what follows, we’ll work with the dat2_train sample, and set aside the dat2_test sample for a while.\n\n\n5.3.2 Transforming the Outcome?\nLet’s use the Box-Cox approach to help us think about which potential transformations of our outcome might be helpful, within our training sample.\n\nm_temp <- lm(HSCRP ~ MEANPULSE + SROH + HOSPITAL, data = dat2_train)\n\nboxCox(m_temp)\n\n\n\n\nThe estimated \\(\\lambda\\) value is very close to 0, which according to the ladder of power transformations, suggests we take the logarithm of our outcome, so as to improve the residual plots for the model. This will also, as it turns out, lead to a much less right-skewed outcome variable.\n\np1 <- ggplot(dat2_train, aes(sample = HSCRP)) +\n  geom_qq() + geom_qq_line(col = \"red\") +\n  labs(title = \"Normal Q-Q: untransformed HSCRP\")\n\np2 <- ggplot(dat2_train, aes(sample = log(HSCRP))) +\n  geom_qq() + geom_qq_line(col = \"red\") +\n  labs(title = \"Normal Q-Q: Natural Log of HSCRP\")\n\np1 + p2\n\n\n\n\nClearly, one benefit of the transformation is some improvement in the Normality of our outcome’s distribution.\n\n\n5.3.3 Scatterplot Matrix and Collinearity\nTo build the relevant scatterplot matrix with our transformed outcome, I’ll create a variable containing the result of the transformation within our training sample.\n\ndat2_train <- dat2_train |>\n  mutate(logHSCRP = log(HSCRP))\n\nnames(dat2_train)\n\n[1] \"SEQN\"      \"HSCRP\"     \"PULSE1\"    \"PULSE2\"    \"SROH\"      \"HOSPITAL\" \n[7] \"MEANPULSE\" \"logHSCRP\" \n\nggpairs(dat2_train, columns = c(7,5,6,8), switch = \"both\",\n        lower=list(combo=wrap(\"facethist\", bins=25)))\n\n\n\n\nAs a collinearity check, we’ll run vif() from the car package here.\n\nm3 <- lm(log(HSCRP) ~ MEANPULSE + SROH + HOSPITAL,\n         data = dat2_train)\n\nvif(m3)\n\n              GVIF Df GVIF^(1/(2*Df))\nMEANPULSE 1.036382  1        1.018028\nSROH      1.063545  4        1.007731\nHOSPITAL  1.027860  1        1.013834\n\n\nAgain, no signs of meaningful collinearity. Note the presentation of the factor variable SROH in the scatterplot matrix, and in the generalized VIF output.\n\n\n5.3.4 Fit Model m3\n\nm3 <- lm(log(HSCRP) ~ MEANPULSE + SROH + HOSPITAL,\n         data = dat2_train)\n\nextract_eq(m3, use_coefs = TRUE, coef_digits = 3,\n           terms_per_line = 3, wrap = TRUE, \n           operator_location = \"start\")\n\n\\[\n\\begin{aligned}\n\\operatorname{\\widehat{log(HSCRP)}} &= -1.108 + 0.02(\\operatorname{MEANPULSE}) + 0.237(\\operatorname{SROH}_{\\operatorname{Very\\ Good}})\\\\\n&\\quad + 0.532(\\operatorname{SROH}_{\\operatorname{Good}}) + 0.641(\\operatorname{SROH}_{\\operatorname{Fair}}) + 0.86(\\operatorname{SROH}_{\\operatorname{Poor}})\\\\\n&\\quad + 0.052(\\operatorname{HOSPITAL})\n\\end{aligned}\n\\]\n\n\ntidy(m3, conf.int = TRUE, conf.level = 0.90) |>\n  mutate(across(.cols = -c(term), num, digits = 3)) |>\n  gt()\n\n\n\n\n\n  \n  \n    \n      term\n      estimate\n      std.error\n      statistic\n      p.value\n      conf.low\n      conf.high\n    \n  \n  \n    (Intercept)\n-1.108\n0.160\n-6.914\n0.000\n-1.372\n-0.845\n    MEANPULSE\n0.020\n0.002\n9.359\n0.000\n0.017\n0.024\n    SROHVery Good\n0.237\n0.081\n2.939\n0.003\n0.104\n0.370\n    SROHGood\n0.532\n0.078\n6.857\n0.000\n0.404\n0.660\n    SROHFair\n0.641\n0.087\n7.353\n0.000\n0.497\n0.784\n    SROHPoor\n0.860\n0.143\n6.020\n0.000\n0.625\n1.095\n    HOSPITAL\n0.052\n0.088\n0.594\n0.552\n-0.092\n0.197\n  \n  \n  \n\n\n\n\n\nIf Harry and Sally have the same values of HOSPITAL and MEANPULSE, but Harry’s SROH is “Very Good” while Sally’s is “Excellent”, then model m3 predicts that Harry will have a log(HSCRP) that is 0.237 (90% CI: 0.104, 0.370) larger than Sally’s log(HSCRP).\nOn the other hand, if Harry and Gary have the same values of HOSPITAL and MEANPULSE, but Harry’s SROH remains “Very Good” while Gary’s is only “Good”, then model m3 predicts that Gary will have a log(HSCRP) that is (0.532 - 0.237 = 0.295) larger than Harry’s log(HSCRP).\n\n\n\n5.3.5 Residual Plots for m3\n\n## don't forget to use #| fig.height: 8\n## to make the residual plots taller\n\npar(mfrow = c(2,2)); plot(m3); par(mfrow = c(1,1))\n\n\n\n\nI see no serious concerns with regression assumptions here. The residuals vs. fitted plot shows no signs of meaningful non-linearity or heteroscedasticity. The standardized residuals in the Normal Q-Q plot follow the reference line closely. There is no clear trend in the scale-location plot, and the residuals vs. leverage plot reveals no particularly influential points.\n\n\n5.3.6 Fit Model m4\nLet’s now fit the simple regression model, m4, with only MEANPULSE as a predictor of the log of HSCRP.\n\nm4 <- lm(log(HSCRP) ~ MEANPULSE,\n         data = dat2_train)\n\nextract_eq(m4, use_coefs = TRUE, coef_digits = 3,\n           wrap = TRUE, operator_location = \"start\")\n\n\\[\n\\begin{aligned}\n\\operatorname{\\widehat{log(HSCRP)}} &= -0.929 + 0.023(\\operatorname{MEANPULSE})\n\\end{aligned}\n\\]\n\nNow, let’s look at the tidied coefficients.\n\ntidy(m4, conf.int = TRUE, conf.level = 0.90) |>\n  mutate(across(.cols = -c(term), num, digits = 3)) |>\n  gt()\n\n\n\n\n\n  \n  \n    \n      term\n      estimate\n      std.error\n      statistic\n      p.value\n      conf.low\n      conf.high\n    \n  \n  \n    (Intercept)\n-0.929\n0.154\n-6.048\n0.000\n-1.181\n-0.676\n    MEANPULSE\n0.023\n0.002\n10.934\n0.000\n0.020\n0.027\n  \n  \n  \n\n\n\n\n\nIf Harry’s mean pulse rate is one beat per minute higher than Sally’s, then model m4 predicts that the logarithm of Harry’s HSCRP will be 0.023 higher than Sally’s, with 90% CI (0.020, 0.027).\nNote that if Harry’s mean pulse rate is ten beats per minute higher than Sally’s, then model m4 predicts that the logarithm of Harry’s HSCRP will be 0.23 higher than Sally’s, with 90% CI (0.20, 0.27).\n\n\n\n5.3.7 Residual Plots for m4\n\npar(mfrow = c(2,2)); plot(m4); par(mfrow = c(1,1))\n\n\n\n\n\n\n5.3.8 In-Sample Fit Quality Comparison (m3 vs. m4)\n\ng3 <- glance(m3) |> \n  select(r2 = r.squared, adjr2 = adj.r.squared, \n         AIC, BIC, sigma, nobs, df, df.res = df.residual) |>\n  mutate(model = \"m3\") |>\n  relocate(model)\n\ng4 <- glance(m4) |> \n  select(r2 = r.squared, adjr2 = adj.r.squared, \n         AIC, BIC, sigma, nobs, df, df.res = df.residual) |>\n  mutate(model = \"m4\") |>\n  relocate(model)\n\nbind_rows(g3, g4) |> gt()\n\n\n\n\n\n  \n  \n    \n      model\n      r2\n      adjr2\n      AIC\n      BIC\n      sigma\n      nobs\n      df\n      df.res\n    \n  \n  \n    m3\n0.09070792\n0.08819837\n6636.024\n6681.525\n1.105532\n2181\n6\n2174\n    m4\n0.05200815\n0.05157309\n6716.928\n6733.990\n1.127517\n2181\n1\n2179\n  \n  \n  \n\n\n\n\nThe larger model (model m3) has better results than model m4 in the sense that it produces a larger adjusted \\(R^2\\), and smaller values for AIC, BIC and \\(\\sigma\\). Based on this comparison within the training sample, we clearly prefer m3, since each model shows reasonable adherence to the assumptions of a linear regression model.\n\n\n5.3.9 Testing the models in new data\nAt last we return to the dat2_test sample which was not used in fitting models m3 and m4 to investigate which of these models has better predictive results in new data. When doing this sort of testing, I recommend a look at the following 4 summaries, each of which is based on the fitted (predicted) and observed values of our outcome in our new data, using the models we want to compare:\n\nsquared correlation of predicted with observed values (validated \\(R^2\\); higher values are better)\nmean absolute prediction error (MAPE; smaller values indicate smaller errors, hence better prediction)\nsquare root of the mean squared prediction error (RMSPE; again, smaller values indicate better predictions)\nmaximum (in absolute value) prediction error (Max Error)\n\nTo obtain observed, predicted, and error (observed - predicted) values for each new data point when we apply model m3, we first use the augment() function from the broom package to obtain our .fitted values.\n\nm3_test_aug <- augment(m3, newdata = dat2_test)\nhead(m3_test_aug)\n\n# A tibble: 6 × 9\n  SEQN   HSCRP PULSE1 PULSE2 SROH      HOSPITAL MEANPULSE .fitted .resid\n  <chr>  <dbl>  <dbl>  <dbl> <fct>        <dbl>     <dbl>   <dbl>  <dbl>\n1 109273  0.98     71     70 Good             0      70.5   0.839 -0.859\n2 109293 15.1      62     64 Good             0      63     0.688  2.03 \n3 109312  0.86     83     87 Very Good        0      85     0.835 -0.985\n4 109332  2.29     63     63 Excellent        0      63     0.156  0.673\n5 109340  4.64     78     78 Fair             0      78     1.10   0.437\n6 109342  5.51     72     70 Good             0      71     0.849  0.858\n\n\nRemember, however, that our models m3 and m4 do not predict HSCRP, but rather the logarithm of HSCRP, so we need to exponentiate the .fitted values to get what we want.\n\nm3_test_aug <- augment(m3, newdata = dat2_test) |>\n  mutate(fits = exp(.fitted),\n         resid = HSCRP - fits) |>\n  select(SEQN, HSCRP, fits, resid, everything())\n\nhead(m3_test_aug)\n\n# A tibble: 6 × 11\n  SEQN   HSCRP  fits resid PULSE1 PULSE2 SROH     HOSPI…¹ MEANP…² .fitted .resid\n  <chr>  <dbl> <dbl> <dbl>  <dbl>  <dbl> <fct>      <dbl>   <dbl>   <dbl>  <dbl>\n1 109273  0.98  2.31 -1.33     71     70 Good           0    70.5   0.839 -0.859\n2 109293 15.1   1.99 13.1      62     64 Good           0    63     0.688  2.03 \n3 109312  0.86  2.30 -1.44     83     87 Very Go…       0    85     0.835 -0.985\n4 109332  2.29  1.17  1.12     63     63 Excelle…       0    63     0.156  0.673\n5 109340  4.64  3.00  1.64     78     78 Fair           0    78     1.10   0.437\n6 109342  5.51  2.34  3.17     72     70 Good           0    71     0.849  0.858\n# … with abbreviated variable names ¹​HOSPITAL, ²​MEANPULSE\n\n\nNow, we can obtain our summaries, as follows.\n\nm3_test_results <- m3_test_aug |>\n  summarize(validated_R_sq = cor(HSCRP, fits)^2,\n            MAPE = mean(abs(resid)),\n            RMSPE = sqrt(mean(resid^2)),\n            max_Error = max(abs(resid)))\n\nm3_test_results\n\n# A tibble: 1 × 4\n  validated_R_sq  MAPE RMSPE max_Error\n           <dbl> <dbl> <dbl>     <dbl>\n1          0.114  3.45  10.7      177.\n\n\nFor model m4, we have:\n\nm4_test_aug <- augment(m4, newdata = dat2_test) |>\n  mutate(fits = exp(.fitted),\n         resid = HSCRP - fits) |>\n  select(SEQN, HSCRP, fits, resid, everything())\n\nm4_test_results <- m4_test_aug |>\n  summarize(validated_R_sq = cor(HSCRP, fits)^2,\n            MAPE = mean(abs(resid)),\n            RMSPE = sqrt(mean(resid^2)),\n            max_Error = max(abs(resid)))\n\nm4_test_results\n\n# A tibble: 1 × 4\n  validated_R_sq  MAPE RMSPE max_Error\n           <dbl> <dbl> <dbl>     <dbl>\n1          0.102  3.50  10.8      177.\n\n\nAnd we can put the two sets of results together into a nice table.\n\nbind_rows(m3_test_results, m4_test_results) |>\n  mutate(model = c(\"m3\", \"m4\")) |>\n  relocate(model) |>\n  gt()\n\n\n\n\n\n  \n  \n    \n      model\n      validated_R_sq\n      MAPE\n      RMSPE\n      max_Error\n    \n  \n  \n    m3\n0.1135858\n3.451295\n10.72894\n176.9855\n    m4\n0.1015280\n3.496673\n10.79613\n177.0716\n  \n  \n  \n\n\n\n\nBased on these out-of-sample validation results, it seems that Model m3 has the better results across each of these four summaries than Model 4 does.\n\n\n5.3.10 Conclusions\nWe fit two models to predict HSCRP, a larger model (m3) containing three predictors (MEANPULSE, SROH and HOSPITAL), and a smaller model (m4) containing only the MEANPULSE as a predictor.\n\nBoth models (after transforming to log(HSCRP) for our outcome) seem to generally meet the assumptions of linear regression\nModel m3 had a raw \\(R^2\\) value of 0.091, so it accounted for about 9.1% of the variation in log(HSCRP) within our training sample. Model m4 accounted for 5.2%.\nIn our in-sample checks, Model m3 had better results in terms of adjusted \\(R^2\\), AIC, BIC and \\(\\sigma\\).\nIn a validation (test) sample, our Model m3 also showed superior predictive performance, including better results in terms of MAPE, RMSPE and maximum absolute error, as well as a validated \\(R^2\\) of 11.4%, higher than model m4’s result of 10.2%.\n\nOverall, model m3 seems like the meaningfully better choice."
  }
]