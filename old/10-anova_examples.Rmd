# Analysis of Variance

## The `bonding` data: A Designed Dental Experiment

The `bonding` data describe a designed experiment into the properties of four different resin types (`resin` = A, B, C, D) and two different curing light sources (`light` = Halogen, LED) as they relate to the resulting bonding strength (measured in MPa^[The MPa is defined as the failure load (in Newtons) divided by the entire bonded area, in mm^2^.]) on the surface of teeth. The source is @Kim2014.

The experiment involved making measurements of bonding strength under a total of 80 experimental setups, or runs, with 10 runs completed at each of the eight combinations of a light source and a resin type. The data are gathered in the `bonding.csv` file.

```{r c3_bonding_tibble}
bonding
```

## A One-Factor Analysis of Variance

Suppose we are interested in the distribution of the `strength` values for the four different types of `resin`. 

```{r}
bonding %>% group_by(resin) %>% summarize(n = n(), mean(strength), median(strength))
```

I'd begin serious work with a plot.

### Look at the Data!

```{r c3_oneway_bonding_resin_boxplot}
ggplot(bonding, aes(x = resin, y = strength)) +
    geom_boxplot()
```

Another good plot for this purpose is a ridgeline plot.

```{r c3_oneway_bonding_resin_ridgelineplot}
ggplot(bonding, aes(x = strength, y = resin, fill = resin)) +
    geom_density_ridges2() +
    guides(fill = "none")
```

### Table of Summary Statistics

With the small size of this experiment (*n* = 20 for each `resin` type), graphical summaries may not perform as well as they often do. We'll also produce a quick table of summary statistics for `strength` within each `resin` type.

```{r}
bonding %$% mosaic::favstats(strength ~ resin)
```

Since the means and medians within each group are fairly close, and the distributions (with the possible exception of `resin` D) are reasonably well approximated by the Normal, I'll fit an ANOVA model^[If the data weren't approximately Normally distributed, we might instead consider a rank-based alternative to ANOVA, like the Kruskal-Wallis test.].

```{r}
anova(lm(strength ~ resin, data = bonding))
```

It appears that the `resin` types have a significant association with mean `strength` of the bonds. Can we identify which `resin` types have generally higher or lower `strength`?

```{r}
TukeyHSD(aov(lm(strength ~ resin, data = bonding)))
```

Based on these confidence intervals (which have a family-wise 95% confidence level), we see that D is associated with significantly larger mean `strength` than A or B or C, and that C is also associated with significantly larger mean `strength` than A. This may be easier to see in a plot of these confidence intervals.

```{r}
plot(TukeyHSD(aov(lm(strength ~ resin, data = bonding))))
```

## A Two-Way ANOVA: Looking at Two Factors

Now, we'll now add consideration of the `light` source into our study. We can look at the distribution of the `strength` values at the combinations of both `light` and `resin`, with a plot like this one.

```{r c3_bonding_points_plot}
ggplot(bonding, aes(x = resin, y = strength, color = light)) +
    geom_point(size = 2, alpha = 0.5) +
    facet_wrap(~ light) +
    guides(color = "none") +
    scale_color_manual(values = c("purple", "darkorange")) +
    theme_bw() 
```

## A Means Plot (with standard deviations) to check for interaction

Sometimes, we'll instead look at a plot simply of the means (and, often, the standard deviations) of `strength` at each combination of `light` and `resin`. We'll start by building up a data set with the summaries we want to plot.

```{r c3_group_to_build_means_plot_bonding}
bond.sum <- bonding %>% 
    group_by(resin, light) %>%
    summarize(mean.str = mean(strength), sd.str = sd(strength))

bond.sum
```

Now, we'll use this new data set to plot the means and standard deviations of `strength` at each combination of `resin` and `light`. 

```{r c3_ggplot_means_plot_bonding}
## The error bars will overlap unless we adjust the position.
pd <- position_dodge(0.2) # move them .1 to the left and right

ggplot(bond.sum, aes(x = resin, y = mean.str, col = light)) +
    geom_errorbar(aes(ymin = mean.str - sd.str, 
                      ymax = mean.str + sd.str),
                  width = 0.2, position = pd) +
    geom_point(size = 2, position = pd) + 
    geom_line(aes(group = light), position = pd) +
    scale_color_manual(values = c("purple", "darkorange")) +
    theme_bw() +
    labs(y = "Bonding Strength (MPa)", x = "Resin Type",
         title = "Observed Means (+/- SD) of Bonding Strength")
```

Is there evidence of a meaningful interaction between the resin type and the `light` source on the bonding strength in this plot? 

- Sure. A meaningful interaction just means that the strength associated with different `resin` types depends on the `light` source. 
    - With LED `light`, it appears that `resin` C leads to the strongest bonding strength.
    - With Halogen `light`, though, it seems that `resin` D is substantially stronger.
- Note that the lines we see here connecting the `light` sources aren't in parallel (as they would be if we had zero interaction between `resin` and `light`), but rather, they cross.

### Summarizing the data after grouping by `resin` and `light`

We might want to look at a numerical summary of the `strengths` within these groups, too.

```{r c3_bonding_anova}
bonding %$% mosaic::favstats(strength ~ resin + light) %>%
    select(resin.light, median, mean, sd, n, missing)
```

## Fitting the Two-Way ANOVA model with Interaction

```{r c3_bonding_anova_with_interaction}
c3_m1 <- lm(strength ~ resin * light, data = bonding)

summary(c3_m1)
```

### The ANOVA table for our model

In a two-way ANOVA model, we begin by assessing the interaction term. If it's important, then our best model is the model including the interaction. If it's not important, we will often move on to consider a new model, fit without an interaction.

The ANOVA table is especially helpful in this case, because it lets us look specifically at the interaction effect.

```{r}
anova(c3_m1)
```

### Is the interaction important?

In this case, the interaction:

- is evident in the means plot, and
- is highly statistically significant, and
- accounts for a sizable fraction (27%) of the overall variation

$$ 
\eta^2_{interaction} = \frac{\mbox{SS(resin:light)}}{SS(Total)}
= \frac{1571.96}{1999.72 + 34.72 + 1571.96 + 2258.52} = 0.268
$$

If the interaction were *either* large or significant we would be inclined to keep it in the model. In this case, it's both, so there's no real reason to remove it.

### Interpreting the Interaction

Recall the model equation, which is:

```{r show_c3_m1_model}
c3_m1
```

so we have:

$$
strength = 17.77 + 2.13 resinB + 4.77 resinC + 22.53 resinD \\
+ 1.29 lightLED + 3.37 resinB*lightLED \\
+ 3.94 resinC*lightLED - 17.74 resinD*lightLED
$$

So, if `light` = Halogen, our equation is:

$$
strength = 17.77 + 2.13 resinB + 4.77 resinC + 22.53 resinD 
$$

And if `light` = LED, our equation is:

$$
strength = 19.06 + 5.50 resinB + 8.71 resinC + 4.79 resinD 
$$

Note that both the intercept and the slopes change as a result of the interaction. The model yields a different prediction for every possible combination of a `resin` type and a `light` source.

## Comparing Individual Combinations of `resin` and `light`

To make comparisons between individual combinations of a `resin` type and a `light` source, using something like Tukey's HSD approach for multiple comparisons, we first refit the model using the `aov` structure, rather than `lm`.

```{r aov_fit_for_c3_m1}
c3m1_aov <- aov(strength ~ resin * light, data = bonding)

summary(c3m1_aov)
```

And now, we can obtain Tukey HSD comparisons (which will maintain an overall 95% family-wise confidence level) across the `resin` types, the `light` sources, and the combinations, with the TukeyHSD command. This approach is only completely appropriate if these comparisons are pre-planned, and if the design is balanced (as this is, with the same sample size for each combination of a `light` source and `resin` type.)

```{r Tukey_HSD_for_aov_fit_for_c3_m1}
TukeyHSD(c3m1_aov)
```

One conclusion from this is that the combination of D and Halogen is significantly stronger than each of the other seven combinations.

## The `bonding` model without Interaction

It seems incorrect in this situation to fit a model without the interaction term, but we'll do so just so you can see what's involved.

```{r c3_m2_bonding_anova_without_interaction}
c3_m2 <- lm(strength ~ resin + light, data = bonding)

summary(c3_m2)
```

In the no-interaction model, if `light` = Halogen, our equation is:

$$
strength = 19.07 + 3.82 resinB + 6.74 resinC + 13.66 resinD
$$

And if `light` = LED, our equation is:

$$
strength = 17.75 + 3.82 resinB + 6.74 resinC + 13.66 resinD
$$

So, in the no-interaction model, only the intercept changes.

```{r anova_c3_m2_without_interaction}
anova(c3_m2)
```

And, it appears, if we ignore the interaction, then `resin` type has a significant impact on `strength` but `light` source doesn't. This is clearer when we look at boxplots of the separated `light` and `resin` groups. 

```{r boxplots_c3_bonding_without_interaction}
p1 <- ggplot(bonding, aes(x = light, y = strength)) + 
    geom_boxplot()
p2 <- ggplot(bonding, aes(x = resin, y = strength)) +
    geom_boxplot()

gridExtra::grid.arrange(p1, p2, nrow = 1)
```

## `cortisol`: A Hypothetical Clinical Trial

156 adults who complained of problems with a high-stress lifestyle were enrolled in a hypothetical clinical trial of the effectiveness of a behavioral intervention designed to help reduce stress levels, as measured by salivary cortisol. 

The subjects were randomly assigned to one of three intervention groups (usual care, low dose, and high dose.) The "low dose" subjects received a one-week intervention with a follow-up at week 5. The "high dose" subjects received a more intensive three-week intervention, with follow up at week 5. 

Since cortisol levels rise and fall with circadian rhythms, the cortisol measurements were taken just after rising for all subjects. These measurements were taken at baseline, and again at five weeks. The difference (baseline - week 5) in cortisol level (in micrograms / l) serves as the primary outcome.

### Codebook and Raw Data for `cortisol`

The data are gathered in the `cortisol` data set. Included are:

Variable  | Description
--------: | --------------------------------
`subject` | subject identification code
`interv`  | intervention group (UC = usual care, Low, High)
`waist`   | waist circumference at baseline (in inches)
`sex`     | male or female
`cort.1`  | salivary cortisol level (microg/l) week 1
`cort.5`  | salivary cortisol level (microg/l) week 5

```{r c3_raw_cortisol_data}
cortisol
```

## Creating a factor combining sex and waist

Next, we'll put the `waist` and `sex` data in the `cortisol` example together. We want to build a second categorical variable (called `fat_est`) combining this information, to indicate "healthy" vs. "unhealthy" levels of fat around the waist. 

- Male subjects whose waist circumference is 40 inches or more, and
- Female subjects whose waist circumference is 35 inches or more, will fall in the "unhealthy" group.

```{r create c3_cortisol_fat_est}
cortisol <- cortisol %>%
    mutate(
        fat_est = factor(case_when(
            sex == "M" & waist >= 40 ~ "unhealthy",
            sex == "F" & waist >= 35 ~ "unhealthy",
            TRUE                     ~ "healthy")),
        cort_diff = cort.1 - cort.5)

summary(cortisol)
```

## A Means Plot for the `cortisol` trial (with standard errors)

Again, we'll start by building up a data set with the summaries we want to plot.

```{r c3_group_to_build_means_plot_cortisol}
cort.sum <- cortisol %>% 
    group_by(interv, fat_est) %>%
    summarize(mean.cort = mean(cort_diff), 
              se.cort = sd(cort_diff)/sqrt(n()))

cort.sum
```

Now, we'll use this new data set to plot the means and standard errors. 

```{r c3_ggplot_means_plot_cortisol}
## The error bars will overlap unless we adjust the position.
pd <- position_dodge(0.2) # move them .1 to the left and right

ggplot(cort.sum, aes(x = interv, y = mean.cort, col = fat_est)) +
    geom_errorbar(aes(ymin = mean.cort - se.cort, 
                      ymax = mean.cort + se.cort),
                  width = 0.2, position = pd) +
    geom_point(size = 2, position = pd) + 
    geom_line(aes(group = fat_est), position = pd) +
    scale_color_manual(values = c("royalblue", "darkred")) +
    theme_bw() +
    labs(y = "Salivary Cortisol Level", x = "Intervention Group",
         title = "Observed Means (+/- SE) of Salivary Cortisol")
```

## A Two-Way ANOVA model for `cortisol` with Interaction

```{r anova_c3_cortisol_with_interaction}
c3_m3 <- lm(cort_diff ~ interv * fat_est, data = cortisol)

anova(c3_m3)
```

Does it seem like we need the interaction term in this case?

```{r}
summary(c3_m3)
```

How do you reconcile the apparent difference in significance levels between this regression summary and the ANOVA table above?

### Notes on this Question

When we're evaluating a two-factor ANOVA model with an interaction, we are choosing between models with either:

1. just one factor
2. both factors, but only as main effects
3. both factors and an interaction between them

But we don't get to pick models that include any other combination of terms. For this two-way ANOVA, then, our choices are:

- a model with `interv only
- a model with `fat_est only
- a model with both `interv` and `fat_est` but not their interaction
- a model with `interv` and `fat_est` and their interaction

Those are the only modeling options available to us. 

First, consider the ANOVA table, repeated below...

```{r}
anova(c3_m3)
```

The conclusions here are as follows:

1. The interaction effect (`interv:fat_est`) has a large p value (0.58554) and assesses whether the two interaction terms (product terms) included in the model add detectable predictive value to the main effects model that includes only interv and fat_est alone. You are right to say that this ANOVA is sequential, which means that the p value for the interaction effect is looking at the additional effect of the interaction once we already have the main effects interv and fat_est included.

2. The `interv` and `fat_est` terms aren't usually evaluated with significance tests or interpreted in the ANOVA for this setting, since if we intend to include the interaction term (as this model does) we also need these main effects. If we wanted to look at those terms individually in a model without the interaction, then we'd want to fit that model (without the interaction term) to do so.

Next, let's look at the summary of the c3_m3 model, specifically the coefficients...

```{r}
summary(c3_m3)
```

So here, we see two p values associated with the interaction terms (the two product terms at the bottom of the regression) but these aren't especially helpful, because we're either going to include the interaction (in which case both of these terms will be in the model) or we're not going to include the interaction (in which case neither of these terms will be in the model.)

So the p values provided here aren't very helpful - like all such p values for t tests, they are looking at the value of the term in their row as the last predictor in to the model, essentially comparing the full model to the model without that specific component, but none of those tests enable us to decide which of the 4 available model choices is our best fit.

Now, let's consider the reason why, for example, the p value for `fat_est` in the `summary()` which is looking at comparing the following models ...

- a model including interv (which has 2 coefficients to account for its 3 categories), `fat_est` (which has 1 coefficient to account for its 2 categories), and the `interv*fat_est` interaction terms (which are 2 terms)
- a model including `interv` and the `interv*fat_est` interaction (but somehow not the main effect of `fat_est`, which actually makes no sense: if we include the interaction we always include the main effect)

to the p value for `fat_est` in the ANOVA which is looking at comparing

- the model with `interv` alone to
- the model with `interv` and `fat_est` as main effects, but no interaction

Only the ANOVA p value is therefore in any way useful, and it suggests that once you have the main effect of `interv`, adding `fat_est`'s main effect adds statistically detectable value (p = 0.023)

## A Two-Way ANOVA model for `cortisol` without Interaction

### The Graph

```{r boxplots_c3_cortisol_without_interaction}
p1 <- ggplot(cortisol, aes(x = interv, y = cort_diff)) + 
    geom_boxplot()
p2 <- ggplot(cortisol, aes(x = fat_est, y = cort_diff)) +
    geom_boxplot()

gridExtra::grid.arrange(p1, p2, nrow = 1)
```

### The ANOVA Model

```{r anova_c3_cortisol_without_interaction}
c3_m4 <- lm(cort_diff ~ interv + fat_est, data = cortisol)

anova(c3_m4)
```

How do these results compare to those we saw in the model with interaction?

### The Regression Summary

```{r}
summary(c3_m4)
```

### Tukey HSD Comparisons

Without the interaction term, we can make direct comparisons between levels of the intervention, and between levels of the `fat_est` variable. This is probably best done here in a Tukey HSD comparison.

```{r}
TukeyHSD(aov(cort_diff ~ interv + fat_est, data = cortisol))
```

What conclusions can we draw, at a 5% significance level?

