# NEW!! Bayes and a Linear Model

Almost all of this material is based on 

- <https://mc-stan.org/rstanarm/articles/continuous.html> and
- <https://easystats.github.io/bayestestR/articles/bayestestR.html> and
- <https://easystats.github.io/bayestestR/articles/example1.html>

There's not a lot that is truly original here. That's a summer project.

## R Setup Used Here

```{r}
#| warning: false
#| message: false

knitr::opts_chunk$set(comment = NA)

library(broom)
library(broom.mixed)
library(gt)
library(janitor)
library(mosaic)
library(bayestestR)
library(insight)
library(rstanarm)
library(conflicted)
library(tidyverse) 

conflicts_prefer(dplyr::select, dplyr::filter, base::mean, base::range)

theme_set(theme_bw())
```

## Return to the `smalldat` Example

Consider the `smalldat.csv` data we discussed in @sec-effect. The data includes 150 observations on 6 variables, and one of our goals was to build a model to predict total cholesterol.

```{r}
smalldat <- read_csv("data/smalldat.csv", show_col_types = FALSE)
```

| Variable  | Description                                 |
|-----------|---------------------------------------------|
| *subject* | Subject identification code                 |
| *smoker*  | 1 = current smoker, 0 = not current smoker  |
| *totchol* | total cholesterol, in mg/dl                 |
| *age*     | age in years                                |
| *sex*     | subject's sex (M or F)                      |
| *educ*    | subject's educational attainment (4 levels) |

The *educ* levels are: 1_Low, 2_Middle, 3_High and 4_VHigh, which stands for Very High

## The Distribution of Total Cholesterol

Across the 150 observations in the `smalldat` data, we have the following summaries of the distribution of total cholesterol, our outcome.

```{r}
favstats(~ totchol, data = smalldat)
```


## Fitting a Linear Model with `lm()` for Total Cholesterol

```{r}
m1 <- lm(totchol ~ age + sex + smoker + factor(educ), data = smalldat)

glance(m1) |> select(r.squared, adj.r.squared, sigma, 
                     AIC, BIC, df, df.residual, nobs) |>
  gt() |> fmt_number(r.squared:sigma, decimals = 3)

tidy(m1, conf.int = TRUE, conf.level = 0.95) |> 
  gt() |> fmt_number(decimals = 3)
```

## Fitting a Bayesian Linear Model

Can we fit a model for the same data using a Bayesian approach?

Yes, we can, for instance using the `stan_glm()` function from the `rstanarm` package.

```{r}
set.seed(43211234) # best to set a random seed first

m2 <- stan_glm(totchol ~ age + sex + smoker + factor(educ), 
               data = smalldat, refresh = 0)
```

Here the `refresh = 0` parameter stops the machine from printing out each of the updates it does while sampling, which is not generally something I need to look at. Here's what's placed in the `m2` object:

```{r}
m2
```

- The first few lines specify the fitting process. 
- Next, for each coefficient, we find the median value from the posterior distribution, and the MAD_SD value, which is an indicator of variation derived from the estimated posterior distribution of the parameters, and is used as a standard error in what follows.
- Finally, we see the estimated root mean squared error (residual standard deviation) `sigma`, again estimated with the median of the `sigma` values in the posterior distribution.

Using the `summary()` function provides some additional information about the parameter estimates, but mostly some convergence diagnostics for the Markov Chain Monte Carlo procedure that the `rstanarm` package used to build the estimates.

```{r}
summary(m2)
```

### Extracting the Posterior

Let's extract the coefficients of our model, using the `get_parameters()` function from the `insight` package:

```{r}
posteriors <- get_parameters(m2)

head(posteriors)
```

In all, we have 4000 observations of this posterior distribution:

```{r}
nrow(posteriors)
```


Let's visualize the posterior distribution of our parameter for `age`.

```{r}
ggplot(posteriors, aes(x = age)) + geom_density(fill = "dodgerblue")
```

This distribution describes the probability (on the vertical axis) of various `age` effects (shown on the horizontal axis). Most of the distribution is between 0.5 and 2, with the peak being around 1.25.

Remember that our `m1` fit with `lm()` had an estimated $\beta$ for `age` of 1.248, so, as is often the case, there is not a lot of difference between the two models in terms of the estimates they make.

Here's the mean and median of the `age` effect, across our 4000 simulations from the posterior distribution.

```{r}
mean(posteriors$age)
median(posteriors$age)
```

Again, these are very close to what we obtained from least squares estimation.

Another option is to take the mode (peak) of the posterior distribution, and this is called the maximum a posteriori (MAP) estimate:

```{r}
map_estimate(posteriors$age)
```

Adding these estimates to our plot, we can see that they are all on top of each other:

```{r}
ggplot(posteriors, aes(x = age)) +
  geom_density(fill = "dodgerblue") +
  # The mean in yellow
  geom_vline(xintercept = mean(posteriors$age), color = "yellow", linewidth = 1) +
  # The median in red
  geom_vline(xintercept = median(posteriors$age), color = "red", linewidth = 1) +
  # The MAP in purple
  geom_vline(xintercept = as.numeric(map_estimate(posteriors$age)), color = "purple", linewidth = 1)
```

### Describing Uncertainty

We might describe the range of estimates for the `age` effect.

```{r}
range(posteriors$age)
```

Instead of showing the whole range, we usually compute the highest density interval at some percentage level, for instance a 95% credible interval which shows the range containing the 95% most probable effect values.

```{r}
hdi(posteriors$age, ci = 0.95)
```

So we conclude that the `age` effect has a 95% chance of falling within the [0.50, 1.98] range.

### Visualizing the Coefficients and Credible Intervals

Here is a plot of the coefficients and parameters estimated in `m2`, along with credible intervals for their values. The inner interval (shaded region) here uses the default choice of 50%, and the outer interval (lines) uses a non-default choice of 95% (90% is the default choice here, as it turns out.) The point estimate shown here is the median of the posterior distribution, which is the default.

```{r}
plot(m2, prob = 0.5, prob_outer = 0.95)
```

## Summarizing the Posterior Distribution

A more detailed set of summaries for the posterior distribution can be obtained from the `describe_posterior()` function from the `bayestestR` package. 

A brief tutorial on what is shown here is available at  <https://easystats.github.io/bayestestR/articles/bayestestR.html> and <https://easystats.github.io/bayestestR/articles/example1.html> and this is the source for much of what I've built in this little chapter.

```{r}
describe_posterior(m2) |> print_md(decimals = 3)
```

Let's walk through all of this output.

### Summarizing the Parameter values

For each parameter, we have:

- its estimated median across the posterior distribution
- its 95% credible interval (highest density interval of values within the posterior distribution)

as we've previously discussed.

### Probability of Direction (`pd`) estimates.

The `pd` estimate helps us understand whether each effect is positive or negative. For instance, regarding `age`, we see the proportion of the posterior that is in the direction of the effect's point estimate, no matter what the "size" of the effect is, will be as follows. 

```{r}
n_positive <- posteriors |> filter(age > 0) |> nrow()
100 * n_positive / nrow(posteriors)
```

So we see that the effect of `age` is positive with a probability of 99.95%, and this is called the **probability of direction** and abbreviated **pd**.

We can also calculate this with

```{r}
p_direction(posteriors$age)
```

As it turns out, this `pd` index is usually highly correlated with the p-value from our `lm()` fit for `age`. We could almost roughly infer the corresponding p-value with a simple transformation:

```{r}
pd <- 99.95
onesided_p <- 1 - pd / 100
twosided_p <- onesided_p * 2
twosided_p
```

This implies that the $p$-value for `age` in our `lm()` fit would be 0.001, and that is, in fact, what we got when we fit model `m1`. 

The probability that each effect is in the direction shown by the point estimate is summarized in the `pd` column of our `describe_posterior()` results.

### The ROPE estimates

Testing whether this distribution is different from 0 doesnâ€™t make sense, as 0 is a single value (and the probability that any distribution is different from a single value is infinite). However, one way to assess significance could be to define an area around 0, which will consider as practically equivalent to zero (i.e., absence of, or a negligible, effect). This is called the Region of Practical Equivalence (ROPE).

The default (semi-objective) way of defining the ROPE is to use the tenth (1/10) of the standard deviation (SD) of the outcome variable `totchol`. This is sometimes considered a "negligible" effect size.

In our case, `totchol` has a standard deviation of 38.8, so the ROPE range is approximately (-3.88, 3.88), or somewhat more precisely:

```{r}
rope_range(m2)
```

So we then compute the **percentage in ROPE** as the percentage of the posterior distribution that falls within this ROPE range. When most of the posterior distribution does not overlap with ROPE, we might conclude that the effect is important enough to be noted.

In our case, 100% of the `age` effects are in the ROPE, so that's not really evidence of an important effect.

For `smoker`, though, only 42% of the effects are in the ROPE, so that's indicative of a somewhat more substantial effect, but we'd only get really excited if a much smaller fraction, say 1%, 5% or maybe 10% were in the ROPE.

### Convergence Diagnostics

The value of Rhat should, ideally, be 1 for each element of the model, and indicate how well the MCMC procedure converged. Generally, values of Rhat below 1.05 are good, values between 1.05 and 1.1 are OK, but values above 1.1 are too high. Here, as you can see, we don't see any serious problems.

As for ESS, we'd like to see values of 1000 or more to ensure sufficiently stable estimates. Here, again, we're OK.

```{r}
broom.mixed::tidy(m2, conf.int = TRUE, conf.level = 0.95) |> 
  gt() |> fmt_number(decimals = 3)
```

## Summarizing the Priors Used

From the `bayestestR` package, we also have the `describe_prior()` and `print_md()` functions to get the following summary of the priors we have assumed. Since we didn't specify anything about the priors in fitting model `m2`, we are looking at the default choices, which are weakly informative priors following Normal distributions. Details on the default prior choices can be found at <https://mc-stan.org/rstanarm/articles/priors.html>.

```{r}
describe_prior(m2) |> print_md(decimals = 3)
```

## Graphical Posterior Predictive Checks

For more on these checks, visit <https://mc-stan.org/rstanarm/articles/continuous.html#the-posterior-predictive-distribution-1>, for example.

Here's the first plot which compares the distribution of the observed outcome $y$ (totchol) to several of the simulated data sets $y_{rep}$ from the posterior predictive distribution using the same predictor values as were used to fit the model.

```{r}
pp_check(m2, plotfun = "hist", nreps = 5, bins = 25)
```

The idea is that if the model is a good fit to the data we should be able to generate data $y_{rep}$ from the posterior predictive distribution that looks a lot like the observed data $y$. That is, given $y$, the $y_{rep}$ we generate should look plausible. We'd worry a bit if this plot showed histograms that were wildly different from one another.

Another useful plot (shown below) made using `pp_check` shows the distribution of a test quantity $T(y_{rep})$ compared to $T(y)$, the value of the quantity in the observed data. I like this scatterplot version which allows us to look at where the simulations' mean and standard deviation fall compared to what the observed `totchol` values show us.

```{r}
pp_check(m2, plotfun = "stat_2d", stat = c("mean", "sd"))
```

We can see that the cloud of simulated means and standard deviations has the observed statistics near its center, although perhaps the standard deviations are a bit higher than we might like to see, typically. Ideally, the dot would be right in the center of this cloud of simulated results.

